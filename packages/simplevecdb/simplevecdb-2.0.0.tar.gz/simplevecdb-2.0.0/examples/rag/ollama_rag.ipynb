{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8812f1d",
   "metadata": {},
   "source": [
    "# SimpleVecDB RAG with Ollama - Multi-Collection Demo\n",
    "\n",
    "This notebook demonstrates retrieval-augmented generation (RAG) using SimpleVecDB with Ollama.\n",
    "Learn how to use multiple collections in a single database for organizing different knowledge domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e438cf",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install the Ollama Python client if not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ollama if needed\n",
    "# !pip install ollama -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6d47f",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from simplevecdb import VectorDB, Quantization\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a557c4fc",
   "metadata": {},
   "source": [
    "## 3. Initialize Vector Database\n",
    "\n",
    "Create a SimpleVecDB instance with BIT quantization for minimal storage overhead.\n",
    "The database supports multiple collections - each collection is a separate namespace for documents.\n",
    "Collections automatically use the embedding server running at `http://localhost:8000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0071f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing database\n",
    "db_path = \"rag_demo.db\"\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "    print(f\"Removed existing database: {db_path}\")\n",
    "\n",
    "# Create new database with BIT quantization (1-bit per dimension = smallest storage)\n",
    "db = VectorDB(\n",
    "    path=db_path,\n",
    "    quantization=Quantization.BIT,\n",
    ")\n",
    "\n",
    "# Get or create a collection for this demo\n",
    "collection = db.collection(\"ollama_demo\")\n",
    "\n",
    "print(f\"Created SimpleVecDB at {db_path}\")\n",
    "print(f\"Collection: {collection.name}\")\n",
    "print(f\"Quantization: {collection.quantization}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04249556",
   "metadata": {},
   "source": [
    "## 4. Prepare Knowledge Base\n",
    "\n",
    "Add documents to the vector database. These will be embedded and stored for later retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2be099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about various topics\n",
    "documents = [\n",
    "    \"SQLite is a C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. It is the most used database engine in the world.\",\n",
    "    \"Vector databases enable semantic search by storing embeddings - numerical representations of text that capture meaning. This allows finding similar content even without exact keyword matches.\",\n",
    "    \"Ollama is a tool that allows you to run large language models locally on your own computer. It supports models like Llama 3, Mistral, and many others without requiring cloud services.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) combines information retrieval with text generation. First, relevant documents are retrieved from a knowledge base, then an LLM generates answers based on that context.\",\n",
    "    \"SimpleVecDB uses usearch HNSW indexing, enabling fast similarity searches on embedded documents stored in a single SQLite file.\",\n",
    "    \"Quantization reduces the memory footprint of vectors by using fewer bits per dimension. BIT quantization uses only 1 bit per dimension, offering 32x compression compared to float32.\",\n",
    "    \"Local-first AI means running models and storing data entirely on your own hardware, without cloud dependencies. This ensures privacy, reduces costs, and works offline.\",\n",
    "    \"Python is a high-level programming language known for its simplicity and readability. It's widely used in data science, machine learning, and web development.\"\n",
    "]\n",
    "\n",
    "# Add documents with metadata to the collection\n",
    "metadatas = [{\"source\": f\"doc_{i}\", \"topic\": \"tech\"} for i in range(len(documents))]\n",
    "\n",
    "print(f\"Adding {len(documents)} documents to collection '{collection.name}'...\")\n",
    "collection.add_texts(texts=documents, metadatas=metadatas)\n",
    "\n",
    "print(f\"✓ Successfully added {len(documents)} documents\")\n",
    "print(f\"Database size: {os.path.getsize(db_path) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381946bd",
   "metadata": {},
   "source": [
    "## 5. Test Retrieval\n",
    "\n",
    "Search the database for documents similar to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query on the collection\n",
    "test_query = \"How does semantic search work?\"\n",
    "\n",
    "# Retrieve top 3 most relevant documents\n",
    "results = collection.similarity_search(query=test_query, k=3)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(f\"Retrieved {len(results)} documents:\\n\")\n",
    "\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"{i}. [Score: {score:.4f}]\")\n",
    "    print(f\"   {doc.page_content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e23a50",
   "metadata": {},
   "source": [
    "## 6. Build RAG Function\n",
    "\n",
    "Create a function that:\n",
    "1. Retrieves relevant documents from the vector database\n",
    "2. Builds a context-aware prompt\n",
    "3. Generates an answer using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str, k: int = 3, model: str = \"llama3.2:3b\", verbose: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        k: Number of documents to retrieve\n",
    "        model: Ollama model to use\n",
    "        verbose: Print retrieval details\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents from the collection\n",
    "    results = collection.similarity_search(query=question, k=k)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Retrieved {len(results)} documents:\")\n",
    "        for i, (doc, score) in enumerate(results, 1):\n",
    "            print(f\"  {i}. Score: {score:.4f} | {doc.page_content[:80]}...\")\n",
    "        print()\n",
    "    \n",
    "    # Step 2: Build context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "    \n",
    "    # Step 3: Create prompt with context\n",
    "    prompt = f\"\"\"You are a helpful assistant. Answer the question based ONLY on the provided context. If the context doesn't contain enough information, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 4: Generate answer with Ollama\n",
    "    if verbose:\n",
    "        print(f\"Generating answer with {model}...\\n\")\n",
    "    \n",
    "    response = ollama.generate(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        options={\n",
    "            \"temperature\": 0.1,  # Low temperature for more factual answers\n",
    "            \"num_predict\": 256   # Limit response length\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response['response']\n",
    "\n",
    "print(\"✓ RAG function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e933e",
   "metadata": {},
   "source": [
    "## 7. Ask Questions\n",
    "\n",
    "Now let's test the RAG system with various questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: About SQLite\n",
    "question1 = \"Why is SQLite good for AI applications?\"\n",
    "\n",
    "print(f\"Question: {question1}\")\n",
    "print(\"=\" * 80)\n",
    "answer1 = rag_query(question1)\n",
    "print(f\"\\nAnswer:\\n{answer1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f628fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: About local AI\n",
    "question2 = \"What are the benefits of running AI models locally?\"\n",
    "\n",
    "print(f\"Question: {question2}\")\n",
    "print(\"=\" * 80)\n",
    "answer2 = rag_query(question2)\n",
    "print(f\"\\nAnswer:\\n{answer2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2096fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: About quantization\n",
    "question3 = \"How does quantization help with vector databases?\"\n",
    "\n",
    "print(f\"Question: {question3}\")\n",
    "print(\"=\" * 80)\n",
    "answer3 = rag_query(question3)\n",
    "print(f\"\\nAnswer:\\n{answer3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f925a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4: Testing with non-verbose mode\n",
    "question4 = \"Explain how RAG works in simple terms.\"\n",
    "\n",
    "print(f\"Question: {question4}\")\n",
    "print(\"=\" * 80)\n",
    "answer4 = rag_query(question4, verbose=False)\n",
    "print(f\"\\nAnswer:\\n{answer4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e607f2d",
   "metadata": {},
   "source": [
    "## 8. Interactive Query\n",
    "\n",
    "Ask your own questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own question\n",
    "my_question = \"What is SimpleVecDB?\"\n",
    "\n",
    "print(f\"Question: {my_question}\")\n",
    "print(\"=\" * 80)\n",
    "my_answer = rag_query(my_question)\n",
    "print(f\"\\nAnswer:\\n{my_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f518b0",
   "metadata": {},
   "source": [
    "## 8.5 Multi-Collection Demo\n",
    "\n",
    "SimpleVecDB supports multiple collections in a single database. This is useful for:\n",
    "- Organizing different knowledge domains\n",
    "- Isolating experiments\n",
    "- Managing different embedding dimensions or quantization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499169e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second collection in the same database for Python-related content\n",
    "python_collection = db.collection(\"python_docs\")\n",
    "\n",
    "# Add Python-specific documents\n",
    "python_docs = [\n",
    "    \"Python uses indentation to define code blocks instead of braces like C or Java.\",\n",
    "    \"List comprehensions in Python provide a concise way to create lists: [x**2 for x in range(10)]\",\n",
    "    \"Python's GIL (Global Interpreter Lock) means only one thread executes Python bytecode at a time.\",\n",
    "]\n",
    "\n",
    "python_collection.add_texts(python_docs, metadatas=[{\"topic\": \"python\"} for _ in python_docs])\n",
    "\n",
    "print(f\"✓ Created second collection: {python_collection.name}\")\n",
    "print(f\"  Documents in '{collection.name}': (check via SQL)\")\n",
    "print(f\"  Documents in '{python_collection.name}': {len(python_docs)}\")\n",
    "\n",
    "# Query the Python collection specifically\n",
    "py_results = python_collection.similarity_search(\"What is the GIL?\", k=1)\n",
    "print(f\"\\nQuery on '{python_collection.name}' collection:\")\n",
    "print(f\"  {py_results[0][0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ba7a6",
   "metadata": {},
   "source": [
    "## 9. Database Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df4a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show database statistics\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Count documents in the collection's table\n",
    "cursor.execute(f\"SELECT COUNT(*) FROM {collection._table_name}\")\n",
    "doc_count = cursor.fetchone()[0]\n",
    "\n",
    "# Get database size\n",
    "db_size_kb = os.path.getsize(db_path) / 1024\n",
    "\n",
    "print(\"Database Statistics:\")\n",
    "print(f\"  Collection: {collection.name}\")\n",
    "print(f\"  Documents: {doc_count}\")\n",
    "print(f\"  Embedding dimension: {collection._dim}\")\n",
    "print(f\"  Quantization: {collection.quantization}\")\n",
    "print(f\"  Database size: {db_size_kb:.2f} KB\")\n",
    "print(f\"  Average size per doc: {db_size_kb/doc_count:.2f} KB\")\n",
    "\n",
    "# For comparison, show what float32 would use\n",
    "if collection._dim:\n",
    "    float32_size = (doc_count * collection._dim * 4) / 1024  # 4 bytes per float32\n",
    "    print(\"\\nComparison:\")\n",
    "    print(f\"  BIT quantization: {db_size_kb:.2f} KB\")\n",
    "    print(f\"  FLOAT32 (uncompressed): ~{float32_size:.2f} KB\")\n",
    "    print(f\"  Compression ratio: {float32_size/db_size_kb:.1f}x\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f9546",
   "metadata": {},
   "source": [
    "## 10. Cleanup (Optional)\n",
    "\n",
    "Remove the demo database when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a980f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to remove the database\n",
    "# db.close()\n",
    "# if os.path.exists(db_path):\n",
    "#     os.remove(db_path)\n",
    "#     print(f\"Removed {db_path}\")\n",
    "\n",
    "print(\"Done! The database will persist for future use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9bfdf0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Setting up SimpleVecDB** with BIT quantization for minimal storage\n",
    "2. **Using collections** to organize documents in namespaces\n",
    "3. **Adding documents** to create a knowledge base\n",
    "4. **Semantic search** using vector similarity\n",
    "5. **RAG pipeline** combining retrieval + generation\n",
    "6. **Local LLM** inference with Ollama\n",
    "7. **Storage efficiency** through quantization (32x compression)\n",
    "8. **Multi-collection support** for organizing different knowledge domains\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- **Fully local**: No cloud dependencies, works offline\n",
    "- **Privacy-preserving**: All data stays on your machine\n",
    "- **Lightweight**: Single SQLite file, minimal dependencies\n",
    "- **Fast**: Efficient vector search with quantization\n",
    "- **Flexible**: Multiple collections per database, easy to customize\n",
    "- **Organized**: Separate collections for different domains or experiments\n",
    "\n",
    "### Multi-Collection Use Cases\n",
    "\n",
    "- **Domain separation**: Tech docs in one collection, legal docs in another\n",
    "- **Experimentation**: Test different embedding models per collection\n",
    "- **Multi-tenant**: Isolate data for different users or projects\n",
    "- **Version control**: Keep different versions of a knowledge base\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different Ollama models (e.g., `gemma3`, `qwen3`, `deepseek-r1`)\n",
    "- Experiment with different quantization levels (INT8, FLOAT)\n",
    "- Create multiple collections for different knowledge domains\n",
    "- Add more documents from your own data sources\n",
    "- Integrate with LangChain or LlamaIndex (see other notebooks)\n",
    "- Build a chat interface with conversation history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
