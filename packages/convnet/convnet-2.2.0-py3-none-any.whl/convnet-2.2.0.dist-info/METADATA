Metadata-Version: 2.4
Name: convnet
Version: 2.2.0
Summary: A high-performance CNN framework: Numba for CPU, JAX for GPU/TPU
Home-page: https://github.com/codinggamer-dev/ConvNet-NumPy
Author: codinggamer-dev
Author-email: ege.tba1940@gmail.com
Project-URL: Bug Reports, https://github.com/codinggamer-dev/ConvNet-NumPy/issues
Project-URL: Source, https://github.com/codinggamer-dev/ConvNet-NumPy
Keywords: deep-learning neural-network cnn convolutional jax gpu education scratch python
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Education
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE.md
Requires-Dist: numpy>=1.20.0
Requires-Dist: numba>=0.56.0
Requires-Dist: tqdm>=4.60.0
Requires-Dist: h5py>=3.0.0
Provides-Extra: jax
Requires-Dist: jax>=0.4.0; extra == "jax"
Requires-Dist: jaxlib>=0.4.0; extra == "jax"
Provides-Extra: gpu
Requires-Dist: jax[cuda12]; extra == "gpu"
Provides-Extra: tpu
Requires-Dist: jax[tpu]; extra == "tpu"
Provides-Extra: dev
Requires-Dist: pytest>=6.0.0; extra == "dev"
Requires-Dist: black>=21.0.0; extra == "dev"
Requires-Dist: flake8>=3.8.0; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license-file
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# ğŸ§  ConvNet

[![PyPI](https://img.shields.io/badge/PyPI-convnet-blue.svg)](https://pypi.org/project/convnet/)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![Numba](https://img.shields.io/badge/Numba-CPU%20JIT-red.svg)](https://numba.pydata.org/)
[![JAX](https://img.shields.io/badge/JAX-GPU%2FTPU-green.svg)](https://jax.readthedocs.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE.md)
[![Status](https://img.shields.io/badge/Status-Educational-orange.svg)]()

> **A high-performance, educational CNN framework: Numba for CPU, JAX for GPU/TPU**

This project was created as a school assignment with the goal of understanding deep learning from the ground up. It's designed to be **easy to understand** and **learn from**, implementing a complete CNN framework with **Numba acceleration** for fast CPU training and optional **JAX acceleration** for GPU/TPU. The framework uses simple, readable code while delivering excellent performance.

---

## ğŸŒŸ Features

### Core Functionality
- âœ… **Pure Python Core** - Clean, educational code
- ğŸ”¥ **Complete CNN Support** - Conv2D, MaxPool2D, Flatten, Dense layers
- ğŸ“Š **Modern Training** - Batch normalization, dropout, early stopping
- ğŸ¯ **Smart Optimizers** - SGD with momentum and Adam optimizer
- ğŸ“ˆ **Learning Rate Scheduling** - Plateau-based LR reduction
- ğŸ’¾ **Model Persistence** - Save/load models in HDF5 or NPZ format
- ğŸ”„ **Data Augmentation Ready** - Thread-pooled data loading

### Performance Options
- ğŸš€ **Numba CPU Acceleration** - Parallel JIT-compiled operations (15+ it/s)
- âš¡ **JAX GPU/TPU Support** - XLA compilation for maximum throughput
- ğŸ“¦ **OpenBLAS Integration** - SIMD-optimized matrix operations

### Developer Experience
- ğŸ“š **Clean Code** - Well-documented and easy to follow
- ğŸ“ **Educational** - Built for learning deep learning fundamentals
- ğŸ”§ **Modular Design** - Easy to extend and customize
- ğŸ’» **Examples Included** - MNIST training example and GUI demo

---

## ğŸš€ Quick Start

### Installation

**Install from PyPI (Recommended):**

```bash
# Install base package (includes Numba for fast CPU)
pip install convnet

# For GPU/TPU support, add JAX:
pip install convnet[jax]      # CPU JAX
pip install convnet[gpu]      # NVIDIA GPU (CUDA 12)
pip install convnet[tpu]      # Google TPU
```

**Install from Source:**

```bash
# Clone the repository
git clone https://github.com/codinggamer-dev/ConvNet.git
cd ConvNet

# Install in development mode
pip install -e .

# Add JAX for GPU acceleration
pip install jax jaxlib  # For CPU JAX
pip install jax[cuda12]  # For NVIDIA GPU
```

### Performance Expectations

| Backend | MNIST Training Speed | Best For |
|---------|---------------------|----------|
| **Numba (CPU)** | **~16 it/s** (model only) | **Fast CPU training** |
| Pure NumPy | ~6-8 it/s | Fallback, compatibility |
| JAX (CPU) | ~10-12 it/s | Development |
| JAX (GPU) | ~50+ it/s | Production training |

> **Note:** Numba uses all available CPU cores with parallel JIT compilation. Actual training speed is ~10-12 it/s due to loss/optimizer/data loading overhead.
> For maximum performance, ensure you have a modern CPU with AVX2 support.

### Your First Neural Network in 10 Lines

```python
from convnet import Model, Conv2D, Activation, MaxPool2D, Flatten, Dense

# Build a simple CNN
model = Model([
    Conv2D(8, (3, 3)), Activation('relu'),
    MaxPool2D((2, 2)),
    Flatten(),
    Dense(10), Activation('softmax')
])

# Configure training
model.compile(loss='categorical_crossentropy', optimizer='adam', lr=0.001)

# Train on your data
history = model.fit(train_dataset, epochs=10, batch_size=32, num_classes=10)
```

---

## ğŸ“– Complete MNIST Example

```python
import numpy as np
from convnet import Model, Conv2D, Activation, MaxPool2D, Flatten, Dense, Dropout, Dataset
from convnet.data import load_mnist_gz

# Load MNIST data
train_data, test_data = load_mnist_gz('mnist_dataset')

# Build the model
model = Model([
    Conv2D(8, (3, 3)), Activation('relu'),
    MaxPool2D((2, 2)),
    Conv2D(16, (3, 3)), Activation('relu'),
    MaxPool2D((2, 2)),
    Flatten(),
    Dense(64), Activation('relu'), Dropout(0.2),
    Dense(10)
])

# Compile and train
model.compile(loss='categorical_crossentropy', optimizer='adam', lr=0.001)
history = model.fit(train_data, epochs=50, batch_size=128, num_classes=10)

# Save the model
model.save('my_model.hdf5')
```

---

## ğŸ§© Architecture Components

### Available Layers

| Layer | Description | Parameters |
|-------|-------------|------------|
| `Conv2D(filters, kernel_size)` | 2D Convolutional layer | `filters`, `kernel_size`, `stride`, `padding` |
| `Dense(units)` | Fully connected layer | `units`, `use_bias` |
| `MaxPool2D(pool_size)` | Max pooling layer | `pool_size`, `stride` |
| `Activation(type)` | Activation function | `'relu'`, `'tanh'`, `'sigmoid'`, `'softmax'` |
| `Flatten()` | Reshape to 1D | None |
| `Dropout(rate)` | Dropout regularization | `rate` (0.0 to 1.0) |
| `BatchNorm2D()` | Batch normalization | `momentum`, `epsilon` |

### Optimizers

- **SGD** - Stochastic Gradient Descent with momentum
  ```python
  model.compile(optimizer='sgd', lr=0.01, momentum=0.9)
  ```

- **Adam** - Adaptive Moment Estimation (recommended)
  ```python
  model.compile(optimizer='adam', lr=0.001, beta1=0.9, beta2=0.999)
  ```

### Loss Functions

- `'categorical_crossentropy'` - For multi-class classification
- `'mse'` - Mean Squared Error for regression

---

## ğŸ® Examples & Demos

The `examples/` directory contains several demonstrations:

### 1. MNIST Training (`mnist_train-example.py`)
Complete training pipeline with early stopping, LR scheduling, and model persistence.

```bash
python examples/mnist_train-example.py
```

### 2. Interactive GUI Demo (`mnist_gui.py`)
Draw digits and see real-time predictions! Requires tkinter.

```bash
python examples/mnist_gui.py
```

### 3. GPU Training Test (`test_gpu_training.py`)
Benchmark GPU vs CPU performance.

```bash
python examples/test_gpu_training.py
```

### 4. JAX Benchmark (`benchmark_jax.py`)
Compare JAX JIT performance vs NumPy baseline.

```bash
python examples/benchmark_jax.py
```

---

## âš™ï¸ Advanced Features

### GPU/TPU Acceleration

ConvNet automatically detects and uses available hardware accelerators via JAX:

```bash
# Install with GPU support
pip install convnet[gpu]

# Or for TPU
pip install convnet[tpu]

# Or install JAX with CUDA manually
pip install jax[cuda12]  # For CUDA 12.x
```

The framework will automatically:
- Detect available GPUs/TPUs
- JIT-compile operations with XLA
- Move tensors to accelerator devices
- Handle device transfers transparently

### Regularization

```python
model.compile(
    optimizer='adam',
    lr=0.001,
    weight_decay=1e-4,  # L2 regularization
    clip_norm=5.0        # Gradient clipping
)
```

### Learning Rate Scheduling

```python
history = model.fit(
    dataset,
    lr_schedule='plateau',  # Reduce LR when validation plateaus
    lr_factor=0.5,         # Multiply LR by 0.5
    lr_patience=5,         # Wait 5 epochs before reducing
    lr_min=1e-6           # Minimum learning rate
)
```

### Early Stopping

```python
history = model.fit(
    dataset,
    val_data=(X_val, y_val),
    early_stopping=True,
    patience=10,      # Stop after 10 epochs without improvement
    min_delta=0.001   # Minimum change to qualify as improvement
)
```

---

## ğŸ“Š Model Inspection

```python
# Print model architecture and parameter counts
model.summary()

# Output:
# Model summary:
# Conv2D: params=80
# Activation: params=0
# MaxPool2D: params=0
# Conv2D: params=1168
# Activation: params=0
# MaxPool2D: params=0
# Flatten: params=0
# Dense: params=40064
# Activation: params=0
# Dropout: params=0
# Dense: params=650
# Total params: 41962
```

---

## ğŸ”§ Configuration

### Thread Configuration

The framework automatically configures BLAS threads for optimal CPU performance:

```python
import os
os.environ['NN_DISABLE_AUTO_THREADS'] = '1'  # Disable auto-configuration
import convnet
```

### Custom RNG Seeds

For reproducibility:

```python
import numpy as np
rng = np.random.default_rng(seed=42)

model = Model([
    Conv2D(8, (3, 3), rng=rng),
    Dense(10, rng=rng)
])
```

---

## ğŸ“š Understanding the Code

This project is designed for learning. Here's how to explore:

### Start Here
1. **`convnet/layers.py`** - See how Conv2D, Dense, and other layers work
2. **`convnet/model.py`** - Understand forward/backward propagation
3. **`convnet/optim.py`** - Learn how optimizers update weights
4. **`examples/mnist_train-example.py`** - Complete training example

### Key Concepts Implemented
- ğŸ”„ **Backpropagation** - Full gradient computation chain
- ğŸ“‰ **Gradient Descent** - SGD and Adam optimization
- ğŸ² **Weight Initialization** - Glorot/Xavier uniform
- ğŸ§® **Convolution Math** - JIT-compiled implementation
- ğŸ“Š **Batch Normalization** - Running mean/variance tracking
- ğŸ¯ **Softmax & Cross-Entropy** - Numerically stable implementation

---

## ğŸ¯ Project Goals

This framework was built to:
1. **Understand** deep learning by implementing it from scratch
2. **Learn** how CNNs actually work under the hood
3. **Teach** others the fundamentals of neural networks
4. **Provide** a clean, readable codebase for education

**Not for production use** - Use PyTorch, TensorFlow, or JAX for real applications!

---

## ğŸ“¦ Project Structure

```
ConvNet/
â”œâ”€â”€ convnet/              # Core framework
â”‚   â”œâ”€â”€ __init__.py       # Package initialization & auto-config
â”‚   â”œâ”€â”€ layers.py         # Layer implementations
â”‚   â”œâ”€â”€ model.py          # Model class with training loop
â”‚   â”œâ”€â”€ optim.py          # Optimizers (SGD, Adam)
â”‚   â”œâ”€â”€ losses.py         # Loss functions
â”‚   â”œâ”€â”€ data.py           # Data loading utilities
â”‚   â”œâ”€â”€ utils.py          # Helper functions
â”‚   â”œâ”€â”€ jax_backend.py    # JAX acceleration backend
â”‚   â””â”€â”€ io.py             # Model save/load
â”œâ”€â”€ examples/             # Example scripts
â”‚   â”œâ”€â”€ mnist_train-example.py
â”‚   â”œâ”€â”€ mnist_gui.py
â”‚   â””â”€â”€ mnist_gui.py
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ setup.py              # Package setup
â”œâ”€â”€ LICENSE.md            # MIT License
â””â”€â”€ README.md             # This file
```

---

## ğŸ¤ Contributing

This is an educational project, but contributions are welcome! Feel free to:
- ğŸ› Report bugs
- ğŸ’¡ Suggest improvements
- ğŸ“– Improve documentation
- âœ¨ Add new features

---

## ğŸ“ Requirements

### Core Dependencies
- **Python** 3.8 or higher
- **JAX** â‰¥ 0.4.0 (high-performance computing! ğŸš€)
- **jaxlib** â‰¥ 0.4.0 (JAX runtime)
- **NumPy** â‰¥ 1.20.0 (compatibility layer)
- **tqdm** â‰¥ 4.60.0 (progress bars)
- **h5py** â‰¥ 3.0.0 (model serialization)

### Optional Dependencies
- **jax[cuda12]** (GPU acceleration)
- **jax[tpu]** (TPU acceleration)
- **tkinter** (for GUI demo, usually included with Python)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.

Copyright (c) 2025 Tim Bauer

---

## ğŸ™ Acknowledgments

- Built as a school project to learn deep learning fundamentals
- Inspired by PyTorch and TensorFlow's clean APIs
- Powered by JAX and XLA for high-performance computation
- MNIST dataset by Yann LeCun and Corinna Cortes - the perfect dataset for learning CNNs

---

## ğŸ’¬ Questions?

Feel free to open an issue on GitHub if you have questions or run into problems!

---

<div align="center">

**Made with â¤ï¸ for learning and education**

â­ If this helped you understand CNNs better, consider giving it a star! â­

</div>
