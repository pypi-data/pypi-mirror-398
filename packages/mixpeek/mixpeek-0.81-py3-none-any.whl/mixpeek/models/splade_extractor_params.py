# coding: utf-8

"""
    Mixpeek API

    This is the Mixpeek API, providing access to various endpoints for data processing and retrieval.

    The version of the OpenAPI document: 0.81
    Contact: info@mixpeek.com
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field
from typing import Any, ClassVar, Dict, List, Optional
from typing_extensions import Annotated
from typing import Optional, Set
from typing_extensions import Self

class SpladeExtractorParams(BaseModel):
    """
    Parameters for the SPLADE extractor.  SPLADE (Sparse Lexical and Expansion Model) generates learned sparse embeddings that combine lexical matching with neural expansion. It learns which keywords are important and expands the query/document with semantically related terms, achieving hybrid search in a single model.  **When to Use**:     - Hybrid search requiring both keyword precision and semantic understanding     - E-commerce product search (exact model numbers + semantic descriptions)     - Technical documentation (specific terms + conceptual matches)     - Multi-language search with important keywords in each language     - Need for explainability (can see which keywords/expansions matched)     - Balance between ColBERT's precision and text_extractor's storage efficiency     - Datasets where exact keyword matching matters but semantic context also needed  **When NOT to Use**:     - Pure semantic search without keyword requirements → Use text_extractor (5x less storage, simpler)     - Highest precision legal/medical search → Use colbert_extractor (better exact matching)     - Real-time search with <10ms requirement → Use text_extractor (2x faster)     - Very large datasets (>50M documents) with budget constraints → Consider text_extractor     - Short keyword queries (1-3 words) → Traditional BM25 may be sufficient  **Comparison with Other Text Extractors**:      | Feature | splade_extractor | text_extractor | colbert_extractor |     |---------|------------------|----------------|-------------------|     | **Accuracy (BEIR avg)** | 90% | 88% | 92% |     | **Precision** | Excellent | Good | Excellent |     | **Recall** | Good | Excellent | Excellent |     | **Speed (per doc)** | 10ms | 5ms | 15ms |     | **Storage per doc** | 20KB | 4KB | 500KB |     | **Storage vs dense** | 5x more | Baseline | 125x more |     | **Query Latency** | 20-30ms | <10ms | 50-100ms |     | **Best For** | Hybrid | General | Precision |     | **Exact Matching** | Excellent | Poor | Excellent |     | **Semantic Matching** | Good | Excellent | Excellent |     | **Multi-language** | Good | Excellent | Good |     | **Explainability** | Excellent (keyword weights) | Poor | Excellent (token-level) |     | **Keyword Expansion** | Yes (learned) | No | No |  **Model Details**:     - Model: SPLADE v1 (based on DistilBERT)     - Sparse Vector: Variable dimensions (only non-zero values stored)     - Typical Sparsity: 200-300 non-zero dimensions out of 30K vocabulary     - Max Sequence Length: Configurable (default 256, up to 512 tokens)     - Top-K Tokens: Configurable (default 200 most important)     - Distance Metric: Dot product on sparse vectors     - Languages: Primarily English, decent multilingual support  **How It Works**:     1. **Tokenization**: Text is split into tokens     2. **BERT Processing**: Contextualized representations computed     3. **MLM Head**: Masked language model head predicts importance scores for all vocab tokens     4. **Expansion**: Model learns to add semantically related terms (e.g., \"wireless\" → \"bluetooth\")     5. **Sparsification**: Only top-K most important tokens retained with their weights     6. **Result**: Sparse vector with explicit keyword matches + learned expansions  **Performance Characteristics**:     - Embedding Generation: 10ms per document (batched: 5ms/doc)     - Index Build: ~2 hours per 10M documents     - Query Time: 20-30ms for top-100 results     - Memory: ~20GB per 1M documents (5x more than dense)     - Storage Cost: $2 per 1M documents vs $0.40 for dense  **Use Case Examples**:     1. **E-commerce Search**: Find products by \"iPhone 15 Pro\" (exact) or \"flagship smartphone\" (semantic)     2. **Technical Documentation**: Search for \"API authentication\" finding both exact matches and related concepts     3. **Enterprise Search**: Hybrid search across documents with technical terms + natural language     4. **Multi-language Product Catalog**: Search with product codes + descriptions in multiple languages     5. **Academic Paper Search**: Find papers by specific terminology + conceptually related work  **Advantages Over Dense Embeddings**:     - Keyword matching: Can match exact technical terms and product codes     - Explainability: Can see which keywords matched and their weights     - Expansion: Learns synonyms and related terms automatically     - BM25 fallback: Gracefully degrades to lexical matching if needed  **Advantages Over ColBERT**:     - 25x less storage: 20KB vs 500KB per document     - 2x faster queries: 20-30ms vs 50-100ms     - Better recall: Expansion helps find semantically related docs     - Lower infrastructure cost  **Limitations**:     - 5x more storage than dense embeddings     - 2x slower than dense embeddings     - Lower precision than ColBERT for exact phrase matching     - Primarily English (multilingual support limited)     - Less effective for very long documents (512 token limit)  **Parameter Tuning**:     - Higher k_tokens (300-500): More expansions, better recall, larger storage     - Lower k_tokens (100-150): Fewer expansions, smaller storage, may miss semantic matches     - Longer max_length (512): Better for long documents, slower processing     - Shorter max_length (128): Faster processing, only for short texts  Requirements:     - text field: REQUIRED (string or text type)     - max_length: OPTIONAL (defaults to 256 tokens)     - k_tokens: OPTIONAL (defaults to 200)
    """ # noqa: E501
    max_length: Optional[Annotated[int, Field(le=512, strict=True, ge=1)]] = Field(default=256, description="NOT REQUIRED. Maximum sequence length for tokenization. Defaults to 256. Longer texts are truncated. Typical: 256 (balanced), 512 (long documents), 128 (short texts).")
    k_tokens: Optional[Annotated[int, Field(le=1000, strict=True, ge=10)]] = Field(default=200, description="NOT REQUIRED. Number of top-k tokens to keep in sparse vector. Defaults to 200. Higher values = more precise but larger vectors. Typical: 100-300.")
    __properties: ClassVar[List[str]] = ["max_length", "k_tokens"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of SpladeExtractorParams from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of SpladeExtractorParams from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "max_length": obj.get("max_length") if obj.get("max_length") is not None else 256,
            "k_tokens": obj.get("k_tokens") if obj.get("k_tokens") is not None else 200
        })
        return _obj


