# coding: utf-8

"""
    Mixpeek API

    This is the Mixpeek API, providing access to various endpoints for data processing and retrieval.

    The version of the OpenAPI document: 0.81
    Contact: info@mixpeek.com
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from typing_extensions import Annotated
from mixpeek.models.response_shape2 import ResponseShape2
from mixpeek.models.text_split_strategy import TextSplitStrategy
from typing import Optional, Set
from typing_extensions import Self

class TextExtractorParams(BaseModel):
    """
    Parameters for the text extractor.  The text extractor generates dense vector embeddings optimized for semantic similarity search. It uses the E5-Large multilingual model to convert text into 1024-dimensional vectors that capture semantic meaning, enabling accurate retrieval of conceptually related documents.  **Text Chunking & Decomposition**:     The extractor supports splitting long texts into multiple documents using various strategies.     This is essential for RAG applications where you need granular retrieval of specific passages.      - split_by: Strategy for splitting text (characters, words, sentences, paragraphs, pages, none)     - chunk_size: Target size for each chunk (interpretation depends on split_by strategy)     - chunk_overlap: Number of units to overlap between chunks (helps preserve context)      **Strategy Guide**:     - CHARACTERS: Fastest, uniform sizes, may break words. Good for quick testing.     - WORDS: Better readability, may break sentences. Good for general text.     - SENTENCES: Preserves semantic units. Best for Q&A and precise retrieval.     - PARAGRAPHS: Natural document structure. Best for articles and documentation.     - PAGES: Explicit page boundaries. Best for PDFs and paginated content.     - NONE: No splitting. Use for short texts (<400 words).  **LLM Structured Extraction (NEW)**:     The extractor now supports LLM-powered structured extraction using `response_shape`.     Instead of just generating embeddings, you can extract custom structured data from text.      - response_shape: Natural language prompt OR explicit JSON schema     - llm_provider: LLM provider to use (openai, google, anthropic)     - llm_model: Specific model for extraction      **When to Use**:     - Extract entities, relationships, sentiment from text     - Generate structured summaries with custom fields     - Classify text into custom taxonomies     - Extract domain-specific metadata  **When to Use**:     - General semantic search (documents, articles, knowledge bases, FAQs)     - RAG (Retrieval Augmented Generation) applications requiring fast context retrieval     - Q&A systems matching questions to answers semantically     - Content recommendation based on similarity     - Chatbots finding relevant information from documentation     - Multi-language search (supports 100+ languages)     - Real-time search requiring low latency (<10ms per query)     - Cost-sensitive applications (free self-hosted embeddings)     - Structured extraction from text using LLMs (NEW)  **When NOT to Use**:     - High-precision legal/medical search requiring exact phrase matching → Use colbert_extractor     - Hybrid semantic + keyword search combining both approaches → Use splade_extractor     - Need for explainability (which keywords matched) → Use splade_extractor     - Documents with critical technical terms that must match exactly → Use colbert_extractor     - Very short texts (1-5 words) where lexical matching is sufficient → Use splade_extractor  **Comparison with Other Text Extractors**:      | Feature | text_extractor | colbert_extractor | splade_extractor |     |---------|----------------|-------------------|------------------|     | **Accuracy (BEIR avg)** | 88% | 92% | 90% |     | **Speed (per doc)** | 5ms | 15ms | 10ms |     | **Storage per doc** | 4KB | 500KB (125x more) | 20KB (5x more) |     | **Query Latency** | <10ms | 50-100ms | 20-30ms |     | **Best For** | General search | Precision | Hybrid |     | **Cost (1M docs)** | Free | Free | Free |     | **Storage Cost (1M)** | $0.40 | $50 | $2 |     | **Multi-language** | Excellent | Good | Good |     | **Exact Matching** | Poor | Excellent | Excellent |     | **Semantic Matching** | Excellent | Excellent | Good |  **Model Details**:     - Embedding Model: E5-Large (intfloat/multilingual-e5-large-instruct)     - Dimensions: 1024     - Context Length: 512 tokens (~400 words)     - Languages: 100+ (trained on multilingual data)     - Distance Metric: Cosine similarity     - Normalization: L2 normalized vectors     - Cost: Free (self-hosted on your infrastructure)  **Performance Characteristics**:     - Embedding Generation: 5ms per document (batched: 2ms/doc)     - Index Build: ~1 hour per 10M documents     - Query Time: 5-10ms for top-100 results     - Memory: ~4GB per 1M documents     - Scales linearly with document count  **Use Case Examples**:     1. **E-commerce Product Search**: Search 1M products by description, find semantically similar items     2. **Customer Support**: Match user questions to 10K FAQ articles with 85%+ accuracy     3. **Document RAG**: Retrieve relevant context chunks from 100K documents for LLM prompts     4. **News Article Discovery**: Find related articles across 1M news items     5. **Research Paper Search**: Semantic search over academic papers and abstracts     6. **Structured Extraction**: Extract entities, sentiment, topics from documents using LLMs (NEW)  **Limitations**:     - Cannot match exact phrases or technical terms reliably     - May miss documents that use different terminology for same concept     - Struggles with very domain-specific jargon or acronyms     - 512 token limit means long documents must be chunked     - Less effective for keyword-heavy queries (e.g., \"iPhone 15 Pro Max 256GB\")     - LLM extraction adds cost and latency (only use when needed)  Requirements:     - text field: REQUIRED (string or text type)     - All chunking parameters are OPTIONAL     - LLM parameters are OPTIONAL (only for structured extraction)
    """ # noqa: E501
    split_by: Optional[TextSplitStrategy] = Field(default=None, description="OPTIONAL. Strategy for splitting text into multiple documents. Default is 'none' (no splitting, entire text becomes one document). Options: 'characters' - Split by character count (fastest, may break words). 'words' - Split by word boundaries (preserves words). 'sentences' - Split by sentence boundaries (preserves semantic units). 'paragraphs' - Split by paragraph boundaries (best for articles). 'pages' - Split by page breaks (best for paginated documents). 'none' - No splitting (default). Choose based on your content structure and retrieval granularity needs.")
    chunk_size: Optional[Annotated[int, Field(le=10000, strict=True, ge=1)]] = Field(default=1000, description="OPTIONAL. Target size for each chunk. Interpretation depends on split_by strategy: - characters: Number of characters per chunk (e.g., 1000 chars). - words: Number of words per chunk (e.g., 200 words). - sentences: Number of sentences per chunk (e.g., 5 sentences). - paragraphs: Number of paragraphs per chunk (e.g., 2 paragraphs). - pages: Number of pages per chunk (e.g., 1 page). - none: Ignored (entire text processed as one document). Default: 1000. Recommended ranges: characters (500-2000), words (100-400), sentences (3-10).")
    chunk_overlap: Optional[Annotated[int, Field(le=5000, strict=True, ge=0)]] = Field(default=0, description="OPTIONAL. Number of units to overlap between consecutive chunks. Helps preserve context across chunk boundaries. Units match split_by strategy (characters, words, sentences, etc.). Example: With chunk_size=1000 and chunk_overlap=100, chunks will be: [0-1000], [900-1900], [1800-2800], etc. Default: 0 (no overlap). Recommended: 10-20% of chunk_size (e.g., 100-200 for chunk_size=1000). Higher overlap improves context but increases storage and processing time.")
    response_shape: Optional[ResponseShape2] = None
    llm_provider: Optional[StrictStr] = Field(default=None, description="OPTIONAL. LLM provider to use for structured extraction. Only required if response_shape is provided. Supported providers: 'openai', 'google', 'anthropic'. Default: 'openai' if not specified.")
    llm_model: Optional[StrictStr] = Field(default=None, description="OPTIONAL. Specific LLM model for structured extraction. Only required if response_shape is provided. Examples: - OpenAI: 'gpt-4o-mini-2024-07-18' (cost-effective), 'gpt-4o-2024-08-06' (best quality) - Google: 'gemini-2.0-flash' (fastest, recommended) - Anthropic: 'claude-3-5-haiku-20241022' (fast), 'claude-3-5-sonnet-20241022' (best reasoning) Default: Uses provider's recommended model if not specified.")
    __properties: ClassVar[List[str]] = ["split_by", "chunk_size", "chunk_overlap", "response_shape", "llm_provider", "llm_model"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of TextExtractorParams from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of response_shape
        if self.response_shape:
            _dict['response_shape'] = self.response_shape.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of TextExtractorParams from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "split_by": obj.get("split_by"),
            "chunk_size": obj.get("chunk_size") if obj.get("chunk_size") is not None else 1000,
            "chunk_overlap": obj.get("chunk_overlap") if obj.get("chunk_overlap") is not None else 0,
            "response_shape": ResponseShape2.from_dict(obj["response_shape"]) if obj.get("response_shape") is not None else None,
            "llm_provider": obj.get("llm_provider"),
            "llm_model": obj.get("llm_model")
        })
        return _obj


