Metadata-Version: 2.4
Name: orka-reasoning
Version: 0.9.12
Summary: Modular agent orchestrator for reasoning pipelines
Author-email: Marco Somma <marcosomma.work@gmail.com>
License-Expression: Apache-2.0
Project-URL: Homepage, https://orkacore.com
Project-URL: Documentation, https://docs.orkacore.com
Project-URL: Repository, https://github.com/marcosomma/orka-reasoning
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: redis[async,search]==4.5.5
Requires-Dist: redis-om==0.3.5
Requires-Dist: rmtest>=0.7.0
Requires-Dist: pyyaml>=6.0.2
Requires-Dist: python-dotenv>=0.19.0
Requires-Dist: GPUtil>=1.4.0
Requires-Dist: psutil>=7.0.0
Requires-Dist: openai>=1.0.0
Requires-Dist: litellm>=1.0.0
Requires-Dist: numpy>=2.3.1
Requires-Dist: sentence_transformers>=5.0.0
Requires-Dist: ddgs>=3.0.0
Requires-Dist: fastapi>=0.68.0
Requires-Dist: uvicorn>=0.15.0
Requires-Dist: pydantic>=1.8.0
Requires-Dist: httpx>=0.24.0
Requires-Dist: jinja2>=3.0.0
Requires-Dist: async-timeout>=4.0.0
Requires-Dist: json-repair>=0.25.0
Requires-Dist: jsonschema>=4.20.0
Requires-Dist: rich>=13.0.0
Requires-Dist: textual>=0.40.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: coverage>=7.0.0; extra == "dev"
Requires-Dist: fakeredis>=2.0.0; extra == "dev"
Requires-Dist: black>=24.2.0; extra == "dev"
Requires-Dist: flake8>=7.0.0; extra == "dev"
Requires-Dist: flake8-docstrings>=1.7.0; extra == "dev"
Requires-Dist: flake8-bugbear>=24.1.0; extra == "dev"
Requires-Dist: flake8-comprehensions>=3.14.0; extra == "dev"
Requires-Dist: flake8-pytest-style>=1.7.0; extra == "dev"
Requires-Dist: mypy>=1.17.0; extra == "dev"
Requires-Dist: types-redis>=4.2.0; extra == "dev"
Requires-Dist: types-PyYAML>=6.0.12; extra == "dev"
Requires-Dist: types-requests>=2.32.0; extra == "dev"
Requires-Dist: types-psutil>=7.0.0; extra == "dev"
Requires-Dist: sphinx>=7.0.0; extra == "dev"
Requires-Dist: sphinx-rtd-theme>=1.0.0; extra == "dev"
Provides-Extra: extra
Provides-Extra: prod
Requires-Dist: gunicorn>=21.0.0; extra == "prod"
Requires-Dist: supervisor>=4.2.0; extra == "prod"
Provides-Extra: ml
Requires-Dist: sentence-transformers>=2.2.0; extra == "ml"
Requires-Dist: numpy>=1.21.0; extra == "ml"
Provides-Extra: all
Requires-Dist: orka-reasoning[dev,ml,prod,schema]; extra == "all"
Dynamic: license-file

<p align="center">

<img src="https://orkacore.com/assets/orka_logo_white.png" alt="OrKa Logo" style="border-radius: 25px; width: 400px; height:400px" />

[![GitHub Tag](https://img.shields.io/github/v/tag/marcosomma/orka-reasoning?color=blue)](https://github.com/marcosomma/orka-reasoning/tags)
[![PyPI - License](https://img.shields.io/pypi/l/orka-reasoning?color=blue)](https://pypi.org/project/orka-reasoning/)

[![Unit Tests](https://github.com/marcosomma/orka-reasoning/actions/workflows/tests.yml/badge.svg)](https://github.com/marcosomma/orka-reasoning/actions/workflows/tests.yml) [![Lint](https://github.com/marcosomma/orka-reasoning/actions/workflows/lint.yml/badge.svg)](https://github.com/marcosomma/orka-reasoning/actions/workflows/lint.yml)

<!-- [![codecov](https://img.shields.io/badge/codecov-76.97%25-yellow?&amp;logo=codecov)](https://codecov.io/gh/marcosomma/orka-reasoning) -->
[![codecov](https://codecov.io/github/marcosomma/orka-reasoning/branch/master/graph/badge.svg?token=V91X4WGBBZ)](https://codecov.io/github/marcosomma/orka-reasoning)
[![orka-reasoning](https://snyk.io/advisor/python/orka-reasoning/badge.svg)](https://snyk.io/advisor/python/orka-reasoning)

[![PyPi](https://img.shields.io/badge/pypi-%23ececec.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=1f73b7)](https://pypi.org/project/orka-reasoning/)[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/r/marcosomma/orka-ui)[![Documentation](https://img.shields.io/badge/Docs-blue?style=for-the-badge&amp;logo=googledocs&amp;logoColor=%23fff&amp;link=https%3A%2F%2Forkacore.com%2Fdocs%2Findex.html)](https://orkacore.com/docs/index.html)

[![orkacore](https://img.shields.io/badge/orkacore-.com-green?labelColor=blue&amp;style=for-the-badge&amp;link=https://orkacore.com/)](https://orkacore.com/)


[![Pepy Total Downloads](https://img.shields.io/pepy/dt/orka-reasoning?style=for-the-badge&amp;label=Downloads%20from%20April%202025&amp;color=blue&amp;link=https%3A%2F%2Fpiptrends.com%2Fpackage%2Forka-reasoning)](https://clickpy.clickhouse.com/dashboard/orka-reasoning)
</p>

## What OrKa Does

OrKa is a **local first** solution that lets you define AI workflows in YAML files instead of writing complex Python code. You describe what you want - like "search memory, then ask an AI, then save the result" - and OrKa handles the execution.

Think of it as a streamlined, open-source alternative to CrewAI or LangChain, but with a focus on:
- **Local LLM** Vendor agnostic
- **YAML configuration** instead of code
- **Built-in memory** that remembers and forgets intelligently  
- **Local LLM support** for privacy
- **Simple setup** with Docker

## Basic Example

Instead of writing Python code like this:
```python
# Complex Python orchestration code
memory_results = search_memory(query)
if not memory_results:
    web_results = search_web(query)
    answer = llm.generate(web_results + query)
else:
    answer = llm.generate(memory_results + query)
save_to_memory(query, answer)
```

You write a YAML file like this:
```yaml
orchestrator:
  id: simple-qa
  agents: [memory_search, web_search, answer, memory_store]

agents:
  - id: memory_search
    type: memory
    operation: read
    prompt: "Find: {{ input }}"
    
  - id: web_search  
    type: search
    prompt: "Search: {{ input }}"
    
  - id: answer
    type: local_llm
    model: llama3.2:3b
    prompt: "Answer based on: {{ previous_outputs }}"
    
  - id: memory_store
    type: memory
    operation: write
    prompt: "Store: {{ input }} -> {{ previous_outputs.answer }}"
```

## Installation

```bash
# Install OrKa
pip install orka-reasoning

# Start RedisStack + Backend + UI
# Automatically tries native RedisStack first, then Docker
# UI available at http://localhost:8080
orka-start

# Memory TUI
orka memory watch

# Run a workflow
orka run my-workflow.yml "What is machine learning?"
```

## Visual Workflow Builder (OrKa UI)

**Don't want to write YAML by hand?** Use **OrKa UI** - a drag-and-drop visual editor:

### Automatic Start (Recommended)
```bash
# UI automatically starts with orka-start
orka-start

# Access at http://localhost:8080
```

### Manual Start (Alternative)
```bash
# Pull and run the UI manually
docker pull marcosomma/orka-ui:latest
docker run -d -p 8080:80 --name orka-ui \
  -e VITE_API_URL_LOCAL=http://localhost:8000/api/run@dist \
  marcosomma/orka-ui:latest

# Access at http://localhost:8080
```

### Configuration
```bash
# Skip UI (Redis + Backend only)
export ORKA_DISABLE_UI=true
orka-start

# Use cached Docker image (faster startup)
export ORKA_UI_SKIP_PULL=true
orka-start
```

## üÜï JSON Input Support

OrKa now supports JSON input for advanced workflows and structured use cases!

### Why use JSON input?
- Pass complex data (objects, arrays, clinical records, etc.) as input to your workflow.
- Enable dynamic prompts and agents that access specific input fields via `{{ input.field }}`.
- Perfect for use cases like: medical assistants, document automation, multi-step workflows with structured data.

### How to use
1. Prepare a JSON file, e.g. `input.json`:
   ```json
   {
     "patient": {
       "name": "Fido",
       "species": "dog",
       "symptoms": ["vomiting", "lethargy"],
       "age": 7
     },
     "history": "No previous major illnesses."
   }
   ```
2. Pass the file as input to the workflow using the `--json-input` flag **before** the `run` command:
   ```bash
   orka --json-input run my-workflow.yml input.json
   ```
   Or pass inline JSON:
   ```bash
   orka --json-input run my-workflow.yml '{"foo": 123, "bar": "baz"}' 
   ```
3. In your YAML, access fields using Jinja2 syntax:
   ```yaml
   prompt: "Patient: {{ input.patient.name }}, Symptoms: {{ input.patient.symptoms }}"
   ```

**Note:** If you use the `--json-input` flag, the plain text input is ignored and only the JSON is used.

For real-world examples, see the `examples/` folder and the documentation.

---

**Features:**
- üé® Drag-and-drop workflow builder
- üîß Visual node configuration
- üì§ One-click YAML export
- üöÄ Built-in workflow execution
- üìö Example workflow library

**[üìñ Read the full OrKa UI documentation ‚Üí](docs/orka-ui.md)**

### What `orka-start` Provides

When you run `orka-start`, it automatically sets up:
1. **RedisStack** (memory backend) - tries native first, then Docker
2. **OrKa Backend API** (port 8000) - workflow execution engine
3. **OrKa UI** (port 8080) - visual workflow builder (if Docker available)

**RedisStack Setup:**
- **Tries native RedisStack** (if installed on your system)
- **Falls back to Docker** (if Docker is running)
- **Shows install instructions** (if neither is available)

**Choose your preferred method:**
- **Docker** (easiest): Just have Docker running, `orka-start` handles everything
- **Native** (no Docker needed):
  - macOS: `brew install redis-stack`
  - Ubuntu: `sudo apt install redis-stack-server`
  - Windows: Download from [redis.io](https://redis.io/download)

## How It Works

### 1. Agent Types
OrKa provides several agent types you can use in your workflows:

- **`memory`** - Read from or write to persistent memory
- **`local_llm`** - Use local models (Ollama, LM Studio)
- **`openai-*`** - Use OpenAI models  
- **`search`** - Web search
- **`router`** - Conditional branching
- **`fork/join`** - Parallel processing
- **`loop`** - Iterative workflows
- **`plan_validator`** - Validate and critique proposed execution paths
- **`graph_scout`** - [BETA] Find best path for workflow execution

### 2. Memory System
OrKa includes a memory system that:
- Stores conversations and facts
- Searches semantically (finds related content, not just exact matches)
- Automatically forgets old, unimportant information
- Uses Redis for fast retrieval

### 3. Workflow Execution
When you run `orka run workflow.yml "input"`, OrKa:
1. Reads your YAML configuration
2. Creates the agents you defined
3. Runs them in the order you specified
4. Passes outputs between agents
5. Returns the final result

### 4. Local LLM Support
OrKa works with local models through:
- **Ollama** - `ollama pull llama3.2` then use `provider: lm_studio`
- **LM Studio** - Point to your local API endpoint
- **Any LLM-compatible API**

---

## üìö **Complete Agent & Node Reference**

> **üéØ NEW: [Comprehensive Documentation for Every Agent, Node & Tool ‚Üí](docs/AGENT_NODE_TOOL_INDEX.md)**
>
> Detailed documentation for all agent types, control flow nodes, and tools:
> - ü§ñ **7 LLM Agents** - OpenAI, Local LLM, Binary, Classification, Validation, PlanValidator
> - üíæ **2 Memory Agents** - Reader & Writer with 100x faster HNSW indexing
> - üîÄ **6 Control Flow Nodes** - Router, Fork/Join, Loop, Failover, GraphScout
> - üîß **2 Search Tools** - DuckDuckGo, RAG
>
> Each with working examples, parameters, best practices, and troubleshooting!

---

## Common Patterns

### Memory-First Q&A
```yaml
# Check memory first, search web if nothing found
agents:
  - id: check_memory
    type: memory
    operation: read

  - id: binary_agent
    type: local_llm
    prompt: |
      Given those memory {{get_agent_response('check_memory')}} and this input {{ input }}
      Is an search on internet required?
      Only answer with 'true' or 'false' 
    
  - id: route_decision
    type: router
    decision_key: 'binary_agent'
    routing_map:
      "true": [answer_from_memory]
      "false": [web_search, answer_from_web]
```

### Parallel Processing
```yaml
# Analyze sentiment and toxicity simultaneously
agents:
  - id: parallel_analysis
    type: fork
    targets:
      - [sentiment_analyzer]
      - [toxicity_checker]
      
  - id: combine_results
    group: parallel_analysis
    type: join
```

### Iterative Improvement
```yaml
# Keep improving until quality threshold met
agents:
  - id: improvement_loop
    type: loop
    max_loops: 5
    score_threshold: 0.85
    internal_workflow:
      agents: [analyzer, scorer]
```

## Comparison to Alternatives

| Feature | OrKa | LangChain | CrewAI |
|---------|------|-----------|---------|
| Configuration | YAML files | Python code | Python code |
| Memory | Built-in with decay | External/manual | External/manual |
| Local LLMs | First-class support | Via adapters | Limited |
| Parallel execution | Native fork/join | Manual threading | Agent-based |
| Learning | Automatic memory management | Manual | Manual |

## Quick Start Examples

### 1. Simple Q&A with Memory
```bash
# Copy example
cp examples/simple_memory_preset_demo.yml my-qa.yml

# Run it
orka run my-qa.yml "What is artificial intelligence?"
```

### 2. Web Search + Memory
```bash
# Copy example  
cp examples/person_routing_with_search.yml web-qa.yml

# Run it
orka run web-qa.yml "Latest news about quantum computing"
```

### 3. Local LLM Chat
```bash
# Start Ollama
ollama pull llama3.2

# Copy example
cp examples/multi_model_local_llm_evaluation.yml local-chat.yml

# Run it
orka run local-chat.yml "Explain machine learning simply"
```

## Documentation

### üìö **[Documentation Index ‚Üí](docs/index.md)** - Start Here!
**Complete documentation hub** with organized guides, tutorials, and references for all OrKa features.

Quick links:
- **[üìò Quickstart](docs/quickstart.md)** - Get running in 5 minutes
- **[üéØ Agent & Node Reference](docs/AGENT_NODE_TOOL_INDEX.md)** - Every agent, node & tool documented
- **[üß† Memory System](docs/MEMORY_SYSTEM_GUIDE.md)** - Intelligent memory configuration
- **[‚öôÔ∏è YAML Configuration](docs/YAML_CONFIGURATION.md)** - Complete workflow reference
- **[üß≠ GraphScout Agent](docs/GRAPH_SCOUT_AGENT.md)** - Dynamic routing system
- **[üìã Examples](examples/README.md)** - 50+ ready-to-use workflow templates

## Getting Help

- [GitHub Issues](https://github.com/marcosomma/orka-reasoning/issues) - Bug reports and feature requests
- [Documentation](https://orkacore.com/docs) - Full documentation
- [Examples](examples/) - Working examples you can copy and modify

## Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## License

Apache 2.0 License - see [LICENSE](LICENSE) for details.
