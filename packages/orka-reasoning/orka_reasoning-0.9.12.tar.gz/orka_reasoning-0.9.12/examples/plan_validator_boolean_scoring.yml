# GraphScout + Boolean Scoring Demo
# ===================================
#
# Basic example of GraphScout validated by PlanValidator
# using boolean scoring for deterministic path evaluation.
#
# Shows the core concept of:
# 1. GraphScout proposes paths from available agents
# 2. PlanValidator scores with boolean criteria
# 3. Loop repeats if quality insufficient
# 4. Feedback guides GraphScout's next proposal

orchestrator:
  id: graphscout-boolean-demo
  strategy: sequential
  agents:
    - routing_loop
    - path_executor
    - execution_summary

agents:
  # Main loop: GraphScout → PlanValidator
  - id: routing_loop
    type: loop
    max_loops: 3
    score_threshold: 0.25  # Lowered from 0.82 to match actual gpt-oss:20b validation performance
    persist_across_runs: true
    
    # Boolean scoring configuration
    scoring:
      preset: moderate  # Options: strict (0.90+), moderate (0.85+), lenient (0.80+)
      context: loop_convergence  # Evaluate iterative improvement
      
      # Optional: Emphasize specific criteria
      custom_weights:
        improvement.better_than_previous: 0.30
        convergence.within_tolerance: 0.25
    
    past_loops_metadata:
      loop_number: "{{ get_loop_number() }}"
      score: "{{ score }}"
      timestamp: "{{ timestamp }}"
      insights: "{{ insights }}"
      improvements: "{{ improvements }}"
      mistakes: "{{ mistakes }}"
    
    internal_workflow:
      orchestrator:
        id: route-validate
        strategy: sequential
        agents: [graphscout_router, path_validator]
      
      agents:
        # GraphScout selects execution path
        - id: graphscout_router
          type: graph-scout
          params:
            k_beam: 4
            max_depth: 2
            commit_margin: 0.15
            require_terminal: true
            score_weights:
              llm: 0.45
              heuristics: 0.30
              prior: 0.15
              cost: 0.05
              latency: 0.05
            evaluation_model: "local_llm"
            evaluation_model_name: "openai/gpt-oss-20b"
            llm_evaluation_enabled: true
            provider: lm_studio
            model_url: http://localhost:1234
            fallback_to_heuristics: true
          prompt: |
            Select optimal agent path for: {{ get_input() }}
            
            {% if has_past_loops() %}
            ## Validation Feedback (Attempt {{ get_loop_number() }})
            {% set last = get_past_loops()[-1] %}
            Previous score: {{ last.score }} (need {{ 0.82 }})
            Failed criteria: {{ last.mistakes }}
            {% endif %}
            
            **Available Agents:**
            - data_retriever: Fetch information (web search, databases)
            - analyzer: Process and analyze data
            - generator: Create final output (REQUIRED for terminal paths)
            
            Select path that addresses the query completely.
        
        # PlanValidator with boolean scoring
        - id: path_validator
          type: plan_validator
          llm_model:  openai/gpt-oss-20b
          model_url: http://localhost:1234
          provider: lm_studio
          temperature: 0.2
          scoring_preset: moderate
          custom_weights:
            completeness.has_all_required_steps: 0.20
            efficiency.uses_appropriate_agents: 0.12

  # ===== EXECUTION AGENTS (Top Level) =====
  # CRITICAL: These agents MUST be at top-level, NOT inside internal_workflow
  # - GraphScout discovers agents from orchestrator.agents (global registry)
  # - PathExecutor can only execute agents in the global registry
  # - Agents inside internal_workflow are scoped locally and NOT accessible
  
  # Supporting agents for GraphScout to route to
  - id: data_retriever
    type: duckduckgo
    capabilities: [data_retrieval, web_search]
    prompt: |
      {{ input }}
  
  - id: analyzer
    type: local_llm
    capabilities: [analysis, reasoning]
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    prompt: |
      Analyze: {{ input }}
      
      {% if previous_outputs.data_retriever %}
      Data: {{ safe_get_response('data_retriever', 'No data available', previous_outputs) }}
      {% endif %}
  
  - id: generator
    type: local_llm
    capabilities: [answer_emit, generation, synthesis]
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    prompt: |
      Generate response for: {{ input }}
      
      Context: {{ previous_outputs }}

  # Execute the validated path
  - id: path_executor
    type: path_executor
    path_source: routing_loop
    on_agent_failure: continue

  # Execution summary
  - id: execution_summary
    type: local_llm
    model: openai/gpt-oss-20b
    model_url: http://localhost:1234
    provider: lm_studio
    temperature: 0.3
    prompt: |
      # Boolean Scoring & Execution Results
      
      {% if previous_outputs.routing_loop %}
      {% set loop = previous_outputs.routing_loop %}
      
      ## Validation Summary
      - Attempts: {{ loop.loops_completed }}
      - Final Score: {{ (loop.final_score | default(0)) | round(3) }}
      - Result: {{ 'APPROVED ✓' if loop.threshold_met else 'REJECTED ✗' }}
      
      ## GraphScout's Path
      {% if loop.response.result.graphscout_router %}
      - Proposed: {{ loop.response.result.graphscout_router.target }}
      - Confidence: {{ loop.response.result.graphscout_router.confidence }}
      {% endif %}
      
      ## Boolean Validation
      {% if loop.response.result.path_validator %}
      {% set val = loop.response.result.path_validator %}
      
      **Score Breakdown:**
      - Overall: {{ (val.validation_score | default(0)) | round(3) }}/1.0
      - Assessment: {{ val.overall_assessment | default('UNKNOWN') }}
      - Passed: {{ val.passed_criteria|length }}/15 criteria
      {% if val.failed_criteria %}
      - Failed: {{ val.failed_criteria }}
      {% endif %}
      
      **Dimension Scores:**
      {% if val.dimension_scores %}
      {% for dim, data in val.dimension_scores.items() %}
      - {{ dim|title }}: {{ (data.percentage | default(0)) | round(0) }}%
      {% endfor %}
      {% endif %}
      {% endif %}
      
      ## Execution Results
      {% if previous_outputs.path_executor %}
      {% set executor = previous_outputs.path_executor %}
      
      **Execution Status:** {{ 'SUCCESS ✓' if executor.status == 'success' else 'FAILED ✗' }}
      **Agents Executed:** {{ executor.executed_path | default([]) | length }}
      
      {% if executor.executed_path %}
      **Path Execution:**
      {% for agent_id in executor.executed_path %}
      - {{ agent_id }}
      {% endfor %}
      
      **Final Result:**
      {{ executor.result | default('No result available') }}
      {% else %}
      The validated path was not executed (validation threshold not met or execution error).
      {% endif %}
      
      {% if executor.errors %}
      **Errors During Execution:**
      {{ executor.errors }}
      {% endif %}
      {% else %}
      PathExecutor did not run (validation loop may have failed).
      {% endif %}
      
      ---
      
      **Analysis:**
      Provide detailed analysis:
      1. Why the path scored {{ (loop.final_score | default(0)) | round(3) }}
      2. Whether validation criteria were appropriate
      3. Execution effectiveness and results quality
      4. Whether the final output answers the original query: "{{ input }}"
      3. Any issues or improvements needed
      
      {% else %}
      No routing data available.
      {% endif %}
