#!/usr/bin/env python3
"""
é«˜å¹¶å‘æ€§èƒ½æµ‹è¯• - TPS=500 ç›®æ ‡
æµ‹è¯•è¦†ç›–ï¼šStreamã€ZSetã€Hash åœºæ™¯
"""

import asyncio
import time
import threading
import statistics
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Callable, Optional
import uuid
import random
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from agent_redis_framework.helper import (
    get_streams_client, get_sorted_set_queue, get_hash_client, get_redis_util
)
from agent_redis_framework.redis_client import (
    get_pool_stats, log_pool_stats
)


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    operation_type: str
    total_operations: int
    successful_operations: int
    failed_operations: int
    total_duration: float
    tps: float
    avg_latency: float
    min_latency: float
    max_latency: float
    p95_latency: float
    p99_latency: float
    error_rate: float
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.operation_times: List[float] = []
        self.errors: List[Exception] = []
        self.start_time: float = 0
        self.end_time: float = 0
        self.lock = threading.Lock()
    
    def start(self):
        """å¼€å§‹ç›‘æ§"""
        self.start_time = time.time()
        self.operation_times.clear()
        self.errors.clear()
    
    def record_operation(self, duration: float, error: Optional[Exception] = None):
        """è®°å½•å•æ¬¡æ“ä½œ"""
        with self.lock:
            if error:
                self.errors.append(error)
            else:
                self.operation_times.append(duration)
    
    def stop(self) -> PerformanceMetrics:
        """åœæ­¢ç›‘æ§å¹¶ç”ŸæˆæŒ‡æ ‡"""
        self.end_time = time.time()
        total_duration = self.end_time - self.start_time
        
        successful_ops = len(self.operation_times)
        failed_ops = len(self.errors)
        total_ops = successful_ops + failed_ops
        
        if successful_ops > 0:
            avg_latency = statistics.mean(self.operation_times)
            min_latency = min(self.operation_times)
            max_latency = max(self.operation_times)
            
            # è®¡ç®—ç™¾åˆ†ä½æ•°
            sorted_times = sorted(self.operation_times)
            p95_idx = int(0.95 * len(sorted_times))
            p99_idx = int(0.99 * len(sorted_times))
            p95_latency = sorted_times[p95_idx] if p95_idx < len(sorted_times) else max_latency
            p99_latency = sorted_times[p99_idx] if p99_idx < len(sorted_times) else max_latency
        else:
            avg_latency = min_latency = max_latency = p95_latency = p99_latency = 0
        
        tps = total_ops / total_duration if total_duration > 0 else 0
        error_rate = failed_ops / total_ops if total_ops > 0 else 0
        
        return PerformanceMetrics(
            operation_type="",
            total_operations=total_ops,
            successful_operations=successful_ops,
            failed_operations=failed_ops,
            total_duration=total_duration,
            tps=tps,
            avg_latency=avg_latency * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
            min_latency=min_latency * 1000,
            max_latency=max_latency * 1000,
            p95_latency=p95_latency * 1000,
            p99_latency=p99_latency * 1000,
            error_rate=error_rate * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        )


class HighConcurrencyTester:
    """é«˜å¹¶å‘æµ‹è¯•å™¨"""
    
    def __init__(self, target_tps: int = 500, test_duration: int = 30):
        self.target_tps = target_tps
        self.test_duration = test_duration
        self.max_workers = min(100, target_tps // 2)  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°
        
        # è®¾ç½®é«˜å¹¶å‘ç¯å¢ƒå˜é‡
        os.environ.update({
            'REDIS_MAX_CONNECTIONS': '1024',
            'REDIS_SOCKET_TIMEOUT': '5',
            'REDIS_SOCKET_CONNECT_TIMEOUT': '3',
            'REDIS_HEALTH_CHECK_INTERVAL': '30'
        })
        
        print(f"ğŸš€ é«˜å¹¶å‘æµ‹è¯•é…ç½®:")
        print(f"   ç›®æ ‡ TPS: {self.target_tps}")
        print(f"   æµ‹è¯•æ—¶é•¿: {self.test_duration}ç§’")
        print(f"   æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
        print(f"   è¿æ¥æ± é…ç½®: max_connections={os.getenv('REDIS_MAX_CONNECTIONS')}")
    
    def _execute_with_rate_limit(self, operation_func: Callable, monitor: PerformanceMonitor):
        """ä»¥æŒ‡å®šé€Ÿç‡æ‰§è¡Œæ“ä½œ"""
        interval = 1.0 / self.target_tps  # æ¯æ¬¡æ“ä½œçš„é—´éš”æ—¶é—´
        
        def worker():
            while time.time() - monitor.start_time < self.test_duration:
                start_time = time.time()
                error = None
                
                try:
                    operation_func()
                except Exception as e:
                    error = e
                
                end_time = time.time()
                duration = end_time - start_time
                monitor.record_operation(duration, error)
                
                # æ§åˆ¶é€Ÿç‡
                elapsed = end_time - start_time
                if elapsed < interval:
                    time.sleep(interval - elapsed)
        
        # å¯åŠ¨å¤šä¸ªå·¥ä½œçº¿ç¨‹
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(worker) for _ in range(self.max_workers)]
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆæˆ–è¶…æ—¶
            for future in as_completed(futures, timeout=self.test_duration + 5):
                try:
                    future.result()
                except Exception as e:
                    print(f"Worker çº¿ç¨‹å¼‚å¸¸: {e}")
    
    def test_stream_operations(self) -> PerformanceMetrics:
        """æµ‹è¯• Stream æ“ä½œæ€§èƒ½"""
        print("\nğŸ“¡ å¼€å§‹ Stream é«˜å¹¶å‘æµ‹è¯•...")
        
        stream_name = f"test_stream_{uuid.uuid4().hex[:8]}"
        consumer_group = "test_group"
        
        # åˆå§‹åŒ– Stream å®¢æˆ·ç«¯
        stream_client = get_streams_client(stream_name)
        
        # åˆ›å»ºæ¶ˆè´¹è€…ç»„
        try:
            stream_client.ensure_group(consumer_group)
        except Exception:
            pass  # ç»„å¯èƒ½å·²å­˜åœ¨
        
        monitor = PerformanceMonitor()
        
        def stream_operation():
            """Stream æ“ä½œï¼šå‘é€æ¶ˆæ¯"""
            from agent_redis_framework.streams import StreamMsg
            message_data = StreamMsg(
                payload=json.dumps({
                    'id': str(uuid.uuid4()),
                    'timestamp': str(int(time.time() * 1000)),
                    'data': f"test_message_{random.randint(1000, 9999)}",
                    'payload': 'x' * random.randint(100, 500)  # éšæœºå¤§å°çš„è´Ÿè½½
                }),
                meta={'test': 'high_concurrency', 'seq': random.randint(1, 1000)}
            )
            stream_client.push(message_data)
        
        monitor.start()
        self._execute_with_rate_limit(stream_operation, monitor)
        metrics = monitor.stop()
        metrics.operation_type = "Stream Operations"
        
        # æ¸…ç†
        try:
            redis_util = get_redis_util()
            redis_util.redis.delete(stream_name)
        except Exception:
            pass
        
        return metrics
    
    def test_zset_operations(self) -> PerformanceMetrics:
        """æµ‹è¯• ZSet (SortedSet) æ“ä½œæ€§èƒ½"""
        print("\nğŸ† å¼€å§‹ ZSet é«˜å¹¶å‘æµ‹è¯•...")
        
        queue_name = f"test_zset_{uuid.uuid4().hex[:8]}"
        sorted_queue = get_sorted_set_queue(queue_name)
        
        monitor = PerformanceMonitor()
        
        def zset_operation():
            """ZSet æ“ä½œï¼šæ·»åŠ ã€æŸ¥è¯¢ã€æ’å"""
            from agent_redis_framework.sortedset import SortedTask
            operation_type = random.choice(['push', 'size'])
            
            if operation_type == 'push':
                # æ·»åŠ å…ƒç´ 
                score = random.uniform(0, 1000)
                task = SortedTask(
                    payload=json.dumps({'task_id': str(uuid.uuid4())}),
                    meta={'priority': score}
                )
                sorted_queue.push(task, score)
            
            elif operation_type == 'size':
                # è·å–é˜Ÿåˆ—å¤§å°
                sorted_queue.size()

        monitor.start()
        self._execute_with_rate_limit(zset_operation, monitor)
        metrics = monitor.stop()
        metrics.operation_type = "ZSet Operations"
        
        # æ¸…ç†
        try:
            redis_util = get_redis_util()
            redis_util.redis.delete(queue_name)
        except Exception:
            pass
        
        return metrics
    
    def test_hash_operations(self) -> PerformanceMetrics:
        """æµ‹è¯• Hash æ“ä½œæ€§èƒ½"""
        print("\nğŸ—‚ï¸ å¼€å§‹ Hash é«˜å¹¶å‘æµ‹è¯•...")
        
        hash_name = f"test_hash_{uuid.uuid4().hex[:8]}"
        hash_client = get_hash_client(hash_name)
        
        monitor = PerformanceMonitor()
        
        def hash_operation():
            """Hash æ“ä½œï¼šè®¾ç½®ã€è·å–ã€æ‰¹é‡æ“ä½œ"""
            operation_type = random.choice(['set', 'get', 'set_many', 'get_many'])
            
            if operation_type == 'set':
                # è®¾ç½®å•ä¸ªå­—æ®µ
                field = f"field_{random.randint(1, 1000)}"
                value = f"value_{uuid.uuid4().hex}"
                hash_client.set(field, value)
            
            elif operation_type == 'get':
                # è·å–å•ä¸ªå­—æ®µ
                field = f"field_{random.randint(1, 1000)}"
                try:
                    hash_client.get(field)
                except Exception:
                    pass  # å­—æ®µå¯èƒ½ä¸å­˜åœ¨
            
            elif operation_type == 'set_many':
                # æ‰¹é‡è®¾ç½® - ç¡®ä¿ç±»å‹å…¼å®¹
                from typing import cast
                from agent_redis_framework.hashes.hash_client import SupportedScalar
                fields = cast(dict[str, SupportedScalar], {
                    f"batch_field_{i}": f"batch_value_{uuid.uuid4().hex[:8]}"
                    for i in range(5)
                })
                hash_client.set_many(fields)
            
            elif operation_type == 'get_many':
                # æ‰¹é‡è·å–
                fields = [f"field_{i}" for i in range(1, 11)]
                try:
                    hash_client.get_many(fields)
                except Exception:
                    pass
        
        monitor.start()
        self._execute_with_rate_limit(hash_operation, monitor)
        metrics = monitor.stop()
        metrics.operation_type = "Hash Operations"
        
        # æ¸…ç†
        try:
            redis_util = get_redis_util()
            redis_util.redis.delete(hash_name)
        except Exception:
            pass
        
        return metrics
    
    def run_comprehensive_test(self) -> Dict[str, PerformanceMetrics]:
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        print("ğŸ¯ å¼€å§‹é«˜å¹¶å‘ç»¼åˆæ€§èƒ½æµ‹è¯•")
        print("=" * 60)
        
        # è®°å½•åˆå§‹è¿æ¥æ± çŠ¶æ€
        print("\nğŸ“Š æµ‹è¯•å‰è¿æ¥æ± çŠ¶æ€:")
        log_pool_stats()
        
        results = {}
        
        # ä¾æ¬¡æµ‹è¯•å„ä¸ªåœºæ™¯
        test_scenarios = [
            ("stream", self.test_stream_operations),
            ("zset", self.test_zset_operations),
            ("hash", self.test_hash_operations)
        ]
        
        for scenario_name, test_func in test_scenarios:
            print(f"\nğŸ”„ ç­‰å¾… 3 ç§’åå¼€å§‹ {scenario_name.upper()} æµ‹è¯•...")
            time.sleep(3)  # ç»™è¿æ¥æ± ä¸€äº›æ¢å¤æ—¶é—´
            
            try:
                metrics = test_func()
                results[scenario_name] = metrics
                
                # è¾“å‡ºå³æ—¶ç»“æœ
                print(f"âœ… {metrics.operation_type} æµ‹è¯•å®Œæˆ:")
                print(f"   TPS: {metrics.tps:.2f}")
                print(f"   æˆåŠŸç‡: {100 - metrics.error_rate:.2f}%")
                print(f"   å¹³å‡å»¶è¿Ÿ: {metrics.avg_latency:.2f}ms")
                
            except Exception as e:
                print(f"âŒ {scenario_name.upper()} æµ‹è¯•å¤±è´¥: {e}")
                results[scenario_name] = None
        
        # è®°å½•æµ‹è¯•åè¿æ¥æ± çŠ¶æ€
        print("\nğŸ“Š æµ‹è¯•åè¿æ¥æ± çŠ¶æ€:")
        log_pool_stats()
        
        return results
    
    def generate_report(self, results: Dict[str, PerformanceMetrics]) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report_lines = [
            "ğŸ¯ é«˜å¹¶å‘æ€§èƒ½æµ‹è¯•æŠ¥å‘Š",
            "=" * 60,
            f"æµ‹è¯•é…ç½®: ç›®æ ‡ TPS={self.target_tps}, æµ‹è¯•æ—¶é•¿={self.test_duration}ç§’",
            f"è¿æ¥æ± é…ç½®: max_connections={os.getenv('REDIS_MAX_CONNECTIONS')}",
            ""
        ]
        
        # æ±‡æ€»ç»Ÿè®¡
        total_tps = 0
        total_operations = 0
        successful_scenarios = 0
        
        for scenario_name, metrics in results.items():
            if metrics:
                total_tps += metrics.tps
                total_operations += metrics.total_operations
                successful_scenarios += 1
        
        report_lines.extend([
            "ğŸ“ˆ æ±‡æ€»ç»Ÿè®¡:",
            f"   æ€» TPS: {total_tps:.2f}",
            f"   æ€»æ“ä½œæ•°: {total_operations}",
            f"   æˆåŠŸåœºæ™¯: {successful_scenarios}/{len(results)}",
            ""
        ])
        
        # è¯¦ç»†ç»“æœ
        for scenario_name, metrics in results.items():
            if metrics:
                report_lines.extend([
                    f"ğŸ” {metrics.operation_type} è¯¦ç»†ç»“æœ:",
                    f"   TPS: {metrics.tps:.2f} (ç›®æ ‡: {self.target_tps})",
                    f"   æ€»æ“ä½œæ•°: {metrics.total_operations}",
                    f"   æˆåŠŸæ“ä½œ: {metrics.successful_operations}",
                    f"   å¤±è´¥æ“ä½œ: {metrics.failed_operations}",
                    f"   æˆåŠŸç‡: {100 - metrics.error_rate:.2f}%",
                    f"   å¹³å‡å»¶è¿Ÿ: {metrics.avg_latency:.2f}ms",
                    f"   P95 å»¶è¿Ÿ: {metrics.p95_latency:.2f}ms",
                    f"   P99 å»¶è¿Ÿ: {metrics.p99_latency:.2f}ms",
                    f"   æœ€å°å»¶è¿Ÿ: {metrics.min_latency:.2f}ms",
                    f"   æœ€å¤§å»¶è¿Ÿ: {metrics.max_latency:.2f}ms",
                    ""
                ])
            else:
                report_lines.extend([
                    f"âŒ {scenario_name.upper()} æµ‹è¯•å¤±è´¥",
                    ""
                ])
        
        # æ€§èƒ½è¯„ä¼°
        report_lines.extend([
            "ğŸ¯ æ€§èƒ½è¯„ä¼°:",
        ])
        
        if total_tps >= self.target_tps * 0.9:
            report_lines.append("   âœ… ä¼˜ç§€: è¾¾åˆ°ç›®æ ‡ TPS çš„ 90% ä»¥ä¸Š")
        elif total_tps >= self.target_tps * 0.7:
            report_lines.append("   âš ï¸ è‰¯å¥½: è¾¾åˆ°ç›®æ ‡ TPS çš„ 70-90%")
        else:
            report_lines.append("   âŒ éœ€è¦ä¼˜åŒ–: æœªè¾¾åˆ°ç›®æ ‡ TPS çš„ 70%")
        
        # ä¼˜åŒ–å»ºè®®
        report_lines.extend([
            "",
            "ğŸ’¡ ä¼˜åŒ–å»ºè®®:",
        ])
        
        if total_tps < self.target_tps * 0.8:
            report_lines.extend([
                "   - è€ƒè™‘å¢åŠ  REDIS_MAX_CONNECTIONS",
                "   - æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿå’Œ Redis æœåŠ¡å™¨æ€§èƒ½",
                "   - ä¼˜åŒ–ä¸šåŠ¡é€»è¾‘å‡å°‘å•æ¬¡æ“ä½œå¤æ‚åº¦"
            ])
        
        avg_error_rate = sum(m.error_rate for m in results.values() if m) / successful_scenarios if successful_scenarios > 0 else 0
        if avg_error_rate > 1:
            report_lines.extend([
                "   - é”™è¯¯ç‡è¾ƒé«˜ï¼Œæ£€æŸ¥ Redis è¿æ¥ç¨³å®šæ€§",
                "   - è€ƒè™‘å¢åŠ é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†"
            ])
        
        return "\n".join(report_lines)


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = HighConcurrencyTester(target_tps=500, test_duration=30)
    
    try:
        # è¿è¡Œæµ‹è¯•
        results = tester.run_comprehensive_test()
        
        # ç”Ÿæˆå¹¶è¾“å‡ºæŠ¥å‘Š
        report = tester.generate_report(results)
        print("\n" + report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = f"performance_test_report_{int(time.time())}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # ä¿å­˜ JSON æ ¼å¼çš„è¯¦ç»†æ•°æ®
        json_data = {
            'test_config': {
                'target_tps': tester.target_tps,
                'test_duration': tester.test_duration,
                'max_workers': tester.max_workers
            },
            'results': {k: v.to_dict() if v else None for k, v in results.items()},
            'timestamp': int(time.time())
        }
        
        json_file = f"performance_test_data_{int(time.time())}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜:")
        print(f"   æ–‡æœ¬æŠ¥å‘Š: {report_file}")
        print(f"   JSON æ•°æ®: {json_file}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()