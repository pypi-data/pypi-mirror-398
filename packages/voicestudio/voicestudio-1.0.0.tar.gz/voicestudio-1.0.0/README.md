# VoiceStudio

<div align="center">

**Your Complete Voice Adaptation Workspace**

[![PyPI version](https://badge.fury.io/py/voicestudio.svg)](https://badge.fury.io/py/voicestudio)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Documentation](https://img.shields.io/badge/docs-passing-brightgreen.svg)](https://latentforge.github.io/VoiceStudio)

[**Installation**](#installation) | [**Quick Start**](#quick-start) | [**Documentation**](https://latentforge.github.io/VoiceStudio) | [**Papers**](#publications)

</div>

---

## ğŸ¯ Overview

VoiceStudio is a unified toolkit for **text-style prompted speech synthesis**, enabling instant voice adaptation and editing through natural language descriptions. Built on cutting-edge research in voice style prompting, LoRA adaptation, and language-audio models.

**Key Features:**
- ğŸ¨ **Text-Style Prompting**: Control voice characteristics with natural language
- âš¡ **Instant Adaptation**: Real-time LoRA generation for any TTS model
- âœ‚ï¸ **Voice Editing**: Modify existing voices with simple instructions
- ğŸ”§ **Architecture Agnostic**: Works with multiple TTS architectures
- ğŸš€ **Production Ready**: Optimized for both research and deployment

---

## ğŸ†• What's New

**v0.1.0** (2025)
- ğŸ” Speaker consistency analysis tools
- ğŸ¨ BOS token P-tuning
- ğŸ“Š Attention visualization

---

## ğŸš€ Installation

### From PyPI (Recommended)

```bash
uv add voicestudio[all]
```

### From Source

```bash
uv add git+https://github.com/LatentForge/voicestudio.git
```

### Requirements

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.8+ (for GPU acceleration)

---

## ğŸ“š Advanced Usage

### Custom TTS Model Integration

VoiceStudio supports any TTS model through a simple adapter interface:

```python
from voicestudio import TTSAdapter, LoRAGenerator

# Wrap your TTS model
class MyTTSAdapter(TTSAdapter):
    def __init__(self, model):
        self.model = model
    
    def get_lora_target_modules(self):
        return ["attention.q_proj", "attention.v_proj"]
    
    def forward(self, text, lora_weights=None):
        if lora_weights:
            self.apply_lora(lora_weights)
        return self.model(text)

# Use with VoiceStudio
adapter = MyTTSAdapter(my_tts_model)
generator = LoRAGenerator.from_pretrained("voicestudio/t2a-lora-base")

lora = generator("professional news anchor voice")
audio = adapter(text="Breaking news tonight...", lora_weights=lora)
```

### Multi-Speaker Voice Blending

```python
from voicestudio import VoiceBlender

blender = VoiceBlender()

# Blend multiple voice characteristics
blended_lora = blender.blend([
    ("warm and friendly", 0.6),
    ("professional and clear", 0.4)
])

audio = tts_model.synthesize(text, lora=blended_lora)
```

### Fine-tuning on Custom Data

```python
from voicestudio import LoRAGenerator
from voicestudio.training import Trainer

# Load pre-trained generator
generator = LoRAGenerator.from_pretrained("voicestudio/t2a-lora-base")

# Fine-tune on your data
trainer = Trainer(
    model=generator,
    train_dataset=your_dataset,
    output_dir="./checkpoints"
)

trainer.train()
```

---

## ğŸ“Š Supported Models

VoiceStudio works with various TTS architectures:

| Model | Status | Notes |
|-------|--------|-------|
| VITS | âœ… Supported | Fully tested |
| FastSpeech2 | âœ… Supported | Fully tested |
| Tacotron2 | âœ… Supported | Requires adapter |
| VALL-E | ğŸ”„ Experimental | Work in progress |
| Bark | ğŸ”„ Experimental | Coming soon |
| YourTTS | âœ… Supported | Community contributed |

**Add your own model**: See our [Integration Guide](docs/integration.md)

---

```bibtex
@inproceedings{voicestudio2027lam,
  title={T2A-LoRA2: Text-Guided Voice Editing with Language-Audio Models},
  author={Your Name},
  booktitle={ICML},
  year={2027}
}
```

---

---

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Areas we need help with:**
- ğŸ”§ Additional TTS model adapters
- ğŸ“š Documentation improvements
- ğŸ› Bug fixes and testing
- ğŸŒ Multi-language support
- ğŸ¨ New voice editing techniques

---

## ğŸ“ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **CLAP**: Microsoft & LAION-AI for CLAP model
- **LoRA**: Microsoft for LoRA technique
- **HuggingFace**: For transformers library and model hub
- **LatentForge Team**: For research support and infrastructure

---

## ğŸŒŸ Citation

If you use VoiceStudio in your research, please cite:

```bibtex
@software{voicestudio2026,
  title={VoiceStudio: A Unified Toolkit for Voice Style Adaptation},
  author={Your Name},
  year={2026},
  url={https://github.com/LatentForge/voicestudio}
}
```

---

<div align="center">

**Made with â¤ï¸ by the LatentForge Team**

[â­ Star us on GitHub](https://github.com/LatentForge/voicestudio) | [ğŸ“– Read the Docs](https://latentforge.github.io/VoiceStudio) | [ğŸ¤— HuggingFace](https://huggingface.co/LatentForge)

</div>
