#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Sep  5 21:19:59 2025

@author: DIYer22
"""
import mxlm
import mximport
from copy import deepcopy

with mximport.inpkg():
    from ..token_level_supervision_utils import unicode_tokenizer

correcting_sft_system_prompt_cn = """- å…ˆå‰çš„ system prompt åªåšè¯„ä¼°ç”¨ï¼Œä¸å¿…å†éµå®ˆ
- ä½ æœ¬ä½“æ˜¯ä¸€ä¸ª GPT æ¶æ„çš„ LLM, ä½ ç°åœ¨çš„è§’è‰²åˆ‡æ¢ä¸ºäº† token-level correcting model
- ç›®æ ‡æ˜¯é€šè¿‡ä¿®æ”¹ä¸æ°å½“çš„ token æ¥ä¼˜åŒ–å·²æœ‰çš„å›ç­”
- ä½ çš„ä»»åŠ¡æ˜¯ï¼š
    1. å®šä½ä¸Šè¿°å›ç­”ä¸­ï¼Œç¬¬ä¸€ä¸ªä¸æ°å½“çš„ tokenï¼Œå³æŒ‡å‡º â€œä¿®æ”¹ä½ç½®â€
    2. å°†â€œä¸æ°å½“ tokenâ€ä¿®æ”¹ä¸ºæ›´åŠ æ°å½“çš„ tokenï¼Œä½¿å¾—åŸºäº â€œæ°å½“ tokenâ€ ç»§ç»­åšè¡¥å…¨èƒ½è·å¾—æœ€å¥½ã€æœ€å‡†ç¡®çš„ç­”å¤
- Correcting èŒƒå›´ï¼šå¤šè½®çš„æƒ…å†µä¸‹ï¼Œåªå®šä½å’Œä¿®æ”¹ä¸Šä¸€è½®ï¼ˆå³æœ€æ–°è½®ï¼‰çš„ç­”å¤ä¸­é¦–ä¸ªâ€œä¸æ°å½“ tokenâ€
- ç”±äºä½ ä½œä¸º LLM åªä¼šè¾“å‡ºæ–‡æœ¬ï¼Œæˆ‘ä»¬æŒ‰ç…§è¿™ä¸ªæ–‡æœ¬æ ¼å¼æ¥è¾“å‡ºä½ çš„ correcting ç­”å¤:
    - `<|split|>{location_tokens}<|split|>{location_index}<|split|>{replacement_token}<|split|>`
    - `<|split|>` æ˜¯åˆ†éš”å†…å®¹çš„ special tokenï¼Œä¸”å›ç­”å¿…é¡»ä»¥ `<|split|>` ä½œä¸ºå¼€å¤´å’Œç»“å°¾
    - `{location_tokens}`: ç”¨æ¥å®šä½ â€œä¿®æ”¹ä½ç½®â€ çš„ä¸€ä¸² tokens
        - å…¶å†…å®¹ä¸ºä»ä¸æ°å½“çš„ token å¼€å§‹ï¼ŒæŒç»­ç”Ÿæˆï¼Œç›´åˆ°è§¦å‘ä»¥ä¸‹ä»»æ„æƒ…å†µï¼š
            1. åœ¨æ‰€æœ‰æ¨¡å‹è¾“å‡ºçš„ tokens ä¸­ (åŒ…æ‹¬æ¨¡å‹çš„å†å²è¾“å‡º) è¢« `{location_tokens}` åŒ¹é…ä¸Šçš„ç¬¬ä¸€å¤„ä½ç½®æ­£å¥½å°±æ˜¯ â€œä¿®æ”¹ä½ç½®â€ 
                - æ­¤æ—¶çš„ `{location_index}` åº”è¯¥ä¸º 0ï¼Œå¹¶åœæ­¢ç”Ÿæˆ
                - è‹¥ç¬¬ä¸€åŒ¹é…å¤„ä¸æ˜¯ â€œä¿®æ”¹ä½ç½®â€ï¼Œåˆ™ç»§ç»­ç”Ÿæˆä¸‹ä¸€ä¸ª token æ¥åšæ›´åŠ ç²¾å‡†çš„å®šä½
            2. `{location_tokens}` é•¿åº¦è¾¾åˆ° 20 ä¸ª tokenï¼Œå°±è¯¥åœæ­¢ç”Ÿæˆäº†
                - ä½†æ˜¯ï¼Œè‹¥æœ€åçš„å‡ ä¸ª token ä¸èƒ½è¢«ä½ è‡ªå·± (correcting model) çš„ tokenizer decode ä¸ºå®Œæ•´å­—ç¬¦ï¼Œéœ€è¦çªç ´ 20 tokens é™åˆ¶ç”Ÿæˆåˆ°èƒ½ decode å‡ºå®Œæ•´å­—ç¬¦ä¸ºæ­¢
                - è‹¥ 20 ä¸ª token éƒ½æ²¡æ³•æŠŠ â€œä¿®æ”¹ä½ç½®â€ å‡†ç¡®å®šä½ï¼Œé‚£å°±éœ€è¦é…åˆ `{location_index}` æ¥ä¸€èµ·å®šä½äº†
            3. ä¸€è½®ç»“æŸäº†ï¼Œå³å·²ç»ç”Ÿæˆäº† stop token: `<|stop|>`ï¼Œä¹Ÿåº”è¯¥åœæ­¢ç”Ÿæˆ
    - `{location_index}` è¡¨ç¤ºåœ¨æ‰€æœ‰æ¨¡å‹è¾“å‡ºçš„ tokens ä¸­, èƒ½è¢« `{location_tokens}` åŒ¹é…ä¸Šçš„æ‰€æœ‰ä½ç½®ä¸­çš„ç¬¬å‡ ä¸ªä½ç½®
        - æ˜¯ä¸€ä¸ª int æ•°å€¼ï¼Œä» 0 å¼€å§‹è®¡æ•°ï¼Œæ”¯æŒè´Ÿæ•°ï¼Œå’Œ Python list çš„ index ä¸€è‡´
        - å½“ç”¨è´Ÿæ•°è¡¨ç¤º index æ—¶çš„ç»å¯¹å€¼æ¯”æ­£æ•° index æ›´åŠ å°çš„æ—¶å€™ï¼Œ`{location_index}` å°±ç”¨è´Ÿæ•°è¡¨ç¤º
        - `{location_tokens}` å’Œ `{location_index}` é…åˆåï¼Œèƒ½åœ¨æ‰€æœ‰ç­”å¤ä¸­å…±åŒå®šä½ä¸€ä¸ªå”¯ä¸€çš„ä½ç½®ï¼Œå³ â€œç¬¬ä¸€ä¸ªä¸æ°å½“ tokenâ€ çš„ä½ç½®
    - `{replacement_token}`: æ›´åŠ æ°å½“çš„ tokenï¼ŒæœŸæœ›æ”¹ä¸ºæ°å½“ token åï¼Œç»§ç»­åšè¡¥å…¨èƒ½è·å¾—æœ€å¥½ã€æœ€å‡†ç¡®çš„ç­”å¤ã€‚åªéœ€è¦ä¸€ä¸ª token å³å¯
    -  stop token: ä¸Šé¢çš„æ¯ä¸€è½®ç­”å¤æœ€åéƒ½æœ‰ stop tokenï¼Œéœ€è¦çš„è¯ï¼Œåœ¨ `{location_tokens}`,`{replacement_token}` ä¸­ä½¿ç”¨ special token `<|stop|>` æ¥è¡¨ç¤º stop token
        - æ¯”å¦‚, è¦ç»­å†™æœ€åä¸€è½®çš„ç­”å¤ `<|split|><|stop|><|split|>-1<|split|>{continue token}<|split|>`
    - tokenizer é—®é¢˜ï¼š
        - ä½ éœ€è¦é€šè¿‡å¤šè¾“å‡º token æˆ–æå‰è¾“å‡º token æ¥é¿å…æ½œåœ¨çš„ tokenizer decode å‡ºä¸åˆè§„æ–‡æœ¬çš„é—®é¢˜ã€‚
        - å³å¤šä¸ª tokens å¯¹åº”ä¸€ä¸ªæ–‡æœ¬å­—ç¬¦çš„æƒ…å†µä¸‹ï¼Œè¦æŠŠå¤šä¸ª token è§†ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œä½¿æ‰€æœ‰è¾“å‡ºçš„ tokens èƒ½å’Œæ–‡æœ¬äº’ç›¸è½¬æ¢ï¼Œè€Œä¸è¦æˆªæ–­ä¸­é—´ token
    - å¦‚æœ Correcting èŒƒå›´å†…çš„å›ç­”éƒ½æ²¡æœ‰é—®é¢˜ï¼Œè¾“å‡º `<|split|><|split|>`

## example 1:
USER:
åˆ—ä¸¾ 3 ç§æ°´æœï¼š
ASSISTANT:
è‹¹æœã€åœŸè±†ã€é¦™è•‰
æœŸæœ›çš„è¾“å‡º: â€œ<|split|>åœŸè±†<|split|>0<|split|>è¥¿ç“œ<|split|>â€

## example 2:
USER:
Just reply 2 times, Using "|" as a separator:
1;2;3;4;5;6;7;8;9;8;
ASSISTANT:
1;2;3;4;5;6;7;8;9;8;|1;2;3;4;5;6;7;8;9;8;
USER:
Reply again
ASSISTANT:
1;2;3;4;5;6;7;8;9;8;|1;2;3;4;5;6;7;8;9;8;|1;2;3;4;5;6;7;8;9;8;

æœŸæœ›çš„è¾“å‡º: â€œ<|split|>|1;2;3;4;5;6;7;8;9;8<|split|>-1<|split|><|stop|><|split|>â€
- â€œç¬¬ä¸€ä¸ªä¸æ°å½“ tokenâ€å¤„å’Œå…¶ä»– ASSISTANT çš„å›ç­”æœ‰é‡å¤ï¼Œæ‰€ä»¥ä¼šç”Ÿæˆå®Œæ•´ä¸ª 20 ä¸ª `{location_tokens}`
- `{location_index}` ç”¨æ­£æ•°è¡¨ç¤ºæ—¶ä¸º 2ï¼Œ ç”¨è´Ÿæ•°ä¸º -1ï¼Œå…¶ä¸­ï¼Œ -1 ç»å¯¹å€¼æ›´åŠ å°ï¼Œæ‰€ä»¥åº”è¯¥ç”¨ -1
- æ­¤å¤„ `{replacement_token}` ä¸º stop token"""

correcting_sft_system_prompt_default = correcting_sft_system_prompt_cn


def next_decodable_num(tokens, current_num, tokenizer):
    """
    ä» tokens çš„ current_num ä½ç½®å¼€å§‹ï¼Œæ‰¾åˆ°ä¸‹ä¸€ä¸ªèƒ½è¢« tokenizer decode å‡ºå®Œæ•´å­—ç¬¦çš„ idx
    """
    for num in range(current_num + 1, len(tokens) + 1):  # number of tokens
        try:
            decoded_text = tokenizer.decode(tokens[0:num])
            if (
                tokenizer.encode(decoded_text, add_special_tokens=False)
                == tokens[0:num]
            ):
                return dict(next_num=num, decoded_text=decoded_text)
        except Exception:
            continue
    raise ValueError(
        "æ— æ³•æ‰¾åˆ°ä¸‹ä¸€ä¸ªå¯è§£ç çš„ä½ç½®",
        getattr(tokenizer, "name_or_path", "unknow_tokenizer"),
        tokens,
    )


class NextTokenPredictionAsCorrectingBuilder:
    def __init__(
        self,
        tokenizer=None,
        SPLIT_TOKEN="<|split|>",  # for qwen 2.5
        STOP_TOKEN="<|stop|>",
        max_location_tokens=20,
        scope_slice=(-1, None),  # TODO: slice of which messages can be correcting
    ):
        self.tokenizer = tokenizer or unicode_tokenizer
        self.SPLIT_TOKEN = SPLIT_TOKEN
        self.STOP_TOKEN = STOP_TOKEN
        self.max_location_tokens = max_location_tokens
        self.scope_slice = scope_slice

    def get_correcting_sft_system_prompt(self, language="cn"):
        if language == "cn":
            prompt = correcting_sft_system_prompt_cn
        else:
            prompt = correcting_sft_system_prompt_default
        return (
            prompt.replace("<|split|>", self.SPLIT_TOKEN)
            .replace("<|stop|>", self.STOP_TOKEN)
            .replace(" 20 ", f" {self.max_location_tokens} ")
        )

    def convert_token_level_to_unicode_location(self, rejected_msgs):
        """
        æ ¹æ® rejected_msgs ä¸­çš„ token_level ä¿¡æ¯è¿”å› unicode_location

        Args:
            rejected_msgs: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            dict: {"message_index": int, "unicode_index": int}
        """
        # æŸ¥æ‰¾é¦–ä¸ªæœ‰ token_level çš„ assistant æ¶ˆæ¯
        for i, msg in enumerate(rejected_msgs):
            if msg["role"] == "assistant" and "token_level" in msg:
                token_level = msg["token_level"]
                unicode_location = token_level["rejected_text_unicode_range"][0]
                return {"message_index": i, "unicode_index": unicode_location}
        return {"not_found": True}

    def parser_ntp_as_correcting_text(self, ntp_as_correcting_text):
        mid_text = ntp_as_correcting_text.removeprefix(self.SPLIT_TOKEN).removesuffix(
            self.SPLIT_TOKEN
        )
        if mid_text:  # correcting
            splits = mid_text.split(self.SPLIT_TOKEN)
            # TODO: How to handle exception?
            assert len(splits) == 3, splits
            ntp_as_correcting = dict(
                zip(["location_text", "location_index", "replacement_token"], splits)
            )
            ntp_as_correcting["location_index"] = int(
                ntp_as_correcting["location_index"]
            )
        else:  # is_good
            ntp_as_correcting = dict(is_good=True, location_text="")
        return ntp_as_correcting

    def get_unicode_location(self, msgs, ntp_as_location=None):
        """
        æ ¹æ® ntp_as_location å®šä½ unicode_location
        å¦‚æœ ntp_as_location is None, åˆ™ä» msgs å¿…é¡»æ˜¯ correcting_sft, ä¼šä»æœ€åä¸€æ¡æ¶ˆæ¯ä¸­è§£æå‡º ntp_as_location
        """
        if ntp_as_location is None:
            sys_msg, correcting_msg = msgs[-2:]
            msgs = msgs[:-2]
            # ntp_as_correcting_gt = correcting_msg.get('correcting')
            ntp_as_correcting_text = mxlm.get_text_content(correcting_msg)
            ntp_as_location = self.parser_ntp_as_correcting_text(ntp_as_correcting_text)
            if ntp_as_location.get("is_good"):
                return dict(not_found=True, is_good=True)
        unicode_location = self._get_unicode_location(msgs, ntp_as_location)
        return unicode_location

    def _get_unicode_location(self, msgs, ntp_as_location):
        """
        Compute unicode_location by ntp_as_location in messages without token_level_info
        if Not found:
            return dict(not_found=True)

        ç”¨ for loop éå†æ‰€æœ‰ assistant æ¶ˆæ¯ï¼Œæ‰¾åˆ°æ‰€æœ‰èƒ½åŒ¹é…ä¸Š location_text çš„ä½ç½®
        å¦‚æœèƒ½æ‰¾åˆ°ï¼Œ è¿”å› location_index å¯¹åº”çš„ä½ç½®çš„ unicode_location
        å¦åˆ™è¿”å› not_found=True
        """
        unicode_sequence_dic = self.messages_to_assistant_unicode_sequence(msgs)
        assistant_sequence = unicode_sequence_dic["assistant_sequence"]
        location_index = ntp_as_location["location_index"]
        location_text = ntp_as_location.get("location_text", "")
        assert location_text, ntp_as_location
        unicode_locations = []
        for message_index, assistant_content in zip(
            unicode_sequence_dic["assistant_indices"],
            assistant_sequence.split(self.STOP_TOKEN),
        ):

            assistant_content += self.STOP_TOKEN
            start = 0
            while True:
                index = assistant_content.find(location_text, start)
                if index == -1:
                    break
                unicode_location = dict(
                    message_index=message_index, unicode_index=index
                )
                unicode_locations.append(unicode_location)
                start = index + 1
        match_num = len(unicode_locations)

        if match_num and -match_num <= location_index and location_index < match_num:
            unicode_location = unicode_locations[location_index]
            unicode_location["match_num"] = match_num
            return unicode_location
        else:
            return dict(not_found=True, match_num=match_num)

    def messages_to_assistant_unicode_sequence(self, msgs, unicode_location=None):
        """
        Convert messages to a single text sequence, if unicode_location is given,
        also compute the sequence_index in the combined text sequence.

        Returns:
            update to unicode_location dict: {"assistant_sequence": str, "sequence_index": int (if unicode_location is given)}
        """

        # æ”¶é›†æ‰€æœ‰assistantæ¶ˆæ¯çš„å†…å®¹ï¼Œå¹¶è®°å½•å…¶åœ¨åŸå§‹æ¶ˆæ¯ä¸­çš„ç´¢å¼•
        assistant_contents = []
        assistant_indices = []
        for i, msg in enumerate(msgs):
            if msg["role"] == "assistant":
                content = mxlm.get_text_content(msg["content"])
                # æ·»åŠ éšè—çš„ STOP_TOKEN
                content += self.STOP_TOKEN
                # content += "\n\n-----\n\n" ä¼šå¯¼è‡´æ½œåœ¨çš„ tokenizer ç²˜è¿é—®é¢˜
                assistant_contents.append(content)
                assistant_indices.append(i)

        assistant_sequence = "".join(assistant_contents)
        if unicode_location is None:
            unicode_location = {}
        else:
            message_index = unicode_location["message_index"]
            target_unicode_index = unicode_location["unicode_index"]
            # è®¡ç®—ç›®æ ‡ä½ç½®çš„unicodeä½ç½®
            # æ‰¾åˆ°ç›®æ ‡æ¶ˆæ¯åœ¨assistantæ¶ˆæ¯åˆ—è¡¨ä¸­çš„ç´¢å¼•
            try:
                assistant_msg_idx = assistant_indices.index(message_index)
            except ValueError:
                raise ValueError(f"æ¶ˆæ¯ç´¢å¼• {message_index} ä¸æ˜¯ assistant æ¶ˆæ¯")

            current_index = 0
            for i in range(assistant_msg_idx):
                current_index += len(assistant_contents[i])
            sequence_index = current_index + target_unicode_index
            # unicode_location = deepcopy(unicode_location)
            unicode_location["sequence_index"] = sequence_index
        unicode_location["assistant_sequence"] = assistant_sequence
        unicode_location["assistant_indices"] = assistant_indices
        # print(unicode_location)
        return unicode_location

    def set_location_index(self, rejected_msgs, ntp_as_location, unicode_location):
        """
        åœ¨æ‰€æœ‰æ¨¡å‹è¾“å‡ºçš„ tokens ä¸­æŸ¥æ‰¾ ntp_as_location.location_text çš„æ‰€æœ‰åŒ¹é…ä½ç½®ï¼Œ
        è¿”å›å¯¹åº”çš„ç´¢å¼•ä½ç½® ntp_as_location.location_index

        Args:
            rejected_msgs: æ¶ˆæ¯åˆ—è¡¨
            ntp_as_location: dict(location_text=...) or è¦æŸ¥æ‰¾çš„å­—ç¬¦ä¸²
            unicode_location: dict, åŒ…å« message_index å’Œ unicode_index, ä¹Ÿå¯ä»¥åŒ…å« assistant_sequence å’Œ sequence_index

        Returns ntp_as_location:
            int: location_indexï¼Œä»0å¼€å§‹è®¡æ•°ï¼Œè´Ÿæ•°è¡¨ç¤ºä»æœ«å°¾å€’æ•°
        """
        if isinstance(ntp_as_location, str):
            ntp_as_location = dict(location_text=ntp_as_location)
        ntp_as_location = deepcopy(ntp_as_location)
        location_text = ntp_as_location["location_text"]
        if "assistant_sequence" not in unicode_location:
            unicode_location = self.messages_to_assistant_unicode_sequence(
                rejected_msgs, unicode_location
            )
        assistant_sequence = unicode_location["assistant_sequence"]
        sequence_index = unicode_location["sequence_index"]

        # åœ¨æ‰€æœ‰assistantå†…å®¹ä¸­æŸ¥æ‰¾location_textçš„æ‰€æœ‰åŒ¹é…ä½ç½®
        matches = []
        start = 0
        while True:
            index = assistant_sequence.find(location_text, start)
            if index == -1:
                break
            matches.append(index)
            start = index + 1

        location_index = None
        # æ‰¾åˆ°ç›®æ ‡ä½ç½®å¯¹åº”çš„åŒ¹é…ç´¢å¼•
        for idx, match_index in enumerate(matches):
            if match_index == sequence_index:
                # å¦‚æœè´Ÿæ•°çš„ç»å¯¹å€¼æ›´å°ï¼Œä½¿ç”¨è´Ÿæ•°è¡¨ç¤º
                negative_idx = idx - len(matches)
                if abs(negative_idx) < idx:
                    location_index = negative_idx
                else:
                    location_index = idx

        ntp_as_location.update(
            unicode_location=unicode_location, match_num=len(matches)
        )
        ntp_as_location["location_index"] = location_index
        if not len(matches):
            ntp_as_location["not_found"] = True
        return ntp_as_location

    def convert_rejected_content_to_ntp_as_location(self, rejected_msgs):
        """
        å°† rejected_msgs å’Œ token_level_info è½¬æ¢ä¸º Next Token Prediction as location æ ¼å¼

        - è·å¾— correcting ä½ç½®çš„ unicode_location
        - ä» unicode_location å¤„å– suffix å† decode
        - å¾ªç¯ next valid decodable ç›´åˆ° location_index ä¸º 0ï¼Œæˆ–è€… token è¶…é•¿
        - ç”Ÿæˆå¹¶è¿”å› location_text å’Œ location_index

        Args:
            rejected_msgs: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            dict: {"location_text": str, "location_index": int}
        """
        # è·å– unicode_location
        unicode_location = self.convert_token_level_to_unicode_location(rejected_msgs)
        message_index = unicode_location["message_index"]
        unicode_index = unicode_location["unicode_index"]

        content = mxlm.get_text_content(rejected_msgs[message_index]["content"])
        content_suffix = content[unicode_index:] + self.STOP_TOKEN
        suffix_tokens = self.tokenizer.encode(content_suffix, add_special_tokens=False)
        decodable_num = 0

        while True:
            decodable_res = next_decodable_num(
                suffix_tokens, decodable_num, self.tokenizer
            )
            decodable_num = decodable_res["next_num"]
            location_text = decodable_res["decoded_text"]
            ntp_as_location = self.set_location_index(
                rejected_msgs,
                location_text,
                unicode_location,
            )
            if ntp_as_location.get("not_found"):
                raise ValueError("æ— æ³•å®šä½åˆ° location_text", ntp_as_location)
            location_index = ntp_as_location.get("location_index", None)
            if location_index == 0:
                break
            if decodable_num >= len(suffix_tokens):
                break
            if decodable_num >= self.max_location_tokens:
                break

        ntp_as_location["location_tokens"] = suffix_tokens[:decodable_num]

        if "asset_location_consistency":
            unicode_location2 = self.get_unicode_location(
                rejected_msgs, ntp_as_location
            )
            assert (
                unicode_location["message_index"] == unicode_location2["message_index"]
                and unicode_location["unicode_index"]
                == unicode_location2["unicode_index"]
            ), (
                "asset_location_consistency: "
                + str(unicode_location)
                + str(unicode_location2)
                + str(ntp_as_location)
            )
        return ntp_as_location

    def build_correcting_sft_by_token_level_SFT(
        self, msgs, is_good=None
    ):  # must be is_good SFT msgs or token_level_SFT msgs
        unicode_location = self.convert_token_level_to_unicode_location(msgs)

        sys_prompt_message = dict(
            role="system",
            content=self.get_correcting_sft_system_prompt(),
        )
        # double check
        if is_good is not None:
            assert bool(is_good) == bool(
                unicode_location.get("not_found")
            ), f"is_good must consistent with token_level_info, is_good: {is_good} != unicode_location: {unicode_location}"

        [msg.update(ignore_loss=True) for msg in msgs if msg["role"] == "assistant"]
        if unicode_location.get(
            "not_found"
        ):  # æ²¡æœ‰ token_level ä¿¡æ¯, å±äº is_good çš„ SFT
            is_good_correcting_msg = dict(
                role="assistant",
                content=self.SPLIT_TOKEN * 2,
                correcting=dict(is_good=True, scope_slice=self.scope_slice),
            )
            correcting_sft = msgs + [sys_prompt_message, is_good_correcting_msg]

            return correcting_sft
        else:  # æœ‰ token_level ä¿¡æ¯, å±äº not is_good çš„ token-level SFT
            token_level_msg = msgs[-1]
            token_level_info = token_level_msg["token_level"]
            rejected_content_chunks = token_level_info.pop("rejected_content")
            token_level_info["chosen_content"] = token_level_msg["content"]

            rejected_content_str = mxlm.get_text_content(rejected_content_chunks)
            rejected_msg = dict(
                role="assistant",
                ignore_loss=True,
                content=rejected_content_str,
                finish_reason=token_level_info.get("rejected_finish_reason", ""),
                token_level=token_level_info,
            )
            rejected_msgs = msgs[:-1] + [rejected_msg]

            ntp_as_location = self.convert_rejected_content_to_ntp_as_location(
                rejected_msgs,
            )
            ntp_as_correcting = deepcopy(ntp_as_location)
            ntp_as_correcting.pop("unicode_location", None)
            replacement_text = (
                token_level_info["chosen_text"] or self.STOP_TOKEN
            )  # if chosen_text is empty mean chosen stop token
            ntp_as_correcting.update(
                replacement_text=replacement_text,
                is_good=False,
                scope_slice=self.scope_slice,
            )

            correcting_content = f"{self.SPLIT_TOKEN}{ntp_as_correcting['location_text']}{self.SPLIT_TOKEN}{ntp_as_correcting['location_index']}{self.SPLIT_TOKEN}{ntp_as_correcting['replacement_text']}{self.SPLIT_TOKEN}"
            correcting_msg = dict(
                role="assistant",
                content=correcting_content,
                correcting=ntp_as_correcting,
            )
            correcting_sft = rejected_msgs + [
                sys_prompt_message,
                correcting_msg,
            ]
        # import boxx.g
        return correcting_sft

    def apply_ntp_as_correcting(self, msgs, ntp_as_correcting: str | dict):
        if isinstance(ntp_as_correcting, str):
            ntp_as_correcting = self.parser_ntp_as_correcting_text(ntp_as_correcting)
        if ntp_as_correcting.get("is_good"):
            return dict(
                ntp_as_correcting=ntp_as_correcting,
            )
        unicode_location = self.get_unicode_location(msgs, ntp_as_correcting)
        if unicode_location.get("not_found"):
            return dict(
                ntp_as_correcting=ntp_as_correcting, unicode_location=unicode_location
            )
        else:
            msg_idx = unicode_location["message_index"]
            partial_msg = deepcopy(msgs[msg_idx])
            if isinstance(partial_msg["content"], list):
                assert all(
                    [d["type"] == "text" for d in partial_msg["content"]]
                ), partial_msg
                partial_msg["content"] = mxlm.get_text_content(partial_msg["content"])
            good_prefix = partial_msg["content"][: unicode_location["unicode_index"]]
            if self.STOP_TOKEN == ntp_as_correcting["replacement_token"]:
                # no need continue final message
                partial_msg["content"] = good_prefix
                partial_msg["finish_reason"] = "stop"
            else:
                partial_msg["content"] = (
                    good_prefix + ntp_as_correcting["replacement_token"]
                )
                if "finish_reason" in partial_msg:
                    del partial_msg["finish_reason"]
            partial_messages = msgs[:msg_idx] + [partial_msg]
            correction = dict(
                ntp_as_correcting=ntp_as_correcting,
                unicode_location=unicode_location,
                partial_messages=partial_messages,
            )
            return correction


if __name__ == "__main__":
    from boxx import *

    with mximport.inpkg():
        from ..test_utils import build_test_tokenizer, get_test_rejected_msgs1
        from ..parser import build_test_panda_tree

    panda_json_dir = "../../../on-panda-example-data/panda_json"
    tokenizer = build_test_tokenizer()
    # build_argkws = dict(tokenizer=unicode_tokenizer)
    build_argkws = dict(
        tokenizer=tokenizer,
        SPLIT_TOKEN="<|fim_pad|>",  # for qwen 2.5
        STOP_TOKEN="<|fim_suffix|>",
    )
    builder = NextTokenPredictionAsCorrectingBuilder(**build_argkws)

    # test next_decodable_num
    complex_emoji_text = "ğŸ§ğŸ¿â€â™‚ï¸â€â¡ï¸"
    decodable = next_decodable_num(tokenizer.encode(complex_emoji_text), 0, tokenizer)
    assert decodable["next_num"] != 1, decodable

    # test sample case
    rejected_msgs1, ntp_as_correcting_text_gt1 = get_test_rejected_msgs1()[:2]

    result1 = builder.convert_rejected_content_to_ntp_as_location(rejected_msgs1)
    assert result1["location_text"] == " potato", result1
    assert result1["location_index"] == 0, result1

    correction1 = builder.apply_ntp_as_correcting(
        rejected_msgs1, ntp_as_correcting_text_gt1
    )
    assert correction1["partial_messages"][-1]["content"] == "Apple, orange"
    assert (
        "finish_reason" not in correction1["partial_messages"][-1]
    ), "Should continue_final_message (no finish_reason)"

    # test correcting_sft extreme cases: chosen stop
    test_json = (
        f"{panda_json_dir}/2025-09-10_correcting_sft_tokenizer-Qwen2.5.panda.json"
    )
    panda_tree = build_test_panda_tree(test_json)
    correcting_sft2 = panda_tree.build_correcting_sft_data_v1(builder)[-1]
    correcting_content2 = correcting_sft2[-1]["content"]
    ntp_as_correcting_text_gt2 = "<|fim_pad|>|1;2;3;4;5;6;7;8;9;8<|fim_pad|>-1<|fim_pad|><|fim_suffix|><|fim_pad|>"
    assert correcting_content2 == ntp_as_correcting_text_gt2, correcting_content2
    correction2 = builder.apply_ntp_as_correcting(
        correcting_sft2[:-2], ntp_as_correcting_text_gt2
    )
    assert correction2["partial_messages"][-1]["finish_reason"] == "stop"

    # test correcting_sft extreme cases: chosen continue
    test_json3 = f"{panda_json_dir}/2025-09-11_correcting_sft_continue_tokenizer-Qwen2.5.panda.json"
    panda_tree3 = build_test_panda_tree(test_json3)
    correcting_sft3 = panda_tree3.build_correcting_sft_data_v1(builder)[-1]
    correcting_content3 = correcting_sft3[-1]["content"]
    assert (
        correcting_content3
        == "<|fim_pad|><|fim_suffix|><|fim_pad|>1<|fim_pad|>|<|fim_pad|>"
    ), correcting_content3

    # test single_char_repeat case: chosen stop
    test_json4 = (
        f"{panda_json_dir}/2025-09-12_single_char_repeat_tokenizer-Qwen2.5.panda.json"
    )
    panda_tree4 = build_test_panda_tree(test_json4)
    correcting_sft4 = panda_tree4.build_correcting_sft_data_v1(builder)[-1]
