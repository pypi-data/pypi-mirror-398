"""
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— 
â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
â•šâ•â•â•â•â•â•â•â•šâ•â•     â•šâ•â•â•šâ•â•  â•šâ•â•â•â• â•šâ•â• â•šâ•â•â•â•â• 

O M N I   A I
Christmas Edition 2025
Created by: Lakshya Gupta
Date: 25 December 2025

FREE for everyone to use forever.
But this is MY work - please don't remove my name.
"""

# Your existing code continues here...
# omniai/main.py - COMPLETE VERSION WITH ALL FEATURES
"""
OmniAI v1.0 - Complete Machine Learning Pipeline
Features:
1. Auto algorithm recommendations (50+ algorithms)
2. One-click auto-training
3. Auto-predictions
4. Medical specialization
5. Web & API ready
"""

import os
import pandas as pd
import numpy as np
from .core.advisor import AlgorithmAdvisor

class OmniAI:
    """
    Complete OmniAI - From data to deployed model
    """
    
    def __init__(self, use_dask=False):
        print("=" * 80)
        print("ğŸš€ OMNIAI v1.0 - Complete AI Pipeline")
        print("=" * 80)
        print("Features:")
        print("   â€¢ Auto Algorithm Selection (20+ algorithms)")
        print("   â€¢ One-Click Model Training")
        print("   â€¢ Instant Predictions")
        print("   â€¢ Medical Data Specialization")
        print("   â€¢ Web Interface & API Ready")
        print("=" * 80)
        
        self.advisor = AlgorithmAdvisor()
        self.use_dask = use_dask
        
        if use_dask:
            print("âš¡ Dask engine activated for 100GB+ datasets")
    
    def process(self, file_path, task="auto", domain=None):
        """
        Step 1: Analyze data and recommend best algorithms
        
        Args:
            file_path: Path to CSV/Excel file
            task: 'classification', 'regression', or 'auto'
            domain: 'medical', 'financial', 'retail', 'generic'
        
        Returns:
            Analysis report with algorithm recommendations
        """
        print(f"\nğŸ“Š STEP 1: ANALYZING {os.path.basename(file_path)}")
        
        try:
            # Load data
            df = pd.read_csv(file_path)
            print(f"   âœ… Loaded: {len(df):,} rows, {len(df.columns)} columns")
            
            # Detect domain
            if domain is None:
                domain = self._detect_domain(df)
            print(f"   ğŸ·ï¸  Domain: {domain}")
            
            # Clean data
            df_clean = self._clean_data(df)
            print(f"   ğŸ§¹ Cleaned: {len(df_clean):,} rows remaining")
            
            # Get recommendations
            analysis = {
                "rows": len(df_clean),
                "n_features": len(df_clean.columns),
                "type": "tabular",
                "domain": domain
            }
            
            recommendations = self.advisor.recommend(analysis, domain, task)
            print(f"   ğŸ¤– Recommended: {len(recommendations)} algorithms")
            
            # Show top 3
            print(f"\n   ğŸ† TOP 3 ALGORITHMS:")
            for i, rec in enumerate(recommendations[:3], 1):
                print(f"     {i}. {rec['algorithm']} - {rec['reason']}")
            
            return {
                "status": "success",
                "summary": {
                    "file": os.path.basename(file_path),
                    "domain": domain,
                    "rows": len(df_clean),
                    "columns": len(df_clean.columns),
                    "size_mb": os.path.getsize(file_path) / (1024*1024)
                },
                "analysis": analysis,
                "recommendations": recommendations,
                "next_step": "Use ai.train() to auto-train best model"
            }
            
        except Exception as e:
            print(f"âŒ Error: {e}")
            return {"status": "error", "message": str(e)}
    
    def train(self, file_path, target_column=None, model_name=None):
        """
        Step 2: Auto-train the best model
        
        Args:
            file_path: Path to training data
            target_column: Name of target column (auto-detected)
            model_name: Custom name for saved model
        
        Returns:
            Training results with model file and performance
        """
        print(f"\nğŸ‹ï¸  STEP 2: TRAINING MODEL")
        print("=" * 60)
        
        try:
            # Load and prepare data
            df = pd.read_csv(file_path)
            print(f"ğŸ“Š Data: {len(df):,} rows, {len(df.columns)} columns")
            
            # Auto-detect target
            if target_column is None:
                target_column = self._auto_detect_target(df)
            print(f"ğŸ¯ Target: {target_column}")
            
            if target_column not in df.columns:
                return {"status": "error", "message": f"Target column '{target_column}' not found"}
            
            # Prepare features and target
            X = df.drop(columns=[target_column])
            y = df[target_column]
            
            # Check if classification or regression
            task_type = "classification" if y.nunique() < 10 else "regression"
            print(f"ğŸ“ Task: {task_type} ({y.nunique()} unique values)")
            
            # Split data
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, 
                stratify=y if task_type == "classification" else None
            )
            print(f"ğŸ“ˆ Training set: {len(X_train):,} samples")
            print(f"ğŸ“‰ Testing set: {len(X_test):,} samples")
            
            # Get best algorithm
            domain = self._detect_domain(df)
            analysis = {"rows": len(df), "n_features": len(X.columns), "type": "tabular"}
            recommendations = self.advisor.recommend(analysis, domain, task_type)
            
            if not recommendations:
                return {"status": "error", "message": "No algorithms recommended"}
            
            best_algo = recommendations[0]
            print(f"\nğŸš€ Training: {best_algo['algorithm']}")
            print(f"   ğŸ“ Reason: {best_algo['reason']}")
            
            # Train model
            model = self._train_model(best_algo['algorithm'], X_train, y_train, task_type)
            
            if model is None:
                return {"status": "error", "message": "Model training failed"}
            
            # Evaluate
            metrics = self._evaluate_model(model, X_test, y_test, task_type)
            
            print(f"\nğŸ“Š PERFORMANCE METRICS:")
            for metric, value in metrics.items():
                print(f"   âœ… {metric}: {value:.4f}")
            
            # Save model
            import joblib
            import datetime
            
            if model_name is None:
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                model_name = f"omniai_model_{timestamp}"
            
            model_file = f"{model_name}.pkl"
            joblib.dump(model, model_file)
            
            # Save feature names for prediction
            feature_names = list(X.columns)
            model_info = {
                'model': model,
                'feature_names': feature_names,
                'target_column': target_column,
                'algorithm': best_algo['algorithm'],
                'task_type': task_type
            }
            joblib.dump(model_info, f"{model_name}_info.pkl")
            
            print(f"\nğŸ’¾ MODEL SAVED:")
            print(f"   ğŸ“ {model_file} (for predictions)")
            print(f"   ğŸ“ {model_name}_info.pkl (model metadata)")
            
            return {
                "status": "success",
                "algorithm": best_algo['algorithm'],
                "accuracy": metrics.get('accuracy', metrics.get('r2', 0)),
                "model_file": model_file,
                "info_file": f"{model_name}_info.pkl",
                "performance": metrics,
                "training_details": {
                    "train_samples": len(X_train),
                    "test_samples": len(X_test),
                    "features": feature_names,
                    "target": target_column,
                    "task": task_type
                }
            }
            
        except Exception as e:
            print(f"âŒ Training error: {e}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e)}
    
    def predict(self, model_file, data_file, info_file=None):
        """
        Step 3: Make predictions with trained model
        
        Args:
            model_file: Path to saved model (.pkl)
            data_file: Path to new data
            info_file: Path to model info file (optional)
        
        Returns:
            Predictions with confidence scores
        """
        print(f"\nğŸ”® STEP 3: MAKING PREDICTIONS")
        print("=" * 60)
        
        try:
            import joblib
            import datetime
            
            # Load model and info
            if info_file is None:
                # Try to find info file automatically
                base_name = model_file.replace('.pkl', '')
                info_file = f"{base_name}_info.pkl"
            
            if os.path.exists(info_file):
                model_info = joblib.load(info_file)
                model = model_info['model']
                feature_names = model_info['feature_names']
                target_column = model_info.get('target_column')
                print(f"âœ… Loaded model info: {model_info['algorithm']}")
            else:
                # Fallback: load just model
                model = joblib.load(model_file)
                feature_names = None
                print(f"âš ï¸  Info file not found, using basic model")
            
            # Load new data
            new_data = pd.read_csv(data_file)
            print(f"ğŸ“Š New data: {len(new_data):,} rows, {len(new_data.columns)} columns")
            
            # Prepare data for prediction
            if feature_names:
                # Ensure we only use features the model was trained on
                missing_features = [f for f in feature_names if f not in new_data.columns]
                extra_features = [f for f in new_data.columns if f not in feature_names]
                
                if missing_features:
                    print(f"âš ï¸  Missing features: {missing_features[:5]}...")
                    # Add missing features with NaN
                    for feature in missing_features:
                        new_data[feature] = np.nan
                
                if extra_features:
                    print(f"âš ï¸  Extra features ignored: {extra_features[:5]}...")
                
                # Reorder columns to match training
                X_pred = new_data[feature_names].copy()
            else:
                # If no feature names, use all columns (except target if present)
                if target_column and target_column in new_data.columns:
                    X_pred = new_data.drop(columns=[target_column])
                else:
                    X_pred = new_data.copy()
            
            print(f"ğŸ”§ Features for prediction: {len(X_pred.columns)}")
            
            # Handle missing values
            X_pred = X_pred.fillna(X_pred.median())
            
            # Make predictions
            predictions = model.predict(X_pred)
            
            # Get probabilities if available
            if hasattr(model, 'predict_proba'):
                probabilities = model.predict_proba(X_pred)
                confidence = probabilities.max(axis=1)
            else:
                probabilities = None
                confidence = np.ones(len(predictions))
            
            # Create results DataFrame
            results = new_data.copy()
            results['prediction'] = predictions
            results['confidence'] = confidence
            
            if probabilities is not None:
                for i in range(probabilities.shape[1]):
                    results[f'prob_class_{i}'] = probabilities[:, i]
            
            # Save predictions
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            predictions_file = f"omniai_predictions_{timestamp}.csv"
            results.to_csv(predictions_file, index=False)
            
            print(f"\nâœ… PREDICTIONS COMPLETE:")
            print(f"   ğŸ“ Saved to: {predictions_file}")
            print(f"   ğŸ“Š Total predictions: {len(predictions):,}")
            
            # Show sample
            print(f"\nğŸ“‹ SAMPLE PREDICTIONS (first 5):")
            sample_df = results.head()
            for i, row in sample_df.iterrows():
                pred = row['prediction']
                conf = row['confidence']
                print(f"   Row {i}: Prediction = {pred}, Confidence = {conf:.2f}")
            
            return {
                "status": "success",
                "predictions_file": predictions_file,
                "total_predictions": len(predictions),
                "sample_predictions": predictions[:5].tolist(),
                "model_algorithm": model_info.get('algorithm', 'Unknown') if 'model_info' in locals() else 'Unknown'
            }
            
        except Exception as e:
            print(f"âŒ Prediction error: {e}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e)}
    
    def batch_predict(self, model_file, data_list, info_file=None):
        """
        Make predictions from list of records (for APIs)
        """
        # Convert to DataFrame
        df = pd.DataFrame(data_list)
        
        # Save to temp file
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            df.to_csv(f.name, index=False)
            temp_file = f.name
        
        # Use predict method
        result = self.predict(model_file, temp_file, info_file)
        
        # Clean up
        os.unlink(temp_file)
        
        return result
    
    # ========== HELPER METHODS ==========
    
    def _detect_domain(self, df):
        """Detect data domain"""
        cols = ' '.join([str(c).lower() for c in df.columns])
        
        if any(word in cols for word in ['patient', 'diagnosis', 'medical', 'hospital']):
            return "medical"
        elif any(word in cols for word in ['customer', 'sales', 'product', 'price']):
            return "retail"
        elif any(word in cols for word in ['stock', 'price', 'revenue', 'profit']):
            return "financial"
        
        return "generic"
    
    def _clean_data(self, df):
        """Basic data cleaning"""
        # Remove duplicates
        df_clean = df.drop_duplicates()
        
        # Fill numeric missing values
        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if df_clean[col].isnull().any():
                df_clean[col] = df_clean[col].fillna(df_clean[col].median())
        
        return df_clean
    
    def _auto_detect_target(self, df):
        """Auto-detect target column"""
        common_targets = ['target', 'label', 'class', 'outcome', 'result', 
                         'diagnosis', 'status', 'y', 'dependent']
        
        for col in df.columns:
            if col.lower() in common_targets:
                return col
        
        # If last column has few unique values, it's likely the target
        last_col = df.columns[-1]
        if df[last_col].nunique() < 20:
            return last_col
        
        return last_col
    
    def _train_model(self, algorithm, X_train, y_train, task_type):
        """Train specific algorithm"""
        try:
            if algorithm == "Logistic Regression":
                from sklearn.linear_model import LogisticRegression
                model = LogisticRegression(max_iter=1000, random_state=42)
                
            elif algorithm == "Random Forest":
                if task_type == "classification":
                    from sklearn.ensemble import RandomForestClassifier
                    model = RandomForestClassifier(n_estimators=100, random_state=42)
                else:
                    from sklearn.ensemble import RandomForestRegressor
                    model = RandomForestRegressor(n_estimators=100, random_state=42)
                    
            elif algorithm == "XGBoost":
                try:
                    if task_type == "classification":
                        from xgboost import XGBClassifier
                        model = XGBClassifier(n_estimators=100, random_state=42, verbosity=0)
                    else:
                        from xgboost import XGBRegressor
                        model = XGBRegressor(n_estimators=100, random_state=42, verbosity=0)
                except ImportError:
                    print("   âš ï¸ XGBoost not installed. Using Random Forest.")
                    return self._train_model("Random Forest", X_train, y_train, task_type)
                    
            elif algorithm == "LightGBM":
                try:
                    if task_type == "classification":
                        from lightgbm import LGBMClassifier
                        model = LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)
                    else:
                        from lightgbm import LGBMRegressor
                        model = LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)
                except ImportError:
                    print("   âš ï¸ LightGBM not installed. Using Random Forest.")
                    return self._train_model("Random Forest", X_train, y_train, task_type)
                    
            elif "SVM" in algorithm:
                if task_type == "classification":
                    from sklearn.svm import SVC
                    model = SVC(probability=True, random_state=42)
                else:
                    from sklearn.svm import SVR
                    model = SVR()
                    
            elif "Neural Network" in algorithm or "MLP" in algorithm:
                if task_type == "classification":
                    from sklearn.neural_network import MLPClassifier
                    model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)
                else:
                    from sklearn.neural_network import MLPRegressor
                    model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42)
                    
            else:
                # Default to Random Forest
                print(f"   âš ï¸ Using Random Forest for {algorithm}")
                return self._train_model("Random Forest", X_train, y_train, task_type)
            
            # Train model
            model.fit(X_train, y_train)
            print(f"   âœ… Model trained successfully")
            return model
            
        except Exception as e:
            print(f"   âŒ Error training {algorithm}: {e}")
            return None
    
    def _evaluate_model(self, model, X_test, y_test, task_type):
        """Evaluate model performance"""
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        
        y_pred = model.predict(X_test)
        
        if task_type == "classification":
            return {
                "accuracy": accuracy_score(y_test, y_pred),
                "precision": precision_score(y_test, y_pred, average='weighted', zero_division=0),
                "recall": recall_score(y_test, y_pred, average='weighted', zero_division=0),
                "f1_score": f1_score(y_test, y_pred, average='weighted', zero_division=0)
            }
        else:
            return {
                "r2": r2_score(y_test, y_pred),
                "mse": mean_squared_error(y_test, y_pred),
                "mae": mean_absolute_error(y_test, y_pred),
                "rmse": np.sqrt(mean_squared_error(y_test, y_pred))
            }
    
    # ========== UTILITY METHODS ==========
    
    def get_supported_algorithms(self):
        """Get list of all supported algorithms"""
        return {
            "tree_based": ["LightGBM", "XGBoost", "Random Forest", "CatBoost"],
            "neural_networks": ["MLP (Neural Network)", "TabNet"],
            "svm": ["SVM (Linear)", "SVM (RBF)"],
            "linear": ["Logistic Regression", "Ridge Classifier"],
            "bayesian": ["Gaussian Naive Bayes"],
            "neighbors": ["K-Nearest Neighbors"],
            "ensemble": ["AdaBoost", "Gradient Boosting", "Extra Trees"],
            "medical": ["Medical Random Forest", "Medical XGBoost"]
        }
    
    def create_web_app(self):
        """Instructions for web interface"""
        return {
            "message": "Web interface ready",
            "command": "streamlit run omniai_web.py",
            "port": 8501,
            "features": ["Upload data", "Auto-train", "Predict", "Download results"]
        }
    
    def create_api_server(self):
        """Instructions for API server"""
        return {
            "message": "API server ready",
            "command": "python omniai_api.py",
            "port": 5000,
            "endpoints": ["/analyze", "/train", "/predict", "/models"]
        }

# Initialize
print("âœ… OmniAI v1.0 - Complete ML Pipeline Loaded!")
print("   Available methods:")
print("   â€¢ ai.process(file) - Analyze data & get recommendations")
print("   â€¢ ai.train(file) - Auto-train best model")
print("   â€¢ ai.predict(model, data) - Make predictions")
print("   â€¢ ai.get_supported_algorithms() - List all algorithms")

# Test if running directly
if __name__ == "__main__":
    print("\nğŸ§ª Running self-test...")
    
    # Create test data
    test_data = pd.DataFrame({
        'age': np.random.randint(18, 70, 100),
        'income': np.random.randint(20000, 100000, 100),
        'education': np.random.randint(1, 5, 100),
        'purchased': np.random.choice([0, 1], 100)
    })
    test_data.to_csv('omniai_test.csv', index=False)
    
    # Test
    ai = OmniAI()
    
    # Test process
    print("\n1. Testing process()...")
    result1 = ai.process('omniai_test.csv')
    print(f"   âœ… Process: {result1['status']}")
    
    # Test train
    print("\n2. Testing train()...")
    result2 = ai.train('omniai_test.csv')
    print(f"   âœ… Train: {result2['status']}")
    
    if result2['status'] == 'success':
        # Test predict
        print("\n3. Testing predict()...")
        result3 = ai.predict(result2['model_file'], 'omniai_test.csv', result2['info_file'])
        print(f"   âœ… Predict: {result3['status']}")
    
    print("\nğŸ‰ Self-test complete! OmniAI is working correctly.")
    print("\nğŸš€ Next steps:")
    print("   â€¢ Try with your own data: ai.process('your_data.csv')")
    print("   â€¢ Web interface: streamlit run omniai_web.py")
    print("   â€¢ API server: python omniai_api.py")