
import os
import sys
import json
import time

# Ensure imports work from project root
sys.path.insert(0, ".")

from kairos_ark import Agent, Policy, Cap
from kairos_ark.connectors import ArkGeminiConnector
from kairos_ark.tools import ArkTools

# --- Configuration ---
# Using the user-requested free tier models
GEN_MODEL = "gemini-2.0-flash-lite"
EMBED_MODEL = "models/text-embedding-004"

def complete_agent_workflow():
    print(f"\nüöÄ Starting KAIROS-ARK Complete Gemini Agent")
    print(f"   - Generation Model: {GEN_MODEL}")
    print(f"   - Embedding Model:  {EMBED_MODEL}")
    print("=" * 60)

    # 1. Initialize Safe Agent (Kernel Policies)
    # ------------------------------------------
    # Define a policy that ALLOWS LLM calls but PREVENTS dangerous actions (like shell exec)
    safe_policy = Policy(
        allowed_capabilities=[Cap.LLM_CALL, Cap.NET_ACCESS, Cap.FILE_SYSTEM_READ], # Whitelist
        name="safe_rag_agent"
    )
    
    agent = Agent(seed=42)
    agent.set_policy(safe_policy)
    
    # 2. Initialize "Smart" Connector
    # -------------------------------
    # Passing the agent injects the policy enforcement
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("! GEMINI_API_KEY missing. Please set it.")
        return

    gemini = ArkGeminiConnector(
        model_name=GEN_MODEL, 
        embedding_model=EMBED_MODEL,
        agent=agent,
        api_key=api_key
    )

    # 3. Define the Workflow Nodes
    # ----------------------------

    # Node A: Retrieve Context (Vector Embed + Retrieval)
    def node_retrieve_context():
        query = "How does KAIROS-ARK improve agent reliability?"
        print(f"   [Retrieval] Embedding query: '{query}'")
        
        # Real call to Gemini Embeddings
        # (This uses the smart connector which checks policy)
        query_vec = gemini.embed(query)
        
        # Verify vector size (text-embedding-004 is 768 dim)
        print(f"   [Retrieval] Got vector of size {len(query_vec)}")
        
        # Mocking the vector DB search result for this demo
        context = """
        KAIROS-ARK uses a high-performance Rust kernel to ensure deterministic scheduling. 
        It provides logical clocks for time-travel debugging and uses a Zero-Copy memory 
        architecture to handle large data efficiently.
        """
        return {"query": query, "context": context}

    # Node B: Analyze & Generate (LLM Generation)
    def node_generate_answer():
        # Get data from previous node (Architecture automatically handles data flow in real app,
        # here we simulate shared state access for simplicity or assume prev node output is available)
        # In ARK, we usually pass data via zero-copy store or explicit args. 
        # For this simple demo, we'll re-run retrieval or just carry context manually if not fully wired.
        # Let's assume retrieval passed data in a state store.
        
        data = node_retrieve_context() # Simplifying for linear flow demo
        
        prompt = f"""
        Context: {data['context']}
        
        Question: {data['query']}
        
        Answer elegantly in one sentence:
        """
        
        print(f"   [Generation] Sending prompt to {GEN_MODEL}...")
        try:
            response = gemini.generate(prompt)
            print(f"   [Generation] Response received.")
            return {"answer": response.strip()}
        except Exception as e:
            if "429" in str(e):
                return {"answer": "Error: Quota Exceeded (Free Tier Limit)."}
            raise e

    # 4. Construct Graph
    # ------------------
    # In a full app: Connect nodes. Here: Simple execution.
    agent.add_node("rag_pipeline", node_generate_answer)

    # 5. Execute
    # ----------
    print("\n--- Executing Workflow ---")
    start_t = time.perf_counter()
    
    results = agent.execute("rag_pipeline")
    
    end_t = time.perf_counter()
    
    # 6. Parse Results
    # ----------------
    output = json.loads(results[0]["output"])
    print(f"\n‚ú® Final Answer:\n{output['answer']}")
    print(f"\n‚è±Ô∏è  Execution time: {(end_t - start_t)*1000:.2f}ms")
    
    # 7. Verify Policy (Audit)
    # ------------------------
    print("\n--- Audit Log (Governance) ---")
    agent.print_audit_log()
    
    # Check if we were secure
    print("\n‚úÖ Verification: Policy was enforced on every API call.")

if __name__ == "__main__":
    complete_agent_workflow()
