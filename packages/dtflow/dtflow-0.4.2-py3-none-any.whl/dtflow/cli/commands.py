"""
CLI å‘½ä»¤å®ç°
"""

import os
import shutil
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional

import orjson

from ..core import DataTransformer, DictWrapper
from ..lineage import format_lineage_report, get_lineage_chain, has_lineage, load_lineage
from ..pipeline import run_pipeline, validate_pipeline
from ..presets import get_preset, list_presets
from ..storage.io import load_data, sample_file, save_data
from ..streaming import load_stream
from ..utils.field_path import get_field_with_spec

# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
SUPPORTED_FORMATS = {".csv", ".jsonl", ".json", ".xlsx", ".xls", ".parquet", ".arrow", ".feather"}

# æ”¯æŒæµå¼å¤„ç†çš„æ ¼å¼ï¼ˆä¸ streaming.py ä¿æŒä¸€è‡´ï¼‰
STREAMING_FORMATS = {".jsonl", ".csv", ".parquet", ".arrow", ".feather"}


def _is_streaming_supported(filepath: Path) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ”¯æŒæµå¼å¤„ç†"""
    return filepath.suffix.lower() in STREAMING_FORMATS


def _check_file_format(filepath: Path) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒï¼Œä¸æ”¯æŒåˆ™æ‰“å°é”™è¯¯ä¿¡æ¯å¹¶è¿”å› False"""
    ext = filepath.suffix.lower()
    if ext not in SUPPORTED_FORMATS:
        print(f"é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ - {ext}")
        print(f"æ”¯æŒçš„æ ¼å¼: {', '.join(sorted(SUPPORTED_FORMATS))}")
        return False
    return True


def sample(
    filename: str,
    num: int = 10,
    type: Literal["random", "head", "tail"] = "head",
    output: Optional[str] = None,
    seed: Optional[int] = None,
    by: Optional[str] = None,
    uniform: bool = False,
    fields: Optional[str] = None,
) -> None:
    """
    ä»æ•°æ®æ–‡ä»¶ä¸­é‡‡æ ·æŒ‡å®šæ•°é‡çš„æ•°æ®ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        num: é‡‡æ ·æ•°é‡ï¼Œé»˜è®¤ 10
            - num > 0: é‡‡æ ·æŒ‡å®šæ•°é‡
            - num = 0: é‡‡æ ·æ‰€æœ‰æ•°æ®
            - num < 0: Python åˆ‡ç‰‡é£æ ¼ï¼ˆå¦‚ -1 è¡¨ç¤ºæœ€å 1 æ¡ï¼Œ-10 è¡¨ç¤ºæœ€å 10 æ¡ï¼‰
        type: é‡‡æ ·æ–¹å¼ï¼Œå¯é€‰ random/head/tailï¼Œé»˜è®¤ head
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™æ‰“å°åˆ°æ§åˆ¶å°
        seed: éšæœºç§å­ï¼ˆä»…åœ¨ type=random æ—¶æœ‰æ•ˆï¼‰
        by: åˆ†å±‚é‡‡æ ·å­—æ®µåï¼ŒæŒ‰è¯¥å­—æ®µçš„å€¼åˆ†ç»„é‡‡æ ·
        uniform: å‡åŒ€é‡‡æ ·æ¨¡å¼ï¼ˆéœ€é…åˆ --by ä½¿ç”¨ï¼‰ï¼Œå„ç»„é‡‡æ ·ç›¸åŒæ•°é‡
        fields: åªæ˜¾ç¤ºæŒ‡å®šå­—æ®µï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œä»…åœ¨é¢„è§ˆæ¨¡å¼ä¸‹æœ‰æ•ˆ

    Examples:
        dt sample data.jsonl 5
        dt sample data.csv 100 --type=head
        dt sample data.xlsx 50 --output=sampled.jsonl
        dt sample data.jsonl 0   # é‡‡æ ·æ‰€æœ‰æ•°æ®
        dt sample data.jsonl -10 # æœ€å 10 æ¡æ•°æ®
        dt sample data.jsonl 1000 --by=category           # æŒ‰æ¯”ä¾‹åˆ†å±‚é‡‡æ ·
        dt sample data.jsonl 1000 --by=category --uniform # å‡åŒ€åˆ†å±‚é‡‡æ ·
        dt sample data.jsonl --fields=question,answer     # åªæ˜¾ç¤ºæŒ‡å®šå­—æ®µ
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # uniform å¿…é¡»é…åˆ by ä½¿ç”¨
    if uniform and not by:
        print("é”™è¯¯: --uniform å¿…é¡»é…åˆ --by ä½¿ç”¨")
        return

    # åˆ†å±‚é‡‡æ ·æ¨¡å¼
    if by:
        try:
            sampled = _stratified_sample(filepath, num, by, uniform, seed, type)
        except Exception as e:
            print(f"é”™è¯¯: {e}")
            return
    else:
        # æ™®é€šé‡‡æ ·
        try:
            sampled = sample_file(
                str(filepath),
                num=num,
                sample_type=type,
                seed=seed,
                output=None,  # å…ˆä¸ä¿å­˜ï¼Œç»Ÿä¸€åœ¨æœ€åå¤„ç†
            )
        except Exception as e:
            print(f"é”™è¯¯: {e}")
            return

    # è¾“å‡ºç»“æœ
    if output:
        save_data(sampled, output)
        print(f"å·²ä¿å­˜ {len(sampled)} æ¡æ•°æ®åˆ° {output}")
    else:
        # è·å–æ–‡ä»¶æ€»è¡Œæ•°ç”¨äºæ˜¾ç¤º
        total_count = _get_file_row_count(filepath)
        # è§£æ fields å‚æ•°
        field_list = _parse_field_list(fields) if fields else None
        _print_samples(sampled, filepath.name, total_count, field_list)


def _stratified_sample(
    filepath: Path,
    num: int,
    stratify_field: str,
    uniform: bool,
    seed: Optional[int],
    sample_type: str,
) -> List[Dict]:
    """
    åˆ†å±‚é‡‡æ ·å®ç°ã€‚

    Args:
        filepath: æ–‡ä»¶è·¯å¾„
        num: ç›®æ ‡é‡‡æ ·æ€»æ•°
        stratify_field: åˆ†å±‚å­—æ®µï¼Œæ”¯æŒåµŒå¥—è·¯å¾„è¯­æ³•ï¼š
            - meta.source        åµŒå¥—å­—æ®µ
            - messages[0].role   æ•°ç»„ç´¢å¼•
            - messages[-1].role  è´Ÿç´¢å¼•
            - messages.#         æ•°ç»„é•¿åº¦
            - messages[*].role   å±•å¼€æ‰€æœ‰å…ƒç´ ï¼ˆå¯åŠ  :join/:unique æ¨¡å¼ï¼‰
        uniform: æ˜¯å¦å‡åŒ€é‡‡æ ·ï¼ˆå„ç»„ç›¸åŒæ•°é‡ï¼‰
        seed: éšæœºç§å­
        sample_type: é‡‡æ ·æ–¹å¼ï¼ˆç”¨äºç»„å†…é‡‡æ ·ï¼‰

    Returns:
        é‡‡æ ·åçš„æ•°æ®åˆ—è¡¨
    """
    import random
    from collections import defaultdict

    if seed is not None:
        random.seed(seed)

    # åŠ è½½æ•°æ®
    data = load_data(str(filepath))
    total = len(data)

    if num <= 0 or num > total:
        num = total

    # æŒ‰å­—æ®µåˆ†ç»„ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„è¯­æ³•ï¼‰
    groups: Dict[Any, List[Dict]] = defaultdict(list)
    for item in data:
        key = get_field_with_spec(item, stratify_field, default="__null__")
        # ç¡®ä¿ key å¯å“ˆå¸Œ
        if isinstance(key, list):
            key = tuple(key)
        groups[key].append(item)

    group_keys = list(groups.keys())
    num_groups = len(group_keys)

    # æ‰“å°åˆ†ç»„ä¿¡æ¯
    print(f"ğŸ“Š åˆ†å±‚é‡‡æ ·: å­—æ®µ={stratify_field}, å…± {num_groups} ç»„")
    for key in sorted(group_keys, key=lambda x: -len(groups[x])):
        count = len(groups[key])
        pct = count / total * 100
        display_key = key if key != "__null__" else "[ç©ºå€¼]"
        print(f"   {display_key}: {count} æ¡ ({pct:.1f}%)")

    # è®¡ç®—å„ç»„é‡‡æ ·æ•°é‡
    if uniform:
        # å‡åŒ€é‡‡æ ·ï¼šå„ç»„æ•°é‡ç›¸ç­‰
        per_group = num // num_groups
        remainder = num % num_groups
        sample_counts = {key: per_group for key in group_keys}
        # ä½™æ•°åˆ†é…ç»™æ•°æ®é‡æœ€å¤šçš„ç»„
        for key in sorted(group_keys, key=lambda x: -len(groups[x]))[:remainder]:
            sample_counts[key] += 1
    else:
        # æŒ‰æ¯”ä¾‹é‡‡æ ·ï¼šä¿æŒåŸæœ‰æ¯”ä¾‹
        sample_counts = {}
        allocated = 0
        # æŒ‰ç»„å¤§å°é™åºå¤„ç†ï¼Œç¡®ä¿å°ç»„ä¹Ÿèƒ½åˆ†åˆ°
        sorted_keys = sorted(group_keys, key=lambda x: -len(groups[x]))
        for i, key in enumerate(sorted_keys):
            if i == len(sorted_keys) - 1:
                # æœ€åä¸€ç»„åˆ†é…å‰©ä½™
                sample_counts[key] = num - allocated
            else:
                # æŒ‰æ¯”ä¾‹è®¡ç®—
                ratio = len(groups[key]) / total
                count = int(num * ratio)
                # ç¡®ä¿è‡³å°‘ 1 æ¡ï¼ˆå¦‚æœç»„æœ‰æ•°æ®ï¼‰
                count = max(1, count) if groups[key] else 0
                sample_counts[key] = count
                allocated += count

    # æ‰§è¡Œå„ç»„é‡‡æ ·
    result = []
    print(f"ğŸ”„ æ‰§è¡Œé‡‡æ ·...")
    for key in group_keys:
        group_data = groups[key]
        target = min(sample_counts[key], len(group_data))

        if target <= 0:
            continue

        # ç»„å†…é‡‡æ ·
        if sample_type == "random":
            sampled = random.sample(group_data, target)
        elif sample_type == "head":
            sampled = group_data[:target]
        else:  # tail
            sampled = group_data[-target:]

        result.extend(sampled)

    # æ‰“å°é‡‡æ ·ç»“æœ
    print(f"\nğŸ“‹ é‡‡æ ·ç»“æœ:")
    result_groups: Dict[Any, int] = defaultdict(int)
    for item in result:
        key = item.get(stratify_field, "__null__")
        result_groups[key] += 1

    for key in sorted(group_keys, key=lambda x: -len(groups[x])):
        orig = len(groups[key])
        sampled_count = result_groups.get(key, 0)
        display_key = key if key != "__null__" else "[ç©ºå€¼]"
        print(f"   {display_key}: {orig} â†’ {sampled_count}")

    print(f"\nâœ… æ€»è®¡: {total} â†’ {len(result)} æ¡")

    return result


def head(
    filename: str,
    num: int = 10,
    output: Optional[str] = None,
    fields: Optional[str] = None,
) -> None:
    """
    æ˜¾ç¤ºæ–‡ä»¶çš„å‰ N æ¡æ•°æ®ï¼ˆdt sample --type=head çš„å¿«æ·æ–¹å¼ï¼‰ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        num: æ˜¾ç¤ºæ•°é‡ï¼Œé»˜è®¤ 10
            - num > 0: æ˜¾ç¤ºæŒ‡å®šæ•°é‡
            - num = 0: æ˜¾ç¤ºæ‰€æœ‰æ•°æ®
            - num < 0: Python åˆ‡ç‰‡é£æ ¼ï¼ˆå¦‚ -10 è¡¨ç¤ºæœ€å 10 æ¡ï¼‰
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™æ‰“å°åˆ°æ§åˆ¶å°
        fields: åªæ˜¾ç¤ºæŒ‡å®šå­—æ®µï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œä»…åœ¨é¢„è§ˆæ¨¡å¼ä¸‹æœ‰æ•ˆ

    Examples:
        dt head data.jsonl          # æ˜¾ç¤ºå‰ 10 æ¡
        dt head data.jsonl 20       # æ˜¾ç¤ºå‰ 20 æ¡
        dt head data.csv 0          # æ˜¾ç¤ºæ‰€æœ‰æ•°æ®
        dt head data.xlsx --output=head.jsonl
        dt head data.jsonl --fields=question,answer
    """
    sample(filename, num=num, type="head", output=output, fields=fields)


def tail(
    filename: str,
    num: int = 10,
    output: Optional[str] = None,
    fields: Optional[str] = None,
) -> None:
    """
    æ˜¾ç¤ºæ–‡ä»¶çš„å N æ¡æ•°æ®ï¼ˆdt sample --type=tail çš„å¿«æ·æ–¹å¼ï¼‰ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        num: æ˜¾ç¤ºæ•°é‡ï¼Œé»˜è®¤ 10
            - num > 0: æ˜¾ç¤ºæŒ‡å®šæ•°é‡
            - num = 0: æ˜¾ç¤ºæ‰€æœ‰æ•°æ®
            - num < 0: Python åˆ‡ç‰‡é£æ ¼ï¼ˆå¦‚ -10 è¡¨ç¤ºæœ€å 10 æ¡ï¼‰
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™æ‰“å°åˆ°æ§åˆ¶å°
        fields: åªæ˜¾ç¤ºæŒ‡å®šå­—æ®µï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œä»…åœ¨é¢„è§ˆæ¨¡å¼ä¸‹æœ‰æ•ˆ

    Examples:
        dt tail data.jsonl          # æ˜¾ç¤ºå 10 æ¡
        dt tail data.jsonl 20       # æ˜¾ç¤ºå 20 æ¡
        dt tail data.csv 0          # æ˜¾ç¤ºæ‰€æœ‰æ•°æ®
        dt tail data.xlsx --output=tail.jsonl
        dt tail data.jsonl --fields=question,answer
    """
    sample(filename, num=num, type="tail", output=output, fields=fields)


def _get_file_row_count(filepath: Path) -> Optional[int]:
    """
    å¿«é€Ÿè·å–æ–‡ä»¶è¡Œæ•°ï¼ˆä¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰ã€‚

    å¯¹äº JSONL æ–‡ä»¶ï¼Œç›´æ¥è®¡ç®—è¡Œæ•°ï¼›å…¶ä»–æ ¼å¼è¿”å› Noneã€‚
    """
    ext = filepath.suffix.lower()
    if ext == ".jsonl":
        try:
            with open(filepath, "rb") as f:
                return sum(1 for _ in f)
        except Exception:
            return None
    # å…¶ä»–æ ¼å¼æš‚ä¸æ”¯æŒå¿«é€Ÿè®¡æ•°
    return None


def _format_value(value: Any, max_len: int = 80) -> str:
    """æ ¼å¼åŒ–å•ä¸ªå€¼ï¼Œé•¿æ–‡æœ¬æˆªæ–­ã€‚"""
    if value is None:
        return "[dim]null[/dim]"
    if isinstance(value, bool):
        return "[cyan]true[/cyan]" if value else "[cyan]false[/cyan]"
    if isinstance(value, (int, float)):
        return f"[cyan]{value}[/cyan]"
    if isinstance(value, str):
        # å¤„ç†å¤šè¡Œæ–‡æœ¬
        if "\n" in value:
            lines = value.split("\n")
            if len(lines) > 3:
                preview = lines[0][:max_len] + f"... [dim]({len(lines)} è¡Œ)[/dim]"
            else:
                preview = value.replace("\n", "\\n")
                if len(preview) > max_len:
                    preview = preview[:max_len] + "..."
            return f'"{preview}"'
        if len(value) > max_len:
            return f'"{value[:max_len]}..." [dim]({len(value)} å­—ç¬¦)[/dim]'
        return f'"{value}"'
    return str(value)


def _format_nested(
    value: Any,
    indent: str = "",
    is_last: bool = True,
    max_len: int = 80,
) -> List[str]:
    """
    é€’å½’æ ¼å¼åŒ–åµŒå¥—ç»“æ„ï¼Œè¿”å›è¡Œåˆ—è¡¨ã€‚

    ä½¿ç”¨æ ‘å½¢ç¬¦å·å±•ç¤ºç»“æ„ï¼š
    â”œâ”€ ä¸­é—´é¡¹
    â””â”€ æœ€åä¸€é¡¹
    """
    lines = []
    branch = "â””â”€ " if is_last else "â”œâ”€ "
    cont = "   " if is_last else "â”‚  "

    if isinstance(value, dict):
        items = list(value.items())
        for i, (k, v) in enumerate(items):
            is_last_item = i == len(items) - 1
            b = "â””â”€ " if is_last_item else "â”œâ”€ "
            c = "   " if is_last_item else "â”‚  "

            if isinstance(v, (dict, list)) and v:
                # åµŒå¥—ç»“æ„
                if isinstance(v, list):
                    # æ£€æµ‹æ˜¯å¦ä¸º messages æ ¼å¼
                    is_messages = (
                        v and isinstance(v[0], dict) and "role" in v[0] and "content" in v[0]
                    )
                    if is_messages:
                        lines.append(
                            f"{indent}{b}[green]{k}[/green]: ({len(v)} items) [dim]â†’ \\[role]: content[/dim]"
                        )
                    else:
                        lines.append(f"{indent}{b}[green]{k}[/green]: ({len(v)} items)")
                else:
                    lines.append(f"{indent}{b}[green]{k}[/green]:")
                lines.extend(_format_nested(v, indent + c, True, max_len))
            else:
                # ç®€å•å€¼
                lines.append(f"{indent}{b}[green]{k}[/green]: {_format_value(v, max_len)}")

    elif isinstance(value, list):
        for i, item in enumerate(value):
            is_last_item = i == len(value) - 1
            b = "â””â”€ " if is_last_item else "â”œâ”€ "
            c = "   " if is_last_item else "â”‚  "

            if isinstance(item, dict):
                # åˆ—è¡¨ä¸­çš„å­—å…¸é¡¹ - æ£€æµ‹æ˜¯å¦ä¸º messages æ ¼å¼
                if "role" in item and "content" in item:
                    role = item.get("role", "")
                    content = item.get("content", "")
                    # æˆªæ–­é•¿å†…å®¹
                    if len(content) > max_len:
                        content = content[:max_len].replace("\n", "\\n") + "..."
                    else:
                        content = content.replace("\n", "\\n")
                    # ä½¿ç”¨ \[ è½¬ä¹‰é¿å…è¢« rich è§£æä¸ºæ ·å¼
                    lines.append(f"{indent}{b}[yellow]\\[{role}]:[/yellow] {content}")
                else:
                    # æ™®é€šå­—å…¸
                    lines.append(f"{indent}{b}[dim]{{...}}[/dim]")
                    lines.extend(_format_nested(item, indent + c, True, max_len))
            elif isinstance(item, list):
                lines.append(f"{indent}{b}[dim][{len(item)} items][/dim]")
                lines.extend(_format_nested(item, indent + c, True, max_len))
            else:
                lines.append(f"{indent}{b}{_format_value(item, max_len)}")

    return lines


def _is_simple_data(samples: List[Dict]) -> bool:
    """åˆ¤æ–­æ•°æ®æ˜¯å¦é€‚åˆè¡¨æ ¼å±•ç¤ºï¼ˆæ— åµŒå¥—ç»“æ„ï¼‰ã€‚"""
    if not samples or not isinstance(samples[0], dict):
        return False
    keys = list(samples[0].keys())
    if len(keys) > 6:
        return False
    for s in samples[:3]:
        for k in keys:
            v = s.get(k)
            if isinstance(v, (dict, list)):
                return False
            if isinstance(v, str) and len(v) > 80:
                return False
    return True


def _print_samples(
    samples: list,
    filename: Optional[str] = None,
    total_count: Optional[int] = None,
    fields: Optional[List[str]] = None,
) -> None:
    """
    æ‰“å°é‡‡æ ·ç»“æœã€‚

    Args:
        samples: é‡‡æ ·æ•°æ®åˆ—è¡¨
        filename: æ–‡ä»¶åï¼ˆç”¨äºæ˜¾ç¤ºæ¦‚è§ˆï¼‰
        total_count: æ–‡ä»¶æ€»è¡Œæ•°ï¼ˆç”¨äºæ˜¾ç¤ºæ¦‚è§ˆï¼‰
        fields: åªæ˜¾ç¤ºæŒ‡å®šå­—æ®µ
    """
    if not samples:
        print("æ²¡æœ‰æ•°æ®")
        return

    # è¿‡æ»¤å­—æ®µ
    if fields and isinstance(samples[0], dict):
        field_set = set(fields)
        samples = [{k: v for k, v in item.items() if k in field_set} for item in samples]

    try:
        from rich.console import Console
        from rich.panel import Panel
        from rich.table import Table

        console = Console()

        # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆå¤´éƒ¨
        if filename:
            all_fields = set()
            for item in samples:
                if isinstance(item, dict):
                    all_fields.update(item.keys())
            field_names = ", ".join(sorted(all_fields))

            if total_count is not None:
                info = f"æ€»è¡Œæ•°: {total_count:,} | é‡‡æ ·: {len(samples)} æ¡ | å­—æ®µ: {len(all_fields)} ä¸ª"
            else:
                info = f"é‡‡æ ·: {len(samples)} æ¡ | å­—æ®µ: {len(all_fields)} ä¸ª"

            console.print(
                Panel(
                    f"[dim]{info}[/dim]\n[dim]å­—æ®µ: {field_names}[/dim]",
                    title=f"[bold]ğŸ“Š {filename}[/bold]",
                    expand=False,
                    border_style="dim",
                )
            )
            console.print()

        # ç®€å•æ•°æ®ç”¨è¡¨æ ¼å±•ç¤º
        if _is_simple_data(samples):
            keys = list(samples[0].keys())
            table = Table(show_header=True, header_style="bold cyan")
            for key in keys:
                table.add_column(key, overflow="fold")
            for item in samples:
                table.add_row(*[str(item.get(k, "")) for k in keys])
            console.print(table)
            return

        # åµŒå¥—æ•°æ®ç”¨æ ‘å½¢ç»“æ„å±•ç¤º
        for i, item in enumerate(samples, 1):
            console.print(f"[bold cyan]--- ç¬¬ {i} æ¡ ---[/bold cyan]")
            if isinstance(item, dict):
                for line in _format_nested(item):
                    console.print(line)
            else:
                console.print(_format_value(item))
            console.print()

    except ImportError:
        # æ²¡æœ‰ richï¼Œä½¿ç”¨æ™®é€šæ‰“å°
        if filename:
            all_fields = set()
            for item in samples:
                if isinstance(item, dict):
                    all_fields.update(item.keys())

            print(f"\nğŸ“Š {filename}")
            if total_count is not None:
                print(
                    f"   æ€»è¡Œæ•°: {total_count:,} | é‡‡æ ·: {len(samples)} æ¡ | å­—æ®µ: {len(all_fields)} ä¸ª"
                )
            else:
                print(f"   é‡‡æ ·: {len(samples)} æ¡ | å­—æ®µ: {len(all_fields)} ä¸ª")
            print(f"   å­—æ®µ: {', '.join(sorted(all_fields))}")
            print()

        for i, item in enumerate(samples, 1):
            print(f"--- ç¬¬ {i} æ¡ ---")
            print(orjson.dumps(item, option=orjson.OPT_INDENT_2).decode("utf-8"))
            print()


# ============ Transform Command ============

CONFIG_DIR = ".dt"


def _get_config_path(input_path: Path, config_override: Optional[str] = None) -> Path:
    """è·å–é…ç½®æ–‡ä»¶è·¯å¾„"""
    if config_override:
        return Path(config_override)

    # ä½¿ç”¨è¾“å…¥æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºé…ç½®æ–‡ä»¶å
    config_name = input_path.stem + ".py"
    return input_path.parent / CONFIG_DIR / config_name


def transform(
    filename: str,
    num: Optional[int] = None,
    preset: Optional[str] = None,
    config: Optional[str] = None,
    output: Optional[str] = None,
) -> None:
    """
    è½¬æ¢æ•°æ®æ ¼å¼ã€‚

    ä¸¤ç§ä½¿ç”¨æ–¹å¼ï¼š
    1. é…ç½®æ–‡ä»¶æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼šè‡ªåŠ¨ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼Œç¼–è¾‘åå†æ¬¡è¿è¡Œ
    2. é¢„è®¾æ¨¡å¼ï¼šä½¿ç”¨ --preset ç›´æ¥è½¬æ¢

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        num: åªè½¬æ¢å‰ N æ¡æ•°æ®ï¼ˆå¯é€‰ï¼‰
        preset: ä½¿ç”¨é¢„è®¾æ¨¡æ¿ï¼ˆopenai_chat, alpaca, sharegpt, dpo_pair, simple_qaï¼‰
        config: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ .dt/<filename>.pyï¼‰
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„

    Examples:
        dt transform data.jsonl                        # é¦–æ¬¡ç”Ÿæˆé…ç½®
        dt transform data.jsonl 10                     # åªè½¬æ¢å‰ 10 æ¡
        dt transform data.jsonl --preset=openai_chat   # ä½¿ç”¨é¢„è®¾
        dt transform data.jsonl 100 --preset=alpaca    # é¢„è®¾ + é™åˆ¶æ•°é‡
    """
    filepath = Path(filename)
    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # é¢„è®¾æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨é¢„è®¾è½¬æ¢
    if preset:
        _execute_preset_transform(filepath, preset, output, num)
        return

    # é…ç½®æ–‡ä»¶æ¨¡å¼
    config_path = _get_config_path(filepath, config)

    if not config_path.exists():
        _generate_config(filepath, config_path)
    else:
        _execute_transform(filepath, config_path, output, num)


def _generate_config(input_path: Path, config_path: Path) -> None:
    """åˆ†æè¾“å…¥æ•°æ®å¹¶ç”Ÿæˆé…ç½®æ–‡ä»¶"""
    print(f"ğŸ“Š åˆ†æè¾“å…¥æ•°æ®: {input_path}")

    # è¯»å–æ•°æ®
    try:
        data = load_data(str(input_path))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    if not data:
        print("é”™è¯¯: æ–‡ä»¶ä¸ºç©º")
        return

    total_count = len(data)
    sample_item = data[0]

    print(f"   æ£€æµ‹åˆ° {total_count} æ¡æ•°æ®")

    # ç”Ÿæˆé…ç½®å†…å®¹
    config_content = _build_config_content(sample_item, input_path.name, total_count)

    # ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨
    config_path.parent.mkdir(parents=True, exist_ok=True)

    # å†™å…¥é…ç½®æ–‡ä»¶
    config_path.write_text(config_content, encoding="utf-8")

    print(f"\nğŸ“ å·²ç”Ÿæˆé…ç½®æ–‡ä»¶: {config_path}")
    print("\nğŸ‘‰ ä¸‹ä¸€æ­¥:")
    print(f"   1. ç¼–è¾‘ {config_path}ï¼Œå®šä¹‰ transform å‡½æ•°")
    print(f"   2. å†æ¬¡æ‰§è¡Œ dt transform {input_path.name} å®Œæˆè½¬æ¢")


def _build_config_content(sample: Dict[str, Any], filename: str, total: int) -> str:
    """æ„å»ºé…ç½®æ–‡ä»¶å†…å®¹"""
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # ç”Ÿæˆ Item ç±»çš„å­—æ®µå®šä¹‰
    fields_def = _generate_fields_definition(sample)

    # ç”Ÿæˆé»˜è®¤çš„ transform å‡½æ•°ï¼ˆç®€å•é‡å‘½åï¼‰
    field_names = list(sample.keys())

    # ç”Ÿæˆè§„èŒƒåŒ–çš„å­—æ®µåç”¨äºç¤ºä¾‹
    safe_field1 = _sanitize_field_name(field_names[0])[0] if field_names else "field1"
    safe_field2 = _sanitize_field_name(field_names[1])[0] if len(field_names) > 1 else "field2"

    # ç”Ÿæˆé»˜è®¤è¾“å‡ºæ–‡ä»¶å
    base_name = Path(filename).stem
    output_filename = f"{base_name}_output.jsonl"

    config = f'''"""
DataTransformer é…ç½®æ–‡ä»¶
ç”Ÿæˆæ—¶é—´: {now}
è¾“å…¥æ–‡ä»¶: {filename} ({total} æ¡)
"""


# ===== è¾“å…¥æ•°æ®ç»“æ„ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼ŒIDE å¯è¡¥å…¨ï¼‰=====

class Item:
{fields_def}


# ===== å®šä¹‰è½¬æ¢é€»è¾‘ =====
# æç¤ºï¼šè¾“å…¥ item. å IDE ä¼šè‡ªåŠ¨è¡¥å…¨å¯ç”¨å­—æ®µ

def transform(item: Item):
    return {{
{_generate_default_transform(field_names)}
    }}


# è¾“å‡ºæ–‡ä»¶è·¯å¾„
output = "{output_filename}"


# ===== ç¤ºä¾‹ =====
#
# ç¤ºä¾‹1: æ„å»º OpenAI Chat æ ¼å¼
# def transform(item: Item):
#     return {{
#         "messages": [
#             {{"role": "user", "content": item.{safe_field1}}},
#             {{"role": "assistant", "content": item.{safe_field2}}},
#         ]
#     }}
#
# ç¤ºä¾‹2: Alpaca æ ¼å¼
# def transform(item: Item):
#     return {{
#         "instruction": item.{safe_field1},
#         "input": "",
#         "output": item.{safe_field2},
#     }}
'''
    return config


def _generate_fields_definition(sample: Dict[str, Any], indent: int = 4) -> str:
    """ç”Ÿæˆ Item ç±»çš„å­—æ®µå®šä¹‰"""
    lines = []
    prefix = " " * indent

    for key, value in sample.items():
        type_name = _get_type_name(value)
        example = _format_example_value(value)
        safe_key, changed = _sanitize_field_name(key)
        comment = f"  # åŸå­—æ®µå: {key}" if changed else ""
        lines.append(f"{prefix}{safe_key}: {type_name} = {example}{comment}")

    return "\n".join(lines) if lines else f"{prefix}pass"


def _get_type_name(value: Any) -> str:
    """è·å–å€¼çš„ç±»å‹åç§°"""
    if value is None:
        return "str"
    if isinstance(value, str):
        return "str"
    if isinstance(value, bool):
        return "bool"
    if isinstance(value, int):
        return "int"
    if isinstance(value, float):
        return "float"
    if isinstance(value, list):
        return "list"
    if isinstance(value, dict):
        return "dict"
    return "str"


def _format_example_value(value: Any, max_len: int = 50) -> str:
    """æ ¼å¼åŒ–ç¤ºä¾‹å€¼"""
    if value is None:
        return '""'
    if isinstance(value, str):
        # æˆªæ–­é•¿å­—ç¬¦ä¸²
        if len(value) > max_len:
            value = value[:max_len] + "..."
        # ä½¿ç”¨ repr() è‡ªåŠ¨å¤„ç†æ‰€æœ‰è½¬ä¹‰å­—ç¬¦
        return repr(value)
    if isinstance(value, bool):
        return str(value)
    if isinstance(value, (int, float)):
        return str(value)
    if isinstance(value, (list, dict)):
        s = orjson.dumps(value).decode("utf-8")
        if len(s) > max_len:
            return repr(s[:max_len] + "...")
        return s
    return '""'


def _sanitize_field_name(name: str) -> tuple:
    """
    å°†å­—æ®µåè§„èŒƒåŒ–ä¸ºåˆæ³•çš„ Python æ ‡è¯†ç¬¦ã€‚

    Returns:
        tuple: (è§„èŒƒåŒ–åçš„åç§°, æ˜¯å¦è¢«ä¿®æ”¹)
    """
    if name.isidentifier():
        return name, False

    # æ›¿æ¢å¸¸è§çš„éæ³•å­—ç¬¦
    sanitized = name.replace("-", "_").replace(" ", "_").replace(".", "_")

    # å¦‚æœä»¥æ•°å­—å¼€å¤´ï¼Œæ·»åŠ å‰ç¼€
    if sanitized and sanitized[0].isdigit():
        sanitized = "f_" + sanitized

    # ç§»é™¤å…¶ä»–éæ³•å­—ç¬¦
    sanitized = "".join(c if c.isalnum() or c == "_" else "_" for c in sanitized)

    # ç¡®ä¿ä¸ä¸ºç©º
    if not sanitized:
        sanitized = "field"

    return sanitized, True


def _generate_default_transform(field_names: List[str]) -> str:
    """ç”Ÿæˆé»˜è®¤çš„ transform å‡½æ•°ä½“"""
    lines = []
    for name in field_names[:5]:  # æœ€å¤šæ˜¾ç¤º 5 ä¸ªå­—æ®µ
        safe_name, _ = _sanitize_field_name(name)
        lines.append(f'        "{name}": item.{safe_name},')
    return "\n".join(lines) if lines else "        # åœ¨è¿™é‡Œå®šä¹‰è¾“å‡ºå­—æ®µ"


def _unwrap(obj: Any) -> Any:
    """é€’å½’å°† DictWrapper è½¬æ¢ä¸ºæ™®é€š dict"""
    if hasattr(obj, "to_dict"):
        return _unwrap(obj.to_dict())
    if isinstance(obj, dict):
        return {k: _unwrap(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_unwrap(v) for v in obj]
    return obj


def _execute_transform(
    input_path: Path,
    config_path: Path,
    output_override: Optional[str],
    num: Optional[int],
) -> None:
    """æ‰§è¡Œæ•°æ®è½¬æ¢ï¼ˆé»˜è®¤æµå¼å¤„ç†ï¼‰"""
    print(f"ğŸ“‚ åŠ è½½é…ç½®: {config_path}")

    # åŠ¨æ€åŠ è½½é…ç½®æ–‡ä»¶
    try:
        config_ns = _load_config(config_path)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ - {e}")
        return

    # è·å– transform å‡½æ•°
    if "transform" not in config_ns:
        print("é”™è¯¯: é…ç½®æ–‡ä»¶ä¸­æœªå®šä¹‰ transform å‡½æ•°")
        return

    transform_func = config_ns["transform"]

    # è·å–è¾“å‡ºè·¯å¾„
    output_path = output_override or config_ns.get("output", "output.jsonl")

    # å¯¹äº JSONL æ–‡ä»¶ä½¿ç”¨æµå¼å¤„ç†
    if _is_streaming_supported(input_path):
        print(f"ğŸ“Š æµå¼åŠ è½½: {input_path}")
        print("ğŸ”„ æ‰§è¡Œè½¬æ¢...")
        try:
            # åŒ…è£…è½¬æ¢å‡½æ•°ä»¥æ”¯æŒå±æ€§è®¿é—®ï¼ˆé…ç½®æ–‡ä»¶ä¸­å®šä¹‰çš„ Item ç±»ï¼‰
            def wrapped_transform(item):
                result = transform_func(DictWrapper(item))
                return _unwrap(result)

            st = load_stream(str(input_path))
            if num:
                st = st.head(num)
            count = st.transform(wrapped_transform).save(output_path)
            print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
            print(f"\nâœ… å®Œæˆ! å·²è½¬æ¢ {count} æ¡æ•°æ®åˆ° {output_path}")
        except Exception as e:
            print(f"é”™è¯¯: è½¬æ¢å¤±è´¥ - {e}")
            import traceback

            traceback.print_exc()
        return

    # é JSONL æ–‡ä»¶ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {input_path}")
    try:
        dt = DataTransformer.load(str(input_path))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    total = len(dt)
    if num:
        dt = DataTransformer(dt.data[:num])
        print(f"   å¤„ç†å‰ {len(dt)}/{total} æ¡æ•°æ®")
    else:
        print(f"   å…± {total} æ¡æ•°æ®")

    # æ‰§è¡Œè½¬æ¢ï¼ˆä½¿ç”¨ Core çš„ to æ–¹æ³•ï¼Œè‡ªåŠ¨æ”¯æŒå±æ€§è®¿é—®ï¼‰
    print("ğŸ”„ æ‰§è¡Œè½¬æ¢...")
    try:
        results = dt.to(transform_func)
    except Exception as e:
        print(f"é”™è¯¯: è½¬æ¢å¤±è´¥ - {e}")
        import traceback

        traceback.print_exc()
        return

    # ä¿å­˜ç»“æœ
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
    try:
        save_data(results, output_path)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•ä¿å­˜æ–‡ä»¶ - {e}")
        return

    print(f"\nâœ… å®Œæˆ! å·²è½¬æ¢ {len(results)} æ¡æ•°æ®åˆ° {output_path}")


def _execute_preset_transform(
    input_path: Path,
    preset_name: str,
    output_override: Optional[str],
    num: Optional[int],
) -> None:
    """ä½¿ç”¨é¢„è®¾æ¨¡æ¿æ‰§è¡Œè½¬æ¢ï¼ˆé»˜è®¤æµå¼å¤„ç†ï¼‰"""
    print(f"ğŸ“‚ ä½¿ç”¨é¢„è®¾: {preset_name}")

    # è·å–é¢„è®¾å‡½æ•°
    try:
        transform_func = get_preset(preset_name)
    except ValueError as e:
        print(f"é”™è¯¯: {e}")
        print(f"å¯ç”¨é¢„è®¾: {', '.join(list_presets())}")
        return

    output_path = output_override or f"{input_path.stem}_{preset_name}.jsonl"

    # æ£€æŸ¥è¾“å…¥è¾“å‡ºæ˜¯å¦ç›¸åŒ
    input_resolved = input_path.resolve()
    output_resolved = Path(output_path).resolve()
    use_temp_file = input_resolved == output_resolved

    # å¯¹äº JSONL æ–‡ä»¶ä½¿ç”¨æµå¼å¤„ç†
    if _is_streaming_supported(input_path):
        print(f"ğŸ“Š æµå¼åŠ è½½: {input_path}")
        print("ğŸ”„ æ‰§è¡Œè½¬æ¢...")

        # å¦‚æœè¾“å…¥è¾“å‡ºç›¸åŒï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        if use_temp_file:
            print("âš  æ£€æµ‹åˆ°è¾“å‡ºæ–‡ä»¶ä¸è¾“å…¥æ–‡ä»¶ç›¸åŒï¼Œå°†ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶")
            temp_fd, temp_path = tempfile.mkstemp(
                suffix=output_resolved.suffix,
                prefix=".tmp_",
                dir=output_resolved.parent,
            )
            os.close(temp_fd)
            actual_output = temp_path
        else:
            actual_output = output_path

        try:
            # åŒ…è£…è½¬æ¢å‡½æ•°ä»¥æ”¯æŒå±æ€§è®¿é—®
            def wrapped_transform(item):
                result = transform_func(DictWrapper(item))
                return _unwrap(result)

            st = load_stream(str(input_path))
            if num:
                st = st.head(num)
            count = st.transform(wrapped_transform).save(actual_output)

            # å¦‚æœä½¿ç”¨äº†ä¸´æ—¶æ–‡ä»¶ï¼Œç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
            if use_temp_file:
                shutil.move(temp_path, output_path)

            print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
            print(f"\nâœ… å®Œæˆ! å·²è½¬æ¢ {count} æ¡æ•°æ®åˆ° {output_path}")
        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if use_temp_file and os.path.exists(temp_path):
                os.unlink(temp_path)
            print(f"é”™è¯¯: è½¬æ¢å¤±è´¥ - {e}")
            import traceback

            traceback.print_exc()
        return

    # é JSONL æ–‡ä»¶ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {input_path}")
    try:
        dt = DataTransformer.load(str(input_path))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    total = len(dt)
    if num:
        dt = DataTransformer(dt.data[:num])
        print(f"   å¤„ç†å‰ {len(dt)}/{total} æ¡æ•°æ®")
    else:
        print(f"   å…± {total} æ¡æ•°æ®")

    # æ‰§è¡Œè½¬æ¢
    print("ğŸ”„ æ‰§è¡Œè½¬æ¢...")
    try:
        results = dt.to(transform_func)
    except Exception as e:
        print(f"é”™è¯¯: è½¬æ¢å¤±è´¥ - {e}")
        import traceback

        traceback.print_exc()
        return

    # ä¿å­˜ç»“æœ
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
    try:
        save_data(results, output_path)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•ä¿å­˜æ–‡ä»¶ - {e}")
        return

    print(f"\nâœ… å®Œæˆ! å·²è½¬æ¢ {len(results)} æ¡æ•°æ®åˆ° {output_path}")


def _load_config(config_path: Path) -> Dict[str, Any]:
    """åŠ¨æ€åŠ è½½ Python é…ç½®æ–‡ä»¶"""
    import importlib.util

    spec = importlib.util.spec_from_file_location("dt_config", config_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)

    return {name: getattr(module, name) for name in dir(module) if not name.startswith("_")}


# ============ Dedupe Command ============


def dedupe(
    filename: str,
    key: Optional[str] = None,
    similar: Optional[float] = None,
    output: Optional[str] = None,
) -> None:
    """
    æ•°æ®å»é‡ã€‚

    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. ç²¾ç¡®å»é‡ï¼ˆé»˜è®¤ï¼‰ï¼šå®Œå…¨ç›¸åŒçš„æ•°æ®æ‰å»é‡
    2. ç›¸ä¼¼åº¦å»é‡ï¼šä½¿ç”¨ MinHash+LSH ç®—æ³•ï¼Œç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼åˆ™å»é‡

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        key: å»é‡ä¾æ®å­—æ®µï¼Œæ”¯æŒåµŒå¥—è·¯å¾„è¯­æ³•ï¼š
            - meta.source        åµŒå¥—å­—æ®µ
            - messages[0].role   æ•°ç»„ç´¢å¼•
            - messages[-1].content  è´Ÿç´¢å¼•
            - messages.#         æ•°ç»„é•¿åº¦
            - messages[*].role:join  å±•å¼€æ‰€æœ‰å…ƒç´ 
            å¤šä¸ªå­—æ®µç”¨é€—å·åˆ†éš”ã€‚ä¸æŒ‡å®šåˆ™å…¨é‡å»é‡
        similar: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼ŒæŒ‡å®šåå¯ç”¨ç›¸ä¼¼åº¦å»é‡æ¨¡å¼ï¼Œéœ€è¦æŒ‡å®š --key
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™è¦†ç›–åŸæ–‡ä»¶

    Examples:
        dt dedupe data.jsonl                       # å…¨é‡ç²¾ç¡®å»é‡
        dt dedupe data.jsonl --key=text            # æŒ‰ text å­—æ®µç²¾ç¡®å»é‡
        dt dedupe data.jsonl --key=user,timestamp  # æŒ‰å¤šå­—æ®µç»„åˆç²¾ç¡®å»é‡
        dt dedupe data.jsonl --key=meta.id         # æŒ‰åµŒå¥—å­—æ®µå»é‡
        dt dedupe data.jsonl --key=messages[0].content   # æŒ‰ç¬¬ä¸€æ¡æ¶ˆæ¯å†…å®¹å»é‡
        dt dedupe data.jsonl --key=text --similar=0.8    # ç›¸ä¼¼åº¦å»é‡
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # ç›¸ä¼¼åº¦å»é‡æ¨¡å¼å¿…é¡»æŒ‡å®š key
    if similar is not None and not key:
        print("é”™è¯¯: ç›¸ä¼¼åº¦å»é‡éœ€è¦æŒ‡å®š --key å‚æ•°")
        return

    if similar is not None and (similar <= 0 or similar > 1):
        print("é”™è¯¯: --similar å‚æ•°å¿…é¡»åœ¨ 0-1 ä¹‹é—´")
        return

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {filepath}")
    try:
        dt = DataTransformer.load(str(filepath))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    original_count = len(dt)
    print(f"   å…± {original_count} æ¡æ•°æ®")

    # æ‰§è¡Œå»é‡
    if similar is not None:
        # ç›¸ä¼¼åº¦å»é‡æ¨¡å¼
        print(f"ğŸ”‘ ç›¸ä¼¼åº¦å»é‡: å­—æ®µ={key}, é˜ˆå€¼={similar}")
        print("ğŸ”„ æ‰§è¡Œå»é‡ï¼ˆMinHash+LSHï¼‰...")
        try:
            result = dt.dedupe_similar(key, threshold=similar)
        except ImportError as e:
            print(f"é”™è¯¯: {e}")
            return
    else:
        # ç²¾ç¡®å»é‡æ¨¡å¼
        dedupe_key: Any = None
        if key:
            keys = [k.strip() for k in key.split(",")]
            if len(keys) == 1:
                dedupe_key = keys[0]
                print(f"ğŸ”‘ æŒ‰å­—æ®µç²¾ç¡®å»é‡: {dedupe_key}")
            else:
                dedupe_key = keys
                print(f"ğŸ”‘ æŒ‰å¤šå­—æ®µç»„åˆç²¾ç¡®å»é‡: {', '.join(dedupe_key)}")
        else:
            print("ğŸ”‘ å…¨é‡ç²¾ç¡®å»é‡")

        print("ğŸ”„ æ‰§è¡Œå»é‡...")
        result = dt.dedupe(dedupe_key)

    dedupe_count = len(result)
    removed_count = original_count - dedupe_count

    # ä¿å­˜ç»“æœ
    output_path = output or str(filepath)
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
    try:
        result.save(output_path)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•ä¿å­˜æ–‡ä»¶ - {e}")
        return

    print(f"\nâœ… å®Œæˆ! å»é™¤ {removed_count} æ¡é‡å¤æ•°æ®ï¼Œå‰©ä½™ {dedupe_count} æ¡")


# ============ Concat Command ============


def concat(
    *files: str,
    output: Optional[str] = None,
    strict: bool = False,
) -> None:
    """
    æ‹¼æ¥å¤šä¸ªæ•°æ®æ–‡ä»¶ï¼ˆæµå¼å¤„ç†ï¼Œå†…å­˜å ç”¨ O(1)ï¼‰ã€‚

    Args:
        *files: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¿…é¡»æŒ‡å®š
        strict: ä¸¥æ ¼æ¨¡å¼ï¼Œå­—æ®µå¿…é¡»å®Œå…¨ä¸€è‡´ï¼Œå¦åˆ™æŠ¥é”™

    Examples:
        dt concat a.jsonl b.jsonl -o merged.jsonl
        dt concat data1.csv data2.csv data3.csv -o all.jsonl
        dt concat a.jsonl b.jsonl --strict -o merged.jsonl
    """
    if len(files) < 2:
        print("é”™è¯¯: è‡³å°‘éœ€è¦ä¸¤ä¸ªæ–‡ä»¶")
        return

    if not output:
        print("é”™è¯¯: å¿…é¡»æŒ‡å®šè¾“å‡ºæ–‡ä»¶ (-o/--output)")
        return

    # éªŒè¯æ‰€æœ‰æ–‡ä»¶
    file_paths = []
    for f in files:
        filepath = Path(f).resolve()  # ä½¿ç”¨ç»å¯¹è·¯å¾„è¿›è¡Œæ¯”è¾ƒ
        if not filepath.exists():
            print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {f}")
            return
        if not _check_file_format(filepath):
            return
        file_paths.append(filepath)

    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦ä¸è¾“å…¥æ–‡ä»¶å†²çª
    output_path = Path(output).resolve()
    use_temp_file = output_path in file_paths
    if use_temp_file:
        print("âš  æ£€æµ‹åˆ°è¾“å‡ºæ–‡ä»¶ä¸è¾“å…¥æ–‡ä»¶ç›¸åŒï¼Œå°†ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶")

    # æµå¼åˆ†æå­—æ®µï¼ˆåªè¯»å–æ¯ä¸ªæ–‡ä»¶çš„ç¬¬ä¸€è¡Œï¼‰
    print("ğŸ“Š æ–‡ä»¶å­—æ®µåˆ†æ:")
    file_fields = []  # [(filepath, fields)]

    for filepath in file_paths:
        try:
            # åªè¯»å–ç¬¬ä¸€è¡Œæ¥è·å–å­—æ®µï¼ˆæ ¹æ®æ ¼å¼é€‰æ‹©åŠ è½½æ–¹å¼ï¼‰
            if _is_streaming_supported(filepath):
                first_row = load_stream(str(filepath)).head(1).collect()
            else:
                # éæµå¼æ ¼å¼ï¼ˆå¦‚ .json, .xlsxï¼‰ä½¿ç”¨å…¨é‡åŠ è½½
                data = load_data(str(filepath))
                first_row = data[:1] if data else []
            if not first_row:
                print(f"è­¦å‘Š: æ–‡ä»¶ä¸ºç©º - {filepath}")
                fields = set()
            else:
                fields = set(first_row[0].keys())
        except Exception as e:
            print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ {filepath} - {e}")
            return

        file_fields.append((filepath, fields))
        fields_str = ", ".join(sorted(fields)) if fields else "(ç©º)"
        print(f"   {filepath.name}: {fields_str}")

    # åˆ†æå­—æ®µå·®å¼‚
    all_fields = set()
    common_fields = None
    for _, fields in file_fields:
        all_fields.update(fields)
        if common_fields is None:
            common_fields = fields.copy()
        else:
            common_fields &= fields

    common_fields = common_fields or set()
    diff_fields = all_fields - common_fields

    if diff_fields:
        if strict:
            print(f"\nâŒ ä¸¥æ ¼æ¨¡å¼: å­—æ®µä¸ä¸€è‡´")
            print(f"   å…±åŒå­—æ®µ: {', '.join(sorted(common_fields)) or '(æ— )'}")
            print(f"   å·®å¼‚å­—æ®µ: {', '.join(sorted(diff_fields))}")
            return
        else:
            print(f"\nâš  å­—æ®µå·®å¼‚: {', '.join(sorted(diff_fields))} ä»…åœ¨éƒ¨åˆ†æ–‡ä»¶ä¸­å­˜åœ¨")

    # æµå¼æ‹¼æ¥
    print("\nğŸ”„ æµå¼æ‹¼æ¥...")

    # å¦‚æœè¾“å‡ºæ–‡ä»¶ä¸è¾“å…¥æ–‡ä»¶å†²çªï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ï¼ˆåœ¨è¾“å‡ºæ–‡ä»¶åŒä¸€ç›®å½•ä¸‹ï¼‰
    if use_temp_file:
        output_dir = output_path.parent
        temp_fd, temp_path = tempfile.mkstemp(
            suffix=output_path.suffix,
            prefix=".tmp_",
            dir=output_dir,
        )
        os.close(temp_fd)
        actual_output = temp_path
        print(f"ğŸ’¾ å†™å…¥ä¸´æ—¶æ–‡ä»¶: {temp_path}")
    else:
        actual_output = output
        print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output}")

    try:
        total_count = _concat_streaming(file_paths, actual_output)

        # å¦‚æœä½¿ç”¨äº†ä¸´æ—¶æ–‡ä»¶ï¼Œé‡å‘½åä¸ºç›®æ ‡æ–‡ä»¶
        if use_temp_file:
            shutil.move(temp_path, output)
            print(f"ğŸ’¾ ç§»åŠ¨åˆ°ç›®æ ‡æ–‡ä»¶: {output}")
    except Exception as e:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if use_temp_file and os.path.exists(temp_path):
            os.unlink(temp_path)
        print(f"é”™è¯¯: æ‹¼æ¥å¤±è´¥ - {e}")
        return

    file_count = len(files)
    print(f"\nâœ… å®Œæˆ! å·²åˆå¹¶ {file_count} ä¸ªæ–‡ä»¶ï¼Œå…± {total_count} æ¡æ•°æ®åˆ° {output}")


def _concat_streaming(file_paths: List[Path], output: str) -> int:
    """æµå¼æ‹¼æ¥å¤šä¸ªæ–‡ä»¶"""
    from ..streaming import (
        StreamingTransformer,
        _stream_arrow,
        _stream_csv,
        _stream_jsonl,
        _stream_parquet,
    )

    def generator():
        for filepath in file_paths:
            ext = filepath.suffix.lower()
            if ext == ".jsonl":
                yield from _stream_jsonl(str(filepath))
            elif ext == ".csv":
                yield from _stream_csv(str(filepath))
            elif ext == ".parquet":
                yield from _stream_parquet(str(filepath))
            elif ext in (".arrow", ".feather"):
                yield from _stream_arrow(str(filepath))
            elif ext in (".json",):
                # JSON éœ€è¦å…¨é‡åŠ è½½
                data = load_data(str(filepath))
                yield from data
            elif ext in (".xlsx", ".xls"):
                # Excel éœ€è¦å…¨é‡åŠ è½½
                data = load_data(str(filepath))
                yield from data
            else:
                yield from _stream_jsonl(str(filepath))

    st = StreamingTransformer(generator())
    return st.save(output, show_progress=True)


# ============ Stats Command ============


def stats(
    filename: str,
    top: int = 10,
    full: bool = False,
) -> None:
    """
    æ˜¾ç¤ºæ•°æ®æ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯ã€‚

    é»˜è®¤å¿«é€Ÿæ¨¡å¼ï¼šåªç»Ÿè®¡è¡Œæ•°å’Œå­—æ®µç»“æ„ã€‚
    å®Œæ•´æ¨¡å¼ï¼ˆ--fullï¼‰ï¼šç»Ÿè®¡å€¼åˆ†å¸ƒã€å”¯ä¸€å€¼ã€é•¿åº¦ç­‰è¯¦ç»†ä¿¡æ¯ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        top: æ˜¾ç¤ºé¢‘ç‡æœ€é«˜çš„å‰ N ä¸ªå€¼ï¼Œé»˜è®¤ 10ï¼ˆä»…å®Œæ•´æ¨¡å¼ï¼‰
        full: å®Œæ•´æ¨¡å¼ï¼Œç»Ÿè®¡å€¼åˆ†å¸ƒã€å”¯ä¸€å€¼ç­‰è¯¦ç»†ä¿¡æ¯

    Examples:
        dt stats data.jsonl            # å¿«é€Ÿæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        dt stats data.jsonl --full     # å®Œæ•´æ¨¡å¼
        dt stats data.csv -f --top=5   # å®Œæ•´æ¨¡å¼ï¼Œæ˜¾ç¤º Top 5
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    if not full:
        _quick_stats(filepath)
        return

    # åŠ è½½æ•°æ®
    try:
        data = load_data(str(filepath))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    if not data:
        print("æ–‡ä»¶ä¸ºç©º")
        return

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total = len(data)
    field_stats = _compute_field_stats(data, top)

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    _print_stats(filepath.name, total, field_stats)


def _quick_stats(filepath: Path) -> None:
    """
    å¿«é€Ÿç»Ÿè®¡æ¨¡å¼ï¼šåªç»Ÿè®¡è¡Œæ•°å’Œå­—æ®µç»“æ„ï¼Œä¸éå†å…¨éƒ¨æ•°æ®ã€‚

    ç‰¹ç‚¹:
    - ä½¿ç”¨æµå¼è®¡æ•°ï¼Œä¸åŠ è½½å…¨éƒ¨æ•°æ®åˆ°å†…å­˜
    - åªè¯»å–å‰å‡ æ¡æ•°æ®æ¥æ¨æ–­å­—æ®µç»“æ„
    - ä¸è®¡ç®—å€¼åˆ†å¸ƒã€å”¯ä¸€å€¼ç­‰è€—æ—¶ç»Ÿè®¡
    """
    import orjson

    from ..streaming import _count_rows_fast

    ext = filepath.suffix.lower()
    file_size = filepath.stat().st_size

    # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
    def format_size(size: int) -> str:
        for unit in ["B", "KB", "MB", "GB"]:
            if size < 1024:
                return f"{size:.1f} {unit}"
            size /= 1024
        return f"{size:.1f} TB"

    # å¿«é€Ÿç»Ÿè®¡è¡Œæ•°
    total = _count_rows_fast(str(filepath))
    if total is None:
        # å›é€€ï¼šæ‰‹åŠ¨è®¡æ•°
        total = 0
        try:
            with open(filepath, "rb") as f:
                for line in f:
                    if line.strip():
                        total += 1
        except Exception:
            total = -1

    # è¯»å–å‰å‡ æ¡æ•°æ®æ¨æ–­å­—æ®µç»“æ„
    sample_data = []
    sample_size = 5
    try:
        if ext == ".jsonl":
            with open(filepath, "rb") as f:
                for i, line in enumerate(f):
                    if i >= sample_size:
                        break
                    line = line.strip()
                    if line:
                        sample_data.append(orjson.loads(line))
        elif ext == ".csv":
            import polars as pl

            df = pl.scan_csv(str(filepath)).head(sample_size).collect()
            sample_data = df.to_dicts()
        elif ext == ".parquet":
            import polars as pl

            df = pl.scan_parquet(str(filepath)).head(sample_size).collect()
            sample_data = df.to_dicts()
        elif ext in (".arrow", ".feather"):
            import polars as pl

            df = pl.scan_ipc(str(filepath)).head(sample_size).collect()
            sample_data = df.to_dicts()
        elif ext == ".json":
            with open(filepath, "rb") as f:
                data = orjson.loads(f.read())
                if isinstance(data, list):
                    sample_data = data[:sample_size]
    except Exception:
        pass

    # åˆ†æå­—æ®µç»“æ„
    fields = []
    if sample_data:
        all_keys = set()
        for item in sample_data:
            all_keys.update(item.keys())

        for key in sorted(all_keys):
            # ä»é‡‡æ ·æ•°æ®ä¸­æ¨æ–­ç±»å‹
            sample_values = [item.get(key) for item in sample_data if key in item]
            non_null = [v for v in sample_values if v is not None]
            if non_null:
                field_type = _infer_type(non_null)
            else:
                field_type = "unknown"
            fields.append({"field": key, "type": field_type})

    # è¾“å‡º
    try:
        from rich.console import Console
        from rich.panel import Panel
        from rich.table import Table

        console = Console()

        # æ¦‚è§ˆ
        console.print(
            Panel(
                f"[bold]æ–‡ä»¶:[/bold] {filepath.name}\n"
                f"[bold]å¤§å°:[/bold] {format_size(file_size)}\n"
                f"[bold]æ€»æ•°:[/bold] {total:,} æ¡\n"
                f"[bold]å­—æ®µ:[/bold] {len(fields)} ä¸ª",
                title="ğŸ“Š å¿«é€Ÿç»Ÿè®¡",
                expand=False,
            )
        )

        if fields:
            table = Table(title="ğŸ“‹ å­—æ®µç»“æ„", show_header=True, header_style="bold cyan")
            table.add_column("#", style="dim", justify="right")
            table.add_column("å­—æ®µ", style="green")
            table.add_column("ç±»å‹", style="yellow")

            for i, f in enumerate(fields, 1):
                table.add_row(str(i), f["field"], f["type"])

            console.print(table)

    except ImportError:
        # æ²¡æœ‰ richï¼Œä½¿ç”¨æ™®é€šæ‰“å°
        print(f"\n{'=' * 40}")
        print("ğŸ“Š å¿«é€Ÿç»Ÿè®¡")
        print(f"{'=' * 40}")
        print(f"æ–‡ä»¶: {filepath.name}")
        print(f"å¤§å°: {format_size(file_size)}")
        print(f"æ€»æ•°: {total:,} æ¡")
        print(f"å­—æ®µ: {len(fields)} ä¸ª")

        if fields:
            print(f"\nğŸ“‹ å­—æ®µç»“æ„:")
            for i, f in enumerate(fields, 1):
                print(f"  {i}. {f['field']} ({f['type']})")


def _compute_field_stats(data: List[Dict], top: int) -> List[Dict[str, Any]]:
    """
    å•æ¬¡éå†è®¡ç®—æ¯ä¸ªå­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯ã€‚

    ä¼˜åŒ–ï¼šå°†å¤šæ¬¡éå†åˆå¹¶ä¸ºå•æ¬¡éå†ï¼Œåœ¨éå†è¿‡ç¨‹ä¸­åŒæ—¶æ”¶é›†æ‰€æœ‰ç»Ÿè®¡æ•°æ®ã€‚
    """
    from collections import Counter, defaultdict

    if not data:
        return []

    total = len(data)

    # å•æ¬¡éå†æ”¶é›†æ‰€æœ‰å­—æ®µçš„å€¼å’Œç»Ÿè®¡ä¿¡æ¯
    field_values = defaultdict(list)  # å­˜å‚¨æ¯ä¸ªå­—æ®µçš„æ‰€æœ‰å€¼
    field_counters = defaultdict(Counter)  # å­˜å‚¨æ¯ä¸ªå­—æ®µçš„å€¼é¢‘ç‡ï¼ˆç”¨äº top Nï¼‰

    for item in data:
        for k, v in item.items():
            field_values[k].append(v)
            # å¯¹å€¼è¿›è¡Œæˆªæ–­åè®¡æ•°ï¼ˆç”¨äº top N æ˜¾ç¤ºï¼‰
            displayable = _truncate(v if v is not None else "", 30)
            field_counters[k][displayable] += 1

    # æ ¹æ®æ”¶é›†çš„æ•°æ®è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    stats_list = []
    for field in sorted(field_values.keys()):
        values = field_values[field]
        non_null = [v for v in values if v is not None and v != ""]
        non_null_count = len(non_null)

        # æ¨æ–­ç±»å‹ï¼ˆä»ç¬¬ä¸€ä¸ªéç©ºå€¼ï¼‰
        field_type = _infer_type(non_null)

        # åŸºç¡€ç»Ÿè®¡
        stat = {
            "field": field,
            "non_null": non_null_count,
            "null_rate": f"{(total - non_null_count) / total * 100:.1f}%",
            "type": field_type,
        }

        # ç±»å‹ç‰¹å®šç»Ÿè®¡
        if non_null:
            # å”¯ä¸€å€¼è®¡æ•°ï¼ˆå¯¹å¤æ‚ç±»å‹ä½¿ç”¨ hash èŠ‚çœå†…å­˜ï¼‰
            stat["unique"] = _count_unique(non_null, field_type)

            # å­—ç¬¦ä¸²ç±»å‹ï¼šè®¡ç®—é•¿åº¦ç»Ÿè®¡
            if field_type == "str":
                lengths = [len(str(v)) for v in non_null]
                stat["len_min"] = min(lengths)
                stat["len_max"] = max(lengths)
                stat["len_avg"] = sum(lengths) / len(lengths)

            # æ•°å€¼ç±»å‹ï¼šè®¡ç®—æ•°å€¼ç»Ÿè®¡
            elif field_type in ("int", "float"):
                nums = [float(v) for v in non_null if _is_numeric(v)]
                if nums:
                    stat["min"] = min(nums)
                    stat["max"] = max(nums)
                    stat["avg"] = sum(nums) / len(nums)

            # åˆ—è¡¨ç±»å‹ï¼šè®¡ç®—é•¿åº¦ç»Ÿè®¡
            elif field_type == "list":
                lengths = [len(v) if isinstance(v, list) else 0 for v in non_null]
                stat["len_min"] = min(lengths)
                stat["len_max"] = max(lengths)
                stat["len_avg"] = sum(lengths) / len(lengths)

            # Top N å€¼ï¼ˆå·²åœ¨éå†æ—¶æ”¶é›†ï¼‰
            stat["top_values"] = field_counters[field].most_common(top)

        stats_list.append(stat)

    return stats_list


def _count_unique(values: List[Any], field_type: str) -> int:
    """
    è®¡ç®—å”¯ä¸€å€¼æ•°é‡ã€‚

    å¯¹äºç®€å•ç±»å‹ç›´æ¥æ¯”è¾ƒï¼Œå¯¹äº list/dict æˆ–æ··åˆç±»å‹ä½¿ç”¨ hashã€‚
    """
    if field_type in ("list", "dict"):
        return _count_unique_by_hash(values)
    else:
        # ç®€å•ç±»å‹ï¼šå°è¯•ç›´æ¥æ¯”è¾ƒï¼Œå¤±è´¥åˆ™å›é€€åˆ° hash æ–¹å¼
        try:
            return len(set(values))
        except TypeError:
            # æ··åˆç±»å‹ï¼ˆå¦‚å­—æ®µä¸­æ—¢æœ‰ str åˆæœ‰ dictï¼‰ï¼Œå›é€€åˆ° hash
            return _count_unique_by_hash(values)


def _count_unique_by_hash(values: List[Any]) -> int:
    """ä½¿ç”¨ orjson åºåˆ—åŒ–åè®¡ç®— hash æ¥ç»Ÿè®¡å”¯ä¸€å€¼"""
    import hashlib

    import orjson

    seen = set()
    for v in values:
        try:
            h = hashlib.md5(orjson.dumps(v, option=orjson.OPT_SORT_KEYS)).digest()
            seen.add(h)
        except TypeError:
            # æ— æ³•åºåˆ—åŒ–çš„å€¼ï¼Œç”¨ repr å…œåº•
            seen.add(repr(v))
    return len(seen)


def _infer_type(values: List[Any]) -> str:
    """æ¨æ–­å­—æ®µç±»å‹"""
    if not values:
        return "unknown"

    sample = values[0]
    if isinstance(sample, bool):
        return "bool"
    if isinstance(sample, int):
        return "int"
    if isinstance(sample, float):
        return "float"
    if isinstance(sample, list):
        return "list"
    if isinstance(sample, dict):
        return "dict"
    return "str"


def _is_numeric(v: Any) -> bool:
    """æ£€æŸ¥å€¼æ˜¯å¦ä¸ºæ•°å€¼"""
    if isinstance(v, (int, float)) and not isinstance(v, bool):
        return True
    return False


def _truncate(v: Any, max_width: int) -> str:
    """æŒ‰æ˜¾ç¤ºå®½åº¦æˆªæ–­å€¼ï¼ˆä¸­æ–‡å­—ç¬¦ç®— 2 å®½åº¦ï¼‰"""
    s = str(v)
    width = 0
    result = []
    for char in s:
        # CJK å­—ç¬¦èŒƒå›´
        if (
            "\u4e00" <= char <= "\u9fff"
            or "\u3000" <= char <= "\u303f"
            or "\uff00" <= char <= "\uffef"
        ):
            char_width = 2
        else:
            char_width = 1
        if width + char_width > max_width - 3:  # é¢„ç•™ ... çš„å®½åº¦
            return "".join(result) + "..."
        result.append(char)
        width += char_width
    return s


def _display_width(s: str) -> int:
    """è®¡ç®—å­—ç¬¦ä¸²çš„æ˜¾ç¤ºå®½åº¦ï¼ˆä¸­æ–‡å­—ç¬¦ç®— 2ï¼ŒASCII å­—ç¬¦ç®— 1ï¼‰"""
    width = 0
    for char in s:
        # CJK å­—ç¬¦èŒƒå›´
        if (
            "\u4e00" <= char <= "\u9fff"
            or "\u3000" <= char <= "\u303f"
            or "\uff00" <= char <= "\uffef"
        ):
            width += 2
        else:
            width += 1
    return width


def _pad_to_width(s: str, target_width: int) -> str:
    """å°†å­—ç¬¦ä¸²å¡«å……åˆ°æŒ‡å®šçš„æ˜¾ç¤ºå®½åº¦"""
    current_width = _display_width(s)
    if current_width >= target_width:
        return s
    return s + " " * (target_width - current_width)


def _print_stats(filename: str, total: int, field_stats: List[Dict[str, Any]]) -> None:
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    try:
        from rich.console import Console
        from rich.panel import Panel
        from rich.table import Table

        console = Console()

        # æ¦‚è§ˆ
        console.print(
            Panel(
                f"[bold]æ–‡ä»¶:[/bold] {filename}\n"
                f"[bold]æ€»æ•°:[/bold] {total:,} æ¡\n"
                f"[bold]å­—æ®µ:[/bold] {len(field_stats)} ä¸ª",
                title="ğŸ“Š æ•°æ®æ¦‚è§ˆ",
                expand=False,
            )
        )

        # å­—æ®µç»Ÿè®¡è¡¨
        table = Table(title="ğŸ“‹ å­—æ®µç»Ÿè®¡", show_header=True, header_style="bold cyan")
        table.add_column("å­—æ®µ", style="green")
        table.add_column("ç±»å‹", style="yellow")
        table.add_column("éç©ºç‡", justify="right")
        table.add_column("å”¯ä¸€å€¼", justify="right")
        table.add_column("ç»Ÿè®¡", style="dim")

        for stat in field_stats:
            non_null_rate = f"{stat['non_null'] / total * 100:.0f}%"
            unique = str(stat.get("unique", "-"))

            # æ„å»ºç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²
            extra = []
            if "len_avg" in stat:
                extra.append(
                    f"é•¿åº¦: {stat['len_min']}-{stat['len_max']} (avg {stat['len_avg']:.0f})"
                )
            if "avg" in stat:
                if stat["type"] == "int":
                    extra.append(
                        f"èŒƒå›´: {int(stat['min'])}-{int(stat['max'])} (avg {stat['avg']:.1f})"
                    )
                else:
                    extra.append(
                        f"èŒƒå›´: {stat['min']:.2f}-{stat['max']:.2f} (avg {stat['avg']:.2f})"
                    )

            table.add_row(
                stat["field"],
                stat["type"],
                non_null_rate,
                unique,
                "; ".join(extra) if extra else "-",
            )

        console.print(table)

        # Top å€¼ç»Ÿè®¡ï¼ˆä»…æ˜¾ç¤ºæœ‰æ„ä¹‰çš„å­—æ®µï¼‰
        for stat in field_stats:
            top_values = stat.get("top_values", [])
            if not top_values:
                continue

            # è·³è¿‡æ•°å€¼ç±»å‹ï¼ˆmin/max/avg å·²è¶³å¤Ÿï¼‰
            if stat["type"] in ("int", "float"):
                continue

            # è·³è¿‡å”¯ä¸€å€¼è¿‡å¤šçš„å­—æ®µï¼ˆåŸºæœ¬éƒ½æ˜¯å”¯ä¸€çš„ï¼‰
            unique_ratio = stat.get("unique", 0) / total if total > 0 else 0
            if unique_ratio > 0.9 and stat.get("unique", 0) > 100:
                continue

            console.print(
                f"\n[bold cyan]{stat['field']}[/bold cyan] å€¼åˆ†å¸ƒ (Top {len(top_values)}):"
            )
            max_count = max(c for _, c in top_values) if top_values else 1
            for value, count in top_values:
                pct = count / total * 100
                bar_len = int(count / max_count * 20)  # æŒ‰ç›¸å¯¹æ¯”ä¾‹ï¼Œæœ€é•¿ 20 å­—ç¬¦
                bar = "â–ˆ" * bar_len
                display_value = value if value else "[ç©º]"
                # ä½¿ç”¨æ˜¾ç¤ºå®½åº¦å¯¹é½ï¼ˆå¤„ç†ä¸­æ–‡å­—ç¬¦ï¼‰
                padded_value = _pad_to_width(display_value, 32)
                console.print(f"  {padded_value} {count:>6} ({pct:>5.1f}%) {bar}")

    except ImportError:
        # æ²¡æœ‰ richï¼Œä½¿ç”¨æ™®é€šæ‰“å°
        print(f"\n{'=' * 50}")
        print(f"ğŸ“Š æ•°æ®æ¦‚è§ˆ")
        print(f"{'=' * 50}")
        print(f"æ–‡ä»¶: {filename}")
        print(f"æ€»æ•°: {total:,} æ¡")
        print(f"å­—æ®µ: {len(field_stats)} ä¸ª")

        print(f"\n{'=' * 50}")
        print(f"ğŸ“‹ å­—æ®µç»Ÿè®¡")
        print(f"{'=' * 50}")
        print(f"{'å­—æ®µ':<20} {'ç±»å‹':<8} {'éç©ºç‡':<8} {'å”¯ä¸€å€¼':<8}")
        print("-" * 50)

        for stat in field_stats:
            non_null_rate = f"{stat['non_null'] / total * 100:.0f}%"
            unique = str(stat.get("unique", "-"))
            print(f"{stat['field']:<20} {stat['type']:<8} {non_null_rate:<8} {unique:<8}")


# ============ Clean Command ============


def clean(
    filename: str,
    drop_empty: Optional[str] = None,
    min_len: Optional[str] = None,
    max_len: Optional[str] = None,
    keep: Optional[str] = None,
    drop: Optional[str] = None,
    strip: bool = False,
    output: Optional[str] = None,
) -> None:
    """
    æ•°æ®æ¸…æ´—ï¼ˆé»˜è®¤æµå¼å¤„ç†ï¼‰ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        drop_empty: åˆ é™¤ç©ºå€¼è®°å½•ï¼Œæ”¯æŒåµŒå¥—è·¯å¾„è¯­æ³•
            - ä¸å¸¦å€¼ï¼šåˆ é™¤ä»»æ„å­—æ®µä¸ºç©ºçš„è®°å½•
            - æŒ‡å®šå­—æ®µï¼šåˆ é™¤æŒ‡å®šå­—æ®µä¸ºç©ºçš„è®°å½•ï¼ˆé€—å·åˆ†éš”ï¼‰
        min_len: æœ€å°é•¿åº¦è¿‡æ»¤ï¼Œæ ¼å¼ "å­—æ®µ:é•¿åº¦"ï¼Œå­—æ®µæ”¯æŒåµŒå¥—è·¯å¾„
        max_len: æœ€å¤§é•¿åº¦è¿‡æ»¤ï¼Œæ ¼å¼ "å­—æ®µ:é•¿åº¦"ï¼Œå­—æ®µæ”¯æŒåµŒå¥—è·¯å¾„
        keep: åªä¿ç•™æŒ‡å®šå­—æ®µï¼ˆé€—å·åˆ†éš”ï¼Œä»…æ”¯æŒé¡¶å±‚å­—æ®µï¼‰
        drop: åˆ é™¤æŒ‡å®šå­—æ®µï¼ˆé€—å·åˆ†éš”ï¼Œä»…æ”¯æŒé¡¶å±‚å­—æ®µï¼‰
        strip: å»é™¤æ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µçš„é¦–å°¾ç©ºç™½
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™è¦†ç›–åŸæ–‡ä»¶

    Examples:
        dt clean data.jsonl --drop-empty                    # åˆ é™¤ä»»æ„ç©ºå€¼è®°å½•
        dt clean data.jsonl --drop-empty=text,answer        # åˆ é™¤æŒ‡å®šå­—æ®µä¸ºç©ºçš„è®°å½•
        dt clean data.jsonl --drop-empty=meta.source        # åˆ é™¤åµŒå¥—å­—æ®µä¸ºç©ºçš„è®°å½•
        dt clean data.jsonl --min-len=text:10               # text å­—æ®µæœ€å°‘ 10 å­—ç¬¦
        dt clean data.jsonl --min-len=messages.#:2          # è‡³å°‘ 2 æ¡æ¶ˆæ¯
        dt clean data.jsonl --max-len=messages[-1].content:500  # æœ€åä¸€æ¡æ¶ˆæ¯æœ€å¤š 500 å­—ç¬¦
        dt clean data.jsonl --keep=question,answer          # åªä¿ç•™è¿™äº›å­—æ®µ
        dt clean data.jsonl --drop=metadata,timestamp       # åˆ é™¤è¿™äº›å­—æ®µ
        dt clean data.jsonl --strip                         # å»é™¤å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # è§£æå‚æ•°
    min_len_field, min_len_value = _parse_len_param(min_len) if min_len else (None, None)
    max_len_field, max_len_value = _parse_len_param(max_len) if max_len else (None, None)
    keep_fields = _parse_field_list(keep) if keep else None
    drop_fields_set = set(_parse_field_list(drop)) if drop else None
    keep_set = set(keep_fields) if keep_fields else None

    # æ„å»ºæ¸…æ´—é…ç½®
    empty_fields = None
    if drop_empty is not None:
        if drop_empty == "" or drop_empty is True:
            print("ğŸ”„ åˆ é™¤ä»»æ„å­—æ®µä¸ºç©ºçš„è®°å½•...")
            empty_fields = []
        else:
            empty_fields = _parse_field_list(drop_empty)
            print(f"ğŸ”„ åˆ é™¤å­—æ®µä¸ºç©ºçš„è®°å½•: {', '.join(empty_fields)}")

    if strip:
        print("ğŸ”„ å»é™¤å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½...")
    if min_len_field:
        print(f"ğŸ”„ è¿‡æ»¤ {min_len_field} é•¿åº¦ < {min_len_value} çš„è®°å½•...")
    if max_len_field:
        print(f"ğŸ”„ è¿‡æ»¤ {max_len_field} é•¿åº¦ > {max_len_value} çš„è®°å½•...")
    if keep_fields:
        print(f"ğŸ”„ åªä¿ç•™å­—æ®µ: {', '.join(keep_fields)}")
    if drop_fields_set:
        print(f"ğŸ”„ åˆ é™¤å­—æ®µ: {', '.join(drop_fields_set)}")

    output_path = output or str(filepath)

    # æ£€æŸ¥è¾“å…¥è¾“å‡ºæ˜¯å¦ç›¸åŒï¼ˆæµå¼å¤„ç†éœ€è¦ä¸´æ—¶æ–‡ä»¶ï¼‰
    input_resolved = filepath.resolve()
    output_resolved = Path(output_path).resolve()
    use_temp_file = input_resolved == output_resolved

    # å¯¹äº JSONL æ–‡ä»¶ä½¿ç”¨æµå¼å¤„ç†
    if _is_streaming_supported(filepath):
        print(f"ğŸ“Š æµå¼åŠ è½½: {filepath}")

        # å¦‚æœè¾“å…¥è¾“å‡ºç›¸åŒï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        if use_temp_file:
            print("âš  æ£€æµ‹åˆ°è¾“å‡ºæ–‡ä»¶ä¸è¾“å…¥æ–‡ä»¶ç›¸åŒï¼Œå°†ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶")
            temp_fd, temp_path = tempfile.mkstemp(
                suffix=output_resolved.suffix,
                prefix=".tmp_",
                dir=output_resolved.parent,
            )
            os.close(temp_fd)
            actual_output = temp_path
        else:
            actual_output = output_path

        try:
            count = _clean_streaming(
                str(filepath),
                actual_output,
                strip=strip,
                empty_fields=empty_fields,
                min_len_field=min_len_field,
                min_len_value=min_len_value,
                max_len_field=max_len_field,
                max_len_value=max_len_value,
                keep_set=keep_set,
                drop_fields_set=drop_fields_set,
            )

            # å¦‚æœä½¿ç”¨äº†ä¸´æ—¶æ–‡ä»¶ï¼Œç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
            if use_temp_file:
                shutil.move(temp_path, output_path)

            print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
            print(f"\nâœ… å®Œæˆ! æ¸…æ´—å {count} æ¡æ•°æ®")
        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if use_temp_file and os.path.exists(temp_path):
                os.unlink(temp_path)
            print(f"é”™è¯¯: æ¸…æ´—å¤±è´¥ - {e}")
            import traceback

            traceback.print_exc()
        return

    # é JSONL æ–‡ä»¶ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {filepath}")
    try:
        dt = DataTransformer.load(str(filepath))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    original_count = len(dt)
    print(f"   å…± {original_count} æ¡æ•°æ®")

    # å•æ¬¡éå†æ‰§è¡Œæ‰€æœ‰æ¸…æ´—æ“ä½œ
    data, step_stats = _clean_data_single_pass(
        dt.data,
        strip=strip,
        empty_fields=empty_fields,
        min_len_field=min_len_field,
        min_len_value=min_len_value,
        max_len_field=max_len_field,
        max_len_value=max_len_value,
        keep_fields=keep_fields,
        drop_fields=drop_fields_set,
    )

    # ä¿å­˜ç»“æœ
    final_count = len(data)
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")

    try:
        save_data(data, output_path)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•ä¿å­˜æ–‡ä»¶ - {e}")
        return

    # æ‰“å°ç»Ÿè®¡
    removed_count = original_count - final_count
    print(f"\nâœ… å®Œæˆ!")
    print(f"   åŸå§‹: {original_count} æ¡ -> æ¸…æ´—å: {final_count} æ¡ (åˆ é™¤ {removed_count} æ¡)")
    if step_stats:
        print(f"   æ­¥éª¤: {' | '.join(step_stats)}")


def _parse_len_param(param: str) -> tuple:
    """è§£æé•¿åº¦å‚æ•°ï¼Œæ ¼å¼ 'field:length'"""
    if ":" not in param:
        raise ValueError(f"é•¿åº¦å‚æ•°æ ¼å¼é”™è¯¯: {param}ï¼Œåº”ä¸º 'å­—æ®µ:é•¿åº¦'")
    parts = param.split(":", 1)
    field = parts[0].strip()
    try:
        length = int(parts[1].strip())
    except ValueError:
        raise ValueError(f"é•¿åº¦å¿…é¡»æ˜¯æ•´æ•°: {parts[1]}")
    return field, length


def _parse_field_list(value: Any) -> List[str]:
    """è§£æå­—æ®µåˆ—è¡¨å‚æ•°ï¼ˆå¤„ç† fire å°†é€—å·åˆ†éš”çš„å€¼è§£æä¸ºå…ƒç»„çš„æƒ…å†µï¼‰"""
    if isinstance(value, (list, tuple)):
        return [str(f).strip() for f in value]
    elif isinstance(value, str):
        return [f.strip() for f in value.split(",")]
    else:
        return [str(value)]


def _is_empty_value(v: Any) -> bool:
    """åˆ¤æ–­å€¼æ˜¯å¦ä¸ºç©º"""
    if v is None:
        return True
    if isinstance(v, str) and v.strip() == "":
        return True
    if isinstance(v, (list, dict)) and len(v) == 0:
        return True
    return False


def _get_value_len(value: Any) -> int:
    """
    è·å–å€¼çš„é•¿åº¦ã€‚

    - str/list/dict: è¿”å› len()
    - int/float: ç›´æ¥è¿”å›è¯¥æ•°å€¼ï¼ˆç”¨äº messages.# è¿™ç§è¿”å›æ•°é‡çš„åœºæ™¯ï¼‰
    - None: è¿”å› 0
    - å…¶ä»–: è½¬ä¸ºå­—ç¬¦ä¸²åè¿”å›é•¿åº¦
    """
    if value is None:
        return 0
    if isinstance(value, (int, float)):
        return int(value)
    if isinstance(value, (str, list, dict)):
        return len(value)
    return len(str(value))


def _clean_data_single_pass(
    data: List[Dict],
    strip: bool = False,
    empty_fields: Optional[List[str]] = None,
    min_len_field: Optional[str] = None,
    min_len_value: Optional[int] = None,
    max_len_field: Optional[str] = None,
    max_len_value: Optional[int] = None,
    keep_fields: Optional[List[str]] = None,
    drop_fields: Optional[set] = None,
) -> tuple:
    """
    å•æ¬¡éå†æ‰§è¡Œæ‰€æœ‰æ¸…æ´—æ“ä½œã€‚

    Args:
        data: åŸå§‹æ•°æ®åˆ—è¡¨
        strip: æ˜¯å¦å»é™¤å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½
        empty_fields: æ£€æŸ¥ç©ºå€¼çš„å­—æ®µåˆ—è¡¨ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰ï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºæ£€æŸ¥æ‰€æœ‰å­—æ®µï¼ŒNone è¡¨ç¤ºä¸æ£€æŸ¥
        min_len_field: æœ€å°é•¿åº¦æ£€æŸ¥çš„å­—æ®µï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        min_len_value: æœ€å°é•¿åº¦å€¼
        max_len_field: æœ€å¤§é•¿åº¦æ£€æŸ¥çš„å­—æ®µï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        max_len_value: æœ€å¤§é•¿åº¦å€¼
        keep_fields: åªä¿ç•™çš„å­—æ®µåˆ—è¡¨ï¼ˆä»…æ”¯æŒé¡¶å±‚å­—æ®µï¼‰
        drop_fields: è¦åˆ é™¤çš„å­—æ®µé›†åˆï¼ˆä»…æ”¯æŒé¡¶å±‚å­—æ®µï¼‰

    Returns:
        (æ¸…æ´—åçš„æ•°æ®, ç»Ÿè®¡ä¿¡æ¯åˆ—è¡¨)
    """
    result = []
    stats = {
        "drop_empty": 0,
        "min_len": 0,
        "max_len": 0,
    }

    # é¢„å…ˆè®¡ç®— keep_fields é›†åˆï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    keep_set = set(keep_fields) if keep_fields else None

    for item in data:
        # 1. strip å¤„ç†ï¼ˆåœ¨è¿‡æ»¤å‰æ‰§è¡Œï¼Œè¿™æ ·ç©ºå€¼æ£€æµ‹æ›´å‡†ç¡®ï¼‰
        if strip:
            item = {k: v.strip() if isinstance(v, str) else v for k, v in item.items()}

        # 2. ç©ºå€¼è¿‡æ»¤
        if empty_fields is not None:
            if len(empty_fields) == 0:
                # æ£€æŸ¥æ‰€æœ‰å­—æ®µ
                if any(_is_empty_value(v) for v in item.values()):
                    stats["drop_empty"] += 1
                    continue
            else:
                # æ£€æŸ¥æŒ‡å®šå­—æ®µï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
                if any(_is_empty_value(get_field_with_spec(item, f)) for f in empty_fields):
                    stats["drop_empty"] += 1
                    continue

        # 3. æœ€å°é•¿åº¦è¿‡æ»¤ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        if min_len_field is not None:
            if _get_value_len(get_field_with_spec(item, min_len_field, default="")) < min_len_value:
                stats["min_len"] += 1
                continue

        # 4. æœ€å¤§é•¿åº¦è¿‡æ»¤ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        if max_len_field is not None:
            if _get_value_len(get_field_with_spec(item, max_len_field, default="")) > max_len_value:
                stats["max_len"] += 1
                continue

        # 5. å­—æ®µç®¡ç†ï¼ˆkeep/dropï¼‰
        if keep_set is not None:
            item = {k: v for k, v in item.items() if k in keep_set}
        elif drop_fields is not None:
            item = {k: v for k, v in item.items() if k not in drop_fields}

        result.append(item)

    # æ„å»ºç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²åˆ—è¡¨
    step_stats = []
    if strip:
        step_stats.append("strip")
    if stats["drop_empty"] > 0:
        step_stats.append(f"drop-empty: -{stats['drop_empty']}")
    if stats["min_len"] > 0:
        step_stats.append(f"min-len: -{stats['min_len']}")
    if stats["max_len"] > 0:
        step_stats.append(f"max-len: -{stats['max_len']}")
    if keep_fields:
        step_stats.append(f"keep: {len(keep_fields)} å­—æ®µ")
    if drop_fields:
        step_stats.append(f"drop: {len(drop_fields)} å­—æ®µ")

    return result, step_stats


def _clean_streaming(
    input_path: str,
    output_path: str,
    strip: bool = False,
    empty_fields: Optional[List[str]] = None,
    min_len_field: Optional[str] = None,
    min_len_value: Optional[int] = None,
    max_len_field: Optional[str] = None,
    max_len_value: Optional[int] = None,
    keep_set: Optional[set] = None,
    drop_fields_set: Optional[set] = None,
) -> int:
    """
    æµå¼æ¸…æ´—æ•°æ®ã€‚

    Returns:
        å¤„ç†åçš„æ•°æ®æ¡æ•°
    """

    def clean_filter(item: Dict) -> bool:
        """è¿‡æ»¤å‡½æ•°ï¼šè¿”å› True ä¿ç•™ï¼ŒFalse è¿‡æ»¤ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰"""
        # ç©ºå€¼è¿‡æ»¤
        if empty_fields is not None:
            if len(empty_fields) == 0:
                if any(_is_empty_value(v) for v in item.values()):
                    return False
            else:
                # æ”¯æŒåµŒå¥—è·¯å¾„
                if any(_is_empty_value(get_field_with_spec(item, f)) for f in empty_fields):
                    return False

        # æœ€å°é•¿åº¦è¿‡æ»¤ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        if min_len_field is not None:
            if _get_value_len(get_field_with_spec(item, min_len_field, default="")) < min_len_value:
                return False

        # æœ€å¤§é•¿åº¦è¿‡æ»¤ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        if max_len_field is not None:
            if _get_value_len(get_field_with_spec(item, max_len_field, default="")) > max_len_value:
                return False

        return True

    def clean_transform(item: Dict) -> Dict:
        """è½¬æ¢å‡½æ•°ï¼šstrip + å­—æ®µç®¡ç†"""
        # strip å¤„ç†
        if strip:
            item = {k: v.strip() if isinstance(v, str) else v for k, v in item.items()}

        # å­—æ®µç®¡ç†
        if keep_set is not None:
            item = {k: v for k, v in item.items() if k in keep_set}
        elif drop_fields_set is not None:
            item = {k: v for k, v in item.items() if k not in drop_fields_set}

        return item

    # æ„å»ºæµå¼å¤„ç†é“¾
    st = load_stream(input_path)

    # å¦‚æœéœ€è¦ stripï¼Œå…ˆæ‰§è¡Œ strip è½¬æ¢ï¼ˆåœ¨è¿‡æ»¤ä¹‹å‰ï¼Œè¿™æ ·ç©ºå€¼æ£€æµ‹æ›´å‡†ç¡®ï¼‰
    if strip:
        st = st.transform(
            lambda x: {k: v.strip() if isinstance(v, str) else v for k, v in x.items()}
        )

    # æ‰§è¡Œè¿‡æ»¤
    if empty_fields is not None or min_len_field is not None or max_len_field is not None:
        st = st.filter(clean_filter)

    # æ‰§è¡Œå­—æ®µç®¡ç†ï¼ˆå¦‚æœæ²¡æœ‰ stripï¼Œä¹Ÿéœ€è¦åœ¨è¿™é‡Œå¤„ç†ï¼‰
    if keep_set is not None or drop_fields_set is not None:

        def field_transform(item):
            if keep_set is not None:
                return {k: v for k, v in item.items() if k in keep_set}
            elif drop_fields_set is not None:
                return {k: v for k, v in item.items() if k not in drop_fields_set}
            return item

        st = st.transform(field_transform)

    return st.save(output_path)


# ============ Run Command ============


def run(
    config: str,
    input: Optional[str] = None,
    output: Optional[str] = None,
) -> None:
    """
    æ‰§è¡Œ Pipeline é…ç½®æ–‡ä»¶ã€‚

    Args:
        config: Pipeline YAML é…ç½®æ–‡ä»¶è·¯å¾„
        input: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆè¦†ç›–é…ç½®ä¸­çš„ inputï¼‰
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆè¦†ç›–é…ç½®ä¸­çš„ outputï¼‰

    Examples:
        dt run pipeline.yaml
        dt run pipeline.yaml --input=new_data.jsonl
        dt run pipeline.yaml --input=data.jsonl --output=result.jsonl
    """
    config_path = Path(config)

    if not config_path.exists():
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ - {config}")
        return

    if config_path.suffix.lower() not in (".yaml", ".yml"):
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶å¿…é¡»æ˜¯ YAML æ ¼å¼ (.yaml æˆ– .yml)")
        return

    # éªŒè¯é…ç½®
    errors = validate_pipeline(config)
    if errors:
        print("âŒ é…ç½®æ–‡ä»¶éªŒè¯å¤±è´¥:")
        for err in errors:
            print(f"   - {err}")
        return

    # æ‰§è¡Œ pipeline
    try:
        run_pipeline(config, input_file=input, output_file=output, verbose=True)
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()


# ============ Token Stats Command ============


def token_stats(
    filename: str,
    field: str = "messages",
    model: str = "cl100k_base",
    detailed: bool = False,
) -> None:
    """
    ç»Ÿè®¡æ•°æ®é›†çš„ Token ä¿¡æ¯ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„
        field: è¦ç»Ÿè®¡çš„å­—æ®µï¼ˆé»˜è®¤ messagesï¼‰ï¼Œæ”¯æŒåµŒå¥—è·¯å¾„è¯­æ³•
        model: åˆ†è¯å™¨: cl100k_base (é»˜è®¤), qwen2.5, llama3, gpt-4 ç­‰
        detailed: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡

    Examples:
        dt token-stats data.jsonl
        dt token-stats data.jsonl --field=text --model=qwen2.5
        dt token-stats data.jsonl --field=conversation.messages
        dt token-stats data.jsonl --field=messages[-1].content   # ç»Ÿè®¡æœ€åä¸€æ¡æ¶ˆæ¯
        dt token-stats data.jsonl --detailed
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {filepath}")
    try:
        data = load_data(str(filepath))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    if not data:
        print("æ–‡ä»¶ä¸ºç©º")
        return

    total = len(data)
    print(f"   å…± {total} æ¡æ•°æ®")
    print(f"ğŸ”¢ ç»Ÿè®¡ Token (æ¨¡å‹: {model}, å­—æ®µ: {field})...")

    # æ£€æŸ¥å­—æ®µç±»å‹å¹¶é€‰æ‹©åˆé€‚çš„ç»Ÿè®¡æ–¹æ³•ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
    sample = data[0]
    field_value = get_field_with_spec(sample, field)

    try:
        if isinstance(field_value, list) and field_value and isinstance(field_value[0], dict):
            # messages æ ¼å¼
            from ..tokenizers import messages_token_stats

            stats = messages_token_stats(data, messages_field=field, model=model)
            _print_messages_token_stats(stats, detailed)
        else:
            # æ™®é€šæ–‡æœ¬å­—æ®µ
            from ..tokenizers import token_stats as compute_token_stats

            stats = compute_token_stats(data, fields=field, model=model)
            _print_text_token_stats(stats, detailed)
    except ImportError as e:
        print(f"é”™è¯¯: {e}")
        return
    except Exception as e:
        print(f"é”™è¯¯: ç»Ÿè®¡å¤±è´¥ - {e}")
        import traceback

        traceback.print_exc()


def _print_messages_token_stats(stats: Dict[str, Any], detailed: bool) -> None:
    """æ‰“å° messages æ ¼å¼çš„ token ç»Ÿè®¡"""
    try:
        from rich.console import Console
        from rich.panel import Panel
        from rich.table import Table

        console = Console()

        # æ¦‚è§ˆ
        overview = (
            f"[bold]æ€»æ ·æœ¬æ•°:[/bold] {stats['count']:,}\n"
            f"[bold]æ€» Token:[/bold] {stats['total_tokens']:,}\n"
            f"[bold]å¹³å‡ Token:[/bold] {stats['avg_tokens']:,}\n"
            f"[bold]ä¸­ä½æ•°:[/bold] {stats['median_tokens']:,}\n"
            f"[bold]èŒƒå›´:[/bold] {stats['min_tokens']:,} - {stats['max_tokens']:,}"
        )
        console.print(Panel(overview, title="ğŸ“Š Token ç»Ÿè®¡æ¦‚è§ˆ", expand=False))

        if detailed:
            # è¯¦ç»†ç»Ÿè®¡
            table = Table(title="ğŸ“‹ åˆ†è§’è‰²ç»Ÿè®¡")
            table.add_column("è§’è‰²", style="cyan")
            table.add_column("Token æ•°", justify="right")
            table.add_column("å æ¯”", justify="right")

            total = stats["total_tokens"]
            for role, key in [
                ("User", "user_tokens"),
                ("Assistant", "assistant_tokens"),
                ("System", "system_tokens"),
            ]:
                tokens = stats.get(key, 0)
                pct = tokens / total * 100 if total > 0 else 0
                table.add_row(role, f"{tokens:,}", f"{pct:.1f}%")

            console.print(table)
            console.print(f"\nå¹³å‡å¯¹è¯è½®æ•°: {stats.get('avg_turns', 0)}")

    except ImportError:
        # æ²¡æœ‰ richï¼Œä½¿ç”¨æ™®é€šæ‰“å°
        print(f"\n{'=' * 40}")
        print("ğŸ“Š Token ç»Ÿè®¡æ¦‚è§ˆ")
        print(f"{'=' * 40}")
        print(f"æ€»æ ·æœ¬æ•°: {stats['count']:,}")
        print(f"æ€» Token: {stats['total_tokens']:,}")
        print(f"å¹³å‡ Token: {stats['avg_tokens']:,}")
        print(f"ä¸­ä½æ•°: {stats['median_tokens']:,}")
        print(f"èŒƒå›´: {stats['min_tokens']:,} - {stats['max_tokens']:,}")

        if detailed:
            print(f"\n{'=' * 40}")
            print("ğŸ“‹ åˆ†è§’è‰²ç»Ÿè®¡")
            print(f"{'=' * 40}")
            total = stats["total_tokens"]
            for role, key in [
                ("User", "user_tokens"),
                ("Assistant", "assistant_tokens"),
                ("System", "system_tokens"),
            ]:
                tokens = stats.get(key, 0)
                pct = tokens / total * 100 if total > 0 else 0
                print(f"{role}: {tokens:,} ({pct:.1f}%)")
            print(f"\nå¹³å‡å¯¹è¯è½®æ•°: {stats.get('avg_turns', 0)}")


def _print_text_token_stats(stats: Dict[str, Any], detailed: bool) -> None:
    """æ‰“å°æ™®é€šæ–‡æœ¬çš„ token ç»Ÿè®¡"""
    try:
        from rich.console import Console
        from rich.panel import Panel

        console = Console()

        overview = (
            f"[bold]æ€»æ ·æœ¬æ•°:[/bold] {stats['count']:,}\n"
            f"[bold]æ€» Token:[/bold] {stats['total_tokens']:,}\n"
            f"[bold]å¹³å‡ Token:[/bold] {stats['avg_tokens']:.1f}\n"
            f"[bold]ä¸­ä½æ•°:[/bold] {stats['median_tokens']:,}\n"
            f"[bold]èŒƒå›´:[/bold] {stats['min_tokens']:,} - {stats['max_tokens']:,}"
        )
        console.print(Panel(overview, title="ğŸ“Š Token ç»Ÿè®¡", expand=False))

    except ImportError:
        print(f"\n{'=' * 40}")
        print("ğŸ“Š Token ç»Ÿè®¡")
        print(f"{'=' * 40}")
        print(f"æ€»æ ·æœ¬æ•°: {stats['count']:,}")
        print(f"æ€» Token: {stats['total_tokens']:,}")
        print(f"å¹³å‡ Token: {stats['avg_tokens']:.1f}")
        print(f"ä¸­ä½æ•°: {stats['median_tokens']:,}")
        print(f"èŒƒå›´: {stats['min_tokens']:,} - {stats['max_tokens']:,}")


# ============ Diff Command ============


def diff(
    file1: str,
    file2: str,
    key: Optional[str] = None,
    output: Optional[str] = None,
) -> None:
    """
    å¯¹æ¯”ä¸¤ä¸ªæ•°æ®é›†çš„å·®å¼‚ã€‚

    Args:
        file1: ç¬¬ä¸€ä¸ªæ–‡ä»¶è·¯å¾„
        file2: ç¬¬äºŒä¸ªæ–‡ä»¶è·¯å¾„
        key: ç”¨äºåŒ¹é…çš„é”®å­—æ®µï¼Œæ”¯æŒåµŒå¥—è·¯å¾„è¯­æ³•ï¼ˆå¯é€‰ï¼‰
        output: å·®å¼‚æŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰

    Examples:
        dt diff v1/train.jsonl v2/train.jsonl
        dt diff a.jsonl b.jsonl --key=id
        dt diff a.jsonl b.jsonl --key=meta.uuid   # æŒ‰åµŒå¥—å­—æ®µåŒ¹é…
        dt diff a.jsonl b.jsonl --output=diff_report.json
    """
    path1 = Path(file1)
    path2 = Path(file2)

    # éªŒè¯æ–‡ä»¶
    for p, name in [(path1, "file1"), (path2, "file2")]:
        if not p.exists():
            print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {p}")
            return
        if not _check_file_format(p):
            return

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½æ•°æ®...")
    try:
        data1 = load_data(str(path1))
        data2 = load_data(str(path2))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    print(f"   æ–‡ä»¶1: {path1.name} ({len(data1)} æ¡)")
    print(f"   æ–‡ä»¶2: {path2.name} ({len(data2)} æ¡)")

    # è®¡ç®—å·®å¼‚
    print("ğŸ” è®¡ç®—å·®å¼‚...")
    diff_result = _compute_diff(data1, data2, key)

    # æ‰“å°å·®å¼‚æŠ¥å‘Š
    _print_diff_report(diff_result, path1.name, path2.name)

    # ä¿å­˜æŠ¥å‘Š
    if output:
        print(f"\nğŸ’¾ ä¿å­˜æŠ¥å‘Š: {output}")
        save_data([diff_result], output)


def _compute_diff(
    data1: List[Dict],
    data2: List[Dict],
    key: Optional[str] = None,
) -> Dict[str, Any]:
    """è®¡ç®—ä¸¤ä¸ªæ•°æ®é›†çš„å·®å¼‚"""
    result = {
        "summary": {
            "file1_count": len(data1),
            "file2_count": len(data2),
            "added": 0,
            "removed": 0,
            "modified": 0,
            "unchanged": 0,
        },
        "field_changes": {},
        "details": {
            "added": [],
            "removed": [],
            "modified": [],
        },
    }

    if key:
        # åŸºäº key çš„ç²¾ç¡®åŒ¹é…ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        dict1 = {get_field_with_spec(item, key): item for item in data1 if get_field_with_spec(item, key) is not None}
        dict2 = {get_field_with_spec(item, key): item for item in data2 if get_field_with_spec(item, key) is not None}

        keys1 = set(dict1.keys())
        keys2 = set(dict2.keys())

        # æ–°å¢
        added_keys = keys2 - keys1
        result["summary"]["added"] = len(added_keys)
        result["details"]["added"] = [dict2[k] for k in list(added_keys)[:10]]  # æœ€å¤šæ˜¾ç¤º 10 æ¡

        # åˆ é™¤
        removed_keys = keys1 - keys2
        result["summary"]["removed"] = len(removed_keys)
        result["details"]["removed"] = [dict1[k] for k in list(removed_keys)[:10]]

        # ä¿®æ”¹/æœªå˜
        common_keys = keys1 & keys2
        for k in common_keys:
            if dict1[k] == dict2[k]:
                result["summary"]["unchanged"] += 1
            else:
                result["summary"]["modified"] += 1
                if len(result["details"]["modified"]) < 10:
                    result["details"]["modified"].append(
                        {
                            "key": k,
                            "before": dict1[k],
                            "after": dict2[k],
                        }
                    )
    else:
        # åŸºäºå“ˆå¸Œçš„æ¯”è¾ƒ
        def _hash_item(item):
            return orjson.dumps(item, option=orjson.OPT_SORT_KEYS)

        set1 = {_hash_item(item) for item in data1}
        set2 = {_hash_item(item) for item in data2}

        added = set2 - set1
        removed = set1 - set2
        unchanged = set1 & set2

        result["summary"]["added"] = len(added)
        result["summary"]["removed"] = len(removed)
        result["summary"]["unchanged"] = len(unchanged)

        # è¯¦æƒ…
        result["details"]["added"] = [orjson.loads(h) for h in list(added)[:10]]
        result["details"]["removed"] = [orjson.loads(h) for h in list(removed)[:10]]

    # å­—æ®µå˜åŒ–åˆ†æ
    fields1 = set()
    fields2 = set()
    for item in data1[:1000]:  # é‡‡æ ·åˆ†æ
        fields1.update(item.keys())
    for item in data2[:1000]:
        fields2.update(item.keys())

    result["field_changes"] = {
        "added_fields": list(fields2 - fields1),
        "removed_fields": list(fields1 - fields2),
        "common_fields": list(fields1 & fields2),
    }

    return result


def _print_diff_report(diff_result: Dict[str, Any], name1: str, name2: str) -> None:
    """æ‰“å°å·®å¼‚æŠ¥å‘Š"""
    summary = diff_result["summary"]
    field_changes = diff_result["field_changes"]

    try:
        from rich.console import Console
        from rich.panel import Panel
        from rich.table import Table

        console = Console()

        # æ¦‚è§ˆ
        overview = (
            f"[bold]{name1}:[/bold] {summary['file1_count']:,} æ¡\n"
            f"[bold]{name2}:[/bold] {summary['file2_count']:,} æ¡\n"
            f"\n"
            f"[green]+ æ–°å¢:[/green] {summary['added']:,} æ¡\n"
            f"[red]- åˆ é™¤:[/red] {summary['removed']:,} æ¡\n"
            f"[yellow]~ ä¿®æ”¹:[/yellow] {summary['modified']:,} æ¡\n"
            f"[dim]= æœªå˜:[/dim] {summary['unchanged']:,} æ¡"
        )
        console.print(Panel(overview, title="ğŸ“Š å·®å¼‚æ¦‚è§ˆ", expand=False))

        # å­—æ®µå˜åŒ–
        if field_changes["added_fields"] or field_changes["removed_fields"]:
            console.print("\n[bold]ğŸ“‹ å­—æ®µå˜åŒ–:[/bold]")
            if field_changes["added_fields"]:
                console.print(
                    f"  [green]+ æ–°å¢å­—æ®µ:[/green] {', '.join(field_changes['added_fields'])}"
                )
            if field_changes["removed_fields"]:
                console.print(
                    f"  [red]- åˆ é™¤å­—æ®µ:[/red] {', '.join(field_changes['removed_fields'])}"
                )

    except ImportError:
        print(f"\n{'=' * 50}")
        print("ğŸ“Š å·®å¼‚æ¦‚è§ˆ")
        print(f"{'=' * 50}")
        print(f"{name1}: {summary['file1_count']:,} æ¡")
        print(f"{name2}: {summary['file2_count']:,} æ¡")
        print()
        print(f"+ æ–°å¢: {summary['added']:,} æ¡")
        print(f"- åˆ é™¤: {summary['removed']:,} æ¡")
        print(f"~ ä¿®æ”¹: {summary['modified']:,} æ¡")
        print(f"= æœªå˜: {summary['unchanged']:,} æ¡")

        if field_changes["added_fields"] or field_changes["removed_fields"]:
            print(f"\nğŸ“‹ å­—æ®µå˜åŒ–:")
            if field_changes["added_fields"]:
                print(f"  + æ–°å¢å­—æ®µ: {', '.join(field_changes['added_fields'])}")
            if field_changes["removed_fields"]:
                print(f"  - åˆ é™¤å­—æ®µ: {', '.join(field_changes['removed_fields'])}")


# ============ History Command ============


def history(
    filename: str,
    json: bool = False,
) -> None:
    """
    æ˜¾ç¤ºæ•°æ®æ–‡ä»¶çš„è¡€ç¼˜å†å²ã€‚

    Args:
        filename: æ•°æ®æ–‡ä»¶è·¯å¾„
        json: ä»¥ JSON æ ¼å¼è¾“å‡º

    Examples:
        dt history data.jsonl
        dt history data.jsonl --json
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not has_lineage(str(filepath)):
        print(f"æ–‡ä»¶ {filename} æ²¡æœ‰è¡€ç¼˜è®°å½•")
        print("\næç¤º: ä½¿ç”¨ track_lineage=True åŠ è½½æ•°æ®ï¼Œå¹¶åœ¨ä¿å­˜æ—¶ä½¿ç”¨ lineage=True æ¥è®°å½•è¡€ç¼˜")
        print("ç¤ºä¾‹:")
        print("  dt = DataTransformer.load('data.jsonl', track_lineage=True)")
        print("  dt.filter(...).transform(...).save('output.jsonl', lineage=True)")
        return

    if json:
        # JSON æ ¼å¼è¾“å‡º
        chain = get_lineage_chain(str(filepath))
        output = [record.to_dict() for record in chain]
        print(orjson.dumps(output, option=orjson.OPT_INDENT_2).decode("utf-8"))
    else:
        # æ ¼å¼åŒ–æŠ¥å‘Š
        report = format_lineage_report(str(filepath))
        print(report)
