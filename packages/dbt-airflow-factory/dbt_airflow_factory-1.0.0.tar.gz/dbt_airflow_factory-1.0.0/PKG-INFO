Metadata-Version: 2.4
Name: dbt-airflow-factory
Version: 1.0.0
Summary: Library to convert DBT manifest metadata to Airflow tasks
Home-page: https://github.com/getindata/dbt-airflow-factory/
Author: Piotr Pekala
Author-email: piotr.pekala@getindata.com
License: Apache Software License (Apache 2.0)
Keywords: dbt airflow manifest parser python
Classifier: Development Status :: 5 - Production/Stable
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pytimeparse<2,>=1.1
Requires-Dist: astronomer-cosmos<2.0,>=1.10.0
Requires-Dist: apache-airflow<3,>=2.5
Requires-Dist: apache-airflow-providers-cncf-kubernetes
Requires-Dist: apache-airflow-providers-docker
Requires-Dist: apache-airflow-providers-slack
Requires-Dist: apache-airflow-providers-airbyte>=3.1
Provides-Extra: tests
Requires-Dist: pytest<7.0.0,>=6.2.2; extra == "tests"
Requires-Dist: pytest-cov<3.0.0,>=2.8.0; extra == "tests"
Requires-Dist: tox==3.21.1; extra == "tests"
Requires-Dist: pre-commit==2.9.3; extra == "tests"
Requires-Dist: pandas<2.0.0,>=1.2.5; extra == "tests"
Provides-Extra: docs
Requires-Dist: sphinx==4.3.1; extra == "docs"
Requires-Dist: sphinx-rtd-theme==1.0.0; extra == "docs"
Requires-Dist: sphinx-click<3.1,>=3.0; extra == "docs"
Requires-Dist: myst-parser<0.17,>=0.16; extra == "docs"
Requires-Dist: docutils<0.17; extra == "docs"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: license-file
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# DBT Airflow Factory

[![Python Version](https://img.shields.io/badge/python-3.9%20%7C%203.10%20%7C%203.11-blue)](https://github.com/getindata/dbt-airflow-factory)
[![PyPI Version](https://badge.fury.io/py/dbt-airflow-factory.svg)](https://pypi.org/project/dbt-airflow-factory/)
[![Downloads](https://pepy.tech/badge/dbt-airflow-factory)](https://pepy.tech/project/dbt-airflow-factory)
[![Maintainability](https://api.codeclimate.com/v1/badges/47fd3570c858b6c166ad/maintainability)](https://codeclimate.com/github/getindata/dbt-airflow-factory/maintainability)
[![Test Coverage](https://api.codeclimate.com/v1/badges/47fd3570c858b6c166ad/test_coverage)](https://codeclimate.com/github/getindata/dbt-airflow-factory/test_coverage)
[![Documentation Status](https://readthedocs.org/projects/dbt-airflow-factory/badge/?version=latest)](https://dbt-airflow-factory.readthedocs.io/en/latest/?badge=latest)

Library to convert DBT manifest metadata to Airflow tasks using [Astronomer Cosmos](https://astronomer.github.io/astronomer-cosmos/)

## What's New in v1.0.0

Version 1.0.0 replaces custom task builders with [Astronomer Cosmos](https://astronomer.github.io/astronomer-cosmos/) integration. This change maintains 100% backward compatibility - no configuration or code changes are required.

**Compatibility:**
- Apache Airflow 2.5 - 2.11
- dbt-core 1.7 - 1.10 (peer dependency via Cosmos)
- Python 3.9 - 3.11

**Key changes:**
- Uses Cosmos DbtTaskGroup for model-level task granularity
- Seeds handled automatically from manifest (no config needed)
- Transparent pass-through of all Kubernetes configuration

See [MIGRATION.md](MIGRATION.md) for upgrade details.

## Documentation

Read the full documentation at [https://dbt-airflow-factory.readthedocs.io/](https://dbt-airflow-factory.readthedocs.io/en/latest/index.html)

## Installation

Use the package manager [pip][pip] to install the library:

```bash
pip install dbt-airflow-factory
```

## Usage

The library is expected to be used inside an Airflow environment with a Kubernetes image referencing **dbt**.

**dbt-airflow-factory**'s main task is to parse `manifest.json` and create Airflow DAG out of it. It also reads config
files from `config` directory and therefore is highly customizable (e.g., user can set path to `manifest.json`).

To start, create a directory with a following structure, where `manifest.json` is a file generated by **dbt**:
```
.
├── config
│   ├── base
│   │   ├── airflow.yml
│   │   ├── dbt.yml
│   │   └── k8s.yml
│   └── dev
│       └── dbt.yml
├── dag.py
└── manifest.json
```

Then, put the following code into `dag.py`:
```python
from dbt_airflow_factory.airflow_dag_factory import AirflowDagFactory
from os import path

dag = AirflowDagFactory(path.dirname(path.abspath(__file__)), "dev").create()
```

When uploaded to Airflow DAGs directory, it will get picked up by Airflow, parse `manifest.json` and prepare a DAG to run.

### Configuration files

It is best to look up the example configuration files in [tests directory][tests] to get a glimpse of correct configs.

You can use [Airflow template variables][airflow-vars] in your `dbt.yml` and `k8s.yml` files, as long as they are inside
quotation marks:
```yaml
target: "{{ var.value.env }}"
some_other_field: "{{ ds_nodash }}"
```

Analogously, you can use `"{{ var.value.VARIABLE_NAME }}"` in `airflow.yml`, but only the Airflow variable getter.
Any other Airflow template variables will not work in `airflow.yml`.

### Creation of the directory with data-pipelines-cli

**DBT Airflow Factory** works best in tandem with [data-pipelines-cli][dp-cli] tool. **dp** not only prepares directory
for the library to digest, but also automates Docker image building and pushes generated directory to the cloud storage
of your choice.

## Development

### Running Tests

```bash
# Install with test dependencies
pip install -e ".[tests]"

# Run tests
pytest tests/

# Run with coverage
pytest tests/ --cov=dbt_airflow_factory --cov-report=term-missing
```

### Known Issue: Installing on Some Systems

If you encounter compilation errors related to `google-re2` (an Airflow dependency), use one of these solutions:

**Option 1 (Recommended) - Use pre-compiled binaries:**

```bash
pip install dbt-airflow-factory --only-binary=google-re2
```

This tells pip to use pre-compiled binary wheels instead of compiling from source.

**Option 2 - Install system dependencies:**

If binary wheels aren't available for your platform, install the system-level RE2 library:

```bash
# Ubuntu/Debian
sudo apt-get install -y libre2-dev

# macOS
brew install re2

# Alpine Linux
apk add --no-cache re2-dev
```

Then retry: `pip install dbt-airflow-factory`

[airflow-vars]: https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#variables
[dp-cli]: https://pypi.org/project/data-pipelines-cli/
[pip]: https://pip.pypa.io/en/stable/
[tests]: https://github.com/getindata/dbt-airflow-factory/tree/develop/tests/config
