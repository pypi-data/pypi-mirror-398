"""
Protocol definitions for engine-agnostic Spark interfaces.

These protocols represent the minimal surface area used by sparkless-testing.
Any engine (PySpark, sparkless, or other) must satisfy these to work with
the testing framework.
"""

from __future__ import annotations

from typing import Any, Protocol, runtime_checkable


@runtime_checkable
class ColumnProtocol(Protocol):
    """Column-like object with comparison and basic ops."""

    def isNotNull(self) -> Any: ...
    def isNull(self) -> Any: ...
    def cast(self, dataType: Any) -> Any: ...
    def __eq__(self, other: Any) -> Any: ...
    def __ne__(self, other: Any) -> Any: ...
    def __gt__(self, other: Any) -> Any: ...
    def __lt__(self, other: Any) -> Any: ...
    def __ge__(self, other: Any) -> Any: ...
    def __le__(self, other: Any) -> Any: ...
    def __add__(self, other: Any) -> Any: ...
    def __sub__(self, other: Any) -> Any: ...
    def __mul__(self, other: Any) -> Any: ...
    def __truediv__(self, other: Any) -> Any: ...


@runtime_checkable
class DataFrameProtocol(Protocol):
    """DataFrame-like object used throughout testing."""

    # Core accessors
    def schema(self) -> Any: ...
    @property
    def columns(self) -> list[str]: ...

    # Actions
    def count(self) -> int: ...
    def collect(self) -> list[Any]: ...
    def show(self, n: int = 20, truncate: bool = True) -> None: ...

    # Transformations
    def filter(self, condition: Any) -> Any: ...
    def select(self, *cols: Any) -> Any: ...
    def withColumn(self, colName: str, col: Any) -> Any: ...
    def withColumnRenamed(self, existing: str, new: str) -> Any: ...
    def groupBy(self, *cols: Any) -> Any: ...
    def agg(self, *exprs: Any, **kwargs: Any) -> Any: ...
    def limit(self, num: int) -> Any: ...
    def cache(self) -> Any: ...

    # SQL helpers
    def createOrReplaceTempView(self, name: str) -> None: ...

    # Writer
    @property
    def write(self) -> Any: ...


@runtime_checkable
class FunctionsProtocol(Protocol):
    """Functions module interface (col, lit, aggregations, etc.)."""

    def col(self, col_name: str) -> ColumnProtocol: ...
    def expr(self, expr: str) -> ColumnProtocol: ...
    def lit(self, value: Any) -> ColumnProtocol: ...
    def when(self, condition: ColumnProtocol, value: Any) -> ColumnProtocol: ...
    def count(self, col: Any = "*") -> ColumnProtocol: ...
    def countDistinct(self, *cols: Any) -> ColumnProtocol: ...
    def sum(self, col: Any) -> ColumnProtocol: ...
    def max(self, col: Any) -> ColumnProtocol: ...
    def min(self, col: Any) -> ColumnProtocol: ...
    def avg(self, col: Any) -> ColumnProtocol: ...
    def length(self, col: Any) -> ColumnProtocol: ...
    def date_trunc(self, fmt: str, col: Any) -> ColumnProtocol: ...
    def dayofweek(self, col: Any) -> ColumnProtocol: ...
    def current_timestamp(self) -> ColumnProtocol: ...


@runtime_checkable
class TypesProtocol(Protocol):
    """Types namespace used for schemas and fields."""

    StructType: Any
    StructField: Any
    StringType: Any
    IntegerType: Any
    FloatType: Any
    DoubleType: Any
    LongType: Any
    TimestampType: Any
    BooleanType: Any


@runtime_checkable
class WindowProtocol(Protocol):
    """Window spec placeholder."""

    def orderBy(self, *cols: Any, **kwargs: Any) -> Any: ...
    def partitionBy(self, *cols: Any) -> Any: ...


@runtime_checkable
class AnalysisExceptionProtocol(Protocol):
    """Exception type placeholder for analysis errors."""

    @property
    def desc(self) -> str: ...


@runtime_checkable
class SparkSessionProtocol(Protocol):
    """SparkSession-like interface."""

    @property
    def catalog(self) -> Any: ...
    @property
    def conf(self) -> Any: ...

    def table(self, name: str) -> DataFrameProtocol: ...
    def createDataFrame(
        self, data: Any, schema: Any = None, samplingRatio: Any = None
    ) -> DataFrameProtocol: ...
    def sql(self, sqlQuery: str) -> DataFrameProtocol: ...
    def stop(self) -> None: ...

    # Builder/config checks
    @property
    def _jsparkSession(self) -> Any: ...  # optional; used for ids


__all__ = [
    "ColumnProtocol",
    "DataFrameProtocol",
    "FunctionsProtocol",
    "TypesProtocol",
    "WindowProtocol",
    "AnalysisExceptionProtocol",
    "SparkSessionProtocol",
]
