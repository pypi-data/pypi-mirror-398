{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db83e36e",
   "metadata": {},
   "source": [
    "# E1 — Chunking strategies evaluation (Finance)\n",
    "\n",
    "This notebook runs the E1 experiment: compares chunking strategies on a financial filings subset.\n",
    "\n",
    "It follows the experiment template in `opik_config.yaml` and saves per-document metrics and chunk outputs in `results/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b818720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup & Imports\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('E1')\n",
    "\n",
    "from src.utils.chunking_master import ChunkingMaster\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Paths\n",
    "ROOT = Path(__file__).parent\n",
    "CONFIG = ROOT / 'opik_config.yaml'\n",
    "DATASET = ROOT / 'dataset' / 'edgar_subset.jsonl'\n",
    "RESULTS = ROOT / 'results'\n",
    "RESULTS.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49098d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Core Functionality\n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    items = []\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "\n",
    "def run_chunking_on_docs(docs: List[Dict], results_dir: Path) -> List[Dict]:\n",
    "    master = ChunkingMaster()\n",
    "    metrics = []\n",
    "    for doc in docs:\n",
    "        doc_id = doc.get('id') or doc.get('filename')\n",
    "        document = Document(page_content=doc['text'], metadata={'id': doc_id, 'filename': doc.get('filename')})\n",
    "        result = master.find_best_chunking_strategy([document], evaluate_all=True)\n",
    "        m = {\n",
    "            'id': doc_id,\n",
    "            'strategy': result.strategy.value,\n",
    "            'num_chunks': result.metrics.num_chunks,\n",
    "            'avg_chunk_size': result.metrics.avg_chunk_size,\n",
    "            'quality_score': master.evaluator.score_strategy(result.metrics)\n",
    "        }\n",
    "        metrics.append(m)\n",
    "        # save chunks\n",
    "        chunk_lines = [{'id': f\"{doc_id}-{i}\", 'content': c.page_content} for i, c in enumerate(result.chunks, start=1)]\n",
    "        with (results_dir / f\"{doc_id}__{result.strategy.value}.jsonl\").open('w', encoding='utf-8') as f:\n",
    "            for ln in chunk_lines:\n",
    "                f.write(json.dumps(ln, ensure_ascii=False) + '\\n')\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Unit Tests (simple assertions inline for notebook demonstration)\n",
    "\n",
    "# We'll run a tiny smoke test on a short synthetic document\n",
    "sample_doc = [{\"id\": \"test-1\", \"filename\": \"sample.txt\", \"text\": \"This is a small financial report.\\n\\nRevenue increased by 10%.\\n\\nSection: Summary\\nThe company reported strong cash flow.\"}]\n",
    "\n",
    "metrics = run_chunking_on_docs(sample_doc, RESULTS)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Example Usage (run on real dataset if available)\n",
    "\n",
    "if DATASET.exists():\n",
    "    docs = load_jsonl(DATASET)\n",
    "    sample = docs[:20]  # run on first 20 for quick iteration\n",
    "    metrics = run_chunking_on_docs(sample, RESULTS)\n",
    "    print(f\"Processed {len(sample)} documents. Collected metrics for {len(metrics)} docs\")\n",
    "else:\n",
    "    print(f\"Dataset not found at {DATASET}. Place a JSONL file as documented in dataset/README.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e598f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualization\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_file = RESULTS / 'metrics' / 'chunking_metrics.jsonl'\n",
    "if metrics_file.exists():\n",
    "    df = pd.read_json(metrics_file, lines=True)\n",
    "    df.groupby('strategy')['avg_chunk_size'].mean().sort_values().plot(kind='bar')\n",
    "    plt.title('Average chunk size by selected strategy (sample)')\n",
    "    plt.ylabel('avg_chunk_size')\n",
    "    plt.xlabel('strategy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No aggregated metrics yet. Run the experiment to generate results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Benchmarking & Profiling (simple timing)\n",
    "\n",
    "import time\n",
    "\n",
    "if DATASET.exists():\n",
    "    docs = load_jsonl(DATASET)\n",
    "    sample = docs[:20]\n",
    "    t0 = time.time()\n",
    "    _ = run_chunking_on_docs(sample, RESULTS)\n",
    "    t1 = time.time()\n",
    "    print(f\"Processed {len(sample)} docs in {t1-t0:.2f}s (avg { (t1-t0)/len(sample):.2f}s/doc)\")\n",
    "else:\n",
    "    print(\"Dataset not available for benchmarking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save and Load Results\n",
    "\n",
    "# Save example: write aggregated metrics to CSV\n",
    "metrics_file = RESULTS / 'metrics' / 'chunking_metrics.jsonl'\n",
    "if metrics_file.exists():\n",
    "    import pandas as pd\n",
    "    df = pd.read_json(metrics_file, lines=True)\n",
    "    df.to_csv(RESULTS / 'metrics' / 'chunking_metrics.csv', index=False)\n",
    "    print(f\"Saved CSV to {RESULTS / 'metrics' / 'chunking_metrics.csv'}\")\n",
    "else:\n",
    "    print(\"No metrics found to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opik quick setup\n",
    "try:\n",
    "    import opik\n",
    "    from opik import Opik\n",
    "    print('Opik SDK available, OPIK_API_KEY present? ->', bool(__import__('os').environ.get('OPIK_API_KEY')))\n",
    "    client = Opik()\n",
    "    print('Opik client created:', type(client))\n",
    "except Exception as e:\n",
    "    print('Opik not available or configuration missing:', e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run E1 Opik smoke test (nbSamples=10)\n",
    "from pathlib import Path\n",
    "from project.experiments.E1_chunking_finance.run_e1_opik import run_smoke\n",
    "\n",
    "res = run_smoke(Path('opik_config.yaml'), nb_samples=10)\n",
    "print('Smoke test result:', res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a73e2",
   "metadata": {},
   "source": [
    "# Smoke test outcome summary\n",
    "\n",
    "- Dataset inserted into Opik (name from config).\n",
    "- Processed 10 sample documents with `ChunkingMaster` (best strategy: **recursive** for these samples).\n",
    "- Local results saved: `results/opik_smoke_results.jsonl` (per-document predictions and chunking metadata).\n",
    "- Backup of the Opik upload exists: `results/opik_experiment_backup_batch_1.jsonl` (server returned an error for one batch).\n",
    "\n",
    "Next steps:\n",
    "1. Add retry/backoff and better error handling for `experiment_items_bulk` uploads (task added to TODOs). ✅\n",
    "2. Optionally re-upload backups when the API stabilizes or chunk into smaller batches.\n",
    "3. Extend `run_e1_opik.py` to optionally call a real LLM for model-graded evaluations (behind a flag).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
