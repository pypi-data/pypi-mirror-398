{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference & Entity Extraction (ETI) - Ungraph\n",
        "\n",
        "Este notebook demuestra la fase **Inference** del patr√≥n ETI (Extract-Transform-Inference) usando spaCy para extracci√≥n de entidades nombradas (NER).\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "1. **Extracci√≥n de entidades** - Usar spaCy NER para extraer personas, organizaciones, lugares, etc.\n",
        "2. **Extracci√≥n de relaciones** - Generar relaciones entre entidades co-ocurrentes\n",
        "3. **Generaci√≥n de facts** - Crear facts estructurados con trazabilidad\n",
        "4. **Persistencia en el grafo** - Guardar entidades y facts en Neo4j\n",
        "5. **Pipeline completo ETI** - Usar IngestDocumentUseCase con inferencia habilitada\n",
        "\n",
        "## Requisitos\n",
        "\n",
        "- `pip install ungraph[infer]`\n",
        "- Modelo de spaCy: `python -m spacy download en_core_web_sm` (ingl√©s)\n",
        "- O: `python -m spacy download es_core_news_sm` (espa√±ol)\n",
        "\n",
        "**Referencias:**\n",
        "- [Patr√≥n ETI](../../docs/concepts/introduction.md)\n",
        "- [SpacyInferenceService](../../src/infrastructure/services/spacy_inference_service.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_src_to_path(path_folder: str):\n",
        "    ''' \n",
        "    Helper function for adding the \"path_folder\" directory to the path.\n",
        "    '''\n",
        "    import sys\n",
        "    from pathlib import Path\n",
        "\n",
        "    base_path = Path().resolve()\n",
        "    for parent in [base_path] + list(base_path.parents):\n",
        "        candidate = parent / path_folder\n",
        "        if candidate.exists():\n",
        "            parent_dir = candidate.parent\n",
        "            if str(parent_dir) not in sys.path:\n",
        "                sys.path.insert(0, str(parent_dir))\n",
        "            if str(candidate) not in sys.path:\n",
        "                sys.path.append(str(candidate))\n",
        "            return\n",
        "\n",
        "# Agregar carpetas necesarias al path\n",
        "add_src_to_path(path_folder=\"src\")\n",
        "add_src_to_path(path_folder=\"src/utils\")\n",
        "add_src_to_path(path_folder=\"src/data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librer√≠as necesarias\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Importar ungraph\n",
        "try:\n",
        "    import ungraph\n",
        "    print(\"‚úÖ Ungraph importado como paquete instalado\")\n",
        "except ImportError:\n",
        "    import src\n",
        "    ungraph = src\n",
        "    print(\"‚úÖ Ungraph importado desde src/ (modo desarrollo)\")\n",
        "\n",
        "# Importar servicios de inferencia\n",
        "from infrastructure.services.spacy_inference_service import SpacyInferenceService\n",
        "from domain.entities.chunk import Chunk\n",
        "from application.dependencies import create_inference_service, create_ingest_document_use_case\n",
        "\n",
        "print(f\"üì¶ Ungraph version: {ungraph.__version__}\")\n",
        "\n",
        "# Verificar si spaCy est√° disponible\n",
        "try:\n",
        "    import spacy\n",
        "    print(\"‚úÖ spaCy est√° instalado\")\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        print(\"‚úÖ Modelo en_core_web_sm disponible\")\n",
        "    except OSError:\n",
        "        print(\"‚ö†Ô∏è  Modelo en_core_web_sm no encontrado. Instalar con: python -m spacy download en_core_web_sm\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå spaCy no est√° instalado. Instalar con: pip install spacy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 1: Extracci√≥n de Entidades con spaCy\n",
        "\n",
        "Primero, veamos c√≥mo extraer entidades nombradas de un chunk de texto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear servicio de inferencia\n",
        "try:\n",
        "    inference_service = SpacyInferenceService(model_name=\"en_core_web_sm\")\n",
        "    print(\"‚úÖ SpacyInferenceService creado\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creando servicio: {e}\")\n",
        "    print(\"   Instalar con: pip install spacy && python -m spacy download en_core_web_sm\")\n",
        "    inference_service = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear un chunk de ejemplo\n",
        "sample_text = \"\"\"\n",
        "Apple Inc. is a technology company founded by Steve Jobs in Cupertino, California.\n",
        "The company was established in 1976 and has since become one of the world's largest tech companies.\n",
        "Tim Cook is the current CEO of Apple Inc.\n",
        "\"\"\"\n",
        "\n",
        "chunk = Chunk(\n",
        "    id=\"chunk_example_1\",\n",
        "    page_content=sample_text,\n",
        "    metadata={\"filename\": \"example.txt\", \"page_number\": 1}\n",
        ")\n",
        "\n",
        "print(\"üìÑ Chunk de ejemplo:\")\n",
        "print(sample_text)\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extraer entidades del chunk\n",
        "if inference_service:\n",
        "    print(\"üîç Extrayendo entidades...\\n\")\n",
        "    entities = inference_service.extract_entities(chunk)\n",
        "    \n",
        "    print(f\"‚úÖ Encontradas {len(entities)} entidades:\\n\")\n",
        "    for i, entity in enumerate(entities, 1):\n",
        "        print(f\"{i}. {entity.name} ({entity.type})\")\n",
        "        print(f\"   ID: {entity.id}\")\n",
        "        print(f\"   Menciones: {entity.mentions}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Servicio de inferencia no disponible\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 2: Extracci√≥n de Relaciones\n",
        "\n",
        "Las relaciones conectan entidades que aparecen juntas en el mismo chunk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extraer relaciones entre entidades\n",
        "if inference_service and entities:\n",
        "    print(\"üîó Extrayendo relaciones...\\n\")\n",
        "    relations = inference_service.extract_relations(chunk, entities)\n",
        "    \n",
        "    print(f\"‚úÖ Encontradas {len(relations)} relaciones:\\n\")\n",
        "    for i, relation in enumerate(relations, 1):\n",
        "        source_entity = next(e for e in entities if e.id == relation.source_entity_id)\n",
        "        target_entity = next(e for e in entities if e.id == relation.target_entity_id)\n",
        "        print(f\"{i}. {source_entity.name} --[{relation.relation_type}]--> {target_entity.name}\")\n",
        "        print(f\"   Confianza: {relation.confidence:.2f}\")\n",
        "        print(f\"   Provenance: {relation.provenance_ref}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No hay entidades para extraer relaciones\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 3: Generaci√≥n de Facts\n",
        "\n",
        "Los facts son tripletas estructuradas (subject-predicate-object) con trazabilidad.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar facts desde el chunk\n",
        "if inference_service:\n",
        "    print(\"üìä Generando facts...\\n\")\n",
        "    facts = inference_service.infer_facts(chunk)\n",
        "    \n",
        "    print(f\"‚úÖ Generados {len(facts)} facts:\\n\")\n",
        "    for i, fact in enumerate(facts, 1):\n",
        "        print(f\"{i}. Tripleta: ({fact.subject}, {fact.predicate}, {fact.object})\")\n",
        "        print(f\"   Confianza: {fact.confidence:.2f}\")\n",
        "        print(f\"   Provenance: {fact.provenance_ref}\")\n",
        "        print(f\"   Alta confianza: {fact.is_high_confidence()}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Servicio de inferencia no disponible\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 4: Pipeline Completo ETI con IngestDocumentUseCase\n",
        "\n",
        "Ahora usemos el caso de uso completo con inferencia habilitada para ingerir un documento y extraer entidades autom√°ticamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear caso de uso con inferencia habilitada\n",
        "use_case = create_ingest_document_use_case(\n",
        "    enable_inference=True,\n",
        "    inference_language=\"en\"  # \"en\" para ingl√©s, \"es\" para espa√±ol\n",
        ")\n",
        "\n",
        "print(\"‚úÖ IngestDocumentUseCase creado con inferencia habilitada\")\n",
        "print(\"   El pipeline ejecutar√°: Extract ‚Üí Transform ‚Üí Inference\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Buscar un archivo de ejemplo para ingerir\n",
        "from src.utils.handlers import find_in_project\n",
        "\n",
        "# Buscar archivos de ejemplo\n",
        "example_files = find_in_project(\"*.md\", \"src/data\")\n",
        "if example_files:\n",
        "    example_file = example_files[0]\n",
        "    print(f\"üìÑ Archivo encontrado: {example_file}\")\n",
        "    print(f\"   Usaremos este archivo para demostrar el pipeline ETI completo\\n\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No se encontraron archivos de ejemplo\")\n",
        "    print(\"   Puedes crear un archivo de texto o usar uno existente\")\n",
        "    example_file = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecutar pipeline ETI completo\n",
        "if example_file and use_case:\n",
        "    print(\"üöÄ Ejecutando pipeline ETI completo...\\n\")\n",
        "    print(\"   Fases:\")\n",
        "    print(\"   1. Extract: Cargar documento\")\n",
        "    print(\"   2. Transform: Dividir en chunks, generar embeddings\")\n",
        "    print(\"   3. Inference: Extraer entidades, relaciones y facts\")\n",
        "    print(\"   4. Persistir: Guardar chunks y facts en el grafo\\n\")\n",
        "    \n",
        "    try:\n",
        "        chunks = use_case.execute(\n",
        "            file_path=Path(example_file),\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=100\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Pipeline completado:\")\n",
        "        print(f\"   - Chunks creados: {len(chunks)}\")\n",
        "        print(f\"   - Entidades y facts extra√≠dos y persistidos en el grafo\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error ejecutando pipeline: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No se puede ejecutar el pipeline (archivo o use_case no disponible)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 5: Consultar Entidades y Facts en el Grafo\n",
        "\n",
        "Ahora consultemos las entidades y facts que fueron persistidos en Neo4j.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Consultar entidades en el grafo\n",
        "from src.utils.graph_operations import graph_session\n",
        "\n",
        "driver = graph_session()\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        # Contar entidades\n",
        "        result = session.run(\"MATCH (e:Entity) RETURN count(e) as count\")\n",
        "        entity_count = result.single()[\"count\"]\n",
        "        print(f\"üìä Entidades en el grafo: {entity_count}\")\n",
        "        \n",
        "        # Mostrar algunas entidades\n",
        "        if entity_count > 0:\n",
        "            result = session.run(\"\"\"\n",
        "                MATCH (e:Entity)\n",
        "                RETURN e.name as name, e.type as type, size(e.mentions) as mention_count\n",
        "                ORDER BY mention_count DESC\n",
        "                LIMIT 10\n",
        "            \"\"\")\n",
        "            \n",
        "            print(\"\\nüîù Top 10 entidades por menciones:\")\n",
        "            for record in result:\n",
        "                print(f\"  - {record['name']} ({record['type']}) - {record['mention_count']} menciones\")\n",
        "finally:\n",
        "    driver.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Consultar facts en el grafo\n",
        "driver = graph_session()\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        # Contar facts\n",
        "        result = session.run(\"MATCH (f:Fact) RETURN count(f) as count\")\n",
        "        fact_count = result.single()[\"count\"]\n",
        "        print(f\"üìä Facts en el grafo: {fact_count}\")\n",
        "        \n",
        "        # Mostrar algunos facts\n",
        "        if fact_count > 0:\n",
        "            result = session.run(\"\"\"\n",
        "                MATCH (f:Fact)\n",
        "                RETURN f.subject as subject, f.predicate as predicate, f.object as object, f.confidence as confidence\n",
        "                ORDER BY f.confidence DESC\n",
        "                LIMIT 10\n",
        "            \"\"\")\n",
        "            \n",
        "            print(\"\\nüîù Top 10 facts por confianza:\")\n",
        "            for record in result:\n",
        "                print(f\"  - ({record['subject']}, {record['predicate']}, {record['object']})\")\n",
        "                print(f\"    Confianza: {record['confidence']:.2f}\")\n",
        "finally:\n",
        "    driver.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 6: Visualizaci√≥n de Entidades con spaCy\n",
        "\n",
        "spaCy incluye herramientas de visualizaci√≥n para mostrar entidades extra√≠das.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar entidades con spaCy displacy\n",
        "try:\n",
        "    import spacy\n",
        "    from spacy import displacy\n",
        "    \n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(sample_text)\n",
        "    \n",
        "    # Visualizar entidades (solo funciona en Jupyter)\n",
        "    print(\"üìä Visualizaci√≥n de entidades (ejecutar en Jupyter para ver visualizaci√≥n):\")\n",
        "    displacy.render(doc, style=\"ent\", jupyter=True)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  No se puede mostrar visualizaci√≥n: {e}\")\n",
        "    print(\"   Esto es normal si no est√°s ejecutando en Jupyter\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen y Mejores Pr√°cticas\n",
        "\n",
        "### Tipos de Entidades Extra√≠das\n",
        "\n",
        "spaCy puede extraer diferentes tipos de entidades:\n",
        "- **PERSON**: Personas (ej: \"Steve Jobs\", \"Tim Cook\")\n",
        "- **ORGANIZATION**: Organizaciones (ej: \"Apple Inc.\")\n",
        "- **LOCATION/GPE**: Lugares (ej: \"Cupertino\", \"California\")\n",
        "- **DATE**: Fechas (ej: \"1976\")\n",
        "- **MONEY**: Cantidades monetarias\n",
        "- **PERCENT**: Porcentajes\n",
        "- Y m√°s...\n",
        "\n",
        "### Flujo del Pipeline ETI\n",
        "\n",
        "1. **Extract**: Cargar documento desde archivo\n",
        "2. **Transform**: \n",
        "   - Dividir en chunks\n",
        "   - Generar embeddings\n",
        "   - Persistir chunks en el grafo\n",
        "3. **Inference**:\n",
        "   - Extraer entidades nombradas (NER)\n",
        "   - Generar relaciones entre entidades\n",
        "   - Crear facts estructurados\n",
        "   - Persistir entidades y facts en el grafo\n",
        "\n",
        "### Configuraci√≥n de Idiomas\n",
        "\n",
        "Para espa√±ol:\n",
        "```python\n",
        "use_case = create_ingest_document_use_case(\n",
        "    enable_inference=True,\n",
        "    inference_language=\"es\"  # Requiere: python -m spacy download es_core_news_sm\n",
        ")\n",
        "```\n",
        "\n",
        "### Mejores Pr√°cticas\n",
        "\n",
        "1. **Instalar modelos apropiados**: Descarga el modelo de spaCy para tu idioma\n",
        "2. **Ajustar chunk_size**: Chunks m√°s peque√±os pueden mejorar la precisi√≥n de NER\n",
        "3. **Revisar confianza**: Los facts tienen niveles de confianza que puedes filtrar\n",
        "4. **Trazabilidad**: Todos los facts tienen `provenance_ref` al chunk origen\n",
        "\n",
        "## Referencias\n",
        "\n",
        "- [Patr√≥n ETI](../../docs/concepts/introduction.md)\n",
        "- [SpacyInferenceService](../../src/infrastructure/services/spacy_inference_service.py)\n",
        "- [spaCy Documentation](https://spacy.io/)\n",
        "el "
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
