{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.1 Graph Pattern Construction - Ungraph\n",
        "\n",
        "Este notebook cubre la fase **TRANSFORM** del patr√≥n ETI: c√≥mo construir estructuras de grafo usando patrones predefinidos y personalizados.\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "1. **Patrones predefinidos** - FILE_PAGE_CHUNK (default), SIMPLE_CHUNK\n",
        "2. **Crear patrones personalizados** - Definir tu propia estructura de grafo\n",
        "3. **Validar patrones** - Verificar que los patrones son v√°lidos\n",
        "4. **Generar queries Cypher** - Ver los queries generados autom√°ticamente\n",
        "5. **Usar patrones en ingesta** - Aplicar patrones al ingerir documentos\n",
        "\n",
        "**Referencias:**\n",
        "- [Patrones de Grafo](../../docs/concepts/graph-patterns.md)\n",
        "- [Patrones Personalizados](../../docs/guides/custom-patterns.md)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Ungraph version: 0.1.0\n"
          ]
        }
      ],
      "source": [
        "def add_src_to_path(path_folder: str):\n",
        "    import sys\n",
        "    from pathlib import Path\n",
        "    base_path = Path().resolve()\n",
        "    for parent in [base_path] + list(base_path.parents):\n",
        "        candidate = parent / path_folder\n",
        "        if candidate.exists():\n",
        "            parent_dir = candidate.parent\n",
        "            if str(parent_dir) not in sys.path:\n",
        "                sys.path.insert(0, str(parent_dir))\n",
        "            if str(candidate) not in sys.path:\n",
        "                sys.path.append(str(candidate))\n",
        "            return\n",
        "\n",
        "add_src_to_path(path_folder=\"src\")\n",
        "add_src_to_path(path_folder=\"src/utils\")\n",
        "add_src_to_path(path_folder=\"src/data\")\n",
        "\n",
        "try:\n",
        "    import ungraph\n",
        "except ImportError:\n",
        "    import src\n",
        "    ungraph = src\n",
        "\n",
        "from domain.value_objects.predefined_patterns import FILE_PAGE_CHUNK_PATTERN\n",
        "from domain.value_objects.graph_pattern import GraphPattern, NodeDefinition, RelationshipDefinition\n",
        "from infrastructure.services.neo4j_pattern_service import Neo4jPatternService\n",
        "from src.utils.handlers import find_in_project\n",
        "\n",
        "print(f\"üì¶ Ungraph version: {ungraph.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 1: Patrones Predefinidos\n",
        "\n",
        "Ungraph incluye patrones predefinidos listos para usar. El patr√≥n por defecto es `FILE_PAGE_CHUNK`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Patr√≥n Predefinido: FILE_PAGE_CHUNK\n",
            "================================================================================\n",
            "Nombre: FILE_PAGE_CHUNK\n",
            "Descripci√≥n: Patr√≥n b√°sico: File contiene Pages, Pages contienen Chunks. Chunks tienen relaciones NEXT_CHUNK entre consecutivos.\n",
            "\n",
            "Nodos (3):\n",
            "  - File\n",
            "    Propiedades requeridas: ['filename']\n",
            "    Propiedades opcionales: ['createdAt']\n",
            "  - Page\n",
            "    Propiedades requeridas: ['filename', 'page_number']\n",
            "  - Chunk\n",
            "    Propiedades requeridas: ['chunk_id', 'page_content', 'embeddings', 'embeddings_dimensions']\n",
            "    Propiedades opcionales: ['is_unitary', 'chunk_id_consecutive', 'embedding_encoder_info']\n",
            "\n",
            "Relaciones (3):\n",
            "  - File --[CONTAINS]--> Page\n",
            "  - Page --[HAS_CHUNK]--> Chunk\n",
            "  - Chunk --[NEXT_CHUNK]--> Chunk\n",
            "\n",
            "Patrones de b√∫squeda soportados: ['basic', 'hybrid']\n"
          ]
        }
      ],
      "source": [
        "# Examinar patr√≥n predefinido FILE_PAGE_CHUNK\n",
        "print(\"üìã Patr√≥n Predefinido: FILE_PAGE_CHUNK\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Nombre: {FILE_PAGE_CHUNK_PATTERN.name}\")\n",
        "print(f\"Descripci√≥n: {FILE_PAGE_CHUNK_PATTERN.description}\")\n",
        "print(f\"\\nNodos ({len(FILE_PAGE_CHUNK_PATTERN.node_definitions)}):\")\n",
        "for node_def in FILE_PAGE_CHUNK_PATTERN.node_definitions:\n",
        "    print(f\"  - {node_def.label}\")\n",
        "    print(f\"    Propiedades requeridas: {list(node_def.required_properties.keys())}\")\n",
        "    if node_def.optional_properties:\n",
        "        print(f\"    Propiedades opcionales: {list(node_def.optional_properties.keys())}\")\n",
        "\n",
        "print(f\"\\nRelaciones ({len(FILE_PAGE_CHUNK_PATTERN.relationship_definitions)}):\")\n",
        "for rel_def in FILE_PAGE_CHUNK_PATTERN.relationship_definitions:\n",
        "    print(f\"  - {rel_def.from_node} --[{rel_def.relationship_type}]--> {rel_def.to_node}\")\n",
        "\n",
        "print(f\"\\nPatrones de b√∫squeda soportados: {FILE_PAGE_CHUNK_PATTERN.search_patterns}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 2: Crear Patrones Personalizados\n",
        "\n",
        "Creemos patrones personalizados seg√∫n nuestras necesidades espec√≠ficas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Patr√≥n Simple: Solo Chunks\n",
        "\n",
        "Un patr√≥n minimalista sin estructura jer√°rquica File-Page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Patr√≥n SIMPLE_CHUNK creado:\n",
            "   Nodos: ['Chunk']\n",
            "   Relaciones: 0\n"
          ]
        }
      ],
      "source": [
        "# Crear patr√≥n SIMPLE_CHUNK\n",
        "simple_chunk_node = NodeDefinition(\n",
        "    label=\"Chunk\",\n",
        "    required_properties={\n",
        "        \"chunk_id\": str,\n",
        "        \"page_content\": str,\n",
        "        \"embeddings\": list,\n",
        "        \"embeddings_dimensions\": int\n",
        "    },\n",
        "    optional_properties={\n",
        "        \"chunk_id_consecutive\": int\n",
        "    },\n",
        "    indexes=[\"chunk_id\"]\n",
        ")\n",
        "\n",
        "SIMPLE_CHUNK_PATTERN = GraphPattern(\n",
        "    name=\"SIMPLE_CHUNK\",\n",
        "    description=\"Solo chunks, sin estructura File-Page. √ötil para documentos simples.\",\n",
        "    node_definitions=[simple_chunk_node],\n",
        "    relationship_definitions=[],\n",
        "    search_patterns=[\"basic\", \"hybrid\"]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Patr√≥n SIMPLE_CHUNK creado:\")\n",
        "print(f\"   Nodos: {[n.label for n in SIMPLE_CHUNK_PATTERN.node_definitions]}\")\n",
        "print(f\"   Relaciones: {len(SIMPLE_CHUNK_PATTERN.relationship_definitions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Patr√≥n con Relaciones Secuenciales\n",
        "\n",
        "Chunks conectados con relaciones NEXT_CHUNK para mantener orden.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Patr√≥n SEQUENTIAL_CHUNKS creado:\n",
            "   Relaciones: ['NEXT_CHUNK']\n"
          ]
        }
      ],
      "source": [
        "# Crear patr√≥n SEQUENTIAL_CHUNKS\n",
        "chunk_node = NodeDefinition(\n",
        "    label=\"Chunk\",\n",
        "    required_properties={\n",
        "        \"chunk_id\": str,\n",
        "        \"page_content\": str,\n",
        "        \"embeddings\": list,\n",
        "        \"embeddings_dimensions\": int\n",
        "    },\n",
        "    optional_properties={\n",
        "        \"chunk_id_consecutive\": int\n",
        "    },\n",
        "    indexes=[\"chunk_id\"]\n",
        ")\n",
        "\n",
        "next_chunk_rel = RelationshipDefinition(\n",
        "    from_node=\"Chunk\",\n",
        "    to_node=\"Chunk\",\n",
        "    relationship_type=\"NEXT_CHUNK\",\n",
        "    direction=\"OUTGOING\"\n",
        ")\n",
        "\n",
        "SEQUENTIAL_CHUNKS_PATTERN = GraphPattern(\n",
        "    name=\"SEQUENTIAL_CHUNKS\",\n",
        "    description=\"Chunks con relaciones NEXT_CHUNK entre consecutivos.\",\n",
        "    node_definitions=[chunk_node],\n",
        "    relationship_definitions=[next_chunk_rel],\n",
        "    search_patterns=[\"basic\", \"hybrid\"]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Patr√≥n SEQUENTIAL_CHUNKS creado:\")\n",
        "print(f\"   Relaciones: {[r.relationship_type for r in SEQUENTIAL_CHUNKS_PATTERN.relationship_definitions]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Patr√≥n L√©xico: Chunks y Entidades\n",
        "\n",
        "Patr√≥n que incluye entidades extra√≠das y sus relaciones con chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Patr√≥n LEXICAL_GRAPH creado:\n",
            "   Nodos: ['Chunk', 'Entity']\n",
            "   Relaciones: ['MENTIONS', 'RELATED_TO']\n"
          ]
        }
      ],
      "source": [
        "# Crear patr√≥n LEXICAL_GRAPH con entidades\n",
        "entity_node = NodeDefinition(\n",
        "    label=\"Entity\",\n",
        "    required_properties={\n",
        "        \"id\": str,\n",
        "        \"name\": str,\n",
        "        \"type\": str\n",
        "    },\n",
        "    optional_properties={\n",
        "        \"mentions\": list\n",
        "    },\n",
        "    indexes=[\"id\", \"name\", \"type\"]\n",
        ")\n",
        "\n",
        "chunk_node_lexical = NodeDefinition(\n",
        "    label=\"Chunk\",\n",
        "    required_properties={\n",
        "        \"chunk_id\": str,\n",
        "        \"page_content\": str,\n",
        "        \"embeddings\": list,\n",
        "        \"embeddings_dimensions\": int\n",
        "    },\n",
        "    indexes=[\"chunk_id\"]\n",
        ")\n",
        "\n",
        "# Relaci√≥n: Chunk menciona Entity\n",
        "mentions_rel = RelationshipDefinition(\n",
        "    from_node=\"Chunk\",\n",
        "    to_node=\"Entity\",\n",
        "    relationship_type=\"MENTIONS\",\n",
        "    direction=\"OUTGOING\"\n",
        ")\n",
        "\n",
        "# Relaci√≥n: Entity relacionada con otra Entity\n",
        "related_rel = RelationshipDefinition(\n",
        "    from_node=\"Entity\",\n",
        "    to_node=\"Entity\",\n",
        "    relationship_type=\"RELATED_TO\",\n",
        "    direction=\"OUTGOING\"\n",
        ")\n",
        "\n",
        "LEXICAL_GRAPH_PATTERN = GraphPattern(\n",
        "    name=\"LEXICAL_GRAPH\",\n",
        "    description=\"Grafo l√©xico con entidades y chunks. √ötil para extracci√≥n de conocimiento.\",\n",
        "    node_definitions=[chunk_node_lexical, entity_node],\n",
        "    relationship_definitions=[mentions_rel, related_rel],\n",
        "    search_patterns=[\"basic\", \"hybrid\", \"graph_enhanced_vector\"]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Patr√≥n LEXICAL_GRAPH creado:\")\n",
        "print(f\"   Nodos: {[n.label for n in LEXICAL_GRAPH_PATTERN.node_definitions]}\")\n",
        "print(f\"   Relaciones: {[r.relationship_type for r in LEXICAL_GRAPH_PATTERN.relationship_definitions]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Patr√≥n Jer√°rquico: Document ‚Üí Section ‚Üí Paragraph\n",
        "\n",
        "Estructura jer√°rquica para documentos con secciones bien definidas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Patr√≥n DOCUMENT_SECTION_PARAGRAPH creado:\n",
            "   Estructura: Document ‚Üí Section ‚Üí Paragraph\n"
          ]
        }
      ],
      "source": [
        "# Crear patr√≥n jer√°rquico DOCUMENT_SECTION_PARAGRAPH\n",
        "document_node = NodeDefinition(\n",
        "    label=\"Document\",\n",
        "    required_properties={\"doc_id\": str, \"title\": str},\n",
        "    indexes=[\"doc_id\"]\n",
        ")\n",
        "\n",
        "section_node = NodeDefinition(\n",
        "    label=\"Section\",\n",
        "    required_properties={\"section_id\": str, \"title\": str},\n",
        "    indexes=[\"section_id\"]\n",
        ")\n",
        "\n",
        "paragraph_node = NodeDefinition(\n",
        "    label=\"Paragraph\",\n",
        "    required_properties={\"para_id\": str, \"content\": str},\n",
        "    indexes=[\"para_id\"]\n",
        ")\n",
        "\n",
        "# Relaciones jer√°rquicas\n",
        "has_section = RelationshipDefinition(\n",
        "    from_node=\"Document\",\n",
        "    to_node=\"Section\",\n",
        "    relationship_type=\"HAS_SECTION\",\n",
        "    direction=\"OUTGOING\"\n",
        ")\n",
        "\n",
        "has_paragraph = RelationshipDefinition(\n",
        "    from_node=\"Section\",\n",
        "    to_node=\"Paragraph\",\n",
        "    relationship_type=\"HAS_PARAGRAPH\",\n",
        "    direction=\"OUTGOING\"\n",
        ")\n",
        "\n",
        "next_paragraph = RelationshipDefinition(\n",
        "    from_node=\"Paragraph\",\n",
        "    to_node=\"Paragraph\",\n",
        "    relationship_type=\"NEXT_PARAGRAPH\",\n",
        "    direction=\"OUTGOING\"\n",
        ")\n",
        "\n",
        "DOCUMENT_SECTION_PARAGRAPH_PATTERN = GraphPattern(\n",
        "    name=\"DOCUMENT_SECTION_PARAGRAPH\",\n",
        "    description=\"Estructura jer√°rquica: Document ‚Üí Section ‚Üí Paragraph\",\n",
        "    node_definitions=[document_node, section_node, paragraph_node],\n",
        "    relationship_definitions=[has_section, has_paragraph, next_paragraph],\n",
        "    search_patterns=[\"basic\", \"parent_child\"]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Patr√≥n DOCUMENT_SECTION_PARAGRAPH creado:\")\n",
        "print(f\"   Estructura: Document ‚Üí Section ‚Üí Paragraph\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 3: Validar Patrones\n",
        "\n",
        "Antes de usar un patr√≥n, debemos validarlo para asegurar que es correcto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Validando patrones:\n",
            "\n",
            "‚úÖ V√ÅLIDO: FILE_PAGE_CHUNK\n",
            "‚úÖ V√ÅLIDO: SIMPLE_CHUNK\n",
            "‚úÖ V√ÅLIDO: SEQUENTIAL_CHUNKS\n",
            "‚úÖ V√ÅLIDO: LEXICAL_GRAPH\n",
            "‚úÖ V√ÅLIDO: DOCUMENT_SECTION_PARAGRAPH\n"
          ]
        }
      ],
      "source": [
        "# Crear servicio de patrones\n",
        "pattern_service = Neo4jPatternService()\n",
        "\n",
        "# Validar todos los patrones creados\n",
        "patterns_to_validate = [\n",
        "    (\"FILE_PAGE_CHUNK\", FILE_PAGE_CHUNK_PATTERN),\n",
        "    (\"SIMPLE_CHUNK\", SIMPLE_CHUNK_PATTERN),\n",
        "    (\"SEQUENTIAL_CHUNKS\", SEQUENTIAL_CHUNKS_PATTERN),\n",
        "    (\"LEXICAL_GRAPH\", LEXICAL_GRAPH_PATTERN),\n",
        "    (\"DOCUMENT_SECTION_PARAGRAPH\", DOCUMENT_SECTION_PARAGRAPH_PATTERN)\n",
        "]\n",
        "\n",
        "print(\"üîç Validando patrones:\\n\")\n",
        "for name, pattern in patterns_to_validate:\n",
        "    is_valid = pattern_service.validate_pattern(pattern)\n",
        "    status = \"‚úÖ V√ÅLIDO\" if is_valid else \"‚ùå INV√ÅLIDO\"\n",
        "    print(f\"{status}: {name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 4: Generar Queries Cypher\n",
        "\n",
        "Podemos ver los queries Cypher que se generan autom√°ticamente para cada patr√≥n.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Query Cypher generado para FILE_PAGE_CHUNK:\n",
            "================================================================================\n",
            "MERGE (n0:File {filename: $filename})\n",
            "ON CREATE SET n0.createdAt = $createdAt\n",
            "MERGE (n1:Page {filename: $filename, page_number: $page_number})\n",
            "MERGE (n2:Chunk {chunk_id: $chunk_id, page_content: $page_content, embeddings: $embeddings, embeddings_dimensions: $embeddings_dimensions})\n",
            "ON CREATE SET n2.is_unitary = $is_unitary, n2.chunk_id_consecutive = $chunk_id_consecutive, n2.embedding_encoder_info = $embedding_encoder_info\n",
            "MERGE (n0)-[:CONTAINS]->(n1)\n",
            "MERGE (n1)-[:HAS_CHUNK]->(n2)\n",
            "MERGE (n2)-[:N...\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Generar query Cypher para FILE_PAGE_CHUNK\n",
        "cypher_query = pattern_service.generate_cypher(FILE_PAGE_CHUNK_PATTERN, \"create\")\n",
        "\n",
        "print(\"üìù Query Cypher generado para FILE_PAGE_CHUNK:\")\n",
        "print(\"=\" * 80)\n",
        "print(cypher_query[:500] + \"...\" if len(cypher_query) > 500 else cypher_query)\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 5: Usar Patrones en Ingesta\n",
        "\n",
        "Aplicamos patrones personalizados al ingerir documentos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ungraph configurado con Neo4j\n",
            "   URI: bolt://localhost:7687\n",
            "   Base de datos: neo4j\n"
          ]
        }
      ],
      "source": [
        "# Configurar conexi√≥n a Neo4j (REQUERIDO antes de ingerir documentos)\n",
        "# Estos son los valores por defecto para desarrollo local\n",
        "ungraph.configure(\n",
        "    neo4j_uri=\"bolt://localhost:7687\",\n",
        "    neo4j_user=\"neo4j\",\n",
        "    neo4j_password=\"Ungraph22\"  # Reemplaza con tu contrase√±a\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Ungraph configurado con Neo4j\")\n",
        "print(f\"   URI: bolt://localhost:7687\")\n",
        "print(f\"   Base de datos: neo4j\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Ingiriendo con patr√≥n SIMPLE_CHUNK: 110225.md\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-25 21:20:13,480 - INFO - CUDA no disponible, usando CPU para embeddings.\n",
            "2025-12-25 21:20:13,484 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
            "2025-12-25 21:20:16,244 - INFO - Embedding service initialized with model: sentence-transformers/all-MiniLM-L6-v2\n",
            "2025-12-25 21:20:16,245 - INFO - Loading spaCy model: en_core_web_sm\n"
          ]
        },
        {
          "ename": "MemoryError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markdown_file.exists():\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müì• Ingiriendo con patr√≥n SIMPLE_CHUNK: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmarkdown_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     chunks = \u001b[43mungraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mingest_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmarkdown_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSIMPLE_CHUNK_PATTERN\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Documento ingerido con patr√≥n personalizado!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Chunks creados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Ungraph\\src\\__init__.py:187\u001b[39m, in \u001b[36mingest_document\u001b[39m\u001b[34m(file_path, chunk_size, chunk_overlap, clean_text, database, embedding_model, pattern)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapplication\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependencies\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_ingest_document_use_case\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m use_case = \u001b[43mcreate_ingest_document_use_case\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43memb_model\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# Ejecutar el caso de uso\u001b[39;00m\n\u001b[32m    194\u001b[39m     chunks = use_case.execute(\n\u001b[32m    195\u001b[39m         file_path=file_path,\n\u001b[32m    196\u001b[39m         chunk_size=chunk_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    199\u001b[39m         pattern=pattern\n\u001b[32m    200\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Ungraph\\src\\application\\dependencies.py:142\u001b[39m, in \u001b[36mcreate_ingest_document_use_case\u001b[39m\u001b[34m(database, embedding_model, enable_inference, inference_model, inference_language)\u001b[39m\n\u001b[32m    139\u001b[39m chunk_repository = Neo4jChunkRepository(database=database)\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Crear servicio de inferencia (opcional)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m inference_service = \u001b[43mcreate_inference_service\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_inference\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_language\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# Crear caso de uso con dependencias inyectadas\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m IngestDocumentUseCase(\n\u001b[32m    150\u001b[39m     document_loader_service=document_loader_service,\n\u001b[32m    151\u001b[39m     chunking_service=chunking_service,\n\u001b[32m   (...)\u001b[39m\u001b[32m    155\u001b[39m     inference_service=inference_service\n\u001b[32m    156\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Ungraph\\src\\application\\dependencies.py:66\u001b[39m, in \u001b[36mcreate_inference_service\u001b[39m\u001b[34m(model_name, enable_inference, language)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Si language no es 'en' ni 'es', usar model_name proporcionado\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSpacyInferenceService\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# Si spaCy no est√° instalado, retornar None en lugar de fallar\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Esto permite usar el pipeline ET sin Inference\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\projects\\Ungraph\\src\\infrastructure\\services\\spacy_inference_service.py:96\u001b[39m, in \u001b[36mSpacyInferenceService.__init__\u001b[39m\u001b[34m(self, model_name, disable)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     95\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading spaCy model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28mself\u001b[39m.nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mspaCy model loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\spacy\\__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\spacy\\util.py:524\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name.replace(\u001b[33m\"\u001b[39m\u001b[33mblank:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))()\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Path(name).exists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), **kwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\spacy\\util.py:560\u001b[39m, in \u001b[36mload_model_from_package\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    543\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[32m    544\u001b[39m \n\u001b[32m    545\u001b[39m \u001b[33;03mname (str): The package name.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    557\u001b[39m \u001b[33;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mcls\u001b[39m = importlib.import_module(name)\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\en_core_web_sm\\__init__.py:10\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(**overrides)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(**overrides):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_init_py\u001b[49m\u001b[43m(\u001b[49m\u001b[34;43m__file__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\spacy\\util.py:741\u001b[39m, in \u001b[36mload_model_from_init_py\u001b[39m\u001b[34m(init_file, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path.exists():\n\u001b[32m    740\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E052.format(path=data_path))\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\spacy\\util.py:606\u001b[39m, in \u001b[36mload_model_from_path\u001b[39m\u001b[34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    597\u001b[39m config = load_config(config_path, overrides=overrides)\n\u001b[32m    598\u001b[39m nlp = load_model_from_config(\n\u001b[32m    599\u001b[39m     config,\n\u001b[32m    600\u001b[39m     vocab=vocab,\n\u001b[32m   (...)\u001b[39m\u001b[32m    604\u001b[39m     meta=meta,\n\u001b[32m    605\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\spacy\\language.py:2245\u001b[39m, in \u001b[36mLanguage.from_disk\u001b[39m\u001b[34m(self, path, exclude, overrides)\u001b[39m\n\u001b[32m   2242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (path / \u001b[33m\"\u001b[39m\u001b[33mvocab\u001b[39m\u001b[33m\"\u001b[39m).exists() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvocab\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[32m   2243\u001b[39m     \u001b[38;5;66;03m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[32m   2244\u001b[39m     exclude = \u001b[38;5;28mlist\u001b[39m(exclude) + [\u001b[33m\"\u001b[39m\u001b[33mvocab\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserializers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   2246\u001b[39m \u001b[38;5;28mself\u001b[39m._path = path  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m   2247\u001b[39m \u001b[38;5;28mself\u001b[39m._link_components()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\spacy\\util.py:1448\u001b[39m, in \u001b[36mfrom_disk\u001b[39m\u001b[34m(path, readers, exclude)\u001b[39m\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, reader \u001b[38;5;129;01min\u001b[39;00m readers.items():\n\u001b[32m   1446\u001b[39m     \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n\u001b[32m   1447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m         \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\spacy\\language.py:2221\u001b[39m, in \u001b[36mLanguage.from_disk.<locals>.deserialize_vocab\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m   2219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeserialize_vocab\u001b[39m(path: Path) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m path.exists():\n\u001b[32m-> \u001b[39m\u001b[32m2221\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\spacy\\vocab.pyx:550\u001b[39m, in \u001b[36mspacy.vocab.Vocab.from_disk\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\spacy\\strings.pyx:339\u001b[39m, in \u001b[36mspacy.strings.StringStore.from_disk\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\srsly\\_json_api.py:53\u001b[39m, in \u001b[36mread_json\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     51\u001b[39m file_path = force_path(path)\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m file_path.open(\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mujson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mMemoryError\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Ingerir documento con patr√≥n personalizado\n",
        "data_path = find_in_project(\"data\", \"folder\", None)\n",
        "markdown_file = data_path / \"110225.md\"\n",
        "\n",
        "if markdown_file.exists():\n",
        "    print(f\"üì• Ingiriendo con patr√≥n SIMPLE_CHUNK: {markdown_file.name}\")\n",
        "    \n",
        "    chunks = ungraph.ingest_document(\n",
        "        markdown_file,\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        pattern=SIMPLE_CHUNK_PATTERN\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Documento ingerido con patr√≥n personalizado!\")\n",
        "    print(f\"   Chunks creados: {len(chunks)}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Archivo no encontrado: {markdown_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen y Comparaci√≥n de Patrones\n",
        "\n",
        "### Patrones Disponibles\n",
        "\n",
        "| Patr√≥n | Estructura | Uso Recomendado |\n",
        "|--------|-----------|-----------------|\n",
        "| **FILE_PAGE_CHUNK** | File ‚Üí Page ‚Üí Chunk | Default, documentos con p√°ginas |\n",
        "| **SIMPLE_CHUNK** | Solo Chunk | Documentos simples, sin estructura |\n",
        "| **SEQUENTIAL_CHUNKS** | Chunk ‚Üí Chunk (NEXT) | Mantener orden secuencial |\n",
        "| **LEXICAL_GRAPH** | Chunk ‚Üí Entity | Extracci√≥n de conocimiento |\n",
        "| **DOCUMENT_SECTION_PARAGRAPH** | Document ‚Üí Section ‚Üí Paragraph | Documentos con secciones |\n",
        "\n",
        "### Reglas de Validaci√≥n\n",
        "\n",
        "- **Labels de nodos**: Deben empezar con may√∫scula (ej: `Chunk`, `File`)\n",
        "- **Tipos de relaci√≥n**: Solo may√∫sculas y underscores (ej: `NEXT_CHUNK`, `HAS_CHUNK`)\n",
        "- **Propiedades**: Nombres v√°lidos de Python, tipos b√°sicos (str, int, list, etc.)\n",
        "\n",
        "### Mejores Pr√°cticas\n",
        "\n",
        "1. **Empezar con FILE_PAGE_CHUNK**: Es el patr√≥n m√°s completo y probado\n",
        "2. **Crear patrones personalizados**: Solo cuando necesites estructuras espec√≠ficas\n",
        "3. **Validar siempre**: Usa `validate_pattern()` antes de usar un patr√≥n\n",
        "4. **Revisar queries**: Genera y revisa los queries Cypher para entender qu√© se crea\n",
        "\n",
        "### Siguiente Paso\n",
        "\n",
        "Una vez que has construido tu grafo con patrones, contin√∫a con:\n",
        "- **2.2 Smart Chunking Strategies** - Optimizar c√≥mo se dividen los documentos\n",
        "- **3.1 Entity Extraction & Facts** - Extraer conocimiento del grafo\n",
        "\n",
        "## Referencias\n",
        "\n",
        "- [Patrones de Grafo](../../docs/concepts/graph-patterns.md)\n",
        "- [Patrones Personalizados](../../docs/guides/custom-patterns.md)\n",
        "- [GraphPattern API](../../src/domain/value_objects/graph_pattern.py)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
