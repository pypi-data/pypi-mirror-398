"""
Production-grade Adaptive Router for zarx-IGRIS.
Determines efficiency gains - the MOST critical component.
MUST converge stably, MUST make smart decisions, MUST train efficiently.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributions as dist
from typing import Dict, List, Tuple, Optional, Union, Any
import math
import numpy as np
from dataclasses import dataclass, field
import warnings

from zarx.config import IgrisConfig as IgrisConfig # Corrected alias
from zarx.utils.logger import get_logger
from zarx.utils.math_utils import TensorStability, InformationTheory, RoutingMathematics

logger = get_logger()


# ==================== DATA STRUCTURES ====================

@dataclass
class RoutingDecision:
    """Complete routing decision for a token."""
    # Depth routing (which layers to use)
    depth_logits: torch.Tensor  # [batch, seq, max_depth]
    depth_probs: torch.Tensor   # [batch, seq, max_depth]
    depth_mask: torch.Tensor    # [batch, seq, max_depth] binary
    
    # Width routing (how much compute per layer)
    width_logits: torch.Tensor  # [batch, seq, num_widths]
    width_probs: torch.Tensor   # [batch, seq, num_widths]
    width_idx: torch.Tensor     # [batch, seq] index
    width_multiplier: torch.Tensor  # [batch, seq, 1] scaled
    
    # Path routing (which HASS pathways to use)
    path_logits: torch.Tensor   # [batch, seq, num_paths]
    path_probs: torch.Tensor    # [batch, seq, num_paths]
    
    # Expert routing (which MoE experts to use)
    expert_logits: torch.Tensor  # [batch, seq, num_experts]
    expert_probs: torch.Tensor   # [batch, seq, num_experts]
    expert_indices: torch.Tensor  # [batch, seq, top_k] indices
    expert_weights: torch.Tensor  # [batch, seq, top_k] weights
    
    # Token complexity
    complexity: torch.Tensor  # [batch, seq, 1]
    
    # Uncertainty
    uncertainty: torch.Tensor  # [batch, seq, 1]
    
    # Auxiliary outputs
    auxiliary: Dict[str, torch.Tensor] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            'depth_probs': self.depth_probs.detach().cpu(),
            'width_idx': self.width_idx.detach().cpu(),
            'path_probs': self.path_probs.detach().cpu(),
            'expert_indices': self.expert_indices.detach().cpu(),
            'expert_weights': self.expert_weights.detach().cpu(),
            'complexity': self.complexity.detach().cpu(),
            'uncertainty': self.uncertainty.detach().cpu(),
        }
    
    def compute_statistics(self) -> Dict[str, float]:
        """Compute routing statistics."""
        stats = {}
        
        # Depth statistics
        depth_active = self.depth_mask.float().mean().item()
        stats['depth_active_mean'] = depth_active
        stats['depth_active_std'] = self.depth_mask.float().std().item()
        
        # Average active layers per token
        active_layers = self.depth_mask.sum(dim=-1).float().mean().item()
        stats['active_layers_per_token'] = active_layers
        
        # Width statistics
        stats['width_mean'] = self.width_multiplier.mean().item()
        stats['width_std'] = self.width_multiplier.std().item()
        
        # Path statistics
        path_entropy = -torch.sum(self.path_probs * torch.log(self.path_probs + 1e-12), dim=-1).mean().item()
        stats['path_entropy'] = path_entropy
        
        # Expert statistics
        expert_utilization = (self.expert_weights > 0.01).float().sum(dim=-1).mean().item()
        stats['expert_utilization'] = expert_utilization
        
        # Complexity statistics
        stats['complexity_mean'] = self.complexity.mean().item()
        stats['complexity_std'] = self.complexity.std().item()
        
        # Uncertainty statistics
        stats['uncertainty_mean'] = self.uncertainty.mean().item()
        
        return stats
    
    def compute_efficiency(self) -> Dict[str, float]:
        """Compute routing efficiency metrics."""
        batch, seq_len, max_depth = self.depth_mask.shape
        
        # Compute cost metrics
        total_possible_compute = batch * seq_len * max_depth
        actual_compute = self.depth_mask.sum().item()
        
        compute_efficiency = 1.0 - (actual_compute / total_possible_compute)
        
        # Width-adjusted compute
        width_adjusted_compute = (self.depth_mask.float() * self.width_multiplier).sum().item()
        max_width_compute = total_possible_compute * self.width_multiplier.max().item()
        
        width_efficiency = 1.0 - (width_adjusted_compute / max_width_compute)
        
        # Overall efficiency
        overall_efficiency = (compute_efficiency + width_efficiency) / 2
        
        return {
            'compute_efficiency': compute_efficiency,
            'width_efficiency': width_efficiency,
            'overall_efficiency': overall_efficiency,
            'active_ratio': actual_compute / total_possible_compute,
            'width_adjusted_ratio': width_adjusted_compute / max_width_compute,
        }


@dataclass 
class RoutingMetrics:
    """Training metrics for router."""
    step: int = 0
    loss_total: float = 0.0
    loss_routing: float = 0.0
    loss_balancing: float = 0.0
    loss_consistency: float = 0.0
    
    # Statistics
    depth_active_mean: float = 0.0
    width_mean: float = 0.0
    complexity_mean: float = 0.0
    uncertainty_mean: float = 0.0
    
    # Efficiency
    compute_efficiency: float = 0.0
    overall_efficiency: float = 0.0
    
    # Gradients
    grad_norm: float = 0.0
    grad_mean: float = 0.0
    
    def update(self, step: int, loss_dict: Dict[str, torch.Tensor], 
               decision: RoutingDecision, grad_stats: Dict[str, float]):
        """Update metrics."""
        self.step = step
        
        # Losses
        self.loss_total = loss_dict.get('total', 0.0)
        self.loss_routing = loss_dict.get('routing', 0.0)
        self.loss_balancing = loss_dict.get('balancing', 0.0)
        self.loss_consistency = loss_dict.get('consistency', 0.0)
        
        # Statistics from decision
        stats = decision.compute_statistics()
        self.depth_active_mean = stats.get('depth_active_mean', 0.0)
        self.width_mean = stats.get('width_mean', 0.0)
        self.complexity_mean = stats.get('complexity_mean', 0.0)
        self.uncertainty_mean = stats.get('uncertainty_mean', 0.0)
        
        # Efficiency
        efficiency = decision.compute_efficiency()
        self.compute_efficiency = efficiency.get('compute_efficiency', 0.0)
        self.overall_efficiency = efficiency.get('overall_efficiency', 0.0)
        
        # Gradients
        self.grad_norm = grad_stats.get('grad_norm', 0.0)
        self.grad_mean = grad_stats.get('grad_mean', 0.0)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            'step': self.step,
            'loss_total': self.loss_total,
            'loss_routing': self.loss_routing,
            'loss_balancing': self.loss_balancing,
            'loss_consistency': self.loss_consistency,
            'depth_active_mean': self.depth_active_mean,
            'width_mean': self.width_mean,
            'complexity_mean': self.complexity_mean,
            'uncertainty_mean': self.uncertainty_mean,
            'compute_efficiency': self.compute_efficiency,
            'overall_efficiency': self.overall_efficiency,
            'grad_norm': self.grad_norm,
            'grad_mean': self.grad_mean,
        }


# ==================== ROUTER CORE ====================

class AdaptiveRouter(nn.Module):
    """
    Core router for zarx-IGRIS.
    Takes token embeddings + CoT features â†’ routing decisions.
    """
    
    def __init__(self, config: IgrisConfig): # Corrected config type
        super().__init__()
        self.config = config
        
        # Input dimensions
        self.hidden_dim = config.hidden_size
        self.cot_dim = config.cot_dim * config.cot_components
        self.input_dim = self.hidden_dim + self.cot_dim
        
        # Output dimensions
        self.max_depth = config.max_depth
        self.num_widths = len(config.width_choices)
        self.num_paths = 3  # Local, Low-rank, SSM
        self.num_experts = config.expert_count
        self.top_k = config.top_k_experts
        
        # Network architecture
        self._build_network()
        
        # Temperature for gumbel softmax
        self.temperature = config.router_temperature
        self.temperature_annealing = config.router_temperature_annealing
        
        # Load balancing
        self.load_balancing_weight = config.load_balancing_weight
        
        # Training state
        self.training_step = 0
        self.metrics_history: List[RoutingMetrics] = []
        
        # Expert usage tracking
        self.expert_usage = torch.zeros(self.num_experts, dtype=torch.float)
        self.expert_load = torch.zeros(self.num_experts, dtype=torch.float)
        
        # Initialize weights
        self._init_weights()
        
        logger.info("router", 
                   f"Initialized AdaptiveRouter: depth={self.max_depth}, "
                   f"widths={self.num_widths}, experts={self.num_experts}, "
                   f"top_k={self.top_k}")
    
    def _build_network(self):
        """Build router network architecture."""
        # Feature encoder (shared backbone)
        self.feature_encoder = nn.Sequential(
            nn.Linear(self.input_dim, 1024),
            nn.LayerNorm(1024),
            nn.GELU(),
            nn.Dropout(self.config.router_dropout),
            
            nn.Linear(1024, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(self.config.router_dropout),
            
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.GELU(),
        )
        
        # Depth router
        self.depth_router = nn.Sequential(
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Linear(128, self.max_depth),
        )
        
        # Width router
        self.width_router = nn.Sequential(
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Linear(128, self.num_widths),
        )
        
        # Path router (HASS pathways)
        self.path_router = nn.Sequential(
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Linear(128, self.num_paths),
        )
        
        # Expert router (MoE)
        self.expert_router = nn.Sequential(
            nn.Linear(256, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Linear(256, self.num_experts),
        )
        
        # Complexity estimator
        self.complexity_estimator = nn.Sequential(
            nn.Linear(256, 64),
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Linear(64, 1),
            nn.Sigmoid(),
        )
        
        # Uncertainty estimator
        self.uncertainty_estimator = nn.Sequential(
            nn.Linear(256, 64),
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Linear(64, 1),
            nn.Sigmoid(),
        )
        
        # Width value network (maps index to multiplier)
        width_tensor = torch.tensor(self.config.width_choices, dtype=torch.float32)
        self.register_buffer('width_values', width_tensor)
        self.width_normalizer = self.config.hidden_size  # Normalize to [0, 1]
    
    def _init_weights(self):
        """Initialize router weights for stable training."""
        # Feature encoder
        for layer in self.feature_encoder:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight, gain=0.5)
                nn.init.zeros_(layer.bias)
        
        # Routers
        for router in [self.depth_router, self.width_router, 
                      self.path_router, self.expert_router]:
            for layer in router:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight, gain=0.1)
                    nn.init.zeros_(layer.bias)
        
        # Estimators
        for estimator in [self.complexity_estimator, self.uncertainty_estimator]:
            for layer in estimator:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight, gain=0.5)
                    nn.init.zeros_(layer.bias)
    
    def forward(
        self,
        x: torch.Tensor,  # Token embeddings [batch, seq, hidden]
        cot_features: torch.Tensor,  # CoT features [batch, seq, cot_dim]
        training: bool = True,
        deterministic: bool = False,
        expert_capacity: Optional[int] = None,
    ) -> RoutingDecision:
        """
        Main forward pass.
        
        Args:
            x: Token embeddings
            cot_features: CoT features
            training: Whether in training mode
            deterministic: Whether to use deterministic routing
            expert_capacity: Capacity for expert routing
            
        Returns:
            RoutingDecision with all routing information
        """
        batch_size, seq_len, _ = x.shape
        
        # Concatenate inputs
        router_input = torch.cat([x, cot_features], dim=-1)  # [batch, seq, input_dim]
        
        # Encode features
        features = self.feature_encoder(router_input)  # [batch, seq, 256]
        
        # Get all logits
        depth_logits = self.depth_router(features)  # [batch, seq, max_depth]
        width_logits = self.width_router(features)  # [batch, seq, num_widths]
        path_logits = self.path_router(features)  # [batch, seq, num_paths]
        expert_logits = self.expert_router(features)  # [batch, seq, num_experts]
        
        # Estimate complexity and uncertainty
        complexity = self.complexity_estimator(features)  # [batch, seq, 1]
        uncertainty = self.uncertainty_estimator(features)  # [batch, seq, 1]
        
        # Apply temperature
        current_temp = self.temperature
        if self.temperature_annealing and training:
            # Anneal temperature over training
            current_temp = max(0.1, self.temperature * (0.99 ** (self.training_step / 1000)))
        
        # Depth routing
        depth_probs, depth_mask = self._route_depth(
            depth_logits, complexity, current_temp, training, deterministic
        )
        
        # Width routing
        width_probs, width_idx, width_multiplier = self._route_width(
            width_logits, complexity, current_temp, training, deterministic
        )
        
        # Path routing
        path_probs = self._route_path(
            path_logits, current_temp, training, deterministic
        )
        
        # Expert routing (most complex)
        expert_probs, expert_indices, expert_weights = self._route_experts(
            expert_logits, current_temp, training, deterministic, expert_capacity
        )
        
        # Update expert usage statistics
        if training:
            self._update_expert_usage(expert_indices, expert_weights)
        
        # Create routing decision
        decision = RoutingDecision(
            depth_logits=depth_logits,
            depth_probs=depth_probs,
            depth_mask=depth_mask,
            width_logits=width_logits,
            width_probs=width_probs,
            width_idx=width_idx,
            width_multiplier=width_multiplier,
            path_logits=path_logits,
            path_probs=path_probs,
            expert_logits=expert_logits,
            expert_probs=expert_probs,
            expert_indices=expert_indices,
            expert_weights=expert_weights,
            complexity=complexity,
            uncertainty=uncertainty,
            auxiliary={
                'features': features,
                'temperature': torch.tensor(current_temp),
            }
        )
        
        return decision
    
    def _route_depth(
        self,
        logits: torch.Tensor,
        complexity: torch.Tensor,
        temperature: float,
        training: bool,
        deterministic: bool
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Route depth (which layers to use).
        
        Complexity influences depth: complex tokens use more layers.
        """
        batch_size, seq_len, max_depth = logits.shape
        
        # Adjust logits with complexity
        # Complex tokens get boosted for deeper layers
        depth_bias = torch.linspace(0, 1, max_depth, device=logits.device).unsqueeze(0).unsqueeze(0)
        adjusted_logits = logits + complexity * depth_bias * 2.0
        
        # Apply temperature
        scaled_logits = adjusted_logits / max(temperature, 1e-8)
        
        if deterministic:
            # Hard routing for inference
            probs = torch.sigmoid(scaled_logits)
            mask = (probs > 0.5).float()
        else:
            # Gumbel-sigmoid for training (differentiable binary)
            if training:
                # Gumbel noise
                gumbel_noise = -torch.log(-torch.log(torch.rand_like(scaled_logits) + 1e-10) + 1e-10)
                noisy_logits = (scaled_logits + gumbel_noise) / temperature
                probs = torch.sigmoid(noisy_logits)
                
                # Straight-through estimator
                mask_hard = (probs > 0.5).float()
                mask = mask_hard - probs.detach() + probs
            else:
                # Soft for evaluation
                probs = torch.sigmoid(scaled_logits)
                mask = (probs > 0.5).float()
        
        # Ensure minimum depth
        min_depth = self.config.min_depth
        for i in range(min_depth):
            mask[..., i] = 1.0
        
        return probs, mask
    
    def _route_width(
        self,
        logits: torch.Tensor,
        complexity: torch.Tensor,
        temperature: float,
        training: bool,
        deterministic: bool
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Route width (how much compute per layer).
        
        Complexity influences width: complex tokens get more compute.
        """
        # Adjust logits with complexity
        # Complex tokens pushed toward higher widths
        width_bias = torch.linspace(-1, 1, self.num_widths, device=logits.device).unsqueeze(0).unsqueeze(0)
        adjusted_logits = logits + complexity * width_bias * 3.0
        
        # Apply temperature
        scaled_logits = adjusted_logits / max(temperature, 1e-8)
        
        if deterministic:
            # Hard selection for inference
            probs = F.softmax(scaled_logits, dim=-1)
            width_idx = torch.argmax(probs, dim=-1)
        else:
            # Gumbel-softmax for training
            if training:
                probs = F.gumbel_softmax(scaled_logits, tau=temperature, hard=False, dim=-1)
                width_idx = torch.argmax(probs, dim=-1)
            else:
                probs = F.softmax(scaled_logits, dim=-1)
                width_idx = torch.argmax(probs, dim=-1)
        
        # Convert index to multiplier
        width_multiplier = self.width_values[width_idx] / self.width_normalizer
        
        return probs, width_idx, width_multiplier.unsqueeze(-1)
    
    def _route_path(
        self,
        logits: torch.Tensor,
        temperature: float,
        training: bool,
        deterministic: bool
    ) -> torch.Tensor:
        """
        Route HASS pathways (Local, Low-rank, SSM).
        
        Returns soft probabilities for each pathway.
        """
        scaled_logits = logits / max(temperature, 1e-8)
        
        if deterministic:
            probs = F.softmax(scaled_logits, dim=-1)
        else:
            if training:
                # Gumbel-softmax
                probs = F.gumbel_softmax(scaled_logits, tau=temperature, hard=False, dim=-1)
            else:
                probs = F.softmax(scaled_logits, dim=-1)
        
        return probs
    
    def _route_experts(
        self,
        logits: torch.Tensor,
        temperature: float,
        training: bool,
        deterministic: bool,
        expert_capacity: Optional[int] = None
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Route to MoE experts.
        
        Implements capacity-aware routing to prevent overload.
        """
        batch_size, seq_len, num_experts = logits.shape
        num_tokens = batch_size * seq_len
        
        # Default capacity: tokens per expert
        if expert_capacity is None:
            expert_capacity = max(1, int(num_tokens * 1.25 / num_experts))
        
        # Reshape for routing
        logits_flat = logits.view(-1, num_experts)  # [num_tokens, num_experts]
        
        if deterministic:
            # Top-k routing for inference
            top_k_weights, top_k_indices = torch.topk(
                F.softmax(logits_flat, dim=-1), 
                self.top_k, 
                dim=-1
            )
            
            # Normalize weights
            top_k_weights = top_k_weights / (top_k_weights.sum(dim=-1, keepdim=True) + 1e-12)
            
        else:
            # Gumbel-softmax top-k for training
            if training:
                # Add gumbel noise
                gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits_flat) + 1e-10) + 1e-10)
                noisy_logits = (logits_flat + gumbel_noise) / max(temperature, 1e-8)
                
                # Top-k with softmax
                top_k_weights = F.softmax(noisy_logits, dim=-1)
                top_k_weights, top_k_indices = torch.topk(top_k_weights, self.top_k, dim=-1)
                
                # Normalize
                top_k_weights = top_k_weights / (top_k_weights.sum(dim=-1, keepdim=True) + 1e-12)
                
                # Apply capacity constraint
                if expert_capacity > 0:
                    top_k_weights = self._apply_capacity_constraint(
                        top_k_weights, top_k_indices, expert_capacity
                    )
            else:
                # Soft top-k for evaluation
                probs = F.softmax(logits_flat / temperature, dim=-1)
                top_k_weights, top_k_indices = torch.topk(probs, self.top_k, dim=-1)
                top_k_weights = top_k_weights / (top_k_weights.sum(dim=-1, keepdim=True) + 1e-12)
        
        # Reshape back
        expert_probs = F.softmax(logits / temperature, dim=-1)
        expert_indices = top_k_indices.view(batch_size, seq_len, self.top_k)
        expert_weights = top_k_weights.view(batch_size, seq_len, self.top_k)
        
        return expert_probs, expert_indices, expert_weights
    
    def _apply_capacity_constraint(
        self,
        weights: torch.Tensor,
        indices: torch.Tensor,
        capacity: int
    ) -> torch.Tensor:
        """
        Apply capacity constraint to expert routing.
        Prevents any expert from getting too many tokens.
        """
        num_tokens, top_k = weights.shape
        num_experts = self.num_experts
        
        # Initialize expert load
        expert_load = torch.zeros(num_experts, device=weights.device)
        
        # Sort tokens by routing weight (highest first)
        token_values, token_order = torch.sort(
            weights.flatten(), descending=True
        )
        
        # Process tokens in order of routing weight
        adjusted_weights = weights.clone()
        
        for sorted_flat_idx in range(len(token_order)):
            original_flat_idx = token_order[sorted_flat_idx] # Get the original flattened index
            token_idx = original_flat_idx // top_k
            k_idx = original_flat_idx % top_k
            
            expert_idx = indices[token_idx, k_idx]
            weight = weights[token_idx, k_idx]
            
            # Check if expert has capacity
            if expert_load[expert_idx] < capacity:
                expert_load[expert_idx] += 1
            else:
                # Expert is full, set weight to 0
                adjusted_weights[token_idx, k_idx] = 0.0
        
        # Renormalize weights
        adjusted_weights = adjusted_weights / (adjusted_weights.sum(dim=-1, keepdim=True) + 1e-12)
        
        return adjusted_weights
    
    def _update_expert_usage(self, expert_indices: torch.Tensor, expert_weights: torch.Tensor):
        """Update expert usage statistics."""
        batch_size, seq_len, top_k = expert_indices.shape
        
        # Flatten
        indices_flat = expert_indices.view(-1)
        weights_flat = expert_weights.view(-1)
        
        # Update usage counts
        for i in range(indices_flat.shape[0]):
            expert_idx = indices_flat[i].item()
            weight = weights_flat[i].item()
            
            if weight > 0.01:  # Threshold
                self.expert_usage[expert_idx] += 1
                self.expert_load[expert_idx] += weight
    
    def compute_loss(
        self,
        decision: RoutingDecision,
        target_complexity: Optional[torch.Tensor] = None,
        balance_experts: bool = True,
    ) -> Dict[str, torch.Tensor]:
        """
        Compute routing losses.
        
        Args:
            decision: Routing decision
            target_complexity: Target complexity if available
            balance_experts: Whether to add expert balancing loss
            
        Returns:
            Dictionary of losses
        """
        losses = {}
        
        # 1. Routing loss: encourage confident decisions
        routing_loss = self._compute_routing_loss(decision)
        losses['routing'] = routing_loss
        
        # 2. Consistency loss: similar tokens should have similar routing
        consistency_loss = self._compute_consistency_loss(decision)
        losses['consistency'] = consistency_loss
        
        # 3. Complexity loss: if targets available
        if target_complexity is not None:
            complexity_loss = self._compute_complexity_loss(decision.complexity, target_complexity)
            losses['complexity'] = complexity_loss
        
        # 4. Expert balancing loss
        if balance_experts:
            balancing_loss = self._compute_balancing_loss(decision)
            losses['balancing'] = balancing_loss * self.load_balancing_weight
        
        # 5. Efficiency loss: encourage efficiency
        efficiency_loss = self._compute_efficiency_loss(decision)
        losses['efficiency'] = efficiency_loss
        
        # Total loss
        total_loss = sum(losses.values())
        losses['total'] = total_loss
        
        return losses
    
    def _compute_routing_loss(self, decision: RoutingDecision) -> torch.Tensor:
        """Loss to encourage confident routing decisions."""
        # Encourage low uncertainty
        uncertainty_loss = decision.uncertainty.mean()
        
        # Encourage sharp distributions (low entropy)
        depth_entropy = -torch.sum(
            decision.depth_probs * torch.log(decision.depth_probs + 1e-12),
            dim=-1
        ).mean()
        
        width_entropy = -torch.sum(
            decision.width_probs * torch.log(decision.width_probs + 1e-12),
            dim=-1
        ).mean()
        
        path_entropy = -torch.sum(
            decision.path_probs * torch.log(decision.path_probs + 1e-12),
            dim=-1
        ).mean()
        
        expert_entropy = -torch.sum(
            decision.expert_probs * torch.log(decision.expert_probs + 1e-12),
            dim=-1
        ).mean()
        
        total_entropy = (depth_entropy + width_entropy + path_entropy + expert_entropy) / 4
        
        # Combined loss
        routing_loss = uncertainty_loss + total_entropy * 0.1
        
        return routing_loss
    
    def _compute_consistency_loss(self, decision: RoutingDecision) -> torch.Tensor:
        """Loss to encourage consistent routing for similar tokens."""
        batch_size, seq_len, _ = decision.depth_probs.shape
        
        if seq_len < 2:
            return torch.tensor(0.0, device=decision.depth_probs.device)
        
        # Compare consecutive tokens
        depth_diff = torch.mean((decision.depth_probs[:, 1:] - decision.depth_probs[:, :-1]) ** 2)
        width_diff = torch.mean((decision.width_multiplier[:, 1:] - decision.width_multiplier[:, :-1]) ** 2)
        complexity_diff = torch.mean((decision.complexity[:, 1:] - decision.complexity[:, :-1]) ** 2)
        
        consistency_loss = (depth_diff + width_diff + complexity_diff) / 3
        
        return consistency_loss
    
    def _compute_complexity_loss(
        self, 
        predicted: torch.Tensor, 
        target: torch.Tensor
    ) -> torch.Tensor:
        """Loss for complexity prediction."""
        return F.mse_loss(predicted, target)
    
    def _compute_balancing_loss(self, decision: RoutingDecision) -> torch.Tensor:
        """Load balancing loss for experts."""
        batch_size, seq_len, num_experts = decision.expert_probs.shape
        
        # Reshape
        probs_flat = decision.expert_probs.view(-1, num_experts)
        
        # Importance (sum of squares)
        importance = torch.sum(probs_flat ** 2, dim=0)
        
        # Load (sum of probabilities)
        load = torch.sum(probs_flat, dim=0)
        
        # Coefficient of variation loss
        importance_mean = torch.mean(importance)
        importance_std = torch.std(importance)
        importance_cv = importance_std / (importance_mean + 1e-12)
        
        load_mean = torch.mean(load)
        load_std = torch.std(load)
        load_cv = load_std / (load_mean + 1e-12)
        
        balancing_loss = importance_cv + load_cv
        
        return balancing_loss
    
    def _compute_efficiency_loss(self, decision: RoutingDecision) -> torch.Tensor:
        """Loss to encourage compute efficiency."""
        efficiency = decision.compute_efficiency()
        
        # Target efficiency (configurable)
        target_efficiency = 0.9  # 90% efficiency target
        
        # Loss: encourage high efficiency
        compute_efficiency = efficiency['compute_efficiency']
        efficiency_loss = F.relu(torch.tensor(target_efficiency - compute_efficiency))
        
        return efficiency_loss
    
    def get_expert_statistics(self) -> Dict[str, Any]:
        """Get expert usage statistics."""
        total_usage = self.expert_usage.sum().item()
        
        if total_usage > 0:
            usage_normalized = self.expert_usage / total_usage
            load_normalized = self.expert_load / (self.expert_load.sum() + 1e-12)
        else:
            usage_normalized = torch.zeros_like(self.expert_usage)
            load_normalized = torch.zeros_like(self.expert_load)
        
        # Statistics
        usage_mean = usage_normalized.mean().item()
        usage_std = usage_normalized.std().item()
        usage_min = usage_normalized.min().item()
        usage_max = usage_normalized.max().item()
        
        load_mean = load_normalized.mean().item()
        load_std = load_normalized.std().item()
        
        # Count experts with significant usage
        active_experts = (usage_normalized > 0.001).sum().item()
        
        return {
            'total_usage': total_usage,
            'usage_mean': usage_mean,
            'usage_std': usage_std,
            'usage_min': usage_min,
            'usage_max': usage_max,
            'load_mean': load_mean,
            'load_std': load_std,
            'active_experts': active_experts,
            'total_experts': self.num_experts,
            'activation_rate': active_experts / self.num_experts,
        }
    
    def reset_expert_statistics(self):
        """Reset expert usage statistics."""
        self.expert_usage.zero_()
        self.expert_load.zero_()
    
    def get_gradient_statistics(self) -> Dict[str, float]:
        """Get gradient statistics for debugging."""
        grad_stats = {}
        
        total_norm = 0.0
        total_mean = 0.0
        param_count = 0
        
        for name, param in self.named_parameters():
            if param.grad is not None:
                grad = param.grad
                grad_stats[f'{name}_norm'] = grad.norm().item()
                grad_stats[f'{name}_mean'] = grad.mean().item()
                if grad.numel() > 1:
                    grad_stats[f'{name}_std'] = grad.std().item()
                else:
                    grad_stats[f'{name}_std'] = 0.0
                
                total_norm += grad.norm().item() ** 2
                total_mean += grad.mean().item()
                param_count += 1
        
        if param_count > 0:
            grad_stats['grad_norm'] = math.sqrt(total_norm)
            grad_stats['grad_mean'] = total_mean / param_count
        
        return grad_stats
    
    def update_training_step(self):
        """Update training step counter for annealing."""
        self.training_step += 1
    
    def get_metrics(self) -> RoutingMetrics:
        """Get current metrics."""
        if self.metrics_history:
            return self.metrics_history[-1]
        return RoutingMetrics()
    
    def log_metrics(self, step: int, loss_dict: Dict[str, torch.Tensor], 
                   decision: RoutingDecision):
        """Log routing metrics."""
        grad_stats = self.get_gradient_statistics()
        
        metrics = RoutingMetrics()
        metrics.update(step, loss_dict, decision, grad_stats)
        
        self.metrics_history.append(metrics)
        
        # Log periodically
        if step % 100 == 0:
            logger.info("router", 
                       f"Step {step}: Loss={metrics.loss_total:.4f}, "
                       f"Depth={metrics.depth_active_mean:.3f}, "
                       f"Width={metrics.width_mean:.3f}, "
                       f"Eff={metrics.overall_efficiency:.3f}, "
                       f"Grad={metrics.grad_norm:.4f}")


# ==================== ROUTER TRAINER ====================

class RouterTrainer:
    """
    Specialized trainer for router to ensure stable convergence.
    Router is the hardest part to train - needs careful handling.
    """
    
    def __init__(
        self,
        router: AdaptiveRouter,
        optimizer: torch.optim.Optimizer,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
        grad_clip: float = 1.0,
        warmup_steps: int = 1000,
        complexity_targets: bool = False,
    ):
        """
        Initialize router trainer.
        
        Args:
            router: AdaptiveRouter instance
            optimizer: Optimizer for router
            scheduler: Learning rate scheduler
            grad_clip: Gradient clipping value
            warmup_steps: Warmup steps for router training
            complexity_targets: Whether complexity targets are available
        """
        self.router = router
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.grad_clip = grad_clip
        self.warmup_steps = warmup_steps
        self.complexity_targets = complexity_targets
        
        # Training state
        self.step = 0
        self.best_loss = float('inf')
        self.patience = 0
        self.max_patience = 1000
        
        # Gradient accumulation
        self.grad_accum_steps = 1
        self.grad_accum_count = 0
        
        # Monitoring
        self.loss_history = []
        self.efficiency_history = []
        
        logger.info("router_trainer", f"Initialized RouterTrainer (warmup={warmup_steps})")
    
    def train_step(
        self,
        x: torch.Tensor,
        cot_features: torch.Tensor,
        target_complexity: Optional[torch.Tensor] = None,
        balance_experts: bool = True,
    ) -> Tuple[RoutingDecision, Dict[str, torch.Tensor]]:
        """
        Perform one training step.
        
        Returns:
            (decision, losses)
        """
        self.router.train()
        self.step += 1
        
        # Forward pass
        decision = self.router(
            x, cot_features, 
            training=True, 
            deterministic=False
        )
        
        # Compute losses
        losses = self.router.compute_loss(
            decision, target_complexity, balance_experts
        )
        
        # Scale loss for gradient accumulation
        loss = losses['total'] / self.grad_accum_steps
        
        # Backward
        loss.backward()
        self.grad_accum_count += 1
        
        # Update if accumulation steps reached
        if self.grad_accum_count >= self.grad_accum_steps:
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(
                self.router.parameters(), 
                self.grad_clip
            )
            
            # Optimizer step
            self.optimizer.step()
            
            # Scheduler step
            if self.scheduler is not None:
                self.scheduler.step()
            
            # Zero gradients
            self.optimizer.zero_grad()
            self.grad_accum_count = 0
            
            # Update router training step
            self.router.update_training_step()
        
        # Log metrics
        if self.step % 10 == 0:
            self.router.log_metrics(self.step, losses, decision)
        
        # Update best loss
        current_loss = losses['total'].item()
        self.loss_history.append(current_loss)
        
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            self.patience = 0
        else:
            self.patience += 1
        
        # Track efficiency
        efficiency = decision.compute_efficiency()['overall_efficiency']
        self.efficiency_history.append(efficiency)
        
        return decision, losses
    
    def evaluate(
        self,
        x: torch.Tensor,
        cot_features: torch.Tensor,
        target_complexity: Optional[torch.Tensor] = None,
    ) -> Tuple[RoutingDecision, Dict[str, float]]:
        """
        Evaluate router.
        
        Returns:
            (decision, metrics)
        """
        self.router.eval()
        
        with torch.no_grad():
            # Forward pass (deterministic)
            decision = self.router(
                x, cot_features,
                training=False,
                deterministic=True
            )
            
            # Compute losses
            losses = self.router.compute_loss(
                decision, target_complexity, balance_experts=False
            )
            
            # Convert to float
            loss_metrics = {k: v.item() for k, v in losses.items()}
            
            # Add statistics
            stats = decision.compute_statistics()
            loss_metrics.update(stats)
            
            # Add efficiency
            efficiency = decision.compute_efficiency()
            loss_metrics.update(efficiency)
            
            # Add expert statistics
            expert_stats = self.router.get_expert_statistics()
            loss_metrics.update(expert_stats)
        
        return decision, loss_metrics
    
    def should_stop(self) -> bool:
        """Check if training should stop."""
        # Early stopping
        if self.patience >= self.max_patience:
            logger.warning("router_trainer", f"Early stopping at step {self.step}")
            return True
        
        # Check for NaN in losses
        if self.loss_history and np.isnan(self.loss_history[-1]):
            logger.error("router_trainer", "Loss became NaN")
            return True
        
        return False
    
    def get_training_summary(self) -> Dict[str, Any]:
        """Get training summary."""
        if not self.loss_history:
            return {}
        
        # Compute statistics
        recent_losses = self.loss_history[-100:] if len(self.loss_history) >= 100 else self.loss_history
        recent_efficiency = self.efficiency_history[-100:] if self.efficiency_history else []
        
        summary = {
            'step': self.step,
            'best_loss': self.best_loss,
            'current_loss': self.loss_history[-1] if self.loss_history else 0.0,
            'loss_mean': np.mean(recent_losses),
            'loss_std': np.std(recent_losses),
            'loss_trend': np.polyfit(range(len(recent_losses)), recent_losses, 1)[0] if len(recent_losses) > 1 else 0.0,
            'efficiency_mean': np.mean(recent_efficiency) if recent_efficiency else 0.0,
            'efficiency_std': np.std(recent_efficiency) if recent_efficiency else 0.0,
            'patience': self.patience,
            'grad_accum_steps': self.grad_accum_steps,
            'grad_clip': self.grad_clip,
        }
        
        # Add router metrics
        router_metrics = self.router.get_metrics()
        summary.update(router_metrics.to_dict())
        
        # Add expert statistics
        expert_stats = self.router.get_expert_statistics()
        summary.update(expert_stats)
        
        return summary
    
    def save_checkpoint(self, path: str):
        """Save trainer checkpoint."""
        checkpoint = {
            'step': self.step,
            'router_state': self.router.state_dict(),
            'optimizer_state': self.optimizer.state_dict(),
            'scheduler_state': self.scheduler.state_dict() if self.scheduler else None,
            'best_loss': self.best_loss,
            'patience': self.patience,
            'loss_history': self.loss_history,
            'efficiency_history': self.efficiency_history,
        }
        
        torch.save(checkpoint, path)
        logger.info("router_trainer", f"Saved checkpoint to {path}")
    
    def load_checkpoint(self, path: str):
        """Load trainer checkpoint."""
        checkpoint = torch.load(path, map_location='cpu')
        
        self.step = checkpoint['step']
        self.router.load_state_dict(checkpoint['router_state'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state'])
        
        if self.scheduler and checkpoint['scheduler_state']:
            self.scheduler.load_state_dict(checkpoint['scheduler_state'])
        
        self.best_loss = checkpoint['best_loss']
        self.patience = checkpoint['patience']
        self.loss_history = checkpoint['loss_history']
        self.efficiency_history = checkpoint['efficiency_history']
        
        logger.info("router_trainer", f"Loaded checkpoint from {path} (step={self.step})")


# ==================== ROUTER VALIDATION ====================

class RouterValidator:
    """
    Validates router decisions against ground truth if available.
    For debugging and analysis.
    """
    
    def __init__(self, config: IgrisConfig): # Corrected config type
        self.config = config
        
        # Ground truth complexity if available
        self.has_ground_truth = False
        self.complexity_correlation = []
        
        # Decision consistency
        self.decision_consistency = []
        
        # Efficiency tracking
        self.efficiency_history = []
    
    def validate_complexity(
        self,
        predicted: torch.Tensor,
        ground_truth: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ) -> Dict[str, float]:
        """Validate complexity predictions."""
        if mask is not None:
            predicted = predicted[mask]
            ground_truth = ground_truth[mask]
        
        # Convert to numpy
        pred_np = predicted.cpu().detach().numpy().flatten()
        gt_np = ground_truth.cpu().detach().numpy().flatten()
        
        # Compute metrics
        mse = np.mean((pred_np - gt_np) ** 2)
        mae = np.mean(np.abs(pred_np - gt_np))
        
        # Correlation
        if len(pred_np) > 1:
            correlation = np.corrcoef(pred_np, gt_np)[0, 1]
        else:
            correlation = 0.0
        
        self.complexity_correlation.append(correlation)
        
        return {
            'complexity_mse': float(mse),
            'complexity_mae': float(mae),
            'complexity_correlation': float(correlation),
        }
    
    def validate_consistency(
        self,
        decision1: RoutingDecision,
        decision2: RoutingDecision,
        similarity_threshold: float = 0.8
    ) -> Dict[str, float]:
        """Validate consistency between two routing decisions."""
        # Compare depth decisions
        depth_similarity = torch.mean(
            (decision1.depth_mask == decision2.depth_mask).float()
        ).item()
        
        # Compare width decisions
        width_agreement = torch.mean(
            (decision1.width_idx == decision2.width_idx).float()
        ).item()
        
        # Compare complexity
        complexity_diff = torch.mean(
            torch.abs(decision1.complexity - decision2.complexity)
        ).item()
        
        consistency_score = (depth_similarity + width_agreement + (1 - complexity_diff)) / 3
        
        self.decision_consistency.append(consistency_score)
        
        return {
            'depth_similarity': depth_similarity,
            'width_agreement': width_agreement,
            'complexity_diff': complexity_diff,
            'consistency_score': consistency_score,
            'is_consistent': consistency_score > similarity_threshold,
        }
    
    def validate_efficiency(
        self,
        decision: RoutingDecision,
        baseline_efficiency: float = 0.1  # Dense model baseline
    ) -> Dict[str, float]:
        """Validate routing efficiency."""
        efficiency = decision.compute_efficiency()
        
        # Gain over baseline
        compute_gain = efficiency['compute_efficiency'] / baseline_efficiency
        overall_gain = efficiency['overall_efficiency'] / baseline_efficiency
        
        self.efficiency_history.append(efficiency['overall_efficiency'])
        
        return {
            **efficiency,
            'compute_gain': compute_gain,
            'overall_gain': overall_gain,
            'is_efficient': efficiency['overall_efficiency'] > 0.5,  # 50% threshold
        }
    
    def validate_expert_usage(
        self,
        decision: RoutingDecision,
        ideal_utilization: float = 0.8  # 80% of experts should be used
    ) -> Dict[str, float]:
        """Validate expert usage."""
        batch_size, seq_len, num_experts = decision.expert_probs.shape
        
        # Count active experts (used by at least one token)
        expert_used = (decision.expert_probs.sum(dim=(0, 1)) > 0.01).float()
        active_experts = expert_used.sum().item()
        
        # Utilization rate
        utilization_rate = active_experts / num_experts
        
        # Load balancing
        expert_load = decision.expert_probs.sum(dim=(0, 1))
        load_std = expert_load.std().item()
        load_mean = expert_load.mean().item()
        load_cv = load_std / (load_mean + 1e-12)
        
        return {
            'total_experts': num_experts,
            'active_experts': active_experts,
            'utilization_rate': utilization_rate,
            'load_mean': load_mean,
            'load_std': load_std,
            'load_cv': load_cv,
            'is_balanced': load_cv < 1.0,  # Coefficient of variation < 1
            'is_utilized': utilization_rate > ideal_utilization,
        }
    
    def get_summary(self) -> Dict[str, Any]:
        """Get validation summary."""
        summary = {}
        
        if self.complexity_correlation:
            summary['avg_complexity_correlation'] = np.mean(self.complexity_correlation)
            summary['std_complexity_correlation'] = np.std(self.complexity_correlation)
        
        if self.decision_consistency:
            summary['avg_decision_consistency'] = np.mean(self.decision_consistency)
            summary['std_decision_consistency'] = np.std(self.decision_consistency)
        
        if self.efficiency_history:
            summary['avg_efficiency'] = np.mean(self.efficiency_history)
            summary['std_efficiency'] = np.std(self.efficiency_history)
            summary['min_efficiency'] = np.min(self.efficiency_history)
            summary['max_efficiency'] = np.max(self.efficiency_history)
        
        return summary
    
    def reset(self):
        """Reset validation statistics."""
        self.complexity_correlation.clear()
        self.decision_consistency.clear()
        self.efficiency_history.clear()


class RoutingRegularizer(nn.Module):
    """
    Regularizer for routing decisions. Placeholder for now.
    """
    def __init__(self, config: IgrisConfig): # Corrected config type
        super().__init__()
        self.config = config

    def forward(self, decision: RoutingDecision) -> torch.Tensor:
        return torch.tensor(0.0, device=decision.depth_mask.device)


# ==================== TESTING ====================

__all__ = [
    'RoutingDecision',
    'RoutingMetrics',
    'AdaptiveRouter',
    'RouterTrainer',
    'RouterValidator',
    'RoutingRegularizer',
]