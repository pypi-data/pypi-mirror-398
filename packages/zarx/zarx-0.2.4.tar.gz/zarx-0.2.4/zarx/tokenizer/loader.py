"""
zarx Tokenizer Loader
Load pretrained tokenizers and manage tokenizer discovery.

This is a CRITICAL module for zarx - it enables:
- Loading zarx pretrained tokenizers by name
- Loading custom tokenizers from file
- Discovering available tokenizers
- Integration with HuggingFace tokenizers

Example:
    >>> from zarx.tokenizer import load_pretrained, list_pretrained
    >>> 
    >>> # List available tokenizers
    >>> print(list_pretrained())
    >>> ['zarx_32k', 'zarx_50k', 'zarx_65k']
    >>> 
    >>> # Load a pretrained tokenizer
    >>> tokenizer = load_pretrained('zarx_32k')
    >>> tokens = tokenizer.encode("Hello world!")
    >>> print(tokens)
    >>> [2, 3245, 8932, 3]
"""

import os
import json
from typing import Optional, List, Dict, Any, Union
from pathlib import Path
import warnings

from .base import BaseTokenizer, TokenizerMetadata, TokenizerRegistry
from zarx.exceptions import TokenizerNotFoundError, TokenizerLoadError
from zarx.utils.logger import get_logger

logger = get_logger()

# Try importing tokenizers library
try:
    from tokenizers import Tokenizer as HFTokenizer
    TOKENIZERS_AVAILABLE = True
except ImportError:
    TOKENIZERS_AVAILABLE = False
    HFTokenizer = None


# =============================================================================
# zarx TOKENIZER WRAPPER (HuggingFace tokenizers library)
# =============================================================================

class ZARXTokenizer(BaseTokenizer):
    """
    zarx tokenizer implementation using HuggingFace tokenizers library.
    
    This wraps the fast tokenizers library for efficient tokenization.
    
    Example:
        >>> tokenizer = ZARXTokenizer.load('/path/to/tokenizer.json')
        >>> tokens = tokenizer.encode("Hello world!")
        >>> text = tokenizer.decode(tokens)
    """
    
    def __init__(self, tokenizer: 'HFTokenizer', metadata: Optional[TokenizerMetadata] = None):
        """
        Initialize zarx tokenizer. 
        
        Args:
            tokenizer: HuggingFace Tokenizer instance
            metadata: Tokenizer metadata
        """
        if not TOKENIZERS_AVAILABLE:
            raise ImportError(
                "tokenizers library required for ZARXTokenizer. "
                "Install with: pip install tokenizers"
            )
        
        super().__init__()
        self._tokenizer = tokenizer
        self._metadata = metadata
        
        # Extract special token IDs
        self._setup_special_tokens()
    
    def _setup_special_tokens(self):
        """Setup special tokens from tokenizer."""
        try:
            # Get special tokens from tokenizer
            special_tokens = self._tokenizer.get_vocab()
            
            # Common special tokens
            if '<pad>' in special_tokens:
                self.pad_token_id = special_tokens['<pad>']
                self.pad_token = '<pad>'
            elif '[PAD]' in special_tokens:
                self.pad_token_id = special_tokens['[PAD]']
                self.pad_token = '[PAD]'
            
            if '<unk>' in special_tokens:
                self.unk_token_id = special_tokens['<unk>']
                self.unk_token = '<unk>'
            elif '[UNK]' in special_tokens:
                self.unk_token_id = special_tokens['[UNK]']
                self.unk_token = '[UNK]'
            
            if '<s>' in special_tokens:
                self.bos_token_id = special_tokens['<s>']
                self.bos_token = '<s>'
            elif '[BOS]' in special_tokens:
                self.bos_token_id = special_tokens['[BOS]']
                self.bos_token = '[BOS]'
            
            if '</s>' in special_tokens:
                self.eos_token_id = special_tokens['</s>']
                self.eos_token = '</s>'
            elif '[EOS]' in special_tokens:
                self.eos_token_id = special_tokens['[EOS]']
                self.eos_token = '[EOS]'
        
        except Exception as e:
            logger.warning("tokenizer.loader", 
                          f"Failed to setup special tokens: {e}. Using defaults.")
    
    def encode(
        self,
        text: str,
        add_special_tokens: bool = True,
        max_length: Optional[int] = None,
        padding: bool = False,
        truncation: bool = False
    ) -> List[int]:
        """
        Encode text to token IDs.
        
        Args:
            text: Input text
            add_special_tokens: Add BOS/EOS tokens
            max_length: Maximum length
            padding: Pad to max_length
            truncation: Truncate to max_length
            
        Returns:
            List of token IDs
        """
        try:
            # Prepare encoding options for the underlying tokenizer
            encode_kwargs = {"add_special_tokens": add_special_tokens}

            if max_length is not None:
                encode_kwargs["truncation"] = truncation
                encode_kwargs["max_length"] = max_length
            
            if padding:
                encode_kwargs["padding"] = "max_length" if max_length else True
                encode_kwargs["pad_id"] = self.pad_token_id
            
            # Encode
            encoding = self._tokenizer.encode(text, **encode_kwargs)
            
            return encoding.ids
        
        except Exception as e:
            raise TokenizerLoadError(
                path=str(text[:50]),
                reason=f"Encoding failed: {e}"
            )
    
    def decode(
        self,
        token_ids: List[int],
        skip_special_tokens: bool = True,
        clean_up_tokenization_spaces: bool = True
    ) -> str:
        """
        Decode token IDs to text.
        
        Args:
            token_ids: List of token IDs
            skip_special_tokens: Skip special tokens
            clean_up_tokenization_spaces: Clean up spaces
            
        Returns:
            Decoded text
        """
        try:
            text = self._tokenizer.decode(
                token_ids,
                skip_special_tokens=skip_special_tokens
            )
            
            if clean_up_tokenization_spaces:
                # Clean up extra spaces
                text = ' '.join(text.split())
            
            return text
        
        except Exception as e:
            raise TokenizerLoadError(
                path=str(token_ids[:10]),
                reason=f"Decoding failed: {e}"
            )
    
    def get_vocab_size(self) -> int:
        """Get vocabulary size."""
        return self._tokenizer.get_vocab_size()
    
    def get_vocab(self) -> Dict[str, int]:
        """Get full vocabulary."""
        return self._tokenizer.get_vocab()
    
    def save(self, path: Union[str, Path], save_metadata: bool = True):
        """
        Save tokenizer to file.
        
        Args:
            path: Path to save tokenizer.json
            save_metadata: Save metadata file alongside
        """
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        # Save tokenizer
        self._tokenizer.save(str(path))
        
        # Save metadata if available
        if save_metadata and self._metadata:
            metadata_path = path.with_suffix('.meta.json')
            self._metadata.save(metadata_path)
        
        logger.info("tokenizer.loader", f"Tokenizer saved to {path}")
    
    @classmethod
    def load(cls, path: Union[str, Path]) -> 'ZARXTokenizer':
        """
        Load tokenizer from file.
        
        Args:
            path: Path to tokenizer.json file
            
        Returns:
            Loaded ZARXTokenizer instance
            
        Raises:
            TokenizerLoadError: If loading fails
        """
        if not TOKENIZERS_AVAILABLE:
            raise ImportError(
                "tokenizers library required. Install with: pip install tokenizers"
            )
        
        path = Path(path)
        
        if not path.exists():
            raise TokenizerLoadError(
                path=str(path),
                reason="File not found"
            )
        
        try:
            # Load tokenizer
            hf_tokenizer = HFTokenizer.from_file(str(path))
            
            # Load metadata if available
            metadata_path = path.with_suffix('.meta.json')
            metadata = None
            
            if metadata_path.exists():
                try:
                    metadata = TokenizerMetadata.load(metadata_path)
                except Exception as e:
                    logger.warning("tokenizer.loader", 
                                  f"Failed to load metadata: {e}")
            
            logger.info("tokenizer.loader", f"Loaded tokenizer from {path}")
            
            return cls(hf_tokenizer, metadata)
        
        except Exception as e:
            raise TokenizerLoadError(
                path=str(path),
                reason=f"Failed to load tokenizer: {e}"
            )


# =============================================================================
# PRETRAINED TOKENIZER DISCOVERY & REGISTRATION
# =============================================================================

_PRETRAINED_TOKENIZERS_REGISTERED = False

def _get_pretrained_tokenizers_dir() -> Path:
    """Get directory containing pretrained tokenizers."""
    # Try to find the pretrained directory
    tokenizer_module_dir = Path(__file__).parent
    pretrained_dir = tokenizer_module_dir / 'pretrained'
    
    if pretrained_dir.exists():
        return pretrained_dir
    
    # Fallback: check package installation directory
    try:
        import zarx
        zarx_dir = Path(zarx.__file__).parent
        pretrained_dir = zarx_dir / 'tokenizer' / 'pretrained'
        
        if pretrained_dir.exists():
            return pretrained_dir
    except:
        pass
    
    # Create directory if it doesn't exist
    pretrained_dir = tokenizer_module_dir / 'pretrained'
    pretrained_dir.mkdir(parents=True, exist_ok=True)
    
    logger.warning("tokenizer.loader", 
                  f"Pretrained tokenizers directory created at {pretrained_dir}. "
                  "No pretrained tokenizers found yet.")
    
    return pretrained_dir

def _load_pretrained_metadata() -> Dict[str, TokenizerMetadata]:
    """
    Load metadata for all pretrained tokenizers from metadata.json.
    
    Returns:
        Dictionary mapping tokenizer names to metadata
    """
    pretrained_dir = _get_pretrained_tokenizers_dir()
    metadata_file = pretrained_dir / 'metadata.json'
    
    if not metadata_file.exists():
        return {}
    
    try:
        with open(metadata_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        result = {}
        for name, meta_dict in data.items():
            if 'name' not in meta_dict:
                meta_dict['name'] = name
            result[name] = TokenizerMetadata.from_dict(meta_dict)
        
        return result
    
    except Exception as e:
        logger.warning("tokenizer.loader", 
                      f"Failed to load pretrained metadata from {metadata_file}: {e}")
        return {}


def _register_pretrained_tokenizers():
    """
    Register all tokenizers listed in metadata.json.

    This function is metadata-driven. It registers all tokenizers found in
    `metadata.json`, making them available via `list_pretrained()`. The actual
    existence of the tokenizer's .json file is checked at load time by
    `load_pretrained()`, which provides a clearer error to the user if a file
    is missing for a registered tokenizer.
    """
    TokenizerRegistry.clear()
    metadata_dict = _load_pretrained_metadata()
    pretrained_dir = _get_pretrained_tokenizers_dir()

    if not metadata_dict:
        logger.warning("tokenizer.loader", "No tokenizer metadata found in metadata.json. No pretrained tokenizers will be registered.")
        return

    # Register each tokenizer from metadata
    for name, metadata in metadata_dict.items():
        path = pretrained_dir / f"{name}.json"
        
        TokenizerRegistry.register(
            name=name,
            tokenizer_class=ZARXTokenizer,
            path=str(path),
            metadata=metadata
        )
    
    if metadata_dict:
        logger.debug("tokenizer.loader", 
                    f"Registered {len(metadata_dict)} pretrained tokenizers from metadata.")


def _ensure_pretrained_registered():
    """Ensure that the pretrained tokenizers are registered, but only once."""
    global _PRETRAINED_TOKENIZERS_REGISTERED
    if _PRETRAINED_TOKENIZERS_REGISTERED:
        return
    
    _register_pretrained_tokenizers()
    _PRETRAINED_TOKENIZERS_REGISTERED = True


# Auto-register on module import
_ensure_pretrained_registered()


# =============================================================================
# PUBLIC API FUNCTIONS
# =============================================================================

def load_pretrained(name: str) -> ZARXTokenizer:
    """
    Load a pretrained zarx tokenizer by name.
    
    This is the PRIMARY way to load tokenizers in zarx.
    
    Args:
        name: Tokenizer name (e.g., 'zarx_32k', 'zarx_50k', 'zarx_65k')
        
    Returns:
        Loaded tokenizer instance
        
    Raises:
        TokenizerNotFoundError: If tokenizer not found in metadata.
        TokenizerLoadError: If loading the tokenizer file fails (e.g., file not found).
        
    Example:
        >>> from zarx.tokenizer import load_pretrained
        >>> tokenizer = load_pretrained('zarx_65k') # Assuming zarx_65k.json exists
        >>> tokens = tokenizer.encode("Hello world!")
        >>> print(tokens)
    """
    _ensure_pretrained_registered()
    
    try:
        tokenizer_class, path, metadata = TokenizerRegistry.get(name)
    except TokenizerNotFoundError:
        available = TokenizerRegistry.list()
        raise TokenizerNotFoundError(name, available_tokenizers=available) from None

    logger.info("tokenizer.loader", f"Loading pretrained tokenizer: {name} from {path}")
    
    # Load tokenizer, which will raise TokenizerLoadError if path doesn't exist
    tokenizer = tokenizer_class.load(path)
    
    # Set metadata if available
    if metadata and not tokenizer.metadata:
        tokenizer.metadata = metadata
    
    return tokenizer


def load_from_path(path: Union[str, Path]) -> ZARXTokenizer:
    """
    Load a tokenizer from a file path.
    
    Use this for loading custom tokenizers not in the pretrained registry.
    
    Args:
        path: Path to tokenizer.json file
        
    Returns:
        Loaded tokenizer instance
        
    Raises:
        TokenizerLoadError: If loading fails
        
    Example:
        >>> from zarx.tokenizer import load_from_path
        >>> tokenizer = load_from_path('/path/to/my_tokenizer.json')
        >>> tokens = tokenizer.encode("Hello!")
    """
    logger.info("tokenizer.loader", f"Loading tokenizer from path: {path}")
    return ZARXTokenizer.load(path)


def list_pretrained() -> List[str]:
    """
    List all available pretrained tokenizers based on metadata.json.
    
    Returns:
        List of tokenizer names
        
    Example:
        >>> from zarx.tokenizer import list_pretrained
        >>> print(list_pretrained())
        ['zarx_32k', 'zarx_50k', 'zarx_65k', 'zarx_opmi_65k']
    """
    _ensure_pretrained_registered()
    return TokenizerRegistry.list()


def list_pretrained_detailed() -> Dict[str, Dict[str, Any]]:
    """
    List pretrained tokenizers with detailed information from metadata.json.
    
    Returns:
        Dictionary with tokenizer info
        
    Example:
        >>> from zarx.tokenizer import list_pretrained_detailed
        >>> info = list_pretrained_detailed()
        >>> print(info['zarx_65k'])
    """
    _ensure_pretrained_registered()
    return TokenizerRegistry.list_detailed()


def get_pretrained_path(name: str) -> Path:
    """
    Get the configured file path for a pretrained tokenizer.
    
    Args:
        name: Tokenizer name
        
    Returns:
        Path to tokenizer file
        
    Raises:
        TokenizerNotFoundError: If tokenizer not found
        
    Example:
        >>> from zarx.tokenizer import get_pretrained_path
        >>> path = get_pretrained_path('zarx_65k')
        >>> print(path)
        /path/to/zarx/tokenizer/pretrained/zarx_65k.json
    """
    _ensure_pretrained_registered()
    _, path, _ = TokenizerRegistry.get(name)
    return Path(path)


def has_pretrained(name: str) -> bool:
    """
    Check if a pretrained tokenizer is listed in metadata.json.
    
    Args:
        name: Tokenizer name
        
    Returns:
        True if tokenizer exists in metadata
        
    Example:
        >>> from zarx.tokenizer import has_pretrained
        >>> if has_pretrained('zarx_65k'):
        ...     tokenizer = load_pretrained('zarx_65k')
    """
    _ensure_pretrained_registered()
    return TokenizerRegistry.has(name)


# This function has served it's purpous of demonstration. So now it is unused and commented.
# def create_empty_pretrained_metadata():
#    """
#    Create an empty metadata.json file for pretrained tokenizers.
   
#    This is a utility function for setting up the pretrained directory.
#    """
#    pretrained_dir = _get_pretrained_tokenizers_dir()
    
#    if not pretrained_dir.exists():
#        logger.warning("tokenizer.loader", 
#                      f"Pretrained tokenizers directory created at {pretrained_dir}. "
#                      "No pretrained tokenizers found yet.")
    
#    metadata_file = pretrained_dir / 'metadata.json'
    
#    if metadata_file.exists():
#        logger.warning("tokenizer.loader", 
#                      f"Metadata file already exists: {metadata_file}")
#        return
    
    # Create template metadata
#    template = {
#        "zarx_32k": {
#           "name": "zarx_32k",
#           "vocab_size": 32000,
#            "version": "1.0.0",
#            "description": "zarx 32K BPE tokenizer trained on diverse corpus",
#            "special_tokens": {
#                "<pad>": 0,
#                "<unk>": 1,
#                "<s>": 2,
#                "</s>": 3
#            },
#            "training_corpus": "mixed_corpus",
#            "training_corpus_size": 100000000,
#            "created_at": "2025-01-01T00:00:00Z",
#            "author": "zarx Team"
#        },
#        "zarx_50k": {
#            "name": "zarx_50k",
#            "vocab_size": 50000,
#            "version": "1.0.0",
#            "description": "zarx 50K BPE tokenizer with extended vocabulary",
#            "special_tokens": {
#                "<pad>": 0,
#                "<unk>": 1,
#               "<s>": 2,
#                "</s>": 3
#            },
#            "training_corpus": "mixed_corpus",
#            "training_corpus_size": 200000000,
#            "created_at": "2025-01-01T00:00:00Z",
#            "author": "zarx Team"
#        },
#        "zarx_65k": {
#            "name": "zarx_65k",
#            "vocab_size": 65536,
#            "version": "1.0.0",
#            "description": "zarx 65K BPE tokenizer (standard GPT-2 size)",
#            "special_tokens": {
#                "<pad>": 0,
#                "<unk>": 1,
#                "<s>": 2,
#                "</s>": 3
#            },
#            "training_corpus": "mixed_corpus",
#            "training_corpus_size": 500000000,
#            "created_at": "2025-01-01T00:00:00Z",
#            "author": "zarx Team"
#        }
#    }
    
#    with open(metadata_file, 'w', encoding='utf-8') as f:
#        json.dump(template, f, indent=2, ensure_ascii=False)
    
#    logger.info("tokenizer.loader", f"Created metadata template: {metadata_file}")
#    print(f"\n‚úÖ Created metadata template: {metadata_file}")
#    print("üìù Edit this file to add your pretrained tokenizers")
#    print(f"üìÅ Place tokenizer files in: {pretrained_dir}")
#    print("\nExample:")
#    print(f"  1. Train tokenizer and save as: {pretrained_dir}/zarx_32k.json")
#    print(f"  2. Update metadata in: {metadata_file}")
#    print(f"  3. Use: load_pretrained('zarx_32k')\n")


# =============================================================================
# INITIALIZATION
# =============================================================================

def _initialize_pretrained_directory():
    """Initialize pretrained directory if needed."""
    pretrained_dir = _get_pretrained_tokenizers_dir()
    
    metadata_file = pretrained_dir / 'metadata.json'
    if not metadata_file.exists() and not any(pretrained_dir.glob('*.json')):
        logger.info("tokenizer.loader", 
                   "Pretrained tokenizers directory is empty. "
                   "Consider running `create_empty_pretrained_metadata()` to set up.")


_initialize_pretrained_directory()


__all__ = [
    'ZARXTokenizer',
    'load_pretrained',
    'load_from_path',
    'list_pretrained',
    'list_pretrained_detailed',
    'get_pretrained_path',
    'has_pretrained',
    # 'create_empty_pretrained_metadata',
]

