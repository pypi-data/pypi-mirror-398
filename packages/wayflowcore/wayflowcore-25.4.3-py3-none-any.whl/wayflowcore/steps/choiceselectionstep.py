# Copyright Â© 2025 Oracle and/or its affiliates.
#
# This software is under the Universal Permissive License
# (UPL) 1.0 (LICENSE-UPL or https://oss.oracle.com/licenses/upl) or Apache License
# 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0), at your option.

import logging
import re
from dataclasses import asdict
from typing import TYPE_CHECKING, Any, Dict, List, Optional

from wayflowcore._metadata import MetadataType
from wayflowcore._utils._templating_helpers import (
    get_variables_names_and_types_from_template,
    render_template_partially,
)
from wayflowcore.models.llmgenerationconfig import LlmGenerationConfig
from wayflowcore.models.llmmodel import LlmModel
from wayflowcore.property import Property, StringProperty
from wayflowcore.steps.step import Step, StepResult

from ..controlconnection import ControlFlowEdge
from ..stepdescription import StepDescription, StepDescriptionInput, make_steps_descriptions
from .branchingstep import BranchingStep
from .completestep import CompleteStep
from .flowexecutionstep import FlowExecutionStep
from .promptexecutionstep import PromptExecutionStep
from .textextractionstep.regexextractionstep import RegexExtractionStep

if TYPE_CHECKING:
    from wayflowcore.executors._flowconversation import FlowConversation
    from wayflowcore.flow import Flow

logger = logging.getLogger(__name__)

_DEFAULT_CHOICE_SELECTION_TEMPLATE = """You are a helpful assistant. You must pick a task to execute, given some user input.
You have several tasks to select from, with the format:
```
task_name: task_description
```

The available tasks are:
```
{% for desc in next_steps -%}
- {{ desc.displayed_step_name }}: {{ desc.description }}
{% endfor -%}
```

Return the task_name (and only the task_name) that you need to execute for this request: {{ input }}"""
"""A template string used for choice selection."""


class ChoiceSelectionStep(Step):
    """
    Step that decides what next step to go to (control flow change) based on an input and description of the next steps,
    powered by an LLM. If the next step named as an explicit mapping to some existing value, please use the
    ``BranchingStep``, which is similar to this step but doesn't use any LLM.

    It outputs the selected choice index so that it can be consumed by downstreams steps if they need it,
    as well as the full text given back by the LLM.
    """

    INPUT = "input"
    """str: Input key for the input to be used to determine the next step to transition to."""
    SELECTED_CHOICE = "selected_choice"
    """str: Output key for the raw next step decision generated by the LLM."""
    LLM_OUTPUT = "llm_output"
    """str: Output key for the final next step decision after parsing the LLM decision."""
    DEFAULT_CHOICE_SELECTION_TEMPLATE = _DEFAULT_CHOICE_SELECTION_TEMPLATE
    """str: Default prompt template to be used by the LLM to determine the next step to transition to."""

    BRANCH_DEFAULT = "default"
    """str: Name of the branch taken in case the LLM is not able to choose a next step"""

    def __init__(
        self,
        llm: LlmModel,
        next_steps: List["StepDescriptionInput"],
        prompt_template: str = "",
        num_tokens: int = 7,  # max number of tokens to return. Need to be enough to encode all step_names
        input_descriptors: Optional[List[Property]] = None,
        output_descriptors: Optional[List[Property]] = None,
        input_mapping: Optional[Dict[str, str]] = None,
        output_mapping: Optional[Dict[str, str]] = None,
        name: Optional[str] = None,
        __metadata_info__: Optional[MetadataType] = None,
    ):
        """
        Note
        ----

        A step has input and output descriptors, describing what values the step requires to run and what values it produces.

        **Input descriptors**

        This step has for input descriptors all the variables extracted from ``prompt_template``. See :ref:`TemplateRenderingStep <TemplateRenderingStep>` for concrete examples on how descriptors are extracted from text prompts.

        **Output descriptors**

        This step has two output descriptors:

        * ``ChoiceSelectionStep.SELECTED_CHOICE``: ``StringProperty()``, the name of the branch selected by the step
        * ``ChoiceSelectionStep.LLM_OUTPUT``: ``StringProperty()``, the raw LLM output before parsing

        **Branches**

        This step can have several next steps and perform conditional branching using the ``llm`` and the given input values. The branches
        this step can take are simply all the branches mentioned in the ``next_step`` argument.


        Parameters
        ----------
        llm:
            Model that is used to determine the choice of next step.
        next_steps:
            List of tuples containing the next step name and a description of it. If the name displayed in prompt is different than
            the real step name, then pass tuples with the format ``Tuple[step_name, step_description, displayed_step_name]``.
            Will be passed in the prompt for making the choice.
        prompt_template:
            Prompt template to be used to have the LLM determine the next step to transition to. Defaults to ``DEFAULT_CHOICE_SELECTION_TEMPLATE``,
        num_tokens:
            Upper limit on the number of tokens that can be generated by the LLM.

            .. note::
                ``num_tokens`` should be as small as possible to ensure low latency, but high enough
                to encode all the displayed_step_names. Adjust the value depending on the length of the step names.
        input_descriptors:
            Input descriptors of the step. ``None`` means the step will resolve the input descriptors automatically using its static configuration in a best effort manner.

        output_descriptors:
            Output descriptors of the step. ``None`` means the step will resolve them automatically using its static
            configuration in a best effort manner.

        name:
            Name of the step.

        input_mapping:
            Mapping between the name of the inputs this step expects and the name to get it from in the conversation input/output dictionary.

        output_mapping:
            Mapping between the name of the outputs this step expects and the name to get it from in the conversation input/output dictionary.

        See Also
        --------
        :class:`~wayflowcore.steps.BranchingStep` : Strict version of the ``ChoiceSelectionStep`` that does not use an LLM.


        Notes
        -----
        The success of this steps depends on the performance of the LLM. You can increase the robustness of this step by:

        - Tweaking the prompt template for your specific use case.
        - Using a better LLM
        - Wrapping the step inside a ``RetryStep``

        Examples
        --------
        >>> from wayflowcore.flow import Flow
        >>> from wayflowcore.steps import ChoiceSelectionStep, OutputMessageStep
        >>>
        >>> CHOICE_SELECTION_STEP = "CHOICE_SELECTION"
        >>> OUTPUT_STEP1 = "OUTPUT1"
        >>> OUTPUT_STEP2 = "OUTPUT2"
        >>> CHOICE_SELECTION_IO = "$choice_selection"
        >>> assistant = Flow(
        ...     begin_step_name=CHOICE_SELECTION_STEP,
        ...     steps={
        ...         CHOICE_SELECTION_STEP: ChoiceSelectionStep(
        ...             llm=llm,
        ...             next_steps=[
        ...                 (OUTPUT_STEP1, "The access is denied", "is_access_denied"),
        ...                 (OUTPUT_STEP2, "The access is granted", "is_access_granted"),
        ...             ],
        ...             input_mapping={ChoiceSelectionStep.INPUT: CHOICE_SELECTION_IO},
        ...         ),
        ...         OUTPUT_STEP1: OutputMessageStep("Access denied. Please exit the conversation."),
        ...         OUTPUT_STEP2: OutputMessageStep("Access granted. Press any key to continue..."),
        ...     },
        ...     transitions={CHOICE_SELECTION_STEP: {OUTPUT_STEP1: OUTPUT_STEP1, OUTPUT_STEP2: OUTPUT_STEP2, ChoiceSelectionStep.BRANCH_DEFAULT: OUTPUT_STEP1}, OUTPUT_STEP1: [None], OUTPUT_STEP2: [None]}
        ... )
        >>> conversation = assistant.start_conversation(inputs={CHOICE_SELECTION_IO: "I grant the access to the user"})
        >>> status = conversation.execute() # doctest: +SKIP
        >>> # conversation.get_last_message().content
        >>> # Access granted. Press any key to continue...
        """
        from wayflowcore.flow import Flow

        PROMPT_EXECUTION = "prompt_execution"
        EXTRACTION = "next_step_extraction"
        BRANCHING = "branching"

        self.num_tokens = num_tokens
        self.prompt_template = (
            prompt_template if prompt_template != "" else self.DEFAULT_CHOICE_SELECTION_TEMPLATE
        )
        self.next_steps = make_steps_descriptions(next_steps)

        next_step_mapping: Dict[str, str] = {
            step.displayed_step_name: step.step_name for step in self.next_steps
        }

        prompt = render_template_partially(
            template=self.prompt_template,
            inputs=dict(next_steps=[asdict(next_step) for next_step in self.next_steps]),
        )
        regex_pattern = r"(" + r"|".join([re.escape(s) for s in next_step_mapping.keys()]) + r")"

        #: use composition over inheritance from FlowExecutionStep so that:
        # 1) we control the visible outputs
        # 2) and we don't get the step builder from FlowExecutionStep as well
        prompt_execution_step = PromptExecutionStep(
            llm=llm,
            prompt_template=prompt,
            generation_config=LlmGenerationConfig(max_tokens=num_tokens),
            output_mapping={PromptExecutionStep.OUTPUT: self.LLM_OUTPUT},
        )
        regex_extraction_step = RegexExtractionStep(
            regex_pattern=regex_pattern,
            input_mapping={RegexExtractionStep.TEXT: self.LLM_OUTPUT},
            output_mapping={RegexExtractionStep.OUTPUT: self.SELECTED_CHOICE},
        )
        branching_step = BranchingStep(
            branch_name_mapping={k: v for k, v in next_step_mapping.items()},
            input_mapping={BranchingStep.NEXT_BRANCH_NAME: self.SELECTED_CHOICE},
        )
        complete_steps = {
            step_name: CompleteStep()
            for step_name in next_step_mapping.values()
            if step_name is not None
        }
        default_complete_step = CompleteStep()

        steps = {
            PROMPT_EXECUTION: prompt_execution_step,
            EXTRACTION: regex_extraction_step,
            BRANCHING: branching_step,
            **complete_steps,
        }

        branching_step_control_flow_edges = [
            # all possible complete steps
            ControlFlowEdge(
                source_step=branching_step, destination_step=step, source_branch=step_name
            )
            for step_name, step in complete_steps.items()
        ]
        if not any(
            branching_step.BRANCH_DEFAULT == edge.source_branch
            for edge in branching_step_control_flow_edges
        ):
            steps[BranchingStep.BRANCH_DEFAULT] = default_complete_step
            branching_step_control_flow_edges += [
                # default complete step
                ControlFlowEdge(
                    branching_step,
                    destination_step=default_complete_step,
                    source_branch=branching_step.BRANCH_DEFAULT,
                )
            ]

        self.subflow = Flow(
            begin_step=prompt_execution_step,
            steps=steps,
            control_flow_edges=[
                # first stages
                ControlFlowEdge(
                    source_step=prompt_execution_step, destination_step=regex_extraction_step
                ),
                ControlFlowEdge(source_step=regex_extraction_step, destination_step=branching_step),
            ]
            + branching_step_control_flow_edges,
        )
        self.subflow_step = FlowExecutionStep(
            flow=self.subflow,
        )

        super().__init__(
            llm=llm,
            step_static_configuration=dict(
                llm=llm,
                next_steps=self.next_steps,
                prompt_template=self.prompt_template,
                num_tokens=num_tokens,
            ),
            input_descriptors=input_descriptors,
            output_descriptors=output_descriptors,
            input_mapping=input_mapping,
            output_mapping=output_mapping,
            name=name,
            __metadata_info__=__metadata_info__,
        )

    @classmethod
    def _get_step_specific_static_configuration_descriptors(
        cls,
    ) -> Dict[str, type]:
        """
        Returns a dictionary in which the keys are the names of the configuration items
        and the values are a descriptor for the expected type
        """
        return {
            "llm": LlmModel,
            "next_steps": List[StepDescription],
            "prompt_template": str,
            "num_tokens": int,
        }

    @classmethod
    def _compute_step_specific_input_descriptors_from_static_config(
        cls,
        llm: LlmModel,
        next_steps: List[StepDescriptionInput],
        prompt_template: str,
        num_tokens: int,
    ) -> List[Property]:
        prompt = render_template_partially(
            template=prompt_template,
            inputs=dict(
                next_steps=[asdict(next_step) for next_step in make_steps_descriptions(next_steps)]
            ),
        )
        input_descriptors = [
            var_info
            for var_info in get_variables_names_and_types_from_template(
                prompt  # inputs should be inputs of the prompt execution step
            )
        ]
        return input_descriptors

    @classmethod
    def _compute_step_specific_output_descriptors_from_static_config(
        cls,
        llm: LlmModel,
        next_steps: List[StepDescriptionInput],
        prompt_template: str,
        num_tokens: int,
    ) -> List[Property]:
        return [
            StringProperty(
                name=cls.SELECTED_CHOICE,
                description="the selected choice",
                default_value=cls.BRANCH_DEFAULT,
            ),
            StringProperty(
                name=cls.LLM_OUTPUT,
                description="the output from the LLM potentially containing an explanation as to why the selected choice has been chosen",
            ),
        ]

    @classmethod
    def _compute_internal_branches_from_static_config(
        cls,
        llm: LlmModel,
        next_steps: List[StepDescriptionInput],
        prompt_template: str,
        num_tokens: int,
    ) -> List[str]:
        # We know the number and names of next steps
        next_steps_descriptions = make_steps_descriptions(next_steps)
        next_step_dict: Dict[str, str] = {
            step.displayed_step_name: step.step_name for step in next_steps_descriptions
        }
        return list(set(list(next_step_dict.values()) + [cls.BRANCH_DEFAULT]))

    async def _invoke_step_async(
        self,
        inputs: Dict[str, Any],
        conversation: "FlowConversation",
    ) -> StepResult:
        return await self.subflow_step._invoke_step_async(inputs, conversation)

    def sub_flow(self) -> Optional["Flow"]:
        return self.subflow
