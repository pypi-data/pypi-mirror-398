model: ple

params:
  num_levels: 2
  num_specific_experts: 4
  num_shared_experts: 4

  shared_expert_params:
    dims: [256, 128]
    activation: relu
    dropout: 0.1

  specific_expert_params:
    dims: [192, 96]
    activation: relu
    dropout: 0.1

  tower_params_list:
    - dims: [96, 48]
      activation: relu
      dropout: 0.2
    - dims: [128, 64]
      activation: relu
      dropout: 0.2


  embedding_l1_reg: 1.e-6
  embedding_l2_reg: 1.e-5     
  dense_l1_reg: 1.e-6
  dense_l2_reg: 1.e-4