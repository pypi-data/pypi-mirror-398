{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab95b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nextrec\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger() \n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))\n",
    "logger.handlers = [handler] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b56c96",
   "metadata": {},
   "source": [
    "# 5-Minute Quick Start\n",
    "\n",
    "该Notebook将为大家了解 NextRec：一个统一、高效、可扩展的推荐系统框架，并带大家从零到一训练并构建一个可上线的推荐模型。示例数据集来自电商场景。\n",
    "\n",
    "开始以前，请通过命令行安装nextrec：\n",
    "\n",
    "```bash\n",
    "# 正式版\n",
    "pip install nextrec\n",
    "\n",
    "# 测试版\n",
    "pip install -i https://test.pypi.org/simple/ nextrec\n",
    "```\n",
    "\n",
    "先介绍一些推荐系统的概念。在推荐系统中，通常会处理多种类型的输入信号，在经过一系列的变换之后转化为向量输入网络：\n",
    "\n",
    "- 稠密特征（数值型）：连续或可序数化的数值，如年龄、价格、时长、打分；常见做法是标准化/归一化或对数变换。\n",
    "- 稀疏特征（类别/ID）：高基数离散字段，如用户 ID、物品 ID、性别、职业、设备类型；通常需要索引化后，在一个embedding lookup matrix中进行嵌入。\n",
    "- 序列特征（行为序列）：可变长的历史行为，如用户的浏览/点击/购买列表。这类特征表征了用户的行为和兴趣变化，通常我们需要截断、padding，嵌入后通过不同聚合方式（如 mean/sum/attention）将其变为定长向量。\n",
    "- 上下文特征：时间、地理、曝光位置等环境信息，可是稠密也可能是稀疏，常与主特征交互。\n",
    "- 多模态特征：文本、图片、视频等经过预训练模型得到的向量，可直接作为稠密输入，或与 ID 交互建模。\n",
    "\n",
    "通常一个标准的训练数据格式如下所示：\n",
    "\n",
    "```text\n",
    "user_id,item_id,gender,age,occupation,history_seq,label\n",
    "1024,501,1,28,3,\"[12,45,18,77]\",1\n",
    "2048,777,0,35,5,\"[8,99]\",0\n",
    "```\n",
    "\n",
    "这里，我们提供了一份脱敏后的数据集供大家使用，该数据集来源于电商场景下，包含用户id，物品id，稠密特征，稀疏特征和序列特征，标签则包含是否点击，与是否转化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97cae0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dense_0</th>\n",
       "      <th>dense_1</th>\n",
       "      <th>dense_2</th>\n",
       "      <th>dense_3</th>\n",
       "      <th>dense_4</th>\n",
       "      <th>dense_5</th>\n",
       "      <th>dense_6</th>\n",
       "      <th>dense_7</th>\n",
       "      <th>...</th>\n",
       "      <th>sparse_5</th>\n",
       "      <th>sparse_6</th>\n",
       "      <th>sparse_7</th>\n",
       "      <th>sparse_8</th>\n",
       "      <th>sparse_9</th>\n",
       "      <th>sparse_10</th>\n",
       "      <th>sparse_11</th>\n",
       "      <th>sequence_0</th>\n",
       "      <th>click</th>\n",
       "      <th>conversion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7817</td>\n",
       "      <td>0.147041</td>\n",
       "      <td>0.310204</td>\n",
       "      <td>0.777809</td>\n",
       "      <td>0.944897</td>\n",
       "      <td>0.623154</td>\n",
       "      <td>0.571242</td>\n",
       "      <td>0.770095</td>\n",
       "      <td>0.321103</td>\n",
       "      <td>...</td>\n",
       "      <td>161</td>\n",
       "      <td>138</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>312</td>\n",
       "      <td>416</td>\n",
       "      <td>188</td>\n",
       "      <td>[90, 54, 86, 5, 121, 138, 45, 100, 0, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3579</td>\n",
       "      <td>0.778112</td>\n",
       "      <td>0.803593</td>\n",
       "      <td>0.518520</td>\n",
       "      <td>0.910912</td>\n",
       "      <td>0.043562</td>\n",
       "      <td>0.821427</td>\n",
       "      <td>0.880369</td>\n",
       "      <td>0.337482</td>\n",
       "      <td>...</td>\n",
       "      <td>252</td>\n",
       "      <td>25</td>\n",
       "      <td>402</td>\n",
       "      <td>7</td>\n",
       "      <td>168</td>\n",
       "      <td>155</td>\n",
       "      <td>154</td>\n",
       "      <td>[3, 95, 31, 124, 56, 79, 109, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2657</td>\n",
       "      <td>0.586647</td>\n",
       "      <td>0.123208</td>\n",
       "      <td>0.203636</td>\n",
       "      <td>0.116398</td>\n",
       "      <td>0.240645</td>\n",
       "      <td>0.882588</td>\n",
       "      <td>0.062836</td>\n",
       "      <td>0.629869</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>62</td>\n",
       "      <td>145</td>\n",
       "      <td>109</td>\n",
       "      <td>432</td>\n",
       "      <td>170</td>\n",
       "      <td>133</td>\n",
       "      <td>[139, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2689</td>\n",
       "      <td>0.337401</td>\n",
       "      <td>0.705511</td>\n",
       "      <td>0.138758</td>\n",
       "      <td>0.945233</td>\n",
       "      <td>0.330333</td>\n",
       "      <td>0.377462</td>\n",
       "      <td>0.121577</td>\n",
       "      <td>0.427124</td>\n",
       "      <td>...</td>\n",
       "      <td>241</td>\n",
       "      <td>144</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>333</td>\n",
       "      <td>175</td>\n",
       "      <td>210</td>\n",
       "      <td>[59, 29, 34, 106, 4, 103, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2495</td>\n",
       "      <td>0.669473</td>\n",
       "      <td>0.564266</td>\n",
       "      <td>0.006319</td>\n",
       "      <td>0.255851</td>\n",
       "      <td>0.698055</td>\n",
       "      <td>0.052065</td>\n",
       "      <td>0.583597</td>\n",
       "      <td>0.590456</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>27</td>\n",
       "      <td>204</td>\n",
       "      <td>129</td>\n",
       "      <td>319</td>\n",
       "      <td>97</td>\n",
       "      <td>168</td>\n",
       "      <td>[52, 122, 104, 116, 5, 138, 37, 30, 59, 10, 19...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id   dense_0   dense_1   dense_2   dense_3   dense_4  \\\n",
       "0        1     7817  0.147041  0.310204  0.777809  0.944897  0.623154   \n",
       "1        1     3579  0.778112  0.803593  0.518520  0.910912  0.043562   \n",
       "2        1     2657  0.586647  0.123208  0.203636  0.116398  0.240645   \n",
       "3        1     2689  0.337401  0.705511  0.138758  0.945233  0.330333   \n",
       "4        1     2495  0.669473  0.564266  0.006319  0.255851  0.698055   \n",
       "\n",
       "    dense_5   dense_6   dense_7  ...  sparse_5  sparse_6  sparse_7  sparse_8  \\\n",
       "0  0.571242  0.770095  0.321103  ...       161       138        88         5   \n",
       "1  0.821427  0.880369  0.337482  ...       252        25       402         7   \n",
       "2  0.882588  0.062836  0.629869  ...        27        62       145       109   \n",
       "3  0.377462  0.121577  0.427124  ...       241       144        40         6   \n",
       "4  0.052065  0.583597  0.590456  ...       152        27       204       129   \n",
       "\n",
       "   sparse_9  sparse_10  sparse_11  \\\n",
       "0       312        416        188   \n",
       "1       168        155        154   \n",
       "2       432        170        133   \n",
       "3       333        175        210   \n",
       "4       319         97        168   \n",
       "\n",
       "                                          sequence_0  click  conversion  \n",
       "0  [90, 54, 86, 5, 121, 138, 45, 100, 0, 0, 0, 0,...      1           0  \n",
       "1  [3, 95, 31, 124, 56, 79, 109, 0, 0, 0, 0, 0, 0...      1           1  \n",
       "2    [139, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]      1           0  \n",
       "3  [59, 29, 34, 106, 4, 103, 0, 0, 0, 0, 0, 0, 0,...      1           0  \n",
       "4  [52, 122, 104, 116, 5, 138, 37, 30, 59, 10, 19...      1           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nextrec.data.preprocessor import DataProcessor\n",
    "\n",
    "df = pd.read_csv('dataset/multitask_task.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353c8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_labels = ['click', 'conversion']\n",
    "dense_features_list = [col for col in df.columns if 'dense' in col]\n",
    "sparse_features_list = [col for col in df.columns if 'sparse' in col] + ['user_id', 'item_id']\n",
    "sequence_features_list = [col for col in df.columns if 'sequence' in col]\n",
    "\n",
    "# 由于csv中的序列特征是以字符串形式存储的列表，我们需要先将其转换为真正的列表对象\n",
    "for col in df.columns:\n",
    "    if 'sequence' in col:\n",
    "        df[col] = df[col].apply(lambda x: eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b3dc5",
   "metadata": {},
   "source": [
    "将数据处理成需要的格式以后，我们将拆分出训练集和推理集，用来给模型进行指标评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886906e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e07b6c",
   "metadata": {},
   "source": [
    "现在我们开始准备模型，我们需要将模型需要的不同特征进行定义，并传给模型，这里需要用到nextrec内置的三种特征DenseFeature, SequenceFeature, SparseFeature。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c38f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nextrec.basic.features import DenseFeature, SequenceFeature, SparseFeature\n",
    "\n",
    "# 我们将所有的数值特征作为DenseFeature，embedding_dim=1表示不进行embedding，当embedding_dim大于1时表示对数值特征进行线性变换，类似于embedding的效果\n",
    "dense_features = [DenseFeature(name=feat, embedding_dim=1) for feat in dense_features_list] \n",
    "\n",
    "# 稀疏特征和序列特征我们一般会进行embedding，embedding_dim可以根据实际情况调整\n",
    "sparse_features = []\n",
    "for feat in sparse_features_list:\n",
    "    vocab_size = 100 # 假设每个稀疏特征的词表大小为100\n",
    "    # sparsefeature还可以设置一些其他的参数，例如初始化器，正则化和embedding_name等，当两个特征共享embedding时，可以设置相同的embedding_name       \n",
    "    sparse_features.append(SparseFeature(name=feat, vocab_size=vocab_size, embedding_dim=4, embedding_name=feat)) \n",
    "\n",
    "# 序列特征的处理和稀疏特征类似，不过还需要设置序列的最大长度max_len和padding_idx等参数\n",
    "sequence_features = []\n",
    "for feat in sequence_features_list:\n",
    "    vocab_size = 5000 # 假设每个序列特征的词表大小为500\n",
    "    sequence_features.append(\n",
    "        SequenceFeature(\n",
    "            name=feat,\n",
    "            vocab_size=vocab_size,\n",
    "            max_len=20,\n",
    "            embedding_dim=8,\n",
    "            padding_idx=0\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71a374",
   "metadata": {},
   "source": [
    "现在，我们要用到DataLoader了，DataLoader通常为模型准备不断迭代的批次的数据。别担心，我们为你准备了RecDataLoader，它用起来不会很复杂。\n",
    "\n",
    "RecDataLoader是一个强大的工具，它支持传入dict，DataFrame，DataLoader以及一个路径，并且支持配置流式加载数据，只要选择streaming=True。这个实例适配了NextRec框架下的一切训练场景，为了避免不必要的困扰，我们强烈建议你试着用它。\n",
    "\n",
    "当然你同样可以不用它，NextRec也支持使用dict，DataFrame直接进行训练，稍后你就能看到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b6d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nextrec.data.dataloader import RecDataLoader\n",
    "\n",
    "task_labels = ['click', 'conversion']\n",
    "\n",
    "dataloader = RecDataLoader(\n",
    "    dense_features=dense_features,\n",
    "    sparse_features=sparse_features,\n",
    "    sequence_features=sequence_features,\n",
    "    target=task_labels,\n",
    ")\n",
    "\n",
    "# 我们需要为训练集和验证集分别创建dataloader\n",
    "train_loader = dataloader.create_dataloader(\n",
    "    data=train_df,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_loader = dataloader.create_dataloader(\n",
    "    data=valid_df,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "# 你也可以传入一个路径来配置流式数据加载器\n",
    "# train_loader = dataloader.create_dataloader(\n",
    "#     data='/path/to/train/data',\n",
    "#     batch_size=512,\n",
    "#     shuffle=True,\n",
    "#     streaming=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e6cd8e",
   "metadata": {},
   "source": [
    "终于来到这一步了，我们来选择想要训练的模型。NextRec提供了超过20种工业界常用的召回，精排，多任务模型。这里我们先用经典的MMOE训练一个模型。在此之前，我们需要实例化模型，并且为模型分配参数。\n",
    "\n",
    "在实例化模型以后，我们需要编译模型，为训练器分配优化器，调度器和损失函数。NextRec支持超过8种优化器，10种调度器和20种损失函数以及不平衡损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a31cce1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94m================================================================================\u001b[0m\n",
      "\u001b[1m\u001b[94mModel Summary: MMOE\u001b[0m\n",
      "\u001b[1m\u001b[94m================================================================================\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36m[1] Feature Configuration\u001b[0m\n",
      "\u001b[36m--------------------------------------------------------------------------------\u001b[0m\n",
      "Dense Features (8):\n",
      "  1. dense_0             \n",
      "  2. dense_1             \n",
      "  3. dense_2             \n",
      "  4. dense_3             \n",
      "  5. dense_4             \n",
      "  6. dense_5             \n",
      "  7. dense_6             \n",
      "  8. dense_7             \n",
      "Sparse Features (14):\n",
      "  #    Name           Vocab Size        Embed Name  Embed Dim\n",
      "  ---- ------------ ------------ ----------------- ----------\n",
      "  1    sparse_0              100          sparse_0          4\n",
      "  2    sparse_1              100          sparse_1          4\n",
      "  3    sparse_2              100          sparse_2          4\n",
      "  4    sparse_3              100          sparse_3          4\n",
      "  5    sparse_4              100          sparse_4          4\n",
      "  6    sparse_5              100          sparse_5          4\n",
      "  7    sparse_6              100          sparse_6          4\n",
      "  8    sparse_7              100          sparse_7          4\n",
      "  9    sparse_8              100          sparse_8          4\n",
      "  10   sparse_9              100          sparse_9          4\n",
      "  11   sparse_10             100         sparse_10          4\n",
      "  12   sparse_11             100         sparse_11          4\n",
      "  13   user_id               100           user_id          4\n",
      "  14   item_id               100           item_id          4\n",
      "Sequence Features (1):\n",
      "  #    Name           Vocab Size        Embed Name  Embed Dim    Max Len\n",
      "  ---- ------------ ------------ ----------------- ---------- ----------\n",
      "  1    sequence_0           5000        sequence_0          8         20\n",
      "\n",
      "\u001b[1m\u001b[36m[2] Model Parameters\u001b[0m\n",
      "\u001b[36m--------------------------------------------------------------------------------\u001b[0m\n",
      "Model Architecture:\n",
      "MMOE(\n",
      "  (embedding): EmbeddingLayer(\n",
      "    (embed_dict): ModuleDict(\n",
      "      (sparse_0): Embedding(100, 4)\n",
      "      (sparse_1): Embedding(100, 4)\n",
      "      (sparse_2): Embedding(100, 4)\n",
      "      (sparse_3): Embedding(100, 4)\n",
      "      (sparse_4): Embedding(100, 4)\n",
      "      (sparse_5): Embedding(100, 4)\n",
      "      (sparse_6): Embedding(100, 4)\n",
      "      (sparse_7): Embedding(100, 4)\n",
      "      (sparse_8): Embedding(100, 4)\n",
      "      (sparse_9): Embedding(100, 4)\n",
      "      (sparse_10): Embedding(100, 4)\n",
      "      (sparse_11): Embedding(100, 4)\n",
      "      (user_id): Embedding(100, 4)\n",
      "      (item_id): Embedding(100, 4)\n",
      "      (sequence_0): Embedding(5000, 8, padding_idx=0)\n",
      "    )\n",
      "    (dense_transforms): ModuleDict(\n",
      "      (dense_0): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_1): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_2): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_3): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_4): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_5): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_6): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_7): Linear(in_features=1, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0-3): 4 x MLP(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=128, bias=True)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.01)\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "        (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): LeakyReLU(negative_slope=0.01)\n",
      "        (7): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gates): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): Linear(in_features=72, out_features=4, bias=True)\n",
      "      (1): Softmax(dim=1)\n",
      "    )\n",
      "  )\n",
      "  (towers): ModuleList(\n",
      "    (0-1): 2 x MLP(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.01)\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): LeakyReLU(negative_slope=0.01)\n",
      "        (7): Dropout(p=0.2, inplace=False)\n",
      "        (8): Linear(in_features=32, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (prediction_layer): PredictionLayer()\n",
      ")\n",
      "\n",
      "Total Parameters:        131,068\n",
      "Trainable Parameters:    131,068\n",
      "Non-trainable Parameters: 0\n",
      "Layer-wise Parameters:\n",
      "  embedding                     : 45,616\n",
      "  experts                       : 71,936\n",
      "  gates                         : 584\n",
      "  towers                        : 12,930\n",
      "  prediction_layer              : 2\n",
      "\n",
      "\u001b[1m\u001b[36m[3] Training Configuration\u001b[0m\n",
      "\u001b[36m--------------------------------------------------------------------------------\u001b[0m\n",
      "Task Type:               ['binary', 'binary']\n",
      "Number of Tasks:         2\n",
      "Metrics:                 ['auc', 'recall', 'precision']\n",
      "Target Columns:          ['click', 'conversion']\n",
      "Device:                  mps\n",
      "Optimizer:               adam\n",
      "  lr                       : 0.001\n",
      "  weight_decay             : 1e-05\n",
      "Loss Function:           ['bce', 'bce']\n",
      "Regularization:\n",
      "  Embedding L1:          1e-06\n",
      "  Embedding L2:          1e-05\n",
      "  Dense L1:              1e-05\n",
      "  Dense L2:              0.0001\n",
      "Other Settings:\n",
      "  Early Stop Patience:   20\n",
      "  Max Gradient Norm:     1.0\n",
      "  Session ID:            mmoe_task\n",
      "  Latest Checkpoint:     /Users/zyaztec/DailyWork/建模代码整理/experiments/NextRec/tutorials/notebooks/zh/nextrec_logs/mmoe_task/MMOE_checkpoint.model\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m================================================================================\u001b[0m\n",
      "\u001b[1mStart training\u001b[0m\n",
      "\u001b[1m================================================================================\u001b[0m\n",
      "\n",
      "\u001b[1mModel device: mps\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 157/157 [00:06<00:00, 23.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mEpoch 1/1 - Train: loss=1.1248, click[auc=0.7952, recall=0.8160, precision=0.8015], conversion[auc=0.6504, recall=0.2708, precision=0.5141]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m  Evaluation batches processed: 40\u001b[0m\n",
      "\u001b[36m  Evaluation samples: 20000\u001b[0m\n",
      "\u001b[36mEpoch 1/1 - Valid: click[auc=0.8207, recall=0.8190, precision=0.8185], conversion[auc=0.6782, recall=0.3249, precision=0.5454]\u001b[0m\n",
      "Validation auc_click improved to 0.8207\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[92mTraining finished.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[94mLoad best model from: /Users/zyaztec/DailyWork/建模代码整理/experiments/NextRec/tutorials/notebooks/zh/nextrec_logs/mmoe_task/MMOE_best.model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MMOE(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (embed_dict): ModuleDict(\n",
       "      (sparse_0): Embedding(100, 4)\n",
       "      (sparse_1): Embedding(100, 4)\n",
       "      (sparse_2): Embedding(100, 4)\n",
       "      (sparse_3): Embedding(100, 4)\n",
       "      (sparse_4): Embedding(100, 4)\n",
       "      (sparse_5): Embedding(100, 4)\n",
       "      (sparse_6): Embedding(100, 4)\n",
       "      (sparse_7): Embedding(100, 4)\n",
       "      (sparse_8): Embedding(100, 4)\n",
       "      (sparse_9): Embedding(100, 4)\n",
       "      (sparse_10): Embedding(100, 4)\n",
       "      (sparse_11): Embedding(100, 4)\n",
       "      (user_id): Embedding(100, 4)\n",
       "      (item_id): Embedding(100, 4)\n",
       "      (sequence_0): Embedding(5000, 8, padding_idx=0)\n",
       "    )\n",
       "    (dense_transforms): ModuleDict(\n",
       "      (dense_0): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_1): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_2): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_3): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_4): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_5): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_6): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_7): Linear(in_features=1, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (experts): ModuleList(\n",
       "    (0-3): 4 x MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=72, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.3, inplace=False)\n",
       "        (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): LeakyReLU(negative_slope=0.01)\n",
       "        (7): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gates): ModuleList(\n",
       "    (0-1): 2 x Sequential(\n",
       "      (0): Linear(in_features=72, out_features=4, bias=True)\n",
       "      (1): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       "  (towers): ModuleList(\n",
       "    (0-1): 2 x MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.2, inplace=False)\n",
       "        (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): LeakyReLU(negative_slope=0.01)\n",
       "        (7): Dropout(p=0.2, inplace=False)\n",
       "        (8): Linear(in_features=32, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (prediction_layer): PredictionLayer()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nextrec.models.multi_task.mmoe import MMOE\n",
    "\n",
    "# mmoe需要设置专家网络和任务塔的参数，这里我们设置4个专家网络，每个专家网络包含两层，任务塔也包含两层\n",
    "# 我们拥有两个任务，分别是click和conversion，每个任务都是二分类任务，因此task参数设置为['binary', 'binary']\n",
    "model = MMOE(\n",
    "    dense_features=dense_features,\n",
    "    sparse_features=sparse_features,\n",
    "    sequence_features=sequence_features,\n",
    "    expert_params= {\"dims\": [128, 64],  \"activation\": \"leaky_relu\", \"dropout\": 0.3},\n",
    "    num_experts=4,  # 4 expert networks\n",
    "    tower_params_list=[{\"dims\": [64, 32], \"activation\": \"leaky_relu\", \"dropout\": 0.2},  # click task\n",
    "                       {\"dims\": [64, 32], \"activation\": \"leaky_relu\", \"dropout\": 0.2},  # conversion task\n",
    "                        ],\n",
    "    target=task_labels,  # multiple task labels\n",
    "    task=['binary', 'binary'],  # each task type\n",
    "    device='mps',\n",
    "    embedding_l1_reg=1e-6,\n",
    "    embedding_l2_reg=1e-5,\n",
    "    dense_l1_reg=1e-5,\n",
    "    dense_l2_reg=1e-4,\n",
    "    session_id=\"mmoe_task\"    # session id用于区分不同的训练任务，会将训练日志，checkpoint，模型参数等保存在以session_id命名的文件夹中\n",
    ")\n",
    "\n",
    "# 编译模型，这是为了设置优化器和损失函数，当然，你也可以在MMOE初始化时传入这些参数，但是我们建议在compile中传入，这样看起来更清晰\n",
    "# 这里我们使用adam优化器，学习率为1e-3，权重衰减为1e-5\n",
    "# 每个任务的损失函数我们都使用二分类交叉熵损失函数\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_params={\"lr\": 1e-3, \"weight_decay\": 1e-5},\n",
    "    loss=['bce', 'bce']  # loss for each task\n",
    ")\n",
    "\n",
    "# 现在我们可以开始训练模型了，这里我们设置训练3个epoch，你可以根据实际情况调整\n",
    "# 同时我们还可以为每个任务设置评估指标，这里我们为每个任务都设置了AUC, Recall和Precision指标\n",
    "# 注意你可以在nextrec_logs/mmoe_iflytek文件夹中查看训练日志和模型checkpoint\n",
    "model.fit(\n",
    "    train_data=train_loader, \n",
    "    valid_data=valid_loader,\n",
    "    metrics={\n",
    "        'click': ['auc', 'recall', 'precision'],\n",
    "        'conversion': ['auc', 'recall', 'precision']\n",
    "    },\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f817212",
   "metadata": {},
   "source": [
    "现在我们再来训练一个精排模型，这里我们训练一个AutoINT作为示例，任务目标从多目标变为单目标。这是一篇北京大学发表在 CIKM 2019 的文章，模型的相关讲解可以在[这里](https://guyuecanhui.github.io/2020/05/09/paper-2019-pku-autoint/)看到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd350faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94m================================================================================\u001b[0m\n",
      "\u001b[1m\u001b[94mModel Summary: AutoInt\u001b[0m\n",
      "\u001b[1m\u001b[94m================================================================================\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36m[1] Feature Configuration\u001b[0m\n",
      "\u001b[36m--------------------------------------------------------------------------------\u001b[0m\n",
      "Dense Features (8):\n",
      "  1. dense_0             \n",
      "  2. dense_1             \n",
      "  3. dense_2             \n",
      "  4. dense_3             \n",
      "  5. dense_4             \n",
      "  6. dense_5             \n",
      "  7. dense_6             \n",
      "  8. dense_7             \n",
      "Sparse Features (14):\n",
      "  #    Name           Vocab Size        Embed Name  Embed Dim\n",
      "  ---- ------------ ------------ ----------------- ----------\n",
      "  1    sparse_0              100          sparse_0          4\n",
      "  2    sparse_1              100          sparse_1          4\n",
      "  3    sparse_2              100          sparse_2          4\n",
      "  4    sparse_3              100          sparse_3          4\n",
      "  5    sparse_4              100          sparse_4          4\n",
      "  6    sparse_5              100          sparse_5          4\n",
      "  7    sparse_6              100          sparse_6          4\n",
      "  8    sparse_7              100          sparse_7          4\n",
      "  9    sparse_8              100          sparse_8          4\n",
      "  10   sparse_9              100          sparse_9          4\n",
      "  11   sparse_10             100         sparse_10          4\n",
      "  12   sparse_11             100         sparse_11          4\n",
      "  13   user_id               100           user_id          4\n",
      "  14   item_id               100           item_id          4\n",
      "Sequence Features (1):\n",
      "  #    Name           Vocab Size        Embed Name  Embed Dim    Max Len\n",
      "  ---- ------------ ------------ ----------------- ---------- ----------\n",
      "  1    sequence_0           5000        sequence_0          8         20\n",
      "\n",
      "\u001b[1m\u001b[36m[2] Model Parameters\u001b[0m\n",
      "\u001b[36m--------------------------------------------------------------------------------\u001b[0m\n",
      "Model Architecture:\n",
      "AutoInt(\n",
      "  (embedding): EmbeddingLayer(\n",
      "    (embed_dict): ModuleDict(\n",
      "      (sparse_0): Embedding(100, 4)\n",
      "      (sparse_1): Embedding(100, 4)\n",
      "      (sparse_2): Embedding(100, 4)\n",
      "      (sparse_3): Embedding(100, 4)\n",
      "      (sparse_4): Embedding(100, 4)\n",
      "      (sparse_5): Embedding(100, 4)\n",
      "      (sparse_6): Embedding(100, 4)\n",
      "      (sparse_7): Embedding(100, 4)\n",
      "      (sparse_8): Embedding(100, 4)\n",
      "      (sparse_9): Embedding(100, 4)\n",
      "      (sparse_10): Embedding(100, 4)\n",
      "      (sparse_11): Embedding(100, 4)\n",
      "      (user_id): Embedding(100, 4)\n",
      "      (item_id): Embedding(100, 4)\n",
      "      (sequence_0): Embedding(5000, 8, padding_idx=0)\n",
      "    )\n",
      "    (dense_transforms): ModuleDict(\n",
      "      (dense_0): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_1): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_2): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_3): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_4): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_5): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_6): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_7): Linear(in_features=1, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (projection_layers): ModuleList(\n",
      "    (0-13): 14 x Linear(in_features=4, out_features=8, bias=False)\n",
      "    (14): Linear(in_features=8, out_features=8, bias=False)\n",
      "  )\n",
      "  (attention_layers): ModuleList(\n",
      "    (0-2): 3 x MultiHeadSelfAttention(\n",
      "      (W_Q): Linear(in_features=8, out_features=8, bias=False)\n",
      "      (W_K): Linear(in_features=8, out_features=8, bias=False)\n",
      "      (W_V): Linear(in_features=8, out_features=8, bias=False)\n",
      "      (W_Res): Linear(in_features=8, out_features=8, bias=False)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=120, out_features=1, bias=True)\n",
      "  (prediction_layer): PredictionLayer()\n",
      ")\n",
      "\n",
      "Total Parameters:        47,018\n",
      "Trainable Parameters:    47,018\n",
      "Non-trainable Parameters: 0\n",
      "Layer-wise Parameters:\n",
      "  embedding                     : 45,616\n",
      "  projection_layers             : 512\n",
      "  attention_layers              : 768\n",
      "  fc                            : 121\n",
      "  prediction_layer              : 1\n",
      "\n",
      "\u001b[1m\u001b[36m[3] Training Configuration\u001b[0m\n",
      "\u001b[36m--------------------------------------------------------------------------------\u001b[0m\n",
      "Task Type:               binary\n",
      "Number of Tasks:         1\n",
      "Metrics:                 ['auc', 'recall', 'precision']\n",
      "Target Columns:          ['conversion']\n",
      "Device:                  mps\n",
      "Optimizer:               adam\n",
      "  lr                       : 0.001\n",
      "  weight_decay             : 1e-05\n",
      "Loss Function:           focal\n",
      "Regularization:\n",
      "  Embedding L1:          1e-06\n",
      "  Embedding L2:          1e-05\n",
      "  Dense L1:              1e-05\n",
      "  Dense L2:              0.0001\n",
      "Other Settings:\n",
      "  Early Stop Patience:   20\n",
      "  Max Gradient Norm:     1.0\n",
      "  Session ID:            autoint_iflytek\n",
      "  Latest Checkpoint:     /Users/zyaztec/DailyWork/建模代码整理/experiments/NextRec/tutorials/notebooks/zh/nextrec_logs/autoint_iflytek/AutoInt_checkpoint.model\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m================================================================================\u001b[0m\n",
      "\u001b[1mStart training\u001b[0m\n",
      "\u001b[1m================================================================================\u001b[0m\n",
      "\n",
      "\u001b[1mModel device: mps\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 157/157 [00:07<00:00, 22.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mEpoch 1/10 - Train: loss=0.0937, auc=0.5008, recall=0.2621, precision=0.3379\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m  Evaluation batches processed: 157\u001b[0m\n",
      "\u001b[36m  Evaluation samples: 80000\u001b[0m\n",
      "\u001b[36mEpoch 1/10 - Valid: auc=0.5000, recall=0.0000, precision=0.0000\u001b[0m\n",
      "Validation auc improved to 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 157/157 [00:05<00:00, 28.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mEpoch 2/10 - Train: loss=0.0737, auc=0.4998, recall=0.0000, precision=0.0000\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m  Evaluation batches processed: 157\u001b[0m\n",
      "\u001b[36m  Evaluation samples: 80000\u001b[0m\n",
      "\u001b[36mEpoch 2/10 - Valid: auc=0.5000, recall=0.0000, precision=0.0000\u001b[0m\n",
      "No improvement for 1 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 157/157 [00:05<00:00, 28.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mEpoch 3/10 - Train: loss=0.0665, auc=0.4988, recall=0.0000, precision=0.0000\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m  Evaluation batches processed: 157\u001b[0m\n",
      "\u001b[36m  Evaluation samples: 80000\u001b[0m\n",
      "\u001b[36mEpoch 3/10 - Valid: auc=0.5000, recall=0.0000, precision=0.0000\u001b[0m\n",
      "No improvement for 2 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 157/157 [00:05<00:00, 28.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mEpoch 4/10 - Train: loss=0.0643, auc=0.4975, recall=0.0000, precision=0.0000\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m  Evaluation batches processed: 157\u001b[0m\n",
      "\u001b[36m  Evaluation samples: 80000\u001b[0m\n",
      "\u001b[36mEpoch 4/10 - Valid: auc=0.5000, recall=0.0000, precision=0.0000\u001b[0m\n",
      "No improvement for 3 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 157/157 [00:05<00:00, 28.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mEpoch 5/10 - Train: loss=0.0638, auc=0.5002, recall=0.0000, precision=0.0000\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m  Evaluation batches processed: 157\u001b[0m\n",
      "\u001b[36m  Evaluation samples: 80000\u001b[0m\n",
      "\u001b[36mEpoch 5/10 - Valid: auc=0.5000, recall=0.0000, precision=0.0000\u001b[0m\n",
      "No improvement for 4 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 157/157 [00:05<00:00, 26.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mEpoch 6/10 - Train: loss=0.0636, auc=0.5024, recall=0.0000, precision=0.0000\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m  Evaluation batches processed: 157\u001b[0m\n",
      "\u001b[36m  Evaluation samples: 80000\u001b[0m\n",
      "\u001b[36mEpoch 6/10 - Valid: auc=0.5000, recall=0.0000, precision=0.0000\u001b[0m\n",
      "No improvement for 5 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 157/157 [00:05<00:00, 28.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mEpoch 7/10 - Train: loss=0.0636, auc=0.4989, recall=0.0000, precision=0.0000\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m  Evaluation batches processed: 157\u001b[0m\n",
      "\u001b[36m  Evaluation samples: 80000\u001b[0m\n",
      "\u001b[36mEpoch 7/10 - Valid: auc=0.5000, recall=0.0000, precision=0.0000\u001b[0m\n",
      "No improvement for 6 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 157/157 [00:05<00:00, 28.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mEpoch 8/10 - Train: loss=0.0636, auc=0.4980, recall=0.0000, precision=0.0000\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m  Evaluation batches processed: 157\u001b[0m\n",
      "\u001b[36m  Evaluation samples: 80000\u001b[0m\n",
      "\u001b[36mEpoch 8/10 - Valid: auc=0.5000, recall=0.0000, precision=0.0000\u001b[0m\n",
      "No improvement for 7 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 157/157 [00:05<00:00, 28.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mEpoch 9/10 - Train: loss=0.0636, auc=0.4971, recall=0.0000, precision=0.0000\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m  Evaluation batches processed: 157\u001b[0m\n",
      "\u001b[36m  Evaluation samples: 80000\u001b[0m\n",
      "\u001b[36mEpoch 9/10 - Valid: auc=0.5000, recall=0.0000, precision=0.0000\u001b[0m\n",
      "No improvement for 8 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 157/157 [00:05<00:00, 28.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mEpoch 10/10 - Train: loss=0.0636, auc=0.4974, recall=0.0000, precision=0.0000\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m  Evaluation batches processed: 157\u001b[0m\n",
      "\u001b[36m  Evaluation samples: 80000\u001b[0m\n",
      "\u001b[36mEpoch 10/10 - Valid: auc=0.5000, recall=0.0000, precision=0.0000\u001b[0m\n",
      "No improvement for 9 epoch(s)\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[92mTraining finished.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[94mLoad best model from: /Users/zyaztec/DailyWork/建模代码整理/experiments/NextRec/tutorials/notebooks/zh/nextrec_logs/autoint_iflytek/AutoInt_best.model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoInt(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (embed_dict): ModuleDict(\n",
       "      (sparse_0): Embedding(100, 4)\n",
       "      (sparse_1): Embedding(100, 4)\n",
       "      (sparse_2): Embedding(100, 4)\n",
       "      (sparse_3): Embedding(100, 4)\n",
       "      (sparse_4): Embedding(100, 4)\n",
       "      (sparse_5): Embedding(100, 4)\n",
       "      (sparse_6): Embedding(100, 4)\n",
       "      (sparse_7): Embedding(100, 4)\n",
       "      (sparse_8): Embedding(100, 4)\n",
       "      (sparse_9): Embedding(100, 4)\n",
       "      (sparse_10): Embedding(100, 4)\n",
       "      (sparse_11): Embedding(100, 4)\n",
       "      (user_id): Embedding(100, 4)\n",
       "      (item_id): Embedding(100, 4)\n",
       "      (sequence_0): Embedding(5000, 8, padding_idx=0)\n",
       "    )\n",
       "    (dense_transforms): ModuleDict(\n",
       "      (dense_0): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_1): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_2): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_3): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_4): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_5): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_6): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_7): Linear(in_features=1, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (projection_layers): ModuleList(\n",
       "    (0-13): 14 x Linear(in_features=4, out_features=8, bias=False)\n",
       "    (14): Linear(in_features=8, out_features=8, bias=False)\n",
       "  )\n",
       "  (attention_layers): ModuleList(\n",
       "    (0-2): 3 x MultiHeadSelfAttention(\n",
       "      (W_Q): Linear(in_features=8, out_features=8, bias=False)\n",
       "      (W_K): Linear(in_features=8, out_features=8, bias=False)\n",
       "      (W_V): Linear(in_features=8, out_features=8, bias=False)\n",
       "      (W_Res): Linear(in_features=8, out_features=8, bias=False)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=120, out_features=1, bias=True)\n",
       "  (prediction_layer): PredictionLayer()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nextrec.models.ranking.autoint import AutoInt\n",
    "\n",
    "target = 'conversion'\n",
    "\n",
    "# 由于目标变了，我们重新创建dataloader\n",
    "dataloader = RecDataLoader(\n",
    "    dense_features=dense_features,\n",
    "    sparse_features=sparse_features,\n",
    "    sequence_features=sequence_features,\n",
    "    target=target,\n",
    ")\n",
    "\n",
    "train_loader = dataloader.create_dataloader(\n",
    "    data=train_df,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_loader = dataloader.create_dataloader(\n",
    "    data=train_df,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "model = AutoInt(\n",
    "    dense_features=dense_features,\n",
    "    sparse_features=sparse_features,\n",
    "    sequence_features=sequence_features,\n",
    "    att_layer_num=3,\n",
    "    att_embedding_dim=8,\n",
    "    att_head_num=2,\n",
    "    att_dropout=0.0,\n",
    "    att_use_residual=True,\n",
    "    target=target,\n",
    "    device='mps',\n",
    "    embedding_l1_reg=1e-6,\n",
    "    dense_l1_reg=1e-5,\n",
    "    embedding_l2_reg=1e-5,\n",
    "    dense_l2_reg=1e-4,\n",
    "    session_id=\"autoint_iflytek\"\n",
    ")\n",
    "\n",
    "# 编译模型\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_params={\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-5\n",
    "    },\n",
    "    loss=\"bce\",\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "model.fit(\n",
    "    train_data=train_loader,\n",
    "    valid_data=valid_loader,\n",
    "    metrics=['auc',\n",
    "             'recall',\n",
    "             'precision'],\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534113f0",
   "metadata": {},
   "source": [
    "觉得手动创建dataloader很麻烦？NextRec同样支持直接传入dataframe或dict -- 只要的你内存够大。（当然，再次强调，RecDataloader是一个更好的选择。）\n",
    "\n",
    "你也可以不传入valid_data，这样模型就会在全量数据集上训练。\n",
    "\n",
    "或者你可以设置validation_split参数，来让模型自动从训练集上拆分出验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d7ad57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94m================================================================================\u001b[0m\n",
      "\u001b[1m\u001b[94mModel Summary: AutoInt\u001b[0m\n",
      "\u001b[1m\u001b[94m================================================================================\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[36m[1] Feature Configuration\u001b[0m\n",
      "\u001b[36m--------------------------------------------------------------------------------\u001b[0m\n",
      "Dense Features (8):\n",
      "  1. dense_0             \n",
      "  2. dense_1             \n",
      "  3. dense_2             \n",
      "  4. dense_3             \n",
      "  5. dense_4             \n",
      "  6. dense_5             \n",
      "  7. dense_6             \n",
      "  8. dense_7             \n",
      "Sparse Features (14):\n",
      "  #    Name           Vocab Size        Embed Name  Embed Dim\n",
      "  ---- ------------ ------------ ----------------- ----------\n",
      "  1    sparse_0              100          sparse_0          4\n",
      "  2    sparse_1              100          sparse_1          4\n",
      "  3    sparse_2              100          sparse_2          4\n",
      "  4    sparse_3              100          sparse_3          4\n",
      "  5    sparse_4              100          sparse_4          4\n",
      "  6    sparse_5              100          sparse_5          4\n",
      "  7    sparse_6              100          sparse_6          4\n",
      "  8    sparse_7              100          sparse_7          4\n",
      "  9    sparse_8              100          sparse_8          4\n",
      "  10   sparse_9              100          sparse_9          4\n",
      "  11   sparse_10             100         sparse_10          4\n",
      "  12   sparse_11             100         sparse_11          4\n",
      "  13   user_id               100           user_id          4\n",
      "  14   item_id               100           item_id          4\n",
      "Sequence Features (1):\n",
      "  #    Name           Vocab Size        Embed Name  Embed Dim    Max Len\n",
      "  ---- ------------ ------------ ----------------- ---------- ----------\n",
      "  1    sequence_0           5000        sequence_0          8         20\n",
      "\n",
      "\u001b[1m\u001b[36m[2] Model Parameters\u001b[0m\n",
      "\u001b[36m--------------------------------------------------------------------------------\u001b[0m\n",
      "Model Architecture:\n",
      "AutoInt(\n",
      "  (embedding): EmbeddingLayer(\n",
      "    (embed_dict): ModuleDict(\n",
      "      (sparse_0): Embedding(100, 4)\n",
      "      (sparse_1): Embedding(100, 4)\n",
      "      (sparse_2): Embedding(100, 4)\n",
      "      (sparse_3): Embedding(100, 4)\n",
      "      (sparse_4): Embedding(100, 4)\n",
      "      (sparse_5): Embedding(100, 4)\n",
      "      (sparse_6): Embedding(100, 4)\n",
      "      (sparse_7): Embedding(100, 4)\n",
      "      (sparse_8): Embedding(100, 4)\n",
      "      (sparse_9): Embedding(100, 4)\n",
      "      (sparse_10): Embedding(100, 4)\n",
      "      (sparse_11): Embedding(100, 4)\n",
      "      (user_id): Embedding(100, 4)\n",
      "      (item_id): Embedding(100, 4)\n",
      "      (sequence_0): Embedding(5000, 8, padding_idx=0)\n",
      "    )\n",
      "    (dense_transforms): ModuleDict(\n",
      "      (dense_0): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_1): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_2): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_3): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_4): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_5): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_6): Linear(in_features=1, out_features=1, bias=True)\n",
      "      (dense_7): Linear(in_features=1, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (projection_layers): ModuleList(\n",
      "    (0-13): 14 x Linear(in_features=4, out_features=8, bias=False)\n",
      "    (14): Linear(in_features=8, out_features=8, bias=False)\n",
      "  )\n",
      "  (attention_layers): ModuleList(\n",
      "    (0-2): 3 x MultiHeadSelfAttention(\n",
      "      (W_Q): Linear(in_features=8, out_features=8, bias=False)\n",
      "      (W_K): Linear(in_features=8, out_features=8, bias=False)\n",
      "      (W_V): Linear(in_features=8, out_features=8, bias=False)\n",
      "      (W_Res): Linear(in_features=8, out_features=8, bias=False)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=120, out_features=1, bias=True)\n",
      "  (prediction_layer): PredictionLayer()\n",
      ")\n",
      "\n",
      "Total Parameters:        47,018\n",
      "Trainable Parameters:    47,018\n",
      "Non-trainable Parameters: 0\n",
      "Layer-wise Parameters:\n",
      "  embedding                     : 45,616\n",
      "  projection_layers             : 512\n",
      "  attention_layers              : 768\n",
      "  fc                            : 121\n",
      "  prediction_layer              : 1\n",
      "\n",
      "\u001b[1m\u001b[36m[3] Training Configuration\u001b[0m\n",
      "\u001b[36m--------------------------------------------------------------------------------\u001b[0m\n",
      "Task Type:               binary\n",
      "Number of Tasks:         1\n",
      "Metrics:                 ['auc', 'recall', 'precision']\n",
      "Target Columns:          ['conversion']\n",
      "Device:                  mps\n",
      "Optimizer:               adam\n",
      "  lr                       : 0.001\n",
      "  weight_decay             : 1e-05\n",
      "Loss Function:           focal\n",
      "Regularization:\n",
      "  Embedding L1:          1e-06\n",
      "  Embedding L2:          1e-05\n",
      "  Dense L1:              1e-05\n",
      "  Dense L2:              0.0001\n",
      "Other Settings:\n",
      "  Early Stop Patience:   20\n",
      "  Max Gradient Norm:     1.0\n",
      "  Session ID:            autoint_iflytek\n",
      "  Latest Checkpoint:     /Users/zyaztec/DailyWork/建模代码整理/experiments/NextRec/tutorials/notebooks/zh/nextrec_logs/autoint_iflytek/AutoInt_checkpoint.model\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m================================================================================\u001b[0m\n",
      "\u001b[1mStart training\u001b[0m\n",
      "\u001b[1m================================================================================\u001b[0m\n",
      "\n",
      "\u001b[1mModel device: mps\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 157/157 [00:05<00:00, 28.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mEpoch 1/1 - Train: loss=0.0679, auc=0.5001, recall=0.0000, precision=0.0000\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[92mTraining finished.\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoInt(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (embed_dict): ModuleDict(\n",
       "      (sparse_0): Embedding(100, 4)\n",
       "      (sparse_1): Embedding(100, 4)\n",
       "      (sparse_2): Embedding(100, 4)\n",
       "      (sparse_3): Embedding(100, 4)\n",
       "      (sparse_4): Embedding(100, 4)\n",
       "      (sparse_5): Embedding(100, 4)\n",
       "      (sparse_6): Embedding(100, 4)\n",
       "      (sparse_7): Embedding(100, 4)\n",
       "      (sparse_8): Embedding(100, 4)\n",
       "      (sparse_9): Embedding(100, 4)\n",
       "      (sparse_10): Embedding(100, 4)\n",
       "      (sparse_11): Embedding(100, 4)\n",
       "      (user_id): Embedding(100, 4)\n",
       "      (item_id): Embedding(100, 4)\n",
       "      (sequence_0): Embedding(5000, 8, padding_idx=0)\n",
       "    )\n",
       "    (dense_transforms): ModuleDict(\n",
       "      (dense_0): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_1): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_2): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_3): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_4): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_5): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_6): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dense_7): Linear(in_features=1, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (projection_layers): ModuleList(\n",
       "    (0-13): 14 x Linear(in_features=4, out_features=8, bias=False)\n",
       "    (14): Linear(in_features=8, out_features=8, bias=False)\n",
       "  )\n",
       "  (attention_layers): ModuleList(\n",
       "    (0-2): 3 x MultiHeadSelfAttention(\n",
       "      (W_Q): Linear(in_features=8, out_features=8, bias=False)\n",
       "      (W_K): Linear(in_features=8, out_features=8, bias=False)\n",
       "      (W_V): Linear(in_features=8, out_features=8, bias=False)\n",
       "      (W_Res): Linear(in_features=8, out_features=8, bias=False)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=120, out_features=1, bias=True)\n",
       "  (prediction_layer): PredictionLayer()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data=train_df,\n",
    "    metrics=['auc',\n",
    "             'recall',\n",
    "             'precision'],\n",
    "    epochs=1,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    # validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82000a76",
   "metadata": {},
   "source": [
    "下面是已经支持的模型，欢迎调用来测试效果\n",
    "\n",
    "### 排序模型\n",
    "\n",
    "| 模型 | 论文 | 年份 | 状态 |\n",
    "|------|------|------|------|\n",
    "| **FM** | Factorization Machines | ICDM 2010 | 已支持 |\n",
    "| **AFM** | Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks | IJCAI 2017 | 已支持 |\n",
    "| **DeepFM** | DeepFM: A Factorization-Machine based Neural Network for CTR Prediction | IJCAI 2017 | 已支持 |\n",
    "| **Wide&Deep** | Wide & Deep Learning for Recommender Systems | DLRS 2016 | 已支持 |\n",
    "| **xDeepFM** | xDeepFM: Combining Explicit and Implicit Feature Interactions | KDD 2018 | 已支持 |\n",
    "| **FiBiNET** | FiBiNET: Combining Feature Importance and Bilinear Feature Interaction for CTR Prediction | RecSys 2019 | 已支持 |\n",
    "| **PNN** | Product-based Neural Networks for User Response Prediction | ICDM 2016 | 已支持 |\n",
    "| **AutoInt** | AutoInt: Automatic Feature Interaction Learning | CIKM 2019 | 已支持 |\n",
    "| **DCN** | Deep & Cross Network for Ad Click Predictions | ADKDD 2017 | 已支持 |\n",
    "| **DIN** | Deep Interest Network for Click-Through Rate Prediction | KDD 2018 | 已支持 |\n",
    "| **DIEN** | Deep Interest Evolution Network for Click-Through Rate Prediction | AAAI 2019 | 已支持 |\n",
    "| **MaskNet** | MaskNet: Introducing Feature-wise Gating Blocks for High-dimensional Sparse Recommendation Data | 2020 | 已支持 |\n",
    "\n",
    "### 召回模型\n",
    "\n",
    "| 模型 | 论文 | 年份 | 状态 |\n",
    "|------|------|------|------|\n",
    "| **DSSM** | Learning Deep Structured Semantic Models | CIKM 2013 | 已支持 |\n",
    "| **DSSM v2** | DSSM with pairwise BPR-style optimization | - | 已支持 |\n",
    "| **YouTube DNN** | Deep Neural Networks for YouTube Recommendations | RecSys 2016 | 已支持 |\n",
    "| **MIND** | Multi-Interest Network with Dynamic Routing | CIKM 2019 | 已支持 |\n",
    "| **SDM** | Sequential Deep Matching Model | - | 已支持 |\n",
    "\n",
    "### 多任务模型\n",
    "\n",
    "| 模型 | 论文 | 年份 | 状态 |\n",
    "|------|------|------|------|\n",
    "| **MMOE** | Modeling Task Relationships in Multi-task Learning | KDD 2018 | 已支持 |\n",
    "| **PLE** | Progressive Layered Extraction | RecSys 2020 | 已支持 |\n",
    "| **ESMM** | Entire Space Multi-Task Model | SIGIR 2018 | 已支持 |\n",
    "| **ShareBottom** | Multitask Learning | - | 已支持 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e8c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
