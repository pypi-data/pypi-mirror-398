# FreeLLM Providers Configuration Example
# Copy this to providers.yaml and configure

providers:
  # OpenRouter - Fetches all available models via API
  - type: openrouter
    enabled: true
    api_key: ${OPENROUTER_API_KEY}

  # Ollama - Auto-discovers locally installed models
  - type: ollama
    enabled: false
    api_base: http://localhost:11434

  # ModelScope - Chinese AI model platform (free quota: 2000 calls/day, 500/model)
  - type: modelscope
    enabled: false
    api_key: ${MODELSCOPE_API_KEY}

  # iFlow - Free Chinese AI models (all free)
  - type: iflow
    enabled: false
    api_key: ${IFLOW_API_KEY}

  # OAI - Generic OpenAI-compatible APIs (auto-fetch models)
  - type: oai
    name: myservice  # Your service name
    enabled: false
    api_base: https://api.example.com/v1
    api_key: ${MYSERVICE_API_KEY}

  # Static - For any custom OpenAI-compatible endpoint (single model)
  - type: static
    enabled: false
    model_name: gpt-3.5-turbo
    provider: openai
    api_base: https://api.example.com/v1
    api_key: ${YOUR_API_KEY}

  # Multiple static providers example
  - type: static
    enabled: false
    model_name: claude-3-sonnet
    provider: anthropic
    api_base: https://api.example.com/v1
    api_key: ${ANOTHER_KEY}
