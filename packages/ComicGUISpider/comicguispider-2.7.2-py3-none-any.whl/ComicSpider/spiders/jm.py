# -*- coding: utf-8 -*-
import re
import typing as t
from urllib.parse import urlencode, urlparse

from utils import convert_punctuation, conf
from utils.website import JmUtils, correct_domain, JmBookInfo
from utils.processed_class import Url
from .basecomicspider import BaseComicSpider2, font_color, scrapy

domain = "18comic-zzz.xyz"


class JmSpider(BaseComicSpider2):
    name = 'jm'
    custom_settings = {
        "ITEM_PIPELINES": {'ComicSpider.pipelines.JmComicPipeline': 50},
        "DOWNLOADER_MIDDLEWARES": {
            'ComicSpider.middlewares.UAMiddleware': 5,
            'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': None,
            'ComicSpider.middlewares.DisableSystemProxyMiddleware': 4,
            'ComicSpider.middlewares.RefererMiddleware': 10,
        }, "COOKIES_ENABLED": not conf.cookies.get(name),
    }
    num_of_row = 4
    domain = domain
    search_url_head = f'https://{domain}/search/photos?main_tag=0&search_query='
    book_id_url = f'https://{domain}/photo/%s'
    transfer_url = staticmethod(lambda url: url.replace('album', 'photo'))
    mappings = {}

    time_regex = re.compile(r".*?([æ—¥å‘¨æœˆæ€»])")
    kind_regex = re.compile(r".*?(æ›´æ–°|ç‚¹å‡»|è¯„åˆ†|è¯„è®º|æ”¶è—)")
    expand_map: t.Dict[str, dict] = {
        "æ—¥": {'t': 't'}, "å‘¨": {'t': 'w'}, "æœˆ": {'t': 'm'}, "æ€»": {'t': 'a'},
        "æ›´æ–°": {'o': 'mr'}, "ç‚¹å‡»": {'o': 'mv'}, "è¯„åˆ†": {'o': 'tr'}, "è¯„è®º": {'o': 'md'}, "æ”¶è—": {'o': 'tf'}
    }
    turn_page_info = (r"page=\d+",)

    @property
    def ua(self):
        _ua = {'Host': self.domain, **JmUtils.headers}
        if conf.cookies.get("jm"):
            _ua.update({'cookie': JmUtils.to_str_(conf.cookies.get(self.name))})
        return _ua

    def preready(self):
        self.domain = JmUtils.get_domain()
        self.book_id_url = correct_domain(self.domain, self.book_id_url)

    def start_requests(self):
        self.preready()
        self.refresh_state('input_state', 'InputFieldQueue')
        keyword = convert_punctuation(self.input_state.keyword).replace(" ", "")
        if ',' in keyword or keyword.isdecimal():
            for key in filter(lambda x: x.isdecimal(), keyword.split(',')):
                yield scrapy.Request(url=self.book_id_url % key, callback=self.parse_section,
                                        headers={**self.ua, 'Referer': self.domain},
                                        meta={'book_id': key}, dont_filter=True)
        else:
            yield from super(JmSpider, self).start_requests()

    def parse_section(self, response):
        def _get_bid():
            if 'book_id' in meta:
                _bid = meta.get('book_id')
            elif 'book' in meta:
                _bid = meta.get('book').id
            else:
                _bid = self.ut.get_uuid(response.request.url, only_id=True) or ''
            return _bid
        meta = response.meta
        bid = _get_bid()
        self.process_state.process = 'parse section'
        self.Q('ProcessQueue').send(self.process_state)
        if response.url.endswith('album_missing'):
            yield self.say(font_color(f'â– æ— æ•ˆè½¦å·ï¼š{bid}', cls='theme-err'))
        elif response.url.endswith('login'):
            yield self.say(font_color(f'âš ï¸ éœ€è¦ç™»å½•/ç”šè‡³JCoinsï¼š{bid}', cls='theme-err'))
        else:
            if not meta.get('title'):
                title = response.xpath('//title/text()').extract_first()
                meta['title'] = title.rsplit('|', 1)[0]
            if not meta.get('book'):
                meta['book'] = JmBookInfo(
                    name=meta['title'],
                    url=response.url,
                ).get_id(response.url)
            yield from super(JmSpider, self).parse_section(response)

    @property
    def search(self):
        self.domain = JmUtils.get_domain()
        keyword = self.input_state.keyword
        __t = self.time_regex.search(keyword)
        __k = self.kind_regex.search(keyword)
        if keyword in self.mappings.keys():
            url = self.mappings[keyword]
            return Url(f"https://{self.domain}{urlparse(url).path}").set_next(*self.turn_page_info)
        elif not bool(__k):  # ä¸å¥½è¯´æ ‡é¢˜åŒ¹é…åˆ°å…³é”®å­—æƒ…å†µï¼Œè§†æƒ…å†µè¿”è‡³å‰ç½®å¸¦*è§¦å‘
            return Url(f"{self.search_url_head}{keyword}").set_next(*self.turn_page_info)
        _t = __t.group(1) if bool(__t) else 'å‘¨'
        _k = __k.group(1) if bool(__k) else 'ç‚¹å‡»'
        params = {**self.expand_map[_t], **self.expand_map[_k]}
        url = f"https://{self.domain}/albums?{urlencode(params)}"
        if len(keyword) > 4:
            url += keyword[4:]
        return Url(url).set_next(*self.turn_page_info)

    def frame_book(self, response):
        frame_results = {}
        self.say(self.say_fm.format('åºå·', 'æ¼«ç”»å') + '<br>')
        targets = response.xpath('//div[contains(@class,"thumb-overlay") and not(@class="thumb-overlay-guess_likes")]')
        for x, target in enumerate(targets):
            book = JmUtils.parse_search_item(target)
            book.idx = x + 1
            book.preview_url = f'https://{self.domain}{book.preview_url}'
            book.url = f'https://{self.domain}{book.url}'
            frame_results[book.idx] = book
        self.say.frame_book_print(frame_results, url=response.url, make_preview=True)
        self.say(font_color("<br>  jmé¢„è§ˆå›¾åŠ è½½æ‡‚å¾—éƒ½æ‡‚ï¼ŒåŠ è½½ä¸å‡ºæ¥æ˜¯æ­£å¸¸ç°è±¡å“¦", cls='theme-highlight'))

    def frame_section(self, response):
        targets = response.xpath(".//img[contains(@id,'album_photo_')]")
        frame_results = {}
        for x, target in enumerate(targets):
            img_url = target.xpath('./@data-original').get()
            frame_results[x + 1] = img_url
        self.say("ğŸ“¢" + font_color(' è¿™æœ¬å·²ç»æ‰”è¿›ä»»åŠ¡äº†', cls='theme-tip'))
        return frame_results
