#!/bin/bash
#SBATCH --job-name={{ job_name }}
#SBATCH --account={{ cluster_account }}
#SBATCH --time={{ time_limit }}
#SBATCH --exclusive
#SBATCH --nodes={{ nodes }}
#SBATCH --partition={{ partition }}
#SBATCH --output={{ home }}/.clserve/logs/%j/log.out
#SBATCH --error={{ home }}/.clserve/logs/%j/log.err


TP_SIZE={{ tp_size }}
EP_SIZE={{ ep_size }}
CUDA_GRAPH_MAX_BS={{ cuda_graph_max_bs }}
GRAMMAR_BACKEND={{ grammar_backend }}
MODEL_PATH={{ model_path }}
NODES={{ nodes }}
NODES_PER_WORKER={{ nodes_per_worker }}
WORKERS={{ workers }}
LOG_DIR="${HOME}/.clserve/logs/${SLURM_JOB_ID}"
ENVIRONMENT={{ environment }}
ROUTER_ENVIRONMENT={{ router_environment }}
ROUTER_POLICY={{ router_policy }}
REASONING_PARSER="{{ reasoning_parser }}"
TOOL_CALL_PARSER="{{ tool_call_parser }}"
NUM_GPUS_PER_WORKER={{ num_gpus_per_worker }}
PROCESSES_PER_NODE=$((4 / NUM_GPUS_PER_WORKER))

{% raw %}

mkdir -p "${LOG_DIR}"

# Calculate total processes to determine if router should be enabled
TOTAL_PROCESSES=$((WORKERS * NODES_PER_WORKER * PROCESSES_PER_NODE))
if [ $TOTAL_PROCESSES -gt 1 ]; then
    USE_ROUTER=true
else
    USE_ROUTER=false
fi

# Write metadata for clserve status command
cat > "${LOG_DIR}/metadata.txt" << METADATA_EOF
MODEL_PATH=${MODEL_PATH}
WORKERS=${WORKERS}
NODES_PER_WORKER=${NODES_PER_WORKER}
TP_SIZE=${TP_SIZE}
EP_SIZE=${EP_SIZE}
USE_ROUTER=${USE_ROUTER}
NUM_GPUS_PER_WORKER=${NUM_GPUS_PER_WORKER}
METADATA_EOF

nodes=($(scontrol show hostnames $SLURM_NODELIST))
if [ ${#nodes[@]} -ne $NODES ]; then
    echo "Error: Expected $NODES nodes but got ${#nodes[@]} nodes"
    exit 1
fi

# Print node information
for i in "${!nodes[@]}"; do
    echo "Node $i: ${nodes[$i]}"
done

# Collect all worker head IPs and URLs
worker_head_ips=()
worker_urls=()
node_ips=()

# First collect all node IPs
for node_idx in $(seq 0 $((NODES - 1))); do
    node=${nodes[$node_idx]}
    node_ip=$(getent hosts ${node} | awk '{print $1}')
    if [ -z "$node_ip" ]; then
        echo "Error: Could not retrieve IP address for node ${node}"
        exit 1
    fi
    node_ips+=("$node_ip")
    echo "Node $node_idx (${node}) IP: $node_ip"
done

# Build worker URLs based on num GPUs per worker
if [ $NUM_GPUS_PER_WORKER -eq 4 ]; then
    # Multi-node worker behavior: only the head node serves the API
    for worker_id in $(seq 0 $((WORKERS - 1))); do
        start_node=$((worker_id * NODES_PER_WORKER))
        worker_host_ip=${node_ips[$start_node]}
        worker_head_ips+=("$worker_host_ip")
        # Only add the head node URL (other nodes are for distributed compute only)
        worker_urls+=("http://${worker_host_ip}:5000")
    done
else
    # Multiple processes per node - build URLs for exactly WORKERS processes
    prev_node_idx=-1
    for worker_idx in $(seq 0 $((WORKERS - 1))); do
        node_idx=$((worker_idx / PROCESSES_PER_NODE))
        proc_id=$((worker_idx % PROCESSES_PER_NODE))
        node_ip=${node_ips[$node_idx]}

        # Add head IP only once per node
        if [ $node_idx -ne $prev_node_idx ]; then
            worker_head_ips+=("$node_ip")
            prev_node_idx=$node_idx
        fi

        port=$((5000 + proc_id))
        worker_urls+=("http://${node_ip}:${port}")
    done
fi

echo "All worker head IPs: $(printf '"%s"' "${worker_head_ips[@]}" | paste -sd ',' - | sed 's/,/, /g')"
echo "All worker URLs: $(printf '"%s"' "${worker_urls[@]}" | paste -sd ',' - | sed 's/,/, /g')"

# Launch workers
for worker_id in $(seq 0 $((WORKERS - 1))); do
    echo "Launching worker $worker_id"

    # Calculate node range for this worker
    start_node=$((worker_id * NODES_PER_WORKER))
    end_node=$((start_node + NODES_PER_WORKER - 1))

    # Get worker nodes
    worker_nodes=()
    for node_idx in $(seq $start_node $end_node); do
        worker_nodes+=("${nodes[$node_idx]}")
    done

    echo "Worker $worker_id nodes: ${worker_nodes[*]}"
    worker_host_ip=${worker_head_ips[$worker_id]}

    # Launch tasks for this worker
    if [ $NUM_GPUS_PER_WORKER -eq 4 ]; then
        # Original behavior: one process per node using all GPUs
        for local_rank in $(seq 0 $((NODES_PER_WORKER - 1))); do
            global_node_idx=$((start_node + local_rank))
            node=${nodes[$global_node_idx]}

            srun --nodes=1 --ntasks=1 --nodelist=$node --container-writable --environment=$ENVIRONMENT --output=${LOG_DIR}/worker${worker_id}_node${local_rank}_${node}.out --error=${LOG_DIR}/worker${worker_id}_node${local_rank}_${node}.err \
                bash --norc --noprofile -c "\
set -ex

export no_proxy=\"0.0.0.0,\$no_proxy\"
export NO_PROXY=\"0.0.0.0,\$NO_PROXY\"

export NCCL_DEBUG=WARN
export CUDA_DEVICE_ORDER=PCI_BUS_ID
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:False
export SGL_ENABLE_JIT_DEEPGEMM=false

python -m sglang.launch_server --model-path ${MODEL_PATH} --host 0.0.0.0 --port 5000 --dist-init-addr ${worker_host_ip}:5757 --nnodes ${NODES_PER_WORKER} --node-rank ${local_rank} --tp-size ${TP_SIZE} --ep-size ${EP_SIZE} --cuda-graph-max-bs ${CUDA_GRAPH_MAX_BS} --grammar-backend ${GRAMMAR_BACKEND} --decode-log-interval 1 --trust-remote-code ${REASONING_PARSER} ${TOOL_CALL_PARSER}" &

            echo "Launched worker $worker_id node $local_rank on $node"
        done
    else
        # Multiple processes per node, each with a subset of GPUs
        # Launch exactly one process per worker_id
        proc_id=$((worker_id % PROCESSES_PER_NODE))
        node_idx=$((worker_id / PROCESSES_PER_NODE))
        node=${nodes[$node_idx]}
        port=$((5000 + proc_id))

        # Calculate GPU binding based on num_gpus_per_worker
        if [ $NUM_GPUS_PER_WORKER -eq 1 ]; then
            gpu_bind="map_gpu:${proc_id}"
            cuda_visible_devices="${proc_id}"
        else
            # NUM_GPUS_PER_WORKER = 2
            first_gpu=$((proc_id * 2))
            second_gpu=$((first_gpu + 1))
            gpu_bind="map_gpu:${first_gpu},${second_gpu}"
            cuda_visible_devices="${first_gpu},${second_gpu}"
        fi

        srun --nodes=1 --ntasks=1 --nodelist=$node --container-writable --environment=$ENVIRONMENT --kill-on-bad-exit=1 --gpus-per-task=${NUM_GPUS_PER_WORKER} --cpus-per-task=50 --gpu-bind=${gpu_bind} --overlap --output=${LOG_DIR}/worker${worker_id}_proc${proc_id}_${node}.out --error=${LOG_DIR}/worker${worker_id}_proc${proc_id}_${node}.err \
            bash --norc --noprofile -c "\
set -ex

export no_proxy=\"0.0.0.0,\$no_proxy\"
export NO_PROXY=\"0.0.0.0,\$NO_PROXY\"

export NCCL_DEBUG=WARN
export CUDA_DEVICE_ORDER=PCI_BUS_ID
export CUDA_VISIBLE_DEVICES=${cuda_visible_devices}
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:False
export SGL_ENABLE_JIT_DEEPGEMM=false

python -m sglang.launch_server --model-path ${MODEL_PATH} --host 0.0.0.0 --port ${port} --tp-size ${NUM_GPUS_PER_WORKER} --cuda-graph-max-bs ${CUDA_GRAPH_MAX_BS} --grammar-backend ${GRAMMAR_BACKEND} --decode-log-interval 1 --trust-remote-code ${REASONING_PARSER} ${TOOL_CALL_PARSER}" &

        echo "Launched worker $worker_id on node $node with GPUs ${cuda_visible_devices} on port ${port}"
    fi
done

# Launch router if there are multiple worker processes
router_url=""
if [ "$USE_ROUTER" = "true" ]; then
    router_host_node=${nodes[0]}
    router_host_ip=${node_ips[0]}
    router_url="http://${router_host_ip}:30000"

    # Build worker URLs string for router
    worker_urls_str=""
    for url in "${worker_urls[@]}"; do
        worker_urls_str="$worker_urls_str $url"
    done

    echo "Starting router on ${router_host_node} (${router_host_ip})"
    echo "Router worker URLs:${worker_urls_str}"

    srun --nodes=1 --ntasks=1 --nodelist=$router_host_node --container-writable --environment=$ROUTER_ENVIRONMENT --cpus-per-task=50 --overlap --output=${LOG_DIR}/router_${router_host_node}.out --error=${LOG_DIR}/router_${router_host_node}.err \
        bash --norc --noprofile -c "\
set -ex

export no_proxy=\"0.0.0.0,\$no_proxy\"
export NO_PROXY=\"0.0.0.0,\$NO_PROXY\"

python -m sglang_router.launch_router --host 0.0.0.0 --port 30000 --worker-urls${worker_urls_str} --model-path ${MODEL_PATH} --policy ${ROUTER_POLICY} --worker-startup-timeout-secs 1200 --max-concurrent-requests -1" &

    echo ""
    echo "Router URL: ${router_url}"
fi

# Write endpoint info for clserve status command
if [ -n "$router_url" ]; then
    echo "ENDPOINT_URL=${router_url}" >> "${LOG_DIR}/metadata.txt"
else
    echo "ENDPOINT_URL=${worker_urls[0]}" >> "${LOG_DIR}/metadata.txt"
fi

echo ""
echo "============================================"
echo "CLSERVE DEPLOYMENT INFO"
echo "============================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Model: ${MODEL_PATH}"
if [ -n "$router_url" ]; then
    echo "Endpoint URL: ${router_url}"
else
    echo "Endpoint URL: ${worker_urls[0]}"
fi
echo "============================================"

echo ""
echo "To connect to the host node:"
echo "srun --jobid $SLURM_JOB_ID -w ${nodes[0]} --overlap --pty bash"

echo ""
echo "Make sure to cancel the job at the end:"
echo "scancel $SLURM_JOB_ID"
echo "Or use: clserve stop $SLURM_JOB_ID"

wait
echo "Script finished at $(date)"

{% endraw %}
