# Enterprise Integration & Extensibility Architecture

**Document Version:** 1.0  
**Date:** 2025-11-16  
**Project:** RabbitMQ MCP Server  
**Phase:** Future Extensibility (Phase 2/Vision)  
**Status:** Architectural Guidance (Not Implemented in MVP)

---

## Executive Summary

This document provides architectural guidance for enterprise integrations deferred to Phase 2 and Vision. While not implemented in MVP (Phase 1), these patterns inform design decisions to avoid costly rework.

**Key Principle**: Design for extensibility without over-engineering MVP.

**Covered Integrations**:
- Authentication & Authorization (LDAP, SSO, RBAC)
- Multi-Region Deployment
- Plugin Architecture
- Enterprise Observability (DataDog, Splunk)
- Compliance & Audit (HIPAA, SOC2, GDPR)

---

## 1. Authentication & Authorization (Phase 2)

### 1.1 Current State (MVP)

**Authentication**:
- HTTP Transport: Bearer token via `MCP_AUTH_TOKEN` environment variable
- Stdio Transport: No authentication (local process, trusted environment)

**Authorization**:
- No granular permissions (all authenticated users have full access)
- RabbitMQ credentials required for all operations

**Limitations**:
- Single token for all users (no user identity)
- No role-based access control
- Credentials managed externally (not integrated with enterprise IAM)

### 1.2 Future Architecture (Phase 2)

**Authentication Abstraction Layer**:

```python
# src/auth/providers.py

from abc import ABC, abstractmethod
from typing import Optional

class AuthProvider(ABC):
    """Abstract authentication provider."""
    
    @abstractmethod
    async def authenticate(self, credentials: dict) -> Optional[User]:
        """Authenticate user and return User object."""
        pass
    
    @abstractmethod
    async def refresh_token(self, refresh_token: str) -> Optional[str]:
        """Refresh authentication token."""
        pass

class BearerTokenProvider(AuthProvider):
    """Bearer token authentication (MVP)."""
    
    async def authenticate(self, credentials: dict) -> Optional[User]:
        token = credentials.get("token")
        if token == os.getenv("MCP_AUTH_TOKEN"):
            return User(id="default", roles=["admin"])
        return None

class LDAPProvider(AuthProvider):
    """LDAP authentication (Phase 2)."""
    
    def __init__(self, ldap_url: str, base_dn: str):
        self.ldap_url = ldap_url
        self.base_dn = base_dn
    
    async def authenticate(self, credentials: dict) -> Optional[User]:
        username = credentials.get("username")
        password = credentials.get("password")
        
        # LDAP bind and search
        conn = await ldap.connect(self.ldap_url)
        user_dn = f"uid={username},{self.base_dn}"
        
        if await conn.bind(user_dn, password):
            # Fetch groups for RBAC
            groups = await conn.search(
                base_dn=self.base_dn,
                filter=f"(memberUid={username})"
            )
            roles = self._map_groups_to_roles(groups)
            return User(id=username, roles=roles)
        return None

class OAuthProvider(AuthProvider):
    """OAuth 2.0 / OIDC authentication (Phase 2)."""
    
    def __init__(self, issuer: str, client_id: str, client_secret: str):
        self.issuer = issuer
        self.client_id = client_id
        self.client_secret = client_secret
    
    async def authenticate(self, credentials: dict) -> Optional[User]:
        access_token = credentials.get("access_token")
        
        # Validate token with OAuth provider
        user_info = await self._validate_token(access_token)
        if user_info:
            roles = user_info.get("groups", [])
            return User(
                id=user_info["sub"],
                email=user_info.get("email"),
                roles=self._map_groups_to_roles(roles)
            )
        return None
```

**Authorization Layer (RBAC)**:

```python
# src/auth/authorization.py

from enum import Enum
from typing import Set

class Permission(Enum):
    """Granular permissions."""
    QUEUE_READ = "queue:read"
    QUEUE_WRITE = "queue:write"
    QUEUE_DELETE = "queue:delete"
    EXCHANGE_READ = "exchange:read"
    EXCHANGE_WRITE = "exchange:write"
    EXCHANGE_DELETE = "exchange:delete"
    MESSAGE_PUBLISH = "message:publish"
    MESSAGE_CONSUME = "message:consume"
    ADMIN = "admin:*"

class Role:
    """Role with associated permissions."""
    
    def __init__(self, name: str, permissions: Set[Permission]):
        self.name = name
        self.permissions = permissions

# Predefined roles
ROLES = {
    "admin": Role("admin", {Permission.ADMIN}),
    "operator": Role("operator", {
        Permission.QUEUE_READ, Permission.QUEUE_WRITE,
        Permission.EXCHANGE_READ, Permission.EXCHANGE_WRITE,
        Permission.MESSAGE_PUBLISH, Permission.MESSAGE_CONSUME
    }),
    "developer": Role("developer", {
        Permission.QUEUE_READ, Permission.EXCHANGE_READ,
        Permission.MESSAGE_PUBLISH, Permission.MESSAGE_CONSUME
    }),
    "viewer": Role("viewer", {
        Permission.QUEUE_READ, Permission.EXCHANGE_READ
    })
}

class AuthorizationService:
    """Check user permissions."""
    
    def has_permission(self, user: User, permission: Permission) -> bool:
        """Check if user has permission."""
        if Permission.ADMIN in self._get_user_permissions(user):
            return True  # Admin has all permissions
        return permission in self._get_user_permissions(user)
    
    def _get_user_permissions(self, user: User) -> Set[Permission]:
        """Get all permissions for user."""
        permissions = set()
        for role_name in user.roles:
            role = ROLES.get(role_name)
            if role:
                permissions.update(role.permissions)
        return permissions
```

**Integration with MCP Tools**:

```python
# src/mcp_server/tools/base.py

from src.auth.authorization import AuthorizationService, Permission

class ProtectedMCPTool:
    """Base class for MCP tools with authorization."""
    
    def __init__(self, auth_service: AuthorizationService):
        self.auth_service = auth_service
    
    async def execute(self, user: User, params: dict):
        """Execute tool with authorization check."""
        required_permission = self.get_required_permission(params)
        
        if not self.auth_service.has_permission(user, required_permission):
            raise UnauthorizedError(
                f"User {user.id} lacks permission: {required_permission.value}"
            )
        
        return await self._execute_impl(params)
    
    @abstractmethod
    def get_required_permission(self, params: dict) -> Permission:
        """Return required permission for this operation."""
        pass
    
    @abstractmethod
    async def _execute_impl(self, params: dict):
        """Actual tool implementation."""
        pass
```

**MVP Design Consideration**:
- Authentication abstraction already present (bearer token provider)
- Authorization hooks prepared (middleware can inject checks)
- RabbitMQ vhost isolation enables tenant separation
- **No rework required** to add LDAP/OAuth in Phase 2

---

## 2. Multi-Region Deployment (Phase 2)

### 2.1 Current State (MVP)

**Deployment Model**:
- Single-region deployment
- Stateless servers (horizontal scaling within region)
- Connection pooling per instance (no cross-region coordination)

**Limitations**:
- No geo-distributed RabbitMQ cluster support
- Latency for remote users
- Single point of failure (region outage)

### 2.2 Future Architecture (Phase 2)

**Regional MCP Server Deployment**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Global Load Balancer                    ‚îÇ
‚îÇ                  (GeoDNS / AWS Route 53)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ               ‚îÇ               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ US-EAST ‚îÇ     ‚îÇ EU-WEST ‚îÇ     ‚îÇ AP-SOUTH‚îÇ
    ‚îÇ  Region ‚îÇ     ‚îÇ  Region ‚îÇ     ‚îÇ  Region ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ               ‚îÇ               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  MCP    ‚îÇ     ‚îÇ  MCP    ‚îÇ     ‚îÇ  MCP    ‚îÇ
    ‚îÇ Servers ‚îÇ     ‚îÇ Servers ‚îÇ     ‚îÇ Servers ‚îÇ
    ‚îÇ (3x)    ‚îÇ     ‚îÇ (3x)    ‚îÇ     ‚îÇ (3x)    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ               ‚îÇ               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇRabbitMQ ‚îÇ     ‚îÇRabbitMQ ‚îÇ     ‚îÇRabbitMQ ‚îÇ
    ‚îÇ Cluster ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚î§Federation‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Cluster ‚îÇ
    ‚îÇ (3 nodes)‚îÇ     ‚îÇ  Links  ‚îÇ     ‚îÇ (3 nodes)‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Regional Configuration**:

```yaml
# config/regions.yaml

regions:
  - name: us-east-1
    endpoint: https://mcp-us-east.example.com
    rabbitmq:
      host: rabbitmq-us-east.internal
      port: 5672
      management_port: 15672
    
  - name: eu-west-1
    endpoint: https://mcp-eu-west.example.com
    rabbitmq:
      host: rabbitmq-eu-west.internal
      port: 5672
      management_port: 15672
    
  - name: ap-south-1
    endpoint: https://mcp-ap-south.example.com
    rabbitmq:
      host: rabbitmq-ap-south.internal
      port: 5672
      management_port: 15672

federation:
  enabled: true
  links:
    - upstream: us-east-1
      downstream: eu-west-1
      patterns: ["federated.*"]
    - upstream: eu-west-1
      downstream: ap-south-1
      patterns: ["federated.*"]
```

**Regional Routing Logic**:

```python
# src/routing/regional.py

import geoip2.database
from typing import Optional

class RegionalRouter:
    """Route requests to nearest regional cluster."""
    
    def __init__(self, regions: List[RegionConfig]):
        self.regions = regions
        self.geoip_reader = geoip2.database.Reader('/usr/share/GeoIP/GeoLite2-City.mmdb')
    
    def get_nearest_region(self, client_ip: str) -> RegionConfig:
        """Determine nearest region for client."""
        try:
            response = self.geoip_reader.city(client_ip)
            client_lat = response.location.latitude
            client_lon = response.location.longitude
            
            # Calculate distance to each region
            distances = []
            for region in self.regions:
                distance = self._haversine_distance(
                    client_lat, client_lon,
                    region.latitude, region.longitude
                )
                distances.append((distance, region))
            
            # Return nearest region
            return min(distances, key=lambda x: x[0])[1]
        
        except Exception:
            # Fallback to default region
            return self.regions[0]
    
    def _haversine_distance(self, lat1, lon1, lat2, lon2) -> float:
        """Calculate distance between two points in km."""
        # Implementation omitted for brevity
        pass
```

**MVP Design Consideration**:
- Stateless design (ADR-005) already enables multi-region deployment
- No shared state (no cross-region coordination needed)
- RabbitMQ federation handles cross-region messaging
- **No architectural changes required** for multi-region

---

## 3. Plugin Architecture (Vision)

### 3.1 Current State (MVP)

**Extensibility**:
- Hardcoded RabbitMQ operations (from OpenAPI spec)
- No plugin mechanism
- Custom operations require code changes

**Limitations**:
- Cannot add custom operations without forking
- No ecosystem for third-party extensions
- Difficult to integrate with proprietary message brokers

### 3.2 Future Architecture (Vision)

**Plugin Interface**:

```python
# src/plugins/interface.py

from abc import ABC, abstractmethod
from typing import List, Dict, Any

class MCPPlugin(ABC):
    """Abstract plugin interface."""
    
    @abstractmethod
    def get_name(self) -> str:
        """Return plugin name."""
        pass
    
    @abstractmethod
    def get_version(self) -> str:
        """Return plugin version."""
        pass
    
    @abstractmethod
    def get_operations(self) -> List[OperationDef]:
        """Return list of operations provided by plugin."""
        pass
    
    @abstractmethod
    async def execute(self, operation_id: str, params: dict) -> Any:
        """Execute plugin operation."""
        pass
    
    @abstractmethod
    async def initialize(self, config: dict) -> None:
        """Initialize plugin with configuration."""
        pass
    
    @abstractmethod
    async def shutdown(self) -> None:
        """Clean up plugin resources."""
        pass

class OperationDef:
    """Operation definition for plugin."""
    
    def __init__(
        self,
        operation_id: str,
        name: str,
        description: str,
        parameters: Dict[str, Any],  # JSON Schema
        examples: List[Dict[str, Any]]
    ):
        self.operation_id = operation_id
        self.name = name
        self.description = description
        self.parameters = parameters
        self.examples = examples
```

**Plugin Loader**:

```python
# src/plugins/loader.py

import importlib
import os
from typing import List, Dict

class PluginLoader:
    """Load and manage plugins."""
    
    def __init__(self, plugin_dir: str = "./plugins"):
        self.plugin_dir = plugin_dir
        self.plugins: Dict[str, MCPPlugin] = {}
    
    async def load_plugins(self) -> None:
        """Discover and load all plugins."""
        for filename in os.listdir(self.plugin_dir):
            if filename.endswith(".py") and not filename.startswith("_"):
                module_name = filename[:-3]
                module = importlib.import_module(f"plugins.{module_name}")
                
                # Find plugin class
                for attr_name in dir(module):
                    attr = getattr(module, attr_name)
                    if isinstance(attr, type) and issubclass(attr, MCPPlugin) and attr != MCPPlugin:
                        plugin_instance = attr()
                        await plugin_instance.initialize(self._get_plugin_config(module_name))
                        self.plugins[plugin_instance.get_name()] = plugin_instance
        
        logger.info(f"Loaded {len(self.plugins)} plugins", plugins=list(self.plugins.keys()))
    
    def get_plugin(self, name: str) -> Optional[MCPPlugin]:
        """Get plugin by name."""
        return self.plugins.get(name)
    
    def get_all_operations(self) -> List[OperationDef]:
        """Get all operations from all plugins."""
        operations = []
        for plugin in self.plugins.values():
            operations.extend(plugin.get_operations())
        return operations
```

**Example Plugin** (Custom Broker Support):

```python
# plugins/kafka_plugin.py

from src.plugins.interface import MCPPlugin, OperationDef
from kafka import KafkaProducer, KafkaConsumer

class KafkaPlugin(MCPPlugin):
    """Plugin to support Apache Kafka operations."""
    
    def get_name(self) -> str:
        return "kafka"
    
    def get_version(self) -> str:
        return "1.0.0"
    
    def get_operations(self) -> List[OperationDef]:
        return [
            OperationDef(
                operation_id="kafka_list_topics",
                name="List Kafka Topics",
                description="List all topics in Kafka cluster",
                parameters={
                    "type": "object",
                    "properties": {
                        "cluster": {"type": "string", "description": "Kafka cluster name"}
                    },
                    "required": ["cluster"]
                },
                examples=[{"cluster": "production"}]
            ),
            OperationDef(
                operation_id="kafka_publish_message",
                name="Publish Kafka Message",
                description="Publish message to Kafka topic",
                parameters={
                    "type": "object",
                    "properties": {
                        "topic": {"type": "string"},
                        "key": {"type": "string"},
                        "value": {"type": "string"}
                    },
                    "required": ["topic", "value"]
                },
                examples=[{"topic": "orders", "key": "order-123", "value": "{}"}]
            )
        ]
    
    async def initialize(self, config: dict) -> None:
        """Initialize Kafka connection."""
        self.bootstrap_servers = config.get("bootstrap_servers", "localhost:9092")
        self.producer = KafkaProducer(bootstrap_servers=self.bootstrap_servers)
        logger.info("Kafka plugin initialized", servers=self.bootstrap_servers)
    
    async def execute(self, operation_id: str, params: dict) -> Any:
        """Execute Kafka operation."""
        if operation_id == "kafka_list_topics":
            consumer = KafkaConsumer(bootstrap_servers=self.bootstrap_servers)
            topics = consumer.topics()
            return {"topics": list(topics)}
        
        elif operation_id == "kafka_publish_message":
            self.producer.send(
                params["topic"],
                key=params.get("key", "").encode(),
                value=params["value"].encode()
            )
            return {"status": "published"}
        
        else:
            raise ValueError(f"Unknown operation: {operation_id}")
    
    async def shutdown(self) -> None:
        """Clean up Kafka connection."""
        self.producer.close()
```

**MVP Design Consideration**:
- Operation registry already abstracted (can load from multiple sources)
- Tool execution delegated to operation handlers (plugin-friendly)
- **Minor refactoring** required to add plugin loader

---

## 4. Enterprise Observability (Phase 2)

### 4.1 Current State (MVP)

**Observability Stack**:
- Structured logging (structlog ‚Üí JSON files)
- OpenTelemetry tracing (OTLP export)
- Prometheus metrics (planned)

**Limitations**:
- No integration with enterprise platforms (DataDog, Splunk, Dynatrace)
- No anomaly detection or alerting
- Limited log retention (30 days local)

### 4.2 Future Architecture (Phase 2)

**Multi-Backend Observability**:

```python
# src/observability/backends.py

from abc import ABC, abstractmethod

class ObservabilityBackend(ABC):
    """Abstract observability backend."""
    
    @abstractmethod
    async def send_logs(self, logs: List[dict]) -> None:
        """Send logs to backend."""
        pass
    
    @abstractmethod
    async def send_metrics(self, metrics: List[Metric]) -> None:
        """Send metrics to backend."""
        pass
    
    @abstractmethod
    async def send_traces(self, traces: List[Trace]) -> None:
        """Send traces to backend."""
        pass

class DataDogBackend(ObservabilityBackend):
    """DataDog observability backend."""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = DatadogHTTPClient(api_key=api_key)
    
    async def send_logs(self, logs: List[dict]) -> None:
        """Send logs to DataDog."""
        await self.client.post("/v1/logs", json=logs)
    
    async def send_metrics(self, metrics: List[Metric]) -> None:
        """Send metrics to DataDog."""
        series = [
            {
                "metric": m.name,
                "points": [(m.timestamp, m.value)],
                "tags": m.tags
            }
            for m in metrics
        ]
        await self.client.post("/v1/series", json={"series": series})

class SplunkBackend(ObservabilityBackend):
    """Splunk observability backend."""
    
    def __init__(self, hec_url: str, hec_token: str):
        self.hec_url = hec_url
        self.hec_token = hec_token
    
    async def send_logs(self, logs: List[dict]) -> None:
        """Send logs to Splunk HEC."""
        events = [
            {
                "time": log["timestamp"],
                "source": "rabbitmq-mcp",
                "sourcetype": "_json",
                "event": log
            }
            for log in logs
        ]
        await httpx.post(
            self.hec_url,
            headers={"Authorization": f"Splunk {self.hec_token}"},
            json=events
        )
```

**Configuration**:

```yaml
# config/observability.yaml

backends:
  - type: datadog
    enabled: true
    api_key: ${DATADOG_API_KEY}
    site: datadoghq.com
    
  - type: splunk
    enabled: true
    hec_url: https://splunk.example.com:8088/services/collector
    hec_token: ${SPLUNK_HEC_TOKEN}
    
  - type: prometheus
    enabled: true
    port: 9090
    
  - type: jaeger
    enabled: true
    endpoint: http://jaeger:14268/api/traces
```

**MVP Design Consideration**:
- OpenTelemetry already supports multiple exporters
- Structured logs easily forwarded to any backend
- **Configuration-only changes** to add enterprise backends

---

## 5. Compliance & Audit (Phase 2)

### 5.1 Current State (MVP)

**Audit Logging**:
- All operations logged with correlation IDs
- Logs include: timestamp, user (if authenticated), operation, result

**Limitations**:
- No tamper-proof audit trail
- No compliance reports (HIPAA, SOC2, GDPR)
- Credential sanitization manual (regex-based)

### 5.2 Future Architecture (Phase 2)

**Immutable Audit Trail**:

```python
# src/audit/immutable_log.py

import hashlib
import json
from typing import List

class ImmutableAuditLog:
    """Blockchain-style immutable audit log."""
    
    def __init__(self):
        self.chain: List[AuditBlock] = []
        self._create_genesis_block()
    
    def _create_genesis_block(self):
        """Create initial block."""
        genesis = AuditBlock(
            index=0,
            timestamp=datetime.utcnow().isoformat(),
            data={"event": "audit_log_initialized"},
            previous_hash="0"
        )
        self.chain.append(genesis)
    
    def add_event(self, event: dict) -> AuditBlock:
        """Add audit event to chain."""
        previous_block = self.chain[-1]
        new_block = AuditBlock(
            index=len(self.chain),
            timestamp=datetime.utcnow().isoformat(),
            data=event,
            previous_hash=previous_block.hash
        )
        self.chain.append(new_block)
        return new_block
    
    def verify_integrity(self) -> bool:
        """Verify audit log has not been tampered."""
        for i in range(1, len(self.chain)):
            current = self.chain[i]
            previous = self.chain[i - 1]
            
            # Verify hash
            if current.hash != current.calculate_hash():
                return False
            
            # Verify chain link
            if current.previous_hash != previous.hash:
                return False
        
        return True

class AuditBlock:
    """Single audit event block."""
    
    def __init__(self, index: int, timestamp: str, data: dict, previous_hash: str):
        self.index = index
        self.timestamp = timestamp
        self.data = data
        self.previous_hash = previous_hash
        self.hash = self.calculate_hash()
    
    def calculate_hash(self) -> str:
        """Calculate block hash."""
        block_string = json.dumps({
            "index": self.index,
            "timestamp": self.timestamp,
            "data": self.data,
            "previous_hash": self.previous_hash
        }, sort_keys=True)
        return hashlib.sha256(block_string.encode()).hexdigest()
```

**Compliance Reports**:

```python
# src/compliance/reports.py

class ComplianceReporter:
    """Generate compliance reports."""
    
    def generate_hipaa_report(self, start_date: date, end_date: date) -> dict:
        """Generate HIPAA compliance report."""
        return {
            "report_type": "HIPAA",
            "period": {"start": start_date, "end": end_date},
            "access_logs": self._get_access_logs(start_date, end_date),
            "data_modifications": self._get_modifications(start_date, end_date),
            "security_incidents": self._get_incidents(start_date, end_date),
            "encryption_status": self._check_encryption(),
            "audit_log_integrity": self.audit_log.verify_integrity()
        }
    
    def generate_gdpr_report(self, user_id: str) -> dict:
        """Generate GDPR data access report for user."""
        return {
            "report_type": "GDPR",
            "user_id": user_id,
            "data_collected": self._get_user_data(user_id),
            "data_shared": self._get_data_sharing(user_id),
            "retention_policy": "30 days",
            "right_to_erasure": self._check_erasure_capability(user_id)
        }
```

**MVP Design Consideration**:
- Audit logging already comprehensive
- Structured format enables easy report generation
- **Add-on module** for compliance reports (no core changes)

---

## 6. API Versioning Strategy (Phase 2+)

### 6.1 Current State (MVP)

**Versioning**:
- Single MCP API version (no backward compatibility burden)
- RabbitMQ API version selected via env var (`RABBITMQ_API_VERSION`)

**Limitations**:
- Breaking changes require coordination with all clients
- No deprecation policy

### 6.2 Future Architecture (Phase 2+)

**Semantic Versioning**:

```
/api/v1/...  ‚Üí Stable, frozen (backward compatible)
/api/v2/...  ‚Üí New features, breaking changes allowed
/api/v3/...  ‚Üí Future version
```

**Deprecation Policy**:
- New versions introduce breaking changes
- Old versions supported for 12 months after new version release
- Deprecation warnings added 6 months before sunset
- Migration guides provided for each major version

**Version Negotiation**:

```python
# src/api/versioning.py

class VersionedAPIRouter:
    """Route requests to appropriate API version."""
    
    def __init__(self):
        self.versions = {
            "v1": V1Router(),
            "v2": V2Router(),
            "v3": V3Router()
        }
        self.default_version = "v2"
    
    async def route(self, request: Request) -> Response:
        """Route request to appropriate version."""
        # Version from URL path
        path_version = self._extract_version_from_path(request.url.path)
        if path_version:
            return await self.versions[path_version].handle(request)
        
        # Version from Accept header
        accept_version = request.headers.get("Accept-Version")
        if accept_version and accept_version in self.versions:
            return await self.versions[accept_version].handle(request)
        
        # Default version
        return await self.versions[self.default_version].handle(request)
```

**MVP Design Consideration**:
- MCP protocol has built-in versioning
- RabbitMQ API versioning already implemented
- **Extend existing version selection** to API versioning

---

## 7. Implementation Roadmap

### 7.1 Phase 1 (MVP) - Completed

‚úÖ **Extensibility Hooks**:
- Authentication abstraction (bearer token provider)
- Stateless design (multi-region ready)
- Operation registry (plugin-friendly)
- OpenTelemetry integration (multiple exporters)

### 7.2 Phase 2 (Growth Features) - 6-12 months

üîµ **Authentication & Authorization**:
- LDAP provider implementation
- OAuth/OIDC provider implementation
- RBAC with granular permissions
- User management API

üîµ **Multi-Region Deployment**:
- Regional configuration support
- GeoDNS routing
- RabbitMQ federation setup
- Cross-region observability

üîµ **Enterprise Observability**:
- DataDog integration
- Splunk integration
- Anomaly detection
- Compliance reports (HIPAA, GDPR, SOC2)

### 7.3 Vision (12+ months)

üîÆ **Plugin Architecture**:
- Plugin interface finalization
- Plugin marketplace
- Third-party plugin SDK
- Kafka/ActiveMQ/NATS plugins

üîÆ **Advanced Features**:
- Immutable audit trail
- Multi-tenancy with resource quotas
- GraphQL API (in addition to MCP)
- Machine learning-based anomaly detection

---

## 8. Decision Log

### 8.1 Why Defer to Phase 2?

**Rationale**:
- MVP focuses on core RabbitMQ operations (Specs 001-008)
- Enterprise features not required for initial adoption
- Design hooks enable future integration without rework
- Avoid over-engineering MVP (YAGNI principle)

### 8.2 Why Document Now?

**Rationale**:
- Inform MVP design decisions (e.g., authentication abstraction)
- Avoid costly architectural rework
- Enable stakeholders to plan Phase 2 budget
- Demonstrate long-term product vision

---

## 9. References

**Related ADRs**:
- ADR-005: Stateless Server Design (enables multi-region)
- ADR-006: Automatic Credential Sanitization (security foundation)

**Related Documents**:
- `architecture/security-architecture.md`: Security design
- `architecture/deployment-architecture.md`: Deployment patterns
- `prd/product-scope.md`: Feature roadmap (MVP vs Growth vs Vision)

**External References**:
- LDAP Authentication: RFC 4511
- OAuth 2.0: RFC 6749
- OIDC: https://openid.net/specs/openid-connect-core-1_0.html
- HIPAA Compliance: https://www.hhs.gov/hipaa/
- GDPR: https://gdpr.eu/

---

**Document End**

**Next Steps**:
1. Review this document during architecture phase (if not already complete)
2. Reference during Sprint Planning (identify extensibility requirements)
3. Update during Phase 2 planning (detailed implementation design)
4. Track integration requirements as tech debt items

**Sign-Off**:
- ‚úÖ Architect (Winston): Extensibility patterns documented
- ‚è≥ PM (John): Phase 2 feature prioritization pending
- ‚è≥ Security: LDAP/OAuth design review pending (Phase 2)
