"""
Run a cat.experiments experiment and stream the results to Phoenix or resume an
existing Phoenix experiment using cat-experiments.

This example mirrors the workflow from the Phoenix documentation:
https://arize.com/docs/phoenix/datasets-and-experiments/how-to-experiments/run-experiments

Practical usage:

  python packages/cat-experiments/examples/phoenix_experiment_example.py
  python packages/cat-experiments/examples/phoenix_experiment_example.py --resume exp_123

The default mode fetches a dataset from Phoenix, runs the task/evaluators, and
streams results via PhoenixExperimentListener. The `--resume` flag fetches the
incomplete example/repetition pairs for `exp_123` and replays only those runs
locally before syncing the new results back to Phoenix.
"""

from __future__ import annotations

import argparse
import os
from datetime import datetime

import httpx
from phoenix.client import Client as PhoenixClient

from cat.experiments import (
    DatasetExample,
    EvaluationContext,
    EvaluationMetric,
    ExperimentConfig,
)
from cat.experiments.adapters.phoenix import PhoenixResumeCoordinator
from cat.experiments.runner_builders import build_phoenix_runner

SAMPLE_SUPPORT_EXAMPLES = [
    {
        "input": {
            "question": "How do I reset my password?",
            "conversation": [
                {"role": "user", "content": "I forgot my password and can't log in."},
                {"role": "assistant", "content": "I can help. Do you have access to your email?"},
            ],
        },
        "output": {"answer": "Visit the reset page and follow the emailed verification link."},
        "metadata": {"category": "authentication", "expected_tone": "empathetic"},
    },
    {
        "input": {
            "question": "Can I upgrade my plan mid-cycle?",
            "conversation": [
                {"role": "user", "content": "I'm on Basic and need Pro features today."},
            ],
        },
        "output": {
            "answer": "Yes. Upgrading mid-cycle prorates the difference on your next invoice."
        },
        "metadata": {"category": "billing", "expected_tone": "encouraging"},
    },
    {
        "input": {
            "question": "Does the desktop app work on Linux?",
            "conversation": [
                {"role": "user", "content": "Trying to install on Ubuntu."},
                {"role": "assistant", "content": "Let me check our supported OS list."},
            ],
        },
        "output": {
            "answer": "We support Windows and macOS; Linux is currently in beta via the browser."
        },
        "metadata": {"category": "product", "expected_tone": "informational"},
    },
]


def fetch_examples_from_phoenix(
    client: PhoenixClient,
    dataset_name: str,
) -> tuple[list[DatasetExample], str, str]:
    """
    Retrieve a Phoenix dataset and convert its examples into cat.experiments objects.

    Returns:
        examples: List of DatasetExample to feed into cat.experiments.
        dataset_id: Phoenix dataset identifier.
        dataset_version_id: Phoenix dataset version identifier.
    """
    try:
        dataset = client.datasets.get_dataset(dataset=dataset_name)
    except (httpx.HTTPStatusError, ValueError):
        print(f"Dataset '{dataset_name}' not found. Creating sample dataset.")
        try:
            dataset = client.datasets.create_dataset(
                name=dataset_name,
                examples=SAMPLE_SUPPORT_EXAMPLES,
                dataset_description=(
                    "Sample support-ticket Q&A dataset generated by cat.experiments example."
                ),
            )
        except (ValueError, httpx.HTTPStatusError):
            # dataset may have been created concurrently; fetch it now
            dataset = client.datasets.get_dataset(dataset=dataset_name)

    examples: list[DatasetExample] = []
    for example in dataset.examples:
        examples.append(
            DatasetExample(
                input=dict(example.get("input", {})),
                output=dict(example.get("output", {})),
                metadata=dict(example.get("metadata", {})),
                id=example.get("id"),
                created_at=_parse_datetime(example.get("created_at")),
                updated_at=_parse_datetime(example.get("updated_at")),
            )
        )

    return examples, dataset.id, dataset.version_id


def _parse_datetime(value: str | None) -> datetime | None:
    """Parse Phoenix timestamp strings into datetime objects for DatasetExample."""
    if not value:
        return None
    try:
        normalized = value.rstrip("Z")
        return datetime.fromisoformat(normalized if normalized else value)
    except ValueError:
        return None


def test_function(example: DatasetExample) -> dict[str, str]:
    """Toy task: echo the question while upper-casing it."""
    question = example.input.get("question", "")
    return {"answer": question.upper()}


def exact_match_evaluator(context: EvaluationContext) -> EvaluationMetric:
    """Simple evaluator that matches Phoenix quick-start behaviour."""
    expected = context.output.get("answer")
    actual = (
        context.actual_output.get("answer")
        if isinstance(context.actual_output, dict)
        else context.actual_output
    )
    score = 1.0 if expected == actual else 0.0
    label = "MATCH" if score == 1.0 else "MISMATCH"
    return EvaluationMetric(
        name="exact_match",
        score=score,
        metadata={"expected": expected, "actual": actual},
        label=label,
        explanation="Checks if predicted answer exactly matches the reference.",
    )


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Phoenix experiment demo with resume support.")
    parser.add_argument(
        "--resume",
        dest="resume_experiment_id",
        help="Phoenix experiment ID to resume. When provided, dataset creation is skipped.",
    )
    parser.add_argument(
        "--dataset-name",
        default=os.getenv("CAT_EVALS_DATASET", "support-ticket-demo"),
        help=(
            "Phoenix dataset name to run when not resuming. Defaults to env "
            "CAT_EVALS_DATASET or support-ticket-demo."
        ),
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    phoenix_client = PhoenixClient()

    runner = build_phoenix_runner(
        client=phoenix_client,
        project_name=os.getenv("CAT_EVALS_PROJECT"),
    )

    if args.resume_experiment_id:
        coordinator = PhoenixResumeCoordinator(phoenix_client)
        summary = coordinator.resume_task_runs(
            experiment_id=args.resume_experiment_id,
            task=test_function,
            evaluators=[exact_match_evaluator],
            runner=runner,
        )
        if summary:
            print("Resumed Phoenix experiment:", args.resume_experiment_id)
            print(f"  Incomplete runs processed: {summary.total_examples}")
            print(f"  Successfully completed: {summary.successful_examples}")
            print(f"  Average scores: {summary.average_scores}")
        else:
            print(f"Phoenix experiment {args.resume_experiment_id} is already complete.")
        return

    examples, dataset_id, dataset_version_id = fetch_examples_from_phoenix(
        phoenix_client, args.dataset_name
    )

    config = ExperimentConfig(
        name="cat-experiments phoenix quick-start",
        description="Reproduces the Phoenix experiment tutorial with cat.experiments.",
        dataset_id=dataset_id,
        dataset_version_id=dataset_version_id,
        metadata={"source": "cat-experiments-example", "tutorial": "phoenix-quickstart"},
    )

    summary = runner.run(
        dataset=examples,
        task=test_function,
        evaluators=[exact_match_evaluator],
        config=config,
    )

    print("Experiment complete")
    print(f"  Total examples: {summary.total_examples}")
    print(f"  Successful examples: {summary.successful_examples}")
    print(f"  Average scores: {summary.average_scores}")


if __name__ == "__main__":
    main()
