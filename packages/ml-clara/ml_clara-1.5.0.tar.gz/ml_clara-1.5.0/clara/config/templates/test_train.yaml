# ML-Clara quick test training config template
# Usage: clara train --config test_train.yaml

output_dir: outputs/test-lora
run_name: test-gpt2-lora

model_path: gpt2
dataset_path: data/test_train.jsonl
val_split: 0.2

num_epochs: 1
batch_size: 2
gradient_accumulation_steps: 1
learning_rate: 1e-4
max_length: 128

optimizer: adamw
lr_scheduler: linear
warmup_ratio: 0.1

gradient_checkpointing: false
bf16: false
fp16: false

save_steps: 100
save_total_limit: 1

eval_steps: 50
eval_strategy: steps

logging_steps: 5
report_to: []

lora:
  r: 4
  lora_alpha: 8
  lora_dropout: 0.0
  target_modules:
    - c_attn
  bias: none
  use_qlora: false

