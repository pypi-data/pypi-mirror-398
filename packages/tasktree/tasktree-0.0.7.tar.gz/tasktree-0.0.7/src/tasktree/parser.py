"""Parse recipe YAML files and handle imports."""

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import yaml


class CircularImportError(Exception):
    """Raised when a circular import is detected."""
    pass


@dataclass
class Environment:
    """Represents an execution environment configuration.

    Can be either a shell environment or a Docker environment:
    - Shell environment: has 'shell' field, executes directly on host
    - Docker environment: has 'dockerfile' field, executes in container
    """

    name: str
    shell: str = ""  # Path to shell (required for shell envs, optional for Docker)
    args: list[str] = field(default_factory=list)
    preamble: str = ""
    # Docker-specific fields (presence of dockerfile indicates Docker environment)
    dockerfile: str = ""  # Path to Dockerfile
    context: str = ""  # Path to build context directory
    volumes: list[str] = field(default_factory=list)  # Volume mounts
    ports: list[str] = field(default_factory=list)  # Port mappings
    env_vars: dict[str, str] = field(default_factory=dict)  # Environment variables
    working_dir: str = ""  # Working directory (container or host)

    def __post_init__(self):
        """Ensure args is always a list."""
        if isinstance(self.args, str):
            self.args = [self.args]


@dataclass
class Task:
    """Represents a task definition."""

    name: str
    cmd: str
    desc: str = ""
    deps: list[str] = field(default_factory=list)
    inputs: list[str] = field(default_factory=list)
    outputs: list[str] = field(default_factory=list)
    working_dir: str = ""
    args: list[str] = field(default_factory=list)
    source_file: str = ""  # Track which file defined this task
    env: str = ""  # Environment name to use for execution

    def __post_init__(self):
        """Ensure lists are always lists."""
        if isinstance(self.deps, str):
            self.deps = [self.deps]
        if isinstance(self.inputs, str):
            self.inputs = [self.inputs]
        if isinstance(self.outputs, str):
            self.outputs = [self.outputs]
        if isinstance(self.args, str):
            self.args = [self.args]


@dataclass
class Recipe:
    """Represents a parsed recipe file with all tasks."""

    tasks: dict[str, Task]
    project_root: Path
    environments: dict[str, Environment] = field(default_factory=dict)
    default_env: str = ""  # Name of default environment
    global_env_override: str = ""  # Global environment override (set via CLI --env)

    def get_task(self, name: str) -> Task | None:
        """Get task by name.

        Args:
            name: Task name (may be namespaced like 'build.compile')

        Returns:
            Task if found, None otherwise
        """
        return self.tasks.get(name)

    def task_names(self) -> list[str]:
        """Get all task names."""
        return list(self.tasks.keys())

    def get_environment(self, name: str) -> Environment | None:
        """Get environment by name.

        Args:
            name: Environment name

        Returns:
            Environment if found, None otherwise
        """
        return self.environments.get(name)


def find_recipe_file(start_dir: Path | None = None) -> Path | None:
    """Find recipe file in current or parent directories.

    Looks for recipe files matching these patterns (in order of preference):
    - tasktree.yaml
    - tasktree.yml
    - tt.yaml
    - *.tasks

    If multiple recipe files are found in the same directory, raises ValueError
    with instructions to use --tasks option.

    Args:
        start_dir: Directory to start searching from (defaults to cwd)

    Returns:
        Path to recipe file if found, None otherwise

    Raises:
        ValueError: If multiple recipe files found in the same directory
    """
    if start_dir is None:
        start_dir = Path.cwd()

    current = start_dir.resolve()

    # Search up the directory tree
    while True:
        candidates = []

        # Check for exact filenames first
        for filename in ["tasktree.yaml", "tasktree.yml", "tt.yaml"]:
            recipe_path = current / filename
            if recipe_path.exists():
                candidates.append(recipe_path)

        # Check for *.tasks files
        for tasks_file in current.glob("*.tasks"):
            if tasks_file.is_file():
                candidates.append(tasks_file)

        if len(candidates) > 1:
            # Multiple recipe files found - ambiguous
            filenames = [c.name for c in candidates]
            raise ValueError(
                f"Multiple recipe files found in {current}:\n"
                f"  {', '.join(filenames)}\n\n"
                f"Please specify which file to use with --tasks (-T):\n"
                f"  tt --tasks {filenames[0]} <task-name>"
            )
        elif len(candidates) == 1:
            return candidates[0]

        # Move to parent directory
        parent = current.parent
        if parent == current:
            # Reached root
            break
        current = parent

    return None


def _parse_file_with_env(
    file_path: Path,
    namespace: str | None,
    project_root: Path,
    import_stack: list[Path] | None = None,
) -> tuple[dict[str, Task], dict[str, Environment], str]:
    """Parse file and extract tasks and environments.

    Args:
        file_path: Path to YAML file
        namespace: Optional namespace prefix for tasks
        project_root: Root directory of the project
        import_stack: Stack of files being imported (for circular detection)

    Returns:
        Tuple of (tasks, environments, default_env_name)
    """
    # Parse tasks normally
    tasks = _parse_file(file_path, namespace, project_root, import_stack)

    # Load YAML again to extract environments (only from root file)
    environments: dict[str, Environment] = {}
    default_env = ""

    # Only parse environments from the root file (namespace is None)
    if namespace is None:
        with open(file_path, "r") as f:
            data = yaml.safe_load(f)

        if data and "environments" in data:
            env_data = data["environments"]
            if isinstance(env_data, dict):
                # Extract default environment name
                default_env = env_data.get("default", "")

                # Parse each environment definition
                for env_name, env_config in env_data.items():
                    if env_name == "default":
                        continue  # Skip the default key itself

                    if not isinstance(env_config, dict):
                        raise ValueError(
                            f"Environment '{env_name}' must be a dictionary"
                        )

                    # Parse common environment configuration
                    shell = env_config.get("shell", "")
                    args = env_config.get("args", [])
                    preamble = env_config.get("preamble", "")
                    working_dir = env_config.get("working_dir", "")

                    # Parse Docker-specific fields
                    dockerfile = env_config.get("dockerfile", "")
                    context = env_config.get("context", "")
                    volumes = env_config.get("volumes", [])
                    ports = env_config.get("ports", [])
                    env_vars = env_config.get("env_vars", {})

                    # Validate environment type
                    if not shell and not dockerfile:
                        raise ValueError(
                            f"Environment '{env_name}' must specify either 'shell' "
                            f"(for shell environments) or 'dockerfile' (for Docker environments)"
                        )

                    # Validate Docker environment requirements
                    if dockerfile and not context:
                        raise ValueError(
                            f"Docker environment '{env_name}' must specify 'context' "
                            f"when 'dockerfile' is specified"
                        )

                    # Validate that Dockerfile exists if specified
                    if dockerfile:
                        dockerfile_path = project_root / dockerfile
                        if not dockerfile_path.exists():
                            raise ValueError(
                                f"Environment '{env_name}': Dockerfile not found at {dockerfile_path}"
                            )

                    # Validate that context directory exists if specified
                    if context:
                        context_path = project_root / context
                        if not context_path.exists():
                            raise ValueError(
                                f"Environment '{env_name}': context directory not found at {context_path}"
                            )
                        if not context_path.is_dir():
                            raise ValueError(
                                f"Environment '{env_name}': context must be a directory, got {context_path}"
                            )

                    # Validate environment name (must be valid Docker tag)
                    if not env_name.replace("-", "").replace("_", "").isalnum():
                        raise ValueError(
                            f"Environment name '{env_name}' must be alphanumeric "
                            f"(with optional hyphens and underscores)"
                        )

                    environments[env_name] = Environment(
                        name=env_name,
                        shell=shell,
                        args=args,
                        preamble=preamble,
                        dockerfile=dockerfile,
                        context=context,
                        volumes=volumes,
                        ports=ports,
                        env_vars=env_vars,
                        working_dir=working_dir,
                    )

    return tasks, environments, default_env


def parse_recipe(recipe_path: Path, project_root: Path | None = None) -> Recipe:
    """Parse a recipe file and handle imports recursively.

    Args:
        recipe_path: Path to the main recipe file
        project_root: Optional project root directory. If not provided, uses recipe file's parent directory.
                     When using --tasks option, this should be the current working directory.

    Returns:
        Recipe object with all tasks (including recursively imported tasks)

    Raises:
        FileNotFoundError: If recipe file doesn't exist
        CircularImportError: If circular imports are detected
        yaml.YAMLError: If YAML is invalid
        ValueError: If recipe structure is invalid
    """
    if not recipe_path.exists():
        raise FileNotFoundError(f"Recipe file not found: {recipe_path}")

    # Default project root to recipe file's parent if not specified
    if project_root is None:
        project_root = recipe_path.parent

    # Parse main file - it will recursively handle all imports
    tasks, environments, default_env = _parse_file_with_env(
        recipe_path, namespace=None, project_root=project_root
    )

    return Recipe(
        tasks=tasks,
        project_root=project_root,
        environments=environments,
        default_env=default_env,
    )


def _parse_file(
    file_path: Path,
    namespace: str | None,
    project_root: Path,
    import_stack: list[Path] | None = None,
) -> dict[str, Task]:
    """Parse a single YAML file and return tasks, recursively processing imports.

    Args:
        file_path: Path to YAML file
        namespace: Optional namespace prefix for tasks
        project_root: Root directory of the project
        import_stack: Stack of files being imported (for circular detection)

    Returns:
        Dictionary of task name to Task objects

    Raises:
        CircularImportError: If a circular import is detected
        FileNotFoundError: If an imported file doesn't exist
        ValueError: If task structure is invalid
    """
    # Initialize import stack if not provided
    if import_stack is None:
        import_stack = []

    # Detect circular imports
    if file_path in import_stack:
        chain = " â†’ ".join(str(f.name) for f in import_stack + [file_path])
        raise CircularImportError(f"Circular import detected: {chain}")

    # Add current file to stack
    import_stack.append(file_path)

    # Load YAML
    with open(file_path, "r") as f:
        data = yaml.safe_load(f)

    if data is None:
        data = {}

    tasks: dict[str, Task] = {}
    file_dir = file_path.parent

    # Default working directory is the project root (where tt is invoked)
    # NOT the directory where the tasks file is located
    default_working_dir = "."

    # Track local import namespaces for dependency rewriting
    local_import_namespaces: set[str] = set()

    # Process nested imports FIRST
    imports = data.get("imports", [])
    if imports:
        for import_spec in imports:
            child_file = import_spec["file"]
            child_namespace = import_spec["as"]

            # Track this namespace as a local import
            local_import_namespaces.add(child_namespace)

            # Build full namespace chain
            full_namespace = f"{namespace}.{child_namespace}" if namespace else child_namespace

            # Resolve import path relative to current file's directory
            child_path = file_path.parent / child_file
            if not child_path.exists():
                raise FileNotFoundError(f"Import file not found: {child_path}")

            # Recursively process with namespace chain and import stack
            nested_tasks = _parse_file(
                child_path,
                full_namespace,
                project_root,
                import_stack.copy(),  # Pass copy to avoid shared mutation
            )

            tasks.update(nested_tasks)

    # Validate top-level keys (only imports, environments, and tasks are allowed)
    VALID_TOP_LEVEL_KEYS = {"imports", "environments", "tasks"}

    # Check if tasks key is missing when there appear to be task definitions at root
    # Do this BEFORE checking for unknown keys, to provide better error message
    if "tasks" not in data and data:
        # Check if there are potential task definitions at root level
        potential_tasks = [
            k for k, v in data.items()
            if isinstance(v, dict) and k not in VALID_TOP_LEVEL_KEYS
        ]

        if potential_tasks:
            raise ValueError(
                f"Invalid recipe format in {file_path}\n\n"
                f"Task definitions must be under a top-level 'tasks:' key.\n\n"
                f"Found these keys at root level: {', '.join(potential_tasks)}\n\n"
                f"Did you mean:\n\n"
                f"tasks:\n"
                + '\n'.join(f"  {k}:" for k in potential_tasks) +
                "\n    cmd: ...\n\n"
                f"Valid top-level keys are: {', '.join(sorted(VALID_TOP_LEVEL_KEYS))}"
            )

    # Now check for other invalid top-level keys (non-dict values)
    invalid_keys = set(data.keys()) - VALID_TOP_LEVEL_KEYS
    if invalid_keys:
        raise ValueError(
            f"Invalid recipe format in {file_path}\n\n"
            f"Unknown top-level keys: {', '.join(sorted(invalid_keys))}\n\n"
            f"Valid top-level keys are:\n"
            f"  - imports      (for importing task files)\n"
            f"  - environments (for shell environment configuration)\n"
            f"  - tasks        (for task definitions)"
        )

    # Extract tasks from "tasks" key
    tasks_data = data.get("tasks", {})
    if tasks_data is None:
        tasks_data = {}

    # Process local tasks
    for task_name, task_data in tasks_data.items():

        if not isinstance(task_data, dict):
            raise ValueError(f"Task '{task_name}' must be a dictionary")

        if "cmd" not in task_data:
            raise ValueError(f"Task '{task_name}' missing required 'cmd' field")

        # Apply namespace if provided
        full_name = f"{namespace}.{task_name}" if namespace else task_name

        # Set working directory
        working_dir = task_data.get("working_dir", default_working_dir)

        # Rewrite dependencies with namespace
        deps = task_data.get("deps", [])
        if isinstance(deps, str):
            deps = [deps]
        if namespace:
            # Rewrite dependencies: only prefix if it's a local reference
            # A dependency is local if:
            # 1. It has no dots (simple name like "init")
            # 2. It starts with a local import namespace (like "base.setup" when "base" is imported)
            rewritten_deps = []
            for dep in deps:
                if "." not in dep:
                    # Simple name - always prefix
                    rewritten_deps.append(f"{namespace}.{dep}")
                else:
                    # Check if it starts with a local import namespace
                    dep_root = dep.split(".", 1)[0]
                    if dep_root in local_import_namespaces:
                        # Local import reference - prefix it
                        rewritten_deps.append(f"{namespace}.{dep}")
                    else:
                        # External reference - keep as-is
                        rewritten_deps.append(dep)
            deps = rewritten_deps

        task = Task(
            name=full_name,
            cmd=task_data["cmd"],
            desc=task_data.get("desc", ""),
            deps=deps,
            inputs=task_data.get("inputs", []),
            outputs=task_data.get("outputs", []),
            working_dir=working_dir,
            args=task_data.get("args", []),
            source_file=str(file_path),
            env=task_data.get("env", ""),
        )

        tasks[full_name] = task

    # Remove current file from stack
    import_stack.pop()

    return tasks


def parse_arg_spec(arg_spec: str) -> tuple[str, str, str | None]:
    """Parse argument specification.

    Format: name:type=default
    - name is required
    - type is optional (defaults to 'str')
    - default is optional

    Args:
        arg_spec: Argument specification string

    Returns:
        Tuple of (name, type, default)

    Examples:
        >>> parse_arg_spec("environment")
        ('environment', 'str', None)
        >>> parse_arg_spec("region=eu-west-1")
        ('region', 'str', 'eu-west-1')
        >>> parse_arg_spec("port:int=8080")
        ('port', 'int', '8080')
    """
    # Split on = to separate name:type from default
    if "=" in arg_spec:
        name_type, default = arg_spec.split("=", 1)
    else:
        name_type = arg_spec
        default = None

    # Split on : to separate name from type
    if ":" in name_type:
        name, arg_type = name_type.split(":", 1)
    else:
        name = name_type
        arg_type = "str"

    return name, arg_type, default
