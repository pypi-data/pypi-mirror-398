Metadata-Version: 2.4
Name: llm-kit-pro
Version: 0.2.3
Summary: A unified toolkit for working with multiple LLM providers
License: MIT
License-File: LICENSE
Keywords: llm,ai,openai,gemini,anthropic,language-models,async
Author: Preetam Raj
Requires-Python: >=3.11,<4.0
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Provides-Extra: bedrock
Provides-Extra: gemini
Requires-Dist: boto3 (>=1.42.14,<2.0.0) ; extra == "bedrock"
Requires-Dist: google-genai (>=1.0.0,<2.0.0) ; extra == "gemini"
Requires-Dist: httpx (>=0.28.1)
Requires-Dist: pydantic (>=2.12.5,<3.0.0)
Requires-Dist: pydantic-settings (>=2.12.0,<3.0.0)
Description-Content-Type: text/markdown

# llm-kit-pro

**llm-kit-pro** is a unified, async-first Python toolkit for interacting with multiple
Large Language Model (LLM) providers through a consistent, provider-agnostic API.

It supports:

- Multiple LLM providers (OpenAI, Gemini, Anthropic, etc.)
- Text and structured JSON generation
- Multimodal inputs (PDFs, images, text files)
- Clean abstractions and strong typing

---

## âœ¨ Core Ideas

### 1. Outputs are explicit

LLMs generate **outputs**, not OCR results.

`llm-kit-pro` exposes two primary operations:

- `generate_text` â†’ free-form text
- `generate_json` â†’ structured, schema-driven output

### 2. Inputs can be text **and/or files**

Files (PDFs, images, etc.) are **first-class inputs** and can be passed directly
to generation methods.

OCR and file parsing are treated as **provider implementation details**.

---

## ðŸ“¦ Installation

```bash
pip install llm-kit-pro
```

Optional provider support (example):

```bash
pip install llm-kit-pro[openai]
pip install llm-kit-pro[gemini]
```

---

## ðŸš€ Quick Start

### Text-only generation

```python
text = await llm.generate_text(
    prompt="Explain what power factor is in simple terms"
)
```

---

### Text generation with a file (PDF, image, etc.)

```python
from llm_kit_pro.core import LLMFile

pdf = LLMFile(
    content=pdf_bytes,
    mime_type="application/pdf",
    filename="bill.pdf",
)

summary = await llm.generate_text(
    prompt="Summarize this electricity bill",
    files=[pdf],
)
```

---

### Structured JSON extraction using Pydantic models

```python
from pydantic import BaseModel
from typing import Optional

class BillDetails(BaseModel):
    consumer_name: str
    bill_amount: float
    due_date: Optional[str] = None

data = await llm.generate_json(
    prompt="Extract billing details from this document",
    schema=BillDetails.model_json_schema(),
    files=[pdf],
)
```

---

## ðŸ§  Design Philosophy

- **Provider-agnostic**: No OpenAI/Gemini specifics in the public API
- **Async-first**: Built for modern Python backends
- **Composable**: Easy to plug into pipelines (FastAPI, background jobs, ETL)
- **Explicit contracts**: Clear separation of inputs, outputs, and providers

---

## ðŸ§© Core Abstractions

### `BaseLLMClient`

All providers implement the same interface:

```python
class BaseLLMClient:
    async def generate_text(...)
    async def generate_json(...)
```

### `LLMFile`

A provider-agnostic representation of file inputs:

```python
from llm_kit_pro.core import LLMFile
```

---

## ðŸ”Œ Providers

Each provider is implemented as an adapter that conforms to `BaseLLMClient`.

Supported / planned:

- OpenAI
- Gemini
- Anthropic
- Bedrock
- Local / OSS models (future)

---

## ðŸ“ Status

ðŸš§ **Under active development**

The public API is stabilizing. Expect rapid iteration until `v1.0`.

---

## ðŸ“„ License

MIT License

