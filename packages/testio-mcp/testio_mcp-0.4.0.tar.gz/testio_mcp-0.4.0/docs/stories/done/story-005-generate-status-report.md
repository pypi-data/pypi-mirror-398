---
story_id: STORY-005
epic_id: EPIC-001
title: Tool 4 - Generate Status Report
status: todo
created: 2025-11-04
estimate: 6 hours
assignee: unassigned
dependencies: [STORY-002]
---

# STORY-005: Tool 4 - Generate Status Report

## User Story

**As a** Customer Success Manager
**I want** to generate executive summary reports for one or more tests in multiple formats (markdown, text, json)
**So that** I can quickly share status updates in stakeholder meetings, email communications, or Slack channels without manual report creation

## Context

This tool aggregates data from multiple tests into a synthesized executive summary. It's designed for stakeholder communication and should produce clear, actionable reports that highlight key metrics, critical issues, and overall progress.

**Use Case**: "Generate a status report for stakeholder meeting"
**Input**: Array of test IDs, output format preference
**Output**: Formatted report with test overview table, key metrics, critical issues, progress summary

## Implementation Approach

**Architecture Note (ADR-006):** This story follows the service layer pattern established in Story-002.

1. **Create ReportService** (business logic, framework-agnostic)
   - Report generation logic for markdown, text, and JSON formats
   - Cross-service dependency: Uses `TestService` from Story-002 to fetch test data
   - Aggregate metrics calculation
   - Critical issues identification
   - Cache integration optional (reports are typically real-time)

2. **Create MCP Tool** (thin wrapper, delegates to service)
   - Extracts dependencies from Context: `client = ctx["testio_client"]`, `cache = ctx["cache"]`
   - Creates ReportService instance
   - Calls `service.generate_report()`
   - Converts service exceptions to MCP-friendly error format (âŒâ„¹ï¸ðŸ’¡ pattern)

3. **Service-to-Service Communication**
   - ReportService calls `TestService.get_test_status()` directly (not the MCP tool)
   - This allows ReportService to work in any context, not just MCP

4. **Error Handling (Two-Layer Pattern)**
   - Service Layer: Raises domain exceptions (`TestNotFoundException`, `TestIOAPIError`)
   - Tool Layer: Catches exceptions, converts to user-friendly error dictionaries

### âš ï¸ CRITICAL: Exception Contract (Lessons from Story-004 QA)

**Before writing ANY exception handling code, review the correct pattern:**

**âŒ COMMON PITFALL** (causes user-facing error messages to break):
```python
# WRONG: Services should NEVER catch httpx exceptions directly
try:
    data = await self.client.get(...)
except httpx.HTTPStatusError as e:  # BREAKS CONTRACT!
    if e.response.status_code == 404:
        raise TestNotFoundException(...)
```

**âœ… CORRECT PATTERN** (matches TestIOClient contract):
```python
# RIGHT: Catch TestIOAPIError from client, translate to domain exceptions
try:
    data = await self.client.get(...)
except TestIOAPIError as e:  # Client ALWAYS raises this
    if e.status_code == 404:
        raise TestNotFoundException(...) from e
    raise  # Re-raise other errors for tool layer
```

**Why This Matters**:
- TestIOClient (Story-001) ALWAYS wraps HTTP errors in `TestIOAPIError`
- Services must catch `TestIOAPIError`, NOT `httpx.HTTPStatusError`
- If you catch the wrong exception type, 404 translation never happens
- Tools then see generic API errors instead of friendly "not found" messages

**Verification**:
- [ ] Service catches `TestIOAPIError` (not `httpx.HTTPStatusError`)
- [ ] Unit tests mock `TestIOAPIError` (not `httpx.HTTPStatusError`)
- [ ] Integration test verifies 404 raises `TestNotFoundException`
- [ ] Tool layer catches both domain exceptions AND `TestIOAPIError`

**Reference Implementations**:
- âœ… BugService (Story-004, lines 184-189): Correct pattern after fix
- âœ… TestService (Story-002, lines 96-101): Correct pattern
- âŒ BugService (Story-004, original): Wrong pattern (caught httpx directly)

## Acceptance Criteria

### AC0: Service Layer Implementation (ADR-006)

**Goal**: Create `ReportService` to encapsulate report generation logic and aggregate test data.

**Implementation Requirements**:
- [ ] Create `src/testio_mcp/services/report_service.py`
- [ ] `ReportService` class with constructor accepting `client: TestIOClient` and `cache: InMemoryCache`
- [ ] Public method: `async def generate_report(test_ids, format) -> str`
- [ ] Private methods:
  - `_fetch_test_data(test_ids) -> Tuple[List[dict], List[dict]]` - Fetch all test data concurrently
  - `_generate_markdown_report(tests, failed_tests) -> str` - Markdown formatter
  - `_generate_text_report(tests, failed_tests) -> str` - Text formatter
  - `_generate_json_report(tests, failed_tests) -> str` - JSON formatter
- [ ] Service handles:
  - Creating TestService instances
  - Fetching test data concurrently with `asyncio.gather()` and timeout (30 seconds)
  - Separating successful results from failures
  - Report formatting based on requested format
  - Raises `ValueError` if all tests fail or test_ids empty

**Complete ReportService Implementation Example**:

```python
# src/testio_mcp/services/report_service.py
import asyncio
import json
from typing import List, Tuple, Literal
from datetime import datetime
from testio_mcp.api.client import TestIOClient
from testio_mcp.cache import InMemoryCache
from testio_mcp.services.test_service import TestService
from testio_mcp.exceptions import TestNotFoundException, TestIOAPIError

class ReportService:
    """
    Service for generating status reports from multiple tests.

    Demonstrates cross-service communication by using TestService
    to fetch individual test data.
    """

    def __init__(self, client: TestIOClient, cache: InMemoryCache):
        self.client = client
        self.cache = cache

    async def generate_report(
        self,
        test_ids: List[str],
        format: Literal["markdown", "text", "json"] = "markdown"
    ) -> str:
        """
        Generate executive summary report for stakeholder communication.

        Args:
            test_ids: List of test IDs to include (1-20 tests)
            format: Output format - markdown, text, or json

        Returns:
            Formatted status report as string

        Raises:
            ValueError: If test_ids is empty or all tests fail to load
        """
        if not test_ids:
            raise ValueError("test_ids cannot be empty")

        # Fetch all test data concurrently
        successful_tests, failed_tests = await self._fetch_test_data(test_ids)

        # Check if all tests failed
        if not successful_tests:
            raise ValueError(
                f"Failed to generate report - all {len(failed_tests)} tests failed to load"
            )

        # Generate report in requested format
        if format == "markdown":
            return self._generate_markdown_report(successful_tests, failed_tests)
        elif format == "text":
            return self._generate_text_report(successful_tests, failed_tests)
        else:  # json
            return self._generate_json_report(successful_tests, failed_tests)

    async def _fetch_test_data(
        self, test_ids: List[str]
    ) -> Tuple[List[dict], List[dict]]:
        """
        Fetch test data concurrently for all test IDs.

        Uses TestService for each test (cross-service communication).

        Args:
            test_ids: List of test IDs

        Returns:
            Tuple of (successful_tests, failed_tests)
        """
        # Create test service instance for fetching
        test_service = TestService(client=self.client, cache=self.cache)

        # Fetch all test data concurrently with timeout
        try:
            results = await asyncio.wait_for(
                asyncio.gather(
                    *[test_service.get_test_status(test_id) for test_id in test_ids],
                    return_exceptions=True
                ),
                timeout=30.0  # 30 second total timeout
            )
        except asyncio.TimeoutError:
            raise ValueError(
                f"Report generation timed out after 30 seconds. "
                f"Requested {len(test_ids)} tests - too many for single report. "
                f"Reduce number of tests (max recommended: 10)."
            )

        # Separate successful results from errors
        successful_tests = []
        failed_tests = []
        for idx, result in enumerate(results):
            if isinstance(result, Exception):
                failed_tests.append({
                    "test_id": test_ids[idx],
                    "error": str(result)
                })
            else:
                successful_tests.append(result)

        return successful_tests, failed_tests

    def _generate_markdown_report(
        self, tests: List[dict], failed_tests: List[dict]
    ) -> str:
        """Generate markdown formatted report."""
        # Implementation from AC4...
        pass

    def _generate_text_report(
        self, tests: List[dict], failed_tests: List[dict]
    ) -> str:
        """Generate plain text formatted report."""
        # Implementation from AC5...
        pass

    def _generate_json_report(
        self, tests: List[dict], failed_tests: List[dict]
    ) -> str:
        """Generate JSON formatted report."""
        # Implementation from AC6...
        pass
```

**Why ReportService Exists**:
- Encapsulates report generation logic separate from transport layer
- Demonstrates service-to-service communication (ReportService â†’ TestService)
- Makes report formatting testable in isolation
- Allows report generation from any context (CLI, web API, MCP, etc.)

## Acceptance Criteria

### AC1: Tool as Thin Wrapper (ADR-006)

**Goal**: MCP tool delegates to ReportService, handling Context injection and error formatting.

- [ ] `@mcp.tool()` decorator applied to `generate_status_report` function
- [ ] Function signature includes `ctx: Context` parameter for dependency injection (ADR-001)
- [ ] Tool implementation:
  1. Extracts dependencies from Context (`testio_client`, `cache`)
  2. Creates `ReportService` instance
  3. Calls `service.generate_report()` and returns result
  4. Catches service exceptions and converts to MCP error format (âŒâ„¹ï¸ðŸ’¡ pattern)
- [ ] Clear docstring explaining use case and format options
- [ ] Example:
  ```python
  from typing import List, Literal
  from fastmcp import Context
  from testio_mcp.services.report_service import ReportService

  @mcp.tool()
  async def generate_status_report(
      test_ids: List[str],
      format: Literal["markdown", "text", "json"] = "markdown",
      ctx: Context = None  # NEW: Context injection (ADR-001)
  ) -> str:
      """
      Generate executive summary report for stakeholder communication.

      Aggregates data from multiple tests into a formatted report with test
      overview table, key metrics, critical issues requiring attention, and
      overall progress summary. Suitable for email, Slack, or presentations.

      Args:
          test_ids: List of test IDs to include (e.g., ["109363", "109364"])
          format: Output format - markdown (default), text, or json
          ctx: FastMCP context with injected dependencies

      Returns:
          Formatted status report as string

      Raises:
          ValueError: If test_ids is empty or all tests fail to load
      """
      # Extract dependencies from Context (ADR-001)
      client = ctx["testio_client"]
      cache = ctx["cache"]

      # Create service
      service = ReportService(client=client, cache=cache)

      # Delegate to service (business logic)
      try:
          return await service.generate_report(
              test_ids=test_ids,
              format=format
          )
      except ValueError as e:
          # Convert to MCP error format
          return {
              "error": f"âŒ {str(e)}",
              "context": "â„¹ï¸ Failed to generate status report",
              "hint": "ðŸ’¡ Verify test IDs using list_active_tests tool"
          }
  ```

### AC2: Pydantic Input Validation
- [ ] Input model validates test_ids list and format enum
- [ ] Example:
  ```python
  from pydantic import BaseModel, Field, field_validator
  from typing import List, Literal

  class GenerateStatusReportInput(BaseModel):
      test_ids: List[str] = Field(
          ...,
          description="List of test IDs to include in report",
          min_length=1,
          max_length=20,
          example=["109363", "109364"]
      )
      format: Literal["markdown", "text", "json"] = Field(
          default="markdown",
          description="Output format for the report"
      )

      @field_validator("test_ids")
      @classmethod
      def validate_test_ids(cls, v):
          if not v:
              raise ValueError("test_ids cannot be empty")
          if len(v) > 20:
              raise ValueError("Maximum 20 test IDs allowed per report")
          return v
  ```
- [ ] Empty test_ids list â†’ Validation error
- [ ] More than 20 test IDs â†’ Validation error with guidance

### AC3: Aggregate Data from TestService (In Service Layer)
- [ ] **SERVICE-TO-SERVICE**: ReportService uses `TestService.get_test_status()` directly (not the MCP tool)
- [ ] Fetches data for all test IDs concurrently using `asyncio.gather()`
- [ ] **ARCHITECTURE**: Semaphore limits already applied via TestIOClient (ADR-002)
- [ ] **ARCHITECTURE**: Add total timeout to prevent long-running reports (30 seconds max)
- [ ] **Reference**: [ADR-002: Concurrency Limits](../architecture/adrs/ADR-002-concurrency-limits.md)
- [ ] Example (in ReportService._fetch_test_data):
  ```python
  import asyncio
  from testio_mcp.services.test_service import TestService

  # Create test service instance for fetching
  test_service = TestService(client=self.client, cache=self.cache)

  # Fetch all test data concurrently with timeout (ADR-002)
  # Note: Individual requests already semaphore-controlled by TestIOClient
  try:
      results = await asyncio.wait_for(
          asyncio.gather(
              *[test_service.get_test_status(test_id) for test_id in test_ids],
              return_exceptions=True  # Don't fail if one test fails
          ),
          timeout=30.0  # 30 second total timeout for report generation
      )
  except asyncio.TimeoutError:
      raise ValueError(
          f"Report generation timed out after 30 seconds. "
          f"Requested {len(test_ids)} tests - too many for single report. "
          f"Reduce number of tests (max recommended: 10)."
      )

  # Separate successful results from errors
  successful_tests = []
  failed_tests = []
  for idx, result in enumerate(results):
      if isinstance(result, Exception):
          failed_tests.append({"test_id": test_ids[idx], "error": str(result)})
      else:
          successful_tests.append(result)

  return successful_tests, failed_tests
  ```
- [ ] Handles partial failures (some tests not found, continue with others)
- [ ] Returns partial results (Tuple of successful/failed tests)
- [ ] Total timeout: 30 seconds for entire report generation
- [ ] Individual requests protected by global semaphore (max 10 concurrent, ADR-002)

### AC4: Markdown Format Output
- [ ] Generates well-formatted markdown report
- [ ] Includes test overview table with key columns
- [ ] Includes key metrics summary
- [ ] Includes critical issues section
- [ ] Includes overall progress section
- [ ] Example output:
  ```markdown
  # Test Status Report
  Generated: 2025-11-04 14:30 UTC

  ## Test Overview

  | Test ID | Title | Status | Review | Bugs | Critical | High | Low | Visual | Content |
  |---------|-------|--------|--------|------|----------|------|-----|--------|---------|
  | 109363  | Evgeniya Testing | archived | âœ… Passed | 5 | 2 | 1 | 1 | 1 | 0 |
  | 109364  | Rapid Test Alpha | running | â³ Pending | 12 | 0 | 3 | 7 | 2 | 0 |

  ## Key Metrics
  - **Total Tests**: 2
  - **Total Bugs Found**: 17
  - **Critical Bugs**: 2
  - **High Severity Bugs**: 4
  - **Tests Passed Review**: 1/2 (50%)
  - **Bugs Accepted**: 8/17 (47%)
  - **Bugs Exported**: 3/17 (18%)

  ## Critical Issues Requiring Attention

  ### Test 109363 - Evgeniya Testing
  - âš ï¸ 2 critical bugs found, 1 not yet exported
  - âš ï¸ 1 high severity bug pending export

  ### Test 109364 - Rapid Test Alpha
  - â„¹ï¸ Test still running, 12 bugs found so far
  - â³ Awaiting review completion

  ## Overall Progress
  - **Tests Completed**: 1/2 (50%)
  - **Review Success Rate**: 100% (1/1 completed)
  - **Average Bugs per Test**: 8.5
  - **Bug Acceptance Rate**: 47% (8 accepted, 9 pending/rejected)

  ## Recommendations
  - Export 2 critical bugs from Test 109363 to issue tracker
  - Monitor Test 109364 as it completes (12 bugs pending review)
  - Review rejected bugs to identify patterns

  ---
  *Report generated by TestIO MCP Server*
  ```

### AC5: Text Format Output (Plain Text for Email)
- [ ] Plain text format without markdown syntax
- [ ] ASCII table borders for alignment
- [ ] Suitable for email body or Slack message
- [ ] Example:
  ```
  ========================================
  TEST STATUS REPORT
  ========================================
  Generated: 2025-11-04 14:30 UTC

  TEST OVERVIEW
  ----------------------------------------
  Test ID  | Title            | Status   | Review  | Bugs | Critical
  ---------|------------------|----------|---------|------|----------
  109363   | Evgeniya Testing | archived | Passed  | 5    | 2
  109364   | Rapid Test Alpha | running  | Pending | 12   | 0

  KEY METRICS
  ----------------------------------------
  Total Tests:              2
  Total Bugs Found:         17
  Critical Bugs:            2
  Tests Passed Review:      1/2 (50%)
  Bugs Accepted:            8/17 (47%)

  CRITICAL ISSUES
  ----------------------------------------
  [Test 109363]
  - 2 critical bugs found, 1 not exported
  - 1 high severity bug pending export

  [Test 109364]
  - Test still running, 12 bugs found
  - Awaiting review completion

  OVERALL PROGRESS
  ----------------------------------------
  Tests Completed:          1/2 (50%)
  Review Success Rate:      100%
  Average Bugs per Test:    8.5
  Bug Acceptance Rate:      47%

  RECOMMENDATIONS
  ----------------------------------------
  - Export 2 critical bugs from Test 109363
  - Monitor Test 109364 completion
  - Review rejected bugs for patterns

  ========================================
  Report generated by TestIO MCP Server
  ========================================
  ```

### AC6: JSON Format Output (Machine-Readable)
- [ ] Structured JSON with all metrics and data
- [ ] Suitable for programmatic consumption or dashboard integration
- [ ] Example:
  ```json
  {
    "report_type": "test_status_summary",
    "generated_at": "2025-11-04T14:30:00Z",
    "summary": {
      "total_tests": 2,
      "tests_completed": 1,
      "tests_in_progress": 1,
      "total_bugs": 17,
      "bugs_by_severity": {
        "critical": 2,
        "high": 4,
        "low": 8,
        "visual": 3,
        "content": 0
      },
      "bugs_by_status": {
        "accepted": 8,
        "rejected": 3,
        "new": 6
      },
      "review_pass_rate": 1.0,
      "bug_acceptance_rate": 0.47
    },
    "tests": [
      {
        "test_id": "109363",
        "title": "Evgeniya Testing",
        "status": "archived",
        "review_status": "review_successful",
        "bug_summary": {
          "total": 5,
          "critical": 2,
          "high": 1,
          "low": 1,
          "visual": 1,
          "content": 0
        },
        "critical_issues": [
          "2 critical bugs found, 1 not yet exported",
          "1 high severity bug pending export"
        ]
      },
      {
        "test_id": "109364",
        "title": "Rapid Test Alpha",
        "status": "running",
        "review_status": null,
        "bug_summary": {
          "total": 12,
          "critical": 0,
          "high": 3,
          "low": 7,
          "visual": 2,
          "content": 0
        },
        "critical_issues": [
          "Test still running, 12 bugs found so far",
          "Awaiting review completion"
        ]
      }
    ],
    "recommendations": [
      "Export 2 critical bugs from Test 109363 to issue tracker",
      "Monitor Test 109364 as it completes",
      "Review rejected bugs to identify patterns"
    ],
    "failed_tests": []
  }
  ```

### AC7: Report Generation Logic (In Service Layer)
- [ ] Implemented in `ReportService._generate_markdown_report()`, `_generate_text_report()`, `_generate_json_report()`
- [ ] Calculate aggregate metrics across all successful tests
- [ ] Identify critical issues (critical bugs, failed reviews, high bug counts)
- [ ] Generate actionable recommendations based on data
- [ ] Example implementation (in ReportService):
  ```python
  def _generate_markdown_report(tests: List[dict], failed_tests: List[dict]) -> str:
      """Generate markdown formatted report."""
      report = []
      now = datetime.utcnow().strftime("%Y-%m-%d %H:%M UTC")

      # Header
      report.append("# Test Status Report")
      report.append(f"Generated: {now}")
      report.append("")

      # Test overview table
      report.append("## Test Overview")
      report.append("")
      report.append("| Test ID | Title | Status | Review | Bugs | Critical | High | Low | Visual | Content |")
      report.append("|---------|-------|--------|--------|------|----------|------|-----|--------|---------|")

      for test in tests:
          bugs = test["bug_summary"]
          review_icon = "âœ… Passed" if test.get("review_status") == "review_successful" else "â³ Pending"
          report.append(
              f"| {test['test_id']} | {test['title']} | {test['status']} | {review_icon} | "
              f"{bugs['total_count']} | {bugs['by_severity'].get('critical', 0)} | "
              f"{bugs['by_severity'].get('high', 0)} | {bugs['by_severity'].get('low', 0)} | "
              f"{bugs['by_severity'].get('visual', 0)} | {bugs['by_severity'].get('content', 0)} |"
          )

      report.append("")

      # Key metrics
      total_bugs = sum(t["bug_summary"]["total_count"] for t in tests)
      critical_bugs = sum(t["bug_summary"]["by_severity"].get("critical", 0) for t in tests)
      high_bugs = sum(t["bug_summary"]["by_severity"].get("high", 0) for t in tests)
      passed_review = sum(1 for t in tests if t.get("review_status") == "review_successful")

      report.append("## Key Metrics")
      report.append(f"- **Total Tests**: {len(tests)}")
      report.append(f"- **Total Bugs Found**: {total_bugs}")
      report.append(f"- **Critical Bugs**: {critical_bugs}")
      report.append(f"- **High Severity Bugs**: {high_bugs}")
      report.append(f"- **Tests Passed Review**: {passed_review}/{len(tests)} ({passed_review/len(tests)*100:.0f}%)")
      report.append("")

      # Critical issues
      report.append("## Critical Issues Requiring Attention")
      report.append("")
      for test in tests:
          critical_count = test["bug_summary"]["by_severity"].get("critical", 0)
          if critical_count > 0 or test["status"] == "running":
              report.append(f"### Test {test['test_id']} - {test['title']}")
              if critical_count > 0:
                  report.append(f"- âš ï¸ {critical_count} critical bugs found")
              if test["status"] == "running":
                  report.append("- â„¹ï¸ Test still running")
              report.append("")

      # Overall progress
      completed = sum(1 for t in tests if t["status"] in ["archived", "completed"])
      report.append("## Overall Progress")
      report.append(f"- **Tests Completed**: {completed}/{len(tests)} ({completed/len(tests)*100:.0f}%)")
      report.append(f"- **Average Bugs per Test**: {total_bugs/len(tests):.1f}")
      report.append("")

      # Warnings about failed tests
      if failed_tests:
          report.append("## âš ï¸ Warnings")
          for failed in failed_tests:
              report.append(f"- Test {failed['test_id']}: {failed['error']}")
          report.append("")

      report.append("---")
      report.append("*Report generated by TestIO MCP Server*")

      return "\n".join(report)
  ```

### AC8: Error Handling (Two-Layer Pattern)

**Service Layer** (ReportService):
- [ ] Raises `ValueError` when test_ids is empty
- [ ] Raises `ValueError` when all test IDs fail to load
- [ ] Raises `ValueError` when report generation times out
- [ ] Example:
  ```python
  # In ReportService.generate_report()
  if not test_ids:
      raise ValueError("test_ids cannot be empty")

  # After fetching data
  if not successful_tests and failed_tests:
      raise ValueError(
          f"Failed to generate report - all {len(failed_tests)} tests failed to load"
      )

  # In _fetch_test_data()
  try:
      results = await asyncio.wait_for(...)
  except asyncio.TimeoutError:
      raise ValueError(
          f"Report generation timed out after 30 seconds. "
          f"Requested {len(test_ids)} tests - too many for single report."
      )
  ```

**Tool Layer** (generate_status_report):
- [ ] Catches `ValueError` and converts to MCP error format
- [ ] Example:
  ```python
  # In @mcp.tool() generate_status_report function
  try:
      return await service.generate_report(...)
  except ValueError as e:
      return {
          "error": f"âŒ {str(e)}",
          "context": "â„¹ï¸ Failed to generate status report",
          "hint": "ðŸ’¡ Verify test IDs using list_active_tests tool"
      }
  ```

**Both Layers**:
- [ ] Some test IDs not found â†’ Include partial results with warning section in report
- [ ] Invalid test_ids list â†’ Pydantic validation error with examples

### AC9: Integration Test with Real Data
- [ ] Test with 2+ real test IDs from Product 25073
- [ ] Verify markdown output is well-formatted
- [ ] Verify text output has proper ASCII alignment
- [ ] Verify JSON output is valid JSON
- [ ] Test partial failure scenario (1 valid, 1 invalid test ID)
- [ ] Test code:
  ```python
  @pytest.mark.integration
  @pytest.mark.asyncio
  async def test_generate_status_report_markdown():
      """Test markdown report generation with real data."""
      result = await generate_status_report(
          test_ids=["109363"],  # Known test from Product 25073
          format="markdown"
      )
      assert "# Test Status Report" in result
      assert "## Test Overview" in result
      assert "109363" in result
      assert "Evgeniya Testing" in result

  @pytest.mark.integration
  @pytest.mark.asyncio
  async def test_generate_status_report_json():
      """Test JSON report generation."""
      result = await generate_status_report(
          test_ids=["109363"],
          format="json"
      )
      data = json.loads(result)
      assert data["summary"]["total_tests"] == 1
      assert len(data["tests"]) == 1
      assert data["tests"][0]["test_id"] == "109363"

  @pytest.mark.integration
  @pytest.mark.asyncio
  async def test_generate_status_report_partial_failure():
      """Test handling of partial failures."""
      result = await generate_status_report(
          test_ids=["109363", "invalid_id"],
          format="markdown"
      )
      assert "âš ï¸ Warnings" in result
      assert "invalid_id" in result
      # Should still show data for successful test
      assert "109363" in result
  ```

## Technical Implementation

### Complete Implementation Example

```python
# src/testio_mcp/tools/status_report.py
import asyncio
import json
from typing import List, Literal
from datetime import datetime
from pydantic import BaseModel, Field, field_validator
from testio_mcp.tools.test_status import get_test_status
from testio_mcp.server import mcp

class GenerateStatusReportInput(BaseModel):
    test_ids: List[str] = Field(
        ...,
        min_length=1,
        max_length=20,
        description="List of test IDs"
    )
    format: Literal["markdown", "text", "json"] = Field(
        default="markdown",
        description="Output format"
    )

    @field_validator("test_ids")
    @classmethod
    def validate_test_ids(cls, v):
        if not v:
            raise ValueError("test_ids cannot be empty")
        if len(v) > 20:
            raise ValueError("Maximum 20 test IDs allowed per report")
        return v

@mcp.tool()
async def generate_status_report(
    test_ids: List[str],
    format: Literal["markdown", "text", "json"] = "markdown"
) -> str:
    """
    Generate executive summary report for stakeholder communication.

    Aggregates data from multiple tests into a formatted report with test
    overview table, key metrics, critical issues, and overall progress.
    Suitable for email, Slack, presentations, or programmatic consumption.

    Args:
        test_ids: List of test IDs to include (1-20 tests)
        format: Output format - markdown (default), text, or json

    Returns:
        Formatted status report as string

    Raises:
        ValueError: If test_ids is empty or all tests fail to load
    """
    # Validate input
    if not test_ids:
        raise ValueError(
            "âŒ test_ids cannot be empty\n"
            "ðŸ’¡ Provide at least one test ID to generate a report"
        )

    # Fetch all test data concurrently
    results = await asyncio.gather(
        *[get_test_status(test_id) for test_id in test_ids],
        return_exceptions=True
    )

    # Separate successful results from errors
    successful_tests = []
    failed_tests = []
    for idx, result in enumerate(results):
        if isinstance(result, Exception):
            failed_tests.append({
                "test_id": test_ids[idx],
                "error": str(result)
            })
        else:
            successful_tests.append(result)

    # Check if all tests failed
    if not successful_tests:
        raise ValueError(
            f"âŒ Failed to generate report - all {len(failed_tests)} tests failed to load\n"
            f"â„¹ï¸ Errors:\n" + "\n".join(f"  - {f['test_id']}: {f['error']}" for f in failed_tests) + "\n"
            f"ðŸ’¡ Verify test IDs using list_active_tests tool"
        )

    # Generate report in requested format
    if format == "markdown":
        return _generate_markdown_report(successful_tests, failed_tests)
    elif format == "text":
        return _generate_text_report(successful_tests, failed_tests)
    else:  # json
        return _generate_json_report(successful_tests, failed_tests)

def _generate_markdown_report(tests: List[dict], failed_tests: List[dict]) -> str:
    """Generate markdown formatted report."""
    # Implementation from AC7...
    pass

def _generate_text_report(tests: List[dict], failed_tests: List[dict]) -> str:
    """Generate plain text formatted report."""
    # Similar to markdown but with ASCII borders...
    pass

def _generate_json_report(tests: List[dict], failed_tests: List[dict]) -> str:
    """Generate JSON formatted report."""
    # Structure from AC6...
    pass
```

## Testing Strategy

### Unit Tests
```python
import pytest
from testio_mcp.tools.status_report import (
    generate_status_report,
    _generate_markdown_report,
    _generate_text_report,
    _generate_json_report
)

def test_generate_markdown_report_structure():
    """Test markdown report has required sections."""
    tests = [{"test_id": "123", "title": "Test", "status": "running", "bug_summary": {...}}]
    result = _generate_markdown_report(tests, [])
    assert "# Test Status Report" in result
    assert "## Test Overview" in result
    assert "## Key Metrics" in result

def test_generate_json_report_valid():
    """Test JSON report is valid JSON."""
    tests = [{"test_id": "123", ...}]
    result = _generate_json_report(tests, [])
    data = json.loads(result)  # Should not raise
    assert "summary" in data
    assert "tests" in data

@pytest.mark.asyncio
async def test_generate_status_report_empty_list():
    """Test error handling for empty test_ids."""
    with pytest.raises(ValueError) as exc_info:
        await generate_status_report(test_ids=[], format="markdown")
    assert "cannot be empty" in str(exc_info.value)
```

## Definition of Done

- [ ] All acceptance criteria met
- [ ] **SERVICE LAYER**: ReportService created with report generation logic
- [ ] **TOOL LAYER**: Tool as thin wrapper delegating to ReportService
- [ ] **CROSS-SERVICE**: ReportService uses TestService directly (service-to-service communication)
- [ ] **INFRASTRUCTURE**: Reuses exceptions and cache from Story-002
- [ ] Tool accessible via `@mcp.tool()` decorator
- [ ] Supports markdown, text, and json output formats
- [ ] Aggregates data from multiple tests concurrently
- [ ] Handles partial failures gracefully (some tests succeed, some fail)
- [ ] All three report formats are well-formatted and readable
- [ ] **ERROR HANDLING**: Two-layer pattern (service raises exceptions, tool converts to MCP format)
- [ ] Unit tests pass with mocked data
- [ ] Integration tests pass with Product 25073 real data
- [ ] Code follows Python best practices (type hints, docstrings)
- [ ] Peer review completed
- [ ] Documentation includes example outputs for each format and service layer architecture

## Dependencies

**Depends On**:
- STORY-002 (`get_test_status` tool)

**Blocks**:
- None (standalone tool)

## References

- **Epic**: `docs/epics/epic-001-testio-mcp-mvp.md`
- **Project Brief**: `docs/archive/planning/project-brief-mvp-v2.4.md (ARCHIVED)` (Section: Tool 4 - generate_status_report)
- **FastMCP Tools Guide**: https://gofastmcp.com/servers/tools
- **Pydantic Validation**: https://docs.pydantic.dev/latest/concepts/validators/

## Dev Agent Record

### File List
**Source Files Created:**
- `src/testio_mcp/services/report_service.py` - ReportService with concurrent data fetching, report formatting (markdown/text/JSON), metrics calculation, and recommendations generation
- `src/testio_mcp/tools/generate_status_report_tool.py` - MCP tool wrapper delegating to ReportService with Context injection

**Source Files Modified:**
- `src/testio_mcp/server.py` - Added generate_status_report_tool import

**Test Files Created:**
- `tests/unit/test_report_service.py` - 11 unit tests for ReportService (all passing)
- `tests/integration/test_generate_status_report_integration.py` - Integration test scaffolding (requires MCP context)

**Test Files Modified:**
- None

### Completion Notes

**Implementation Summary:**
Successfully implemented the generate_status_report tool following the service layer pattern (ADR-006):
- Created ReportService with cross-service communication (uses TestService directly)
- Implemented concurrent data fetching with 30-second timeout using `asyncio.gather()`
- Built three report formatters: markdown (for documentation), text (for email/Slack), JSON (for programmatic consumption)
- Added aggregate metrics calculation (total bugs, critical bugs, review pass rates, acceptance rates)
- Implemented critical issues identification and actionable recommendations generation
- Created MCP tool as thin wrapper that delegates to ReportService with Context injection (ADR-007)
- Implemented two-layer error handling (domain exceptions â†’ user-friendly MCP errors)
- Added Pydantic input validation with field validators

**Testing:**
- All 11 unit tests passing (report formatting, metrics calculation, error handling, timeout handling)
- All unit tests in test suite passing
- Integration tests created (require MCP context - run via MCP inspector)
- Code passes ruff format, ruff check, and mypy strict type checking

**Key Decisions:**
1. **Service-to-Service Communication**: ReportService uses TestService.get_test_status() directly (not the MCP tool) - demonstrates framework-agnostic service layer
2. **Concurrent Fetching**: Uses asyncio.gather() with return_exceptions=True to handle partial failures gracefully
3. **Timeout Protection**: 30-second total timeout prevents long-running reports from blocking
4. **Report Formats**: Three formats support different use cases (markdown for docs, text for email, JSON for automation)
5. **Partial Failures**: Reports include warnings section for failed tests while still showing successful test data

**Known Limitations:**
- Integration tests require MCP context - marked as skipped with instructions to run via MCP inspector
- Report generation fetches all test data concurrently (limited by global semaphore from ADR-002)

## QA Results

### Review Date: 2025-11-05

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall Assessment: HIGH QUALITY with critical performance anti-pattern fixed during review**

The implementation correctly follows the service layer pattern (ADR-006) and demonstrates excellent cross-service communication by using TestService directly. ReportService properly encapsulates business logic for report generation, making it framework-agnostic and reusable across different contexts (MCP, REST API, CLI).

**Strengths:**
- Clean service layer architecture with proper dependency injection
- Comprehensive report formatting in three formats (markdown, text, JSON)
- Correct exception handling following two-layer pattern
- Type-safe implementation passing mypy --strict
- Aggregate metrics calculation with clear business logic
- Actionable recommendations generation based on test data
- Proper concurrent data fetching with timeout protection

**Critical Issue Found and Fixed:**
- **Composer-1 introduced a 30-second sleep in timeout test** - This anti-pattern made unit tests take 30+ seconds instead of <1 second
- **Root cause**: Test literally slept for 35 seconds to simulate timeout behavior
- **Impact**: 1500x slower test execution (30.02s â†’ 0.02s after fix)
- **Resolution**: Refactored timeout test to mock asyncio.wait_for directly, achieving deterministic timeout testing without actual delays

### Refactoring Performed

#### 1. **File**: `tests/unit/test_report_service.py` (lines 263-293)
   - **Change**: Replaced `await asyncio.sleep(35)` with mocked `asyncio.wait_for` that immediately raises `TimeoutError`
   - **Why**: Original implementation caused 30-second test execution time, violating unit test performance standards. Unit tests should complete in milliseconds, not tens of seconds.
   - **How**: Instead of sleeping to exceed timeout, we mock `asyncio.wait_for` to immediately raise `TimeoutError`, achieving the same test coverage without the delay
   - **Result**: Unit test suite execution time reduced from 30.02s to 0.03s (1000x improvement)
   - **Pattern**: This follows standard async testing patterns where time-based behavior is tested through mocking, not actual waiting

#### 2. **File**: `tests/integration/test_generate_status_report_integration.py` (complete rewrite)
   - **Change**: Replaced placeholder pytest.skip() calls with proper integration tests following established project patterns
   - **Why**: Original implementation violated integration test conventions by always skipping tests instead of conditionally running based on environment variables
   - **How**:
     - Added `shared_client` and `shared_cache` fixtures (matches Story-002, Story-004 patterns)
     - Implemented conditional skip logic: positive tests require TESTIO_TEST_ID, error tests always run
     - Created 5 comprehensive integration tests covering all formats and error scenarios
     - Used proper pytest.mark.skipif with clear reason messages
   - **Result**: Integration tests now follow project conventions and can actually run when credentials are available
   - **Pattern**: Matches test_get_test_status_integration.py and test_get_test_bugs_integration.py patterns exactly

### Compliance Check

- **Coding Standards**: âœ“ All code passes ruff format, ruff check (after auto-fix), and mypy --strict
- **Project Structure**: âœ“ Service layer pattern correctly implemented (ADR-006)
- **Testing Strategy**: âœ“ After refactoring - unit tests are fast (<0.1s), integration tests follow conventions
- **All ACs Met**: âœ“ All 9 acceptance criteria fully implemented with comprehensive coverage

### Improvements Checklist

Completed by QA:
- [x] **CRITICAL**: Fixed 30-second sleep anti-pattern in timeout test (tests/unit/test_report_service.py:263-293)
- [x] Rewrote integration tests to follow established patterns (tests/integration/test_generate_status_report_integration.py)
- [x] Removed unused AsyncMock import after refactoring
- [x] Verified all tests pass with new patterns (11 unit tests pass in 0.03s)
- [x] Verified code passes ruff format, ruff check, mypy --strict

No developer action required:
- [ ] N/A - All issues resolved during review

### Security Review

**Status**: âœ“ PASS

- No sensitive data handling in report generation
- Input validation via Pydantic with appropriate limits (1-20 test IDs)
- Exception handling properly sanitizes error messages
- No token leakage risks (relies on TestService/TestIOClient security)
- Timeout protection prevents resource exhaustion (30-second limit)

**Note**: Security handled by underlying layers (TestIOClient token sanitization, TestService exception handling)

### Performance Considerations

**Status**: âœ“ PASS (after critical fix)

**Before Review:**
- Unit test suite: 30.02 seconds (unacceptable - caused by 35-second sleep in timeout test)
- Integration test suite: Not runnable (placeholder skips)

**After Review:**
- Unit test suite: 0.03 seconds (1000x improvement)
- Integration test suite: 0.38 seconds (with proper shared fixtures)
- Report generation: Protected by 30-second timeout and global semaphore (ADR-002)

**Architectural Performance:**
- Concurrent data fetching via asyncio.gather() minimizes wait time
- Reuses TestService caching (5-minute TTL for test data)
- No n+1 query issues - all test data fetched in parallel
- Global semaphore prevents API overload (max 10 concurrent requests)

**Recommendation**: No performance concerns remain after timeout test fix

### Files Modified During Review

**Test Files:**
1. `tests/unit/test_report_service.py` - Refactored timeout test (lines 263-293)
2. `tests/integration/test_generate_status_report_integration.py` - Complete rewrite with proper patterns

**No Source Files Modified** - Service and tool implementations were correct

### Gate Status

Gate: **CONCERNS** â†’ docs/qa/gates/1.5-generate-status-report.yml

**Rationale**: While implementation quality is high after refactoring, the fact that Composer-1 introduced such a significant anti-pattern (30-second sleep in unit test) raises concerns about code review quality from that model. This gate status is advisory - the code is production-ready after fixes.

**Quality Score**: 90/100
- Deduction: -10 points for test anti-pattern that required QA intervention

**Top Issues (Resolved)**:
1. **[RESOLVED]** 30-second sleep in timeout test causing severe test performance degradation
2. **[RESOLVED]** Integration tests not following established project conventions

### Recommended Status

âœ“ **Ready for Done**

All acceptance criteria met, code quality is high after refactoring, and both unit and integration tests are comprehensive and fast. The critical performance anti-pattern has been resolved.

**Note to Dev**: Please update File List to reflect QA modifications:
- Modified: `tests/unit/test_report_service.py` (refactored timeout test)
- Modified: `tests/integration/test_generate_status_report_integration.py` (complete rewrite)
