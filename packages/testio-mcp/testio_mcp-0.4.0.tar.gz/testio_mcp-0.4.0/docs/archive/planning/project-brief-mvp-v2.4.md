---
document_type: project-brief
status: completed
archived: true
archive_date: 2025-01-20
completion_status: 100% - All epics delivered
release_version: 0.2.0
epic_coverage: [EPIC-001, EPIC-002, EPIC-003, EPIC-004]
original_version: v2.4
finalized_date: 2025-11-04
note: |
  This was the MVP planning document that guided development from November 2024 to January 2025.
  All planned features have been successfully implemented and released in v0.2.0.
  Future planning will be based on real-world usage feedback post-PyPI release.
see_also:
  - EPIC_COMPLETION_SUMMARY.md
  - CHANGELOG.md
  - docs/epics/epic-001-testio-mcp-mvp.md
---

# TestIO MCP Server - Project Brief (ARCHIVED)

> **âœ… STATUS: COMPLETED - 2025-01-20**
> **ğŸ“¦ RELEASE: v0.2.0**
> **ğŸ¯ EPIC COVERAGE: 100% (4/4 epics completed)**
>
> This document served as the MVP planning foundation. All features have been implemented.
> See [EPIC_COMPLETION_SUMMARY.md](../EPIC_COMPLETION_SUMMARY.md) for final delivery status.

## Executive Summary

Build an MVP MCP (Model Context Protocol) server that provides AI-first access to TestIO's **Customer API**, enabling non-developer stakeholders (CSMs, PMs, QA leads) to rapidly prototype integrations and automate workflows without traditional development bottlenecks.

**Strategic Goal:** Empower AI-enabled extensibility for TestIO as part of EPAM's AI-first initiative.

**Scope:** Customer API only for MVP. Tester API requires tester-specific permissions and is deferred to future enhancements.

---

## Problem Statement

### Current State
- Limited development capacity creates bottlenecks for customizations and enhancements
- Customer Success Managers and other non-dev collaborators cannot quickly prototype solutions
- TestIO's APIs are powerful but require traditional development for integration
- Gap between test execution and real-time visibility for stakeholders

### Desired State
- Non-technical stakeholders can "vibe code" new functionality through AI tools (Claude, Cursor, etc.)
- Rapid creation of "throwaway prototypes" to validate ideas before committing dev resources
- AI-first integration layer that makes TestIO capabilities accessible to AI agents
- Real-time test cycle visibility without UI navigation

---

## MVP Scope: Test Cycle Visibility & Management

### Core Use Cases

#### 1. "What's the status of test X?"
**Input:** Test ID or product name
**Output:** Synthesized report including:
- Test configuration (title, goal, testing type, duration, requirements)
- Bugs found (count by severity, status breakdown, recent submissions)
- Test status (running, review, completed)
- Review status and feedback
- Time/date information (start, end, created)

#### 2. "Show me all active tests for Product Y"
**Input:** Product name or ID
**Output:** List of running tests with high-level status:
- Test ID and title
- Start/end dates
- Current status (running, review, completed)
- Review status
- Bug count summary
- Testing type and requirements

#### 3. "What bugs have been found in test X?"
**Input:** Test ID
**Output:** Structured bug list with:
- Bug ID, title, severity
- Status (new, accepted, rejected, known, fixed)
- Tester information
- Steps to reproduce
- Attachments/screenshots
- Export status

#### 4. "Generate a status report for stakeholder meeting"
**Input:** Test ID(s) or Product name
**Output:** Executive summary suitable for:
- Email/Slack communication
- Stakeholder presentations
- Status reports
- Include key metrics, critical bugs, blockers

#### 5. "Show me testing activity across my products this quarter"
**Input:** Product IDs and date range
**Output:** Activity analysis including:
- Tests created, started, or completed in timeframe
- Product-wise breakdown
- Testing type distribution
- Optional bug metrics
- Trend data for visualization

### Out of Scope (MVP)
- **Tester API integration** (requires tester-specific permissions, not suitable for CSM use case)
- Real-time tester session monitoring
- Tester feedback/sentiment analysis
- Real-time notifications/polling
- Test creation/modification via MCP (read-only for MVP)
- Bulk bug operations (accept/reject in batch)
- Advanced analytics/dashboards

---

## Technical Architecture

### MCP Server Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          AI Client (Claude, Cursor)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ MCP Protocol
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            TestIO MCP Server                    â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Tools (API Endpoints)            â”‚   â”‚
â”‚  â”‚  - get_test_status                       â”‚   â”‚
â”‚  â”‚  - list_active_tests                     â”‚   â”‚
â”‚  â”‚  - get_test_bugs                         â”‚   â”‚
â”‚  â”‚  - generate_status_report                â”‚   â”‚
â”‚  â”‚  - get_test_activity_by_timeframe        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚      Resources (Data Access)             â”‚   â”‚
â”‚  â”‚  - products://list                       â”‚   â”‚
â”‚  â”‚  - tests://list                          â”‚   â”‚
â”‚  â”‚  - bugs://list                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚       Authentication Layer               â”‚   â”‚
â”‚  â”‚  - Customer API Token (MVP)              â”‚   â”‚
â”‚  â”‚  - Environment: Staging                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Customer API (v2)  â”‚
         â”‚                      â”‚
         â”‚  - Products          â”‚
         â”‚  - Features          â”‚
         â”‚  - Exploratory Tests â”‚
         â”‚  - Bugs              â”‚
         â”‚  - Test Environments â”‚
         â”‚  - Connections       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

**MCP Server Framework:** Python or TypeScript (TBD based on team preference)

**Key Dependencies:**
- MCP SDK (official Model Context Protocol implementation)
- HTTP client (requests/axios)
- JSON Schema validation
- Environment variable management (python-dotenv or similar)

**Configuration (.env):**
```bash
# API Endpoints (VERIFIED - 2025-11-04)
TESTIO_CUSTOMER_API_BASE_URL=https://api.stage-a.space/customer/v2

# Authentication
# Customer API uses: Authorization: Token <token>
TESTIO_CUSTOMER_API_TOKEN=your_customer_token_here

```

---

## MCP Tools Definition

### Tool 1: `get_test_status`
**Purpose:** Get comprehensive status of a single exploratory test

**Input Schema:**
```json
{
  "test_id": {
    "type": "string",
    "description": "Exploratory test ID",
    "required": true
  }
}
```

**API Calls:**
- `GET /exploratory_tests/{test_id}` - Get test configuration, status, requirements
- `GET /bugs?filter_test_cycle_ids={test_id}` - Get bugs for this test

**Output:** Structured JSON with:
- Test configuration (title, goal, testing_type, duration, requirements)
- Test status and review_status
- Bug summary (count by severity, status breakdown)
- Time/date information (created_at, starts_at, ends_at)
- Product and feature context

---

### Tool 2: `list_active_tests`
**Purpose:** List all active/running tests for a product

**Input Schema:**
```json
{
  "product_id": {
    "type": "string",
    "description": "Product ID or name",
    "required": true
  },
  "status": {
    "type": "string",
    "enum": ["running", "review_successful", "all"],
    "default": "running"
  }
}
```

**API Calls:**
- `GET /products/{product_id}` - Get product details
- `GET /products/{product_id}/exploratory_tests` - Get all tests for product
- Optional: `GET /bugs?filter_product_ids={product_id}` - Get bug counts per test

**Output:** Array of test summaries with:
- Test ID, title, and goal
- Status and review_status
- Testing type and duration
- Start/end dates
- Bug count summary (if requested)

---

### Tool 3: `get_test_bugs`
**Purpose:** Get detailed bug information for a test

**Input Schema:**
```json
{
  "test_id": {
    "type": "string",
    "description": "Exploratory test ID",
    "required": true
  },
  "bug_type": {
    "type": "string",
    "enum": ["functional", "visual", "content", "all"],
    "default": "all",
    "description": "Filter by bug type"
  },
  "severity": {
    "type": "string",
    "enum": ["low", "high", "critical", "all"],
    "default": "all",
    "description": "Filter by severity (applies only to functional bugs)"
  },
  "status": {
    "type": "string",
    "enum": ["accepted", "rejected", "forwarded"],
    "default": "all"
  }
}
```

**API Calls:**
- `GET /bugs?filter_test_cycle_ids={test_id}` - Get bugs, then filter by type/severity in MCP server

**Implementation Note:**
The API's `severity` field is overloaded (contains both bug type and severity). The MCP server must:
1. Query all bugs for the test
2. Client-side filter by `bug_type`:
   - `functional` â†’ severity in ["low", "high", "critical"]
   - `visual` â†’ severity == "visual"
   - `content` â†’ severity == "content"
3. If severity filter specified and bug_type is functional, further filter by severity value

**Output:** Array of bugs with:
- Bug ID, title
- Bug type (derived from severity field: functional/visual/content)
- Severity level (only for functional bugs: low/high/critical)
- Status (accepted, rejected, new)
- Author and tester information
- Steps to reproduce, expected vs actual results
- Location/URL where bug occurs
- Attachments and screenshots
- Comments
- Known and exported flags
- Device information

---

### Tool 4: `generate_status_report`
**Purpose:** Generate executive summary report for stakeholders

**Input Schema:**
```json
{
  "test_ids": {
    "type": "array",
    "items": {"type": "string"},
    "description": "One or more test IDs",
    "required": true
  },
  "format": {
    "type": "string",
    "enum": ["markdown", "text", "json"],
    "default": "markdown"
  }
}
```

**API Calls:**
- Aggregates data from `get_test_status` for each test
- Synthesizes into human-readable report

**Output:** Formatted report with:
- Test overview table
- Key metrics (total bugs, severity breakdown, export status)
- Critical issues requiring attention
- Overall progress summary

---

### Tool 5: `get_test_activity_by_timeframe`
**Purpose:** Query test activity across multiple products within a specific date range

**Input Schema:**
```json
{
  "product_ids": {
    "type": "array",
    "items": {"type": "string"},
    "description": "List of product IDs to query",
    "required": true
  },
  "start_date": {
    "type": "string",
    "format": "date",
    "description": "Start date (YYYY-MM-DD)",
    "required": true
  },
  "end_date": {
    "type": "string",
    "format": "date",
    "description": "End date (YYYY-MM-DD)",
    "required": true
  },
  "include_bugs": {
    "type": "boolean",
    "default": false,
    "description": "Include bug counts in results"
  }
}
```

**API Calls:**
- `GET /products/{id}/exploratory_tests` for each product
- Filter results by `created_at`, `starts_at`, or `ends_at` within date range
- Optional: `GET /bugs?filter_product_ids={ids}` for bug aggregation

**Output:** Activity summary with:
- Product-wise test breakdown
- Total tests created, started, or completed in timeframe
- Testing types distribution (rapid, focused, coverage, usability)
- Optional bug metrics per product/test
- Timeline visualization data (tests by week/month)

**Use Case Examples:**
- "Show me all test activity for products A, B, C in Q4 2024"
- "What tests ran last week across my product portfolio?"
- "Compare testing activity between January and February"

---

## MCP Resources Definition

### Resource 1: `products://list`
**Purpose:** Browse available products

**URI:** `products://list`

**Returns:** List of products with IDs, names, types, sections

---

### Resource 2: `tests://active`
**Purpose:** View all currently active tests across products

**URI:** `tests://active`

**Returns:** Real-time list of running tests

---

### Resource 3: `bugs://recent`
**Purpose:** View recently submitted bugs across all tests

**URI:** `bugs://recent?limit=50`

**Returns:** Last N bugs submitted, sorted by creation date

---

## Implementation Plan

### Phase 1: Foundation (Week 1)
- [ ] Set up MCP server boilerplate
- [x] Verify API connectivity with production sandbox âœ… (2025-11-04)
- [ ] Implement authentication layer (Token-based)
- [ ] Create Customer API client wrapper
- [ ] Implement `get_test_status` tool (basic version)
- [ ] Test with real production data (225 products, 89+ tests available)

### Phase 2: Core Tools (Week 2)
- [ ] Implement `list_active_tests` tool
- [ ] Implement `get_test_bugs` tool with filtering
- [ ] Implement `get_test_activity_by_timeframe` tool
- [ ] Add error handling and retry logic
- [ ] Test all 5 tools with real production data
- [ ] Document tool usage with examples

### Phase 3: Reporting & Polish (Week 3)
- [ ] Implement `generate_status_report` tool
- [ ] Add MCP resources for browsing
- [ ] Optimize API call patterns (reduce redundant calls)
- [ ] Add caching layer for frequently accessed data
- [ ] Create user documentation

### Phase 4: Integration & Demo (Week 4)
- [ ] Test with Claude Desktop integration
- [ ] Test with Cursor integration
- [ ] Create demo scenarios with real production data
- [ ] Prepare demo presentation for stakeholders

---

## Success Criteria

### MVP Launch Criteria
1. âœ… CSM can query test status via Claude without touching TestIO UI
2. âœ… All 5 core use cases work with real production data
3. âœ… Response times < 5 seconds for typical queries (225 products, 89+ tests)
4. âœ… Error messages are clear and actionable
5. âœ… Documentation allows another CSM to install and use independently

### Future Success Indicators (Post-MVP)
- Number of "throwaway prototypes" created by non-dev stakeholders
- Reduction in "What's the status?" questions to dev team
- Time saved in manual test monitoring
- Feedback from CSMs on usefulness
- Extension to additional workflows (bug triage, test creation)

---

## Schema Validation & API Verification

### âœ… VERIFIED - Production API Testing (2025-11-04)

**Environment:** Production sandbox (testcloud.test.io)
**Base URL:** `https://api.test.io/customer/v2` âœ… **CONFIRMED**
**Authentication:** `Authorization: Token <token>` âœ… **WORKING**

**Dataset Size:**
- **225 products** available
- **89 exploratory tests** for sample product (ID: 598)
- Real bug data confirmed

### âœ… Verified Schema Values

#### Test Status (Customer API) - CONFIRMED
**Observed values in production (Product 25073 - Affinity Studio Website):**
- `locked` - Test is locked/in progress
- `archived` - Completed/archived tests

**Expected additional values (need verification with more test samples):**
- `initialized` - New/initialized tests
- `running` - Test actively running
- `review` - Test under review

**Note:** Status values vary by test lifecycle stage

#### Test Review Status - CONFIRMED
**Observed:** `review_successful`
**Expected additional values:** `review_in_progress`, `review_failed` (need verification)

#### Bug Severity Field - âš ï¸ OVERLOADED FIELD (CRITICAL CORRECTION)

**IMPORTANT:** The `severity` field serves dual purposes:

**1. Bug Type Classification (non-functional bugs):**
- `"visual"` - Visual/UI bugs (layout issues, misaligned elements, UI problems)
- `"content"` - Content bugs (text errors, copy issues, typos)

**2. Severity Levels (functional bugs ONLY):**
- `"critical"` - Critical severity (functional bugs)
- `"high"` - High severity (functional bugs)
- `"low"` - Low severity (functional bugs)

**Logic:**
- If `severity = "visual"` â†’ Bug is visual type, no severity level
- If `severity = "content"` â†’ Bug is content type, no severity level
- If `severity = "low|high|critical"` â†’ Bug is functional type with that severity

#### Bug Fields - CONFIRMED
**Complete verified structure:**
```json
{
  "id": 2149563,
  "title": "The order submit button doesn't work",
  "severity": "low",
  "status": "accepted",
  "auto_accepted": true,
  "known": false,
  "exported_at": null,
  "external_idx": null,
  "language": "en",
  "location": "www.test.io",
  "expected_result": "You can click on button and place an order",
  "actual_result": "Button is inactive",
  "steps": ["Open the site", "Click on the link", "..."],
  "author": {"name": "yevgenia_averina"},
  "devices": [...],
  "attachments": [...],
  "feature": {...},
  "test": {"id": 109363, "title": "Evgeniya Testing"}
}
```

**Response format includes `meta` object:**
```json
{
  "bugs": [...],
  "meta": {
    "record_count": 1
  }
}
```

#### Bug Status Values - CONFIRMED
**Observed in production:**
- `"accepted"` - Bug accepted by customer

**Expected additional values (need verification):**
- `"new"` - Newly reported bug
- `"rejected"` - Bug rejected by customer
- `"fixed"` - Bug marked as fixed

#### Export Status (Customer API) - CONFIRMED
**From Bug Fields:**
- `exported_at: null` - Not exported
- `exported_at: "2024-10-15T10:30:00Z"` - Exported with timestamp

**API Blueprint documented filter values:**
- `export_requested`
- `not_exported`
- `exported`

**Status:** âœ… Field exists, enumerated filter values need verification

### âš ï¸ Still Need Verification

#### Test Status - Complete List
**Confirmed:** `locked`, `archived`
**Expected but need verification:** `initialized`, `running`, `review`, `pending`, `cancelled`, `completed`
**Workaround:** Accept any string value for filtering, log unknown values for documentation

### Verified API Endpoints

#### GET /products âœ…
**Response:** Array of 225 products
**Sample structure:**
```json
{
  "products": [{
    "id": 598,
    "name": "test IO - HALO",
    "type": "website",
    "default_section_id": 598,
    "sections": [],
    "connection": null
  }]
}
```

#### GET /products/{id}/exploratory_tests âœ…
**Response:** Array of exploratory tests for product
**Sample:** 89 tests for product ID 598
**Structure:**
```json
{
  "exploratory_tests": [{
    "id": 109363,
    "title": "Evgeniya Testing",
    "status": "archived",
    "review_status": "review_successful",
    "testing_type": "coverage",
    "starts_at": null,
    "ends_at": null
  }]
}
```

#### GET /bugs?filter_test_cycle_ids={id} âœ…
**Response:** Bugs for specified test cycle
**Structure:**
```json
{
  "bugs": [{
    "id": 2149563,
    "title": "The order submit button doesn't work",
    "severity": "low",
    "known": false,
    "fixed": null,
    "exported_at": null
  }],
  "meta": {
    "record_count": 1
  }
}
```

---

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| API rate limits in staging | High | Implement caching, throttling, and graceful degradation |
| Authentication complexity | Medium | Clear .env template, validation on startup |
| API schema changes | Medium | Version API clients, add schema validation |
| Staging data availability | Low | Coordinate with team to ensure active test cycles exist |
| MCP protocol complexity | Medium | Use official SDK, reference existing MCP servers |
| Incomplete API documentation | Medium | Verify all enums/values with real API calls, document discrepancies |

### Error Handling Scenarios

The MCP server must gracefully handle these failure modes:

#### Authentication Errors
**Scenario:** Invalid or expired API token
**Detection:** HTTP 401 Unauthorized or 403 Forbidden
**Response:** Clear error message indicating which API (Customer/Tester) failed auth, with instructions to check `.env` configuration
**Example:** `"âŒ Tester API authentication failed. Please verify TESTIO_TESTER_API_TOKEN in .env"`

#### API Unavailability
**Scenario:** Staging environment down or unreachable
**Detection:** Connection timeout, DNS failure, HTTP 5xx errors
**Response:** Retry with exponential backoff (3 attempts), then fail gracefully
**Example:** `"âš ï¸ TestIO API unavailable (timeout after 3 retries). Please try again later."`

#### Rate Limiting
**Scenario:** Too many requests to API
**Detection:** HTTP 429 Too Many Requests
**Response:** Implement token bucket or sliding window, queue requests, inform user of delay
**Example:** `"â³ Rate limit reached. Waiting 30 seconds before retry..."`

#### Resource Not Found
**Scenario:** Test ID, product ID, or bug ID doesn't exist
**Detection:** HTTP 404 Not Found
**Response:** Validate IDs before making requests when possible, provide helpful error
**Example:** `"âŒ Test ID 'ABC-123' not found. Use list_active_tests to see available tests."`

#### Invalid Parameters
**Scenario:** User provides invalid filter values (e.g., severity="medium" when only low/high/critical exist)
**Detection:** HTTP 400 Bad Request or validation in MCP server
**Response:** Pre-validate known enums, provide valid options in error message
**Example:** `"âŒ Invalid severity 'medium'. Valid options: low, high, critical, all"`

#### Partial Data Availability
**Scenario:** Customer API works but Tester API fails (or vice versa)
**Detection:** One API returns 200, other returns error
**Response:** Return partial data with warning, don't fail completely
**Example:** `"âš ï¸ Retrieved test configuration (Customer API) but couldn't fetch active sessions (Tester API error). Showing partial data."`

#### Empty Results
**Scenario:** Valid query but no matching data (e.g., no active tests)
**Detection:** HTTP 200 with empty array
**Response:** Inform user, suggest alternative queries
**Example:** `"â„¹ï¸ No active tests found for product 'MyApp'. Try list_active_tests with status='all' to see completed tests."`

#### Timeout
**Scenario:** API request takes too long
**Detection:** Request exceeds timeout threshold (e.g., 30 seconds)
**Response:** Cancel request, suggest narrowing query
**Example:** `"â±ï¸ Request timed out after 30 seconds. Try filtering by specific product or date range."`

---

## Future Enhancements (Post-MVP)

### Tester API Integration
**Why deferred:** Tester API uses tester-specific authentication, limiting access to only test cycles that specific tester is invited to. Not suitable for CSM use case where visibility across all tests is required.

**Potential future use cases:**
- **Tester-facing MCP tools** - Allow testers to use AI to submit bugs, manage sessions, view their stats
- **Admin/super-user access** - If TestIO provides elevated Tester API permissions, could enrich Customer API data with:
  - Real-time active tester counts
  - Tester session activity timelines
  - Device configuration usage statistics
  - Tester feedback and sentiment analysis

### Additional Customer API Features
- **Test creation via MCP** - Allow AI to draft/create exploratory tests from specs
- **Bulk bug operations** - Accept/reject/export bugs in batch based on AI analysis
- **Product and feature management** - CRUD operations for products/features
- **Connection management** - Configure Jira, GitHub integrations via MCP
- **Binary app upload** - Upload APK/IPA builds for mobile testing

### Cross-System Intelligence
- **Issue tracker integration** - Auto-create issues in Jira/Linear from critical bugs
- **Slack/Teams notifications** - Push test status updates to team channels
- **CI/CD integration** - Trigger tests from pipeline, post results back
- **AI-powered bug triage** - Automatically categorize, prioritize, deduplicate bugs
- **Trend analysis** - Historical bug patterns, quality metrics, test effectiveness

### Advanced Reporting
- **Custom report templates** - User-defined report formats with saved filters
- **Scheduled reports** - Automated weekly/daily status digests
- **Export formats** - PDF, CSV, Excel outputs for stakeholder sharing
- **Dashboard integration** - Embed MCP-generated insights in existing dashboards

---

## Implementation Decisions

### âœ… Resolved

**1. Language Choice: Python**
- **Decision:** Use Python for MVP implementation
- **Rationale:** Faster prototyping, excellent MCP SDK support, team familiarity
- **Future:** Can migrate to TypeScript if type safety becomes critical

**2. Error Handling: Moderately Verbose with Actionable Guidance**
- **Decision:** Provide context and suggested next steps
- **Include:** What failed, why it failed, actionable guidance
- **Exclude:** Stack traces, internal implementation details
- **Example:** `"âŒ Test ID '12345' not found. Use list_active_tests to see available tests for this product."`

**3. Caching Strategy: In-Memory with Short TTL**
- **Decision:** Start with in-memory caching, short TTLs
- **TTL Values:**
  - Products list: 1 hour (rarely changes)
  - Test lists: 5 minutes (moderate updates)
  - Bug data: 1 minute (changes frequently)
- **Future:** Upgrade to Redis/file-based if performance requires

**4. MVP Scope: Read-Only Operations**
- **Decision:** MVP focuses on read-only visibility
- **Rationale:** Lower risk, faster delivery, aligns with "Test Cycle Visibility" goal
- **Excluded from MVP:** Bug acceptance/rejection, test creation/modification
- **Phase 2:** Add write operations for bug triage if stakeholders require

---

## Appendix

### API Endpoints Reference

**Customer API Base:** `https://api.test.io/customer/v2` âœ… **VERIFIED (Production Sandbox)**

**Key Endpoints for MVP:**
- `GET /products` - List products
- `GET /products/{id}` - Get product details
- `GET /products/{id}/exploratory_tests` - List tests for product
- `GET /exploratory_tests/{id}` - Get test details
- `GET /bugs` - List bugs (with filters: product_ids, section_ids, test_cycle_ids, export_status)
- `GET /bugs/{id}` - Get detailed bug information
- `POST /bugs` - Search bugs with complex filters
- `GET /bugs/reject_reasons` - Get available rejection reasons

**Optional Endpoints (for future enhancements):**
- `PUT /bugs/{id}/accept` - Accept a bug
- `PUT /bugs/{id}/reject` - Reject a bug with reason
- `PUT /bugs/{id}/mark_as_exported` - Mark bug as exported
- `PUT /bugs/{id}/mark_as_known` - Mark bug as known issue
- `PUT /bugs/{id}/mark_as_fixed` - Mark bug as fixed

### Authentication Headers

**Customer API:**
```
Authorization: Token YOUR_CUSTOMER_API_TOKEN
```

---

## Next Steps

### 1. Project Planning & Documentation (Pre-Implementation)
**Action:** Create BMAD Epic and Stories
- **Epic:** TestIO MCP Server MVP
- **Stories to create:**
  - Story 1: Project setup and boilerplate (MCP server initialization, API client)
  - Story 2: Tool 1 - `get_test_status` implementation
  - Story 3: Tool 2 - `list_active_tests` implementation
  - Story 4: Tool 3 - `get_test_bugs` with bug type/severity filtering
  - Story 5: Tool 4 - `generate_status_report` implementation
  - Story 6: Tool 5 - `get_test_activity_by_timeframe` implementation
  - Story 7: Caching layer and error handling
  - Story 8: Documentation and integration testing
  - Story 9: Claude Desktop & Cursor integration demos

**Tools:** Use BMAD `/pm` agent to create epic and stories from this brief

### 2. Stakeholder Communication
- Share finalized brief with CTO and Product Team
- Schedule Week 4 demo date
- Set up weekly progress check-ins

### 3. Development Environment Setup (After Stories Created)
- Initialize Python MCP server project
- Configure Customer API token in environment
- Set up testing framework
- Validate against Product 25073 (Affinity Studio - known working dataset)

### Open for Implementation Details
- Tool parameter naming conventions
- Response format standards (JSON vs structured text)
- Logging and monitoring approach
- Testing strategy (unit tests, integration tests)

---

**Document Status:** âœ… **FINALIZED - Ready for Implementation**

**Document Version:** 2.4
**Last Updated:** 2025-11-04
**Owner:** Ricardo Leon (CSM)
**Stakeholders:** CTO, Product Team, CSM Team

**Changelog:**
- v2.4 (2025-11-04): **FINALIZED** - Resolved all open questions, added implementation decisions (Python, read-only MVP, caching strategy), added "Next Steps" section. Ready for development.
- v2.3 (2025-11-04): **CRITICAL SCHEMA CORRECTION** - Documented that `severity` field is overloaded (contains both bug type and severity). Updated Tool 3 to handle functional/visual/content bug types correctly. Verified test statuses (locked, archived) on Product 25073.
- v2.2 (2025-11-04): Verified production API connectivity, documented actual schema values, confirmed 225 products and 89+ tests available
- v2.1 (2025-11-04): Removed Linear references, added Tool 5 (test activity by timeframe)
- v2.0 (2025-11-04): Revised MVP scope to Customer API only after discovering Tester API authentication limitations
- v1.0 (2025-11-04): Initial draft with dual-API approach
