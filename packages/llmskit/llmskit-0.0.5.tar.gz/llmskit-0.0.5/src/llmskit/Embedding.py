# -*- coding: UTF-8 -*-
# @Time : 2025/12/15 23:18 
# @Author : åˆ˜æ´ªæ³¢

from __future__ import annotations
import httpx
from typing import List, Optional
from functools import lru_cache
from openai import OpenAI, APIError, RateLimitError, APIConnectionError, InternalServerError, AsyncOpenAI
from tenacity import retry, AsyncRetrying, stop_after_attempt, wait_random_exponential, retry_if_exception_type, before_sleep_log, retry_if_exception
import logging


__all__ = ["OpenAIEmbeddings", "AsyncOpenAIEmbeddings"]


def _should_retry(exc: Exception) -> bool:
    # æ˜Žç¡®ä¸å¯é‡è¯•çš„é”™è¯¯ï¼ˆå³ä½¿å±žäºŽ APIErrorï¼‰
    if isinstance(exc, APIError):
        err_code = getattr(exc, 'code', None) or ""
        err_type = getattr(exc, 'type', "") or ""
        # å¦‚ï¼šcontext_length_exceeded / invalid_request_error / auth error éƒ½ä¸è¯¥é‡è¯•
        if err_code in ("context_length_exceeded", "invalid_request_error") or "auth" in err_type:
            return False
    # å…œåº•ï¼šå¯¹å·²çŸ¥å¯æ¢å¤é”™è¯¯é‡è¯•
    return isinstance(exc, (RateLimitError, APIConnectionError, InternalServerError, httpx.TimeoutException, httpx.NetworkError))


class OpenAIEmbeddings:
    """OpenAI å…¼å®¹çš„åµŒå…¥æ¨¡åž‹å°è£…ç±»ï¼ˆæ”¯æŒ vLLM / LocalAI / Ollama / å…¶ä»– OpenAI å…¼å®¹æœåŠ¡ï¼‰

    ç‰¹æ€§ï¼š
      - æ‰¹é‡å¤„ç† + æŒ‡æ•°é€€é¿é‡è¯•
      - è¾“å…¥æˆªæ–­ä¸Žè­¦å‘Š
      - ç»´åº¦æŽ¢æµ‹ç¼“å­˜
      - æ”¯æŒåŒæ­¥/å¼‚æ­¥æ‰©å±•ï¼ˆæœ¬ç‰ˆæœ¬ä¸ºåŒæ­¥ï¼‰
    """

    def __init__(self, base_url: str, model_name: str, api_key: str = None, batch_size: int = 32, max_retries: int = 3,
        retry_delay: float = 1.0, max_retry_delay: int = 10, *, client: Optional[OpenAI] = None,
        max_input_length: int = 8191,  logger: Optional[logging.Logger] = None):
        """
        init
        :param base_url: é“¾æŽ¥åœ°å€
        :param api_key: å¯†é’¥
        :param model_name: æ¨¡åž‹å
        :param batch_size: æ‰¹æ¬¡å¤§å°
        :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        :param retry_delay: é‡è¯•å»¶è¿Ÿå€¼ï¼Œæ”¯æŒæµ®ç‚¹ç§’æ•°ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥
        :param max_retry_delay: æœ€å¤§é‡è¯•å»¶è¿Ÿå€¼ï¼Œå¿…é¡»ä¸ºæ•´æ•°
        :param client: å¤–éƒ¨ä¼ å…¥client    # å…è®¸æ³¨å…¥å·²æœ‰ clientï¼ˆæå‡æµ‹è¯•æ€§ï¼‰
        :param max_input_length:  æ¨¡åž‹æœ€å¤§è¾“å…¥é•¿åº¦ï¼Œæˆªæ–­çš„ä¾æ®  # OpenAI å®˜æ–¹ä¸Šé™ä¸º 8191 tokensï¼Œä½†æŒ‰å­—ç¬¦æˆªæ›´å®‰å…¨
        :param logger: å¯ä¼ å…¥ è‡ªå®šä¹‰logger
        """
        if not base_url:
            raise ValueError("base_url ä¸èƒ½ä¸ºç©º")

        self.base_url = base_url.rstrip("/")
        self.api_key = api_key if api_key else ""
        self.model_name = model_name
        self.batch_size = max(1, batch_size)
        self.max_retries = max(0, max_retries)
        self.retry_delay = max(0.1, retry_delay)  # è‡³å°‘ 0.1s
        self.max_retry_delay = max(1, max_retry_delay)  # è‡³å°‘ 1s
        self.max_input_length = max_input_length

        self.logger = logger or logging.getLogger(__name__)
        # å®¢æˆ·ç«¯æ³¨å…¥æˆ–æ–°å»º
        self.client = client or OpenAI(base_url=self.base_url, api_key=self.api_key)

        self.logger.info(f"âœ… åˆå§‹åŒ– OpenAIEmbeddings: model={model_name!r}, endpoint={self.base_url}")

        # å®šä¹‰é‡è¯•ç­–ç•¥
        self.retry_policy = {
                "stop": stop_after_attempt(self.max_retries),
                "wait": wait_random_exponential(multiplier=self.retry_delay, max=self.max_retry_delay),
                "retry": retry_if_exception(_should_retry),
                "before_sleep": before_sleep_log(self.logger, logging.INFO),
                "reraise": True,
            }
        self.logger.info(f'é‡è¯•ç­–ç•¥: {self.retry_policy}')


    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥æ–‡æ¡£åˆ—è¡¨ï¼ˆåŒæ­¥ï¼‰"""
        if not texts:
            return []
        if not isinstance(texts, list):
            raise TypeError("texts must be a list")
        return self.batch_embed_documents(texts)

    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥å•ä¸ªæŸ¥è¯¢"""
        if not text:
            text = ""
        if not isinstance(text, str):
            raise TypeError("text must be a string")
        return self.batch_embed_documents([text])[0]

    def _request_embeddings(self, texts: List[str]) -> List[List[float]]:
        """å¸¦ä¸“ä¸šé‡è¯•çš„åµŒå…¥è¯·æ±‚, æ ¸å¿ƒï¼šç”¨ @retry è£…é¥°"""
        @retry(**self.retry_policy)
        def cell():
            if not texts:
                return []
            response = self.client.embeddings.create(
                model=self.model_name,
                input=texts
            )
            if len(response.data) != len(texts):
                raise ValueError(f"å“åº”é•¿åº¦ {len(response.data)} â‰  è¾“å…¥ {len(texts)}")

            return [[float(x) for x in item.embedding] for item in response.data]
        return cell()

    # ====== 3. æ‰¹å¤„ç†é€»è¾‘å¤§å¹…ç®€åŒ– ======
    def batch_embed_documents(self, texts: List[str], *, batch_size: Optional[int] = None) -> List[List[float]]:
        if not texts:
            return []

        batch_size = batch_size or self.batch_size
        all_embeddings: List[List[float]] = []

        self.logger.info(f"ðŸ“¦ å¼€å§‹åµŒå…¥ {len(texts)} æ–‡æœ¬ (batch_size={batch_size})")

        for i in range(0, len(texts), batch_size):
            batch = texts[i: i + batch_size]
            safe_batch = self._prepare_texts(batch)
            batch_embeddings = self._request_embeddings(safe_batch)
            all_embeddings.extend(batch_embeddings)

            processed = min(i + len(batch), len(texts))
            self.logger.info(f"ðŸ“ˆ è¿›åº¦: {processed}/{len(texts)} ({processed / len(texts) * 100:.1f}%)")

        self.logger.info(f"âœ… æ‰¹é‡åµŒå…¥å®Œæˆ: {len(all_embeddings)} å‘é‡")
        return all_embeddings

    def _prepare_texts(self, texts: List[str]) -> List[str]:
        """é¢„å¤„ç†æ–‡æœ¬ï¼šæˆªæ–­ + è­¦å‘Š"""
        prepared = []
        for text in texts:
            if not isinstance(text, str):
                text = str(text)
            if len(text) > self.max_input_length:
                self.logger.warning(f"æ–‡æœ¬é•¿åº¦ ({len(text)}) > max_input_length ({self.max_input_length})ï¼Œå·²æˆªæ–­")
                text = text[: self.max_input_length]
            prepared.append(text)
        return prepared

    @lru_cache(maxsize=1)
    def get_embedding_dimension(self) -> int:
        """æŽ¢æµ‹å¹¶ç¼“å­˜åµŒå…¥ç»´åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        try:
            test_emb = self.embed_query("ç»´åº¦æŽ¢æµ‹æ–‡æœ¬")
            dim = len(test_emb)
            self.logger.info(f"ðŸ” æŽ¢æµ‹åˆ°åµŒå…¥ç»´åº¦: {dim}")
            return dim
        except Exception as e:
            self.logger.error(f"ç»´åº¦æŽ¢æµ‹å¤±è´¥: {e}")
            raise RuntimeError("æ— æ³•ç¡®å®šåµŒå…¥ç»´åº¦") from e


    def __repr__(self) -> str:
        return (
            f"OpenAIEmbeddings(model={self.model_name!r}, "
            f"endpoint={self.base_url}, batch_size={self.batch_size})"
        )


class AsyncOpenAIEmbeddings:
    """
    å®Œå…¨å¼‚æ­¥ OpenAI å…¼å®¹ Embeddings å°è£…
    - æ”¯æŒ vLLM / LocalAI / Ollama / OpenAI
    - æ‰¹é‡å¤„ç†
    - å¼‚æ­¥æŒ‡æ•°é€€é¿é‡è¯•
    """

    def __init__(
        self,
        base_url: str,
        model_name: str,
        api_key: str | None = None,
        batch_size: int = 32,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        max_retry_delay: int = 10,
        *,
        client: Optional[AsyncOpenAI] = None,
        max_input_length: int = 8191,
        logger: Optional[logging.Logger] = None,
    ):

        """
        init
        :param base_url: é“¾æŽ¥åœ°å€
        :param api_key: å¯†é’¥
        :param model_name: æ¨¡åž‹å
        :param batch_size: æ‰¹æ¬¡å¤§å°
        :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        :param retry_delay: é‡è¯•å»¶è¿Ÿå€¼ï¼Œæ”¯æŒæµ®ç‚¹ç§’æ•°ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥
        :param max_retry_delay: æœ€å¤§é‡è¯•å»¶è¿Ÿå€¼ï¼Œå¿…é¡»ä¸ºæ•´æ•°
        :param client: å¤–éƒ¨ä¼ å…¥client    # å…è®¸æ³¨å…¥å·²æœ‰ clientï¼ˆæå‡æµ‹è¯•æ€§ï¼‰
        :param max_input_length:  æ¨¡åž‹æœ€å¤§è¾“å…¥é•¿åº¦ï¼Œæˆªæ–­çš„ä¾æ®  # OpenAI å®˜æ–¹ä¸Šé™ä¸º 8191 tokensï¼Œä½†æŒ‰å­—ç¬¦æˆªæ›´å®‰å…¨
        :param logger: å¯ä¼ å…¥ è‡ªå®šä¹‰logger
        """
        if not base_url:
            raise ValueError("base_url ä¸èƒ½ä¸ºç©º")

        self.base_url = base_url.rstrip("/")
        self.api_key = api_key or ""
        self.model_name = model_name
        self.batch_size = max(1, batch_size)
        self.max_retries = max(0, max_retries)
        self.retry_delay = max(0.1, retry_delay)
        self.max_retry_delay = max(1, max_retry_delay)
        self.max_input_length = max_input_length

        self.logger = logger or logging.getLogger(__name__)

        self.client = client or AsyncOpenAI(
            base_url=self.base_url,
            api_key=self.api_key,
        )

        self.logger.info(
            f"âœ… åˆå§‹åŒ– AsyncOpenAIEmbeddings: model={model_name!r}, endpoint={self.base_url}"
        )

        # å®šä¹‰é‡è¯•ç­–ç•¥
        self.retry_policy = {
            "stop": stop_after_attempt(self.max_retries),
            "wait": wait_random_exponential(multiplier=self.retry_delay, max=self.max_retry_delay),
            "retry": retry_if_exception(_should_retry),
            "before_sleep": before_sleep_log(self.logger, logging.INFO),
            "reraise": True,
        }
        self.logger.info(f'é‡è¯•ç­–ç•¥: {self.retry_policy}')

    # =========================
    # å¯¹å¤– API
    # =========================
    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        if not texts:
            return []
        if not isinstance(texts, list):
            raise TypeError("texts must be a list")
        return await self.batch_embed_documents(texts)

    async def embed_query(self, text: str) -> List[float]:
        if not text:
            text = ""
        if not isinstance(text, str):
            raise TypeError("text must be a string")
        return (await self.batch_embed_documents([text]))[0]

    # =========================
    # æ ¸å¿ƒè¯·æ±‚ï¼ˆå¼‚æ­¥é‡è¯•ï¼‰
    # =========================
    async def _request_embeddings(self, texts: List[str]) -> List[List[float]]:
        if not texts:
            return []

        async for attempt in AsyncRetrying(**self.retry_policy):
            with attempt:
                response = await self.client.embeddings.create(
                    model=self.model_name,
                    input=texts,
                )

                if len(response.data) != len(texts):
                    raise ValueError(
                        f"å“åº”é•¿åº¦ {len(response.data)} â‰  è¾“å…¥ {len(texts)}"
                    )

                return [
                    [float(x) for x in item.embedding]
                    for item in response.data
                ]

        return []

    # =========================
    # æ‰¹å¤„ç†ï¼ˆå¼‚æ­¥é¡ºåºæ‰§è¡Œï¼‰
    # =========================
    async def batch_embed_documents(
        self, texts: List[str], *, batch_size: Optional[int] = None
    ) -> List[List[float]]:
        if not texts:
            return []

        batch_size = batch_size or self.batch_size
        all_embeddings: List[List[float]] = []

        self.logger.info(
            f"ðŸ“¦ å¼€å§‹åµŒå…¥ {len(texts)} æ–‡æœ¬ (batch_size={batch_size})"
        )

        for i in range(0, len(texts), batch_size):
            batch = texts[i : i + batch_size]
            safe_batch = self._prepare_texts(batch)

            batch_embeddings = await self._request_embeddings(safe_batch)
            all_embeddings.extend(batch_embeddings)

            processed = min(i + len(batch), len(texts))
            self.logger.info(
                f"ðŸ“ˆ è¿›åº¦: {processed}/{len(texts)} "
                f"({processed / len(texts) * 100:.1f}%)"
            )

        self.logger.info(f"âœ… æ‰¹é‡åµŒå…¥å®Œæˆ: {len(all_embeddings)} å‘é‡")
        return all_embeddings

    # =========================
    # æ–‡æœ¬é¢„å¤„ç†
    # =========================
    def _prepare_texts(self, texts: List[str]) -> List[str]:
        prepared = []
        for text in texts:
            if not isinstance(text, str):
                text = str(text)
            if len(text) > self.max_input_length:
                self.logger.warning(
                    f"æ–‡æœ¬é•¿åº¦ ({len(text)}) > max_input_length "
                    f"({self.max_input_length})ï¼Œå·²æˆªæ–­"
                )
                text = text[: self.max_input_length]
            prepared.append(text)
        return prepared

    # =========================
    # åµŒå…¥ç»´åº¦æŽ¢æµ‹ï¼ˆå¼‚æ­¥ + ç¼“å­˜ï¼‰
    # =========================
    @lru_cache(maxsize=1)
    async def get_embedding_dimension(self) -> int:
        try:
            emb = await self.embed_query("ç»´åº¦æŽ¢æµ‹æ–‡æœ¬")
            dim = len(emb)
            self.logger.info(f"ðŸ” æŽ¢æµ‹åˆ°åµŒå…¥ç»´åº¦: {dim}")
            return dim
        except Exception as e:
            self.logger.error(f"ç»´åº¦æŽ¢æµ‹å¤±è´¥: {e}")
            raise RuntimeError("æ— æ³•ç¡®å®šåµŒå…¥ç»´åº¦") from e

    def __repr__(self) -> str:
        return (
            f"AsyncOpenAIEmbeddings(model={self.model_name!r}, "
            f"endpoint={self.base_url}, batch_size={self.batch_size})"
        )
