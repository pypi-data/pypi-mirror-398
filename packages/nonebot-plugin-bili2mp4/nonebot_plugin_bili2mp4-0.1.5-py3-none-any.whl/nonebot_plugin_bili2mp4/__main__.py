from __future__ import annotations

import asyncio
import json
import os
import re
import shutil
import subprocess
import sys
import time
import urllib.request
from pathlib import Path
from typing import List, Optional, Set, Tuple
from urllib.parse import parse_qs, unquote, urlparse

from nonebot import logger, on_message, require

require("nonebot_plugin_localstore")
import nonebot_plugin_localstore as store
from nonebot.adapters.onebot.v11 import (
    Bot,
    Event,
    GroupMessageEvent,
    Message,
    MessageSegment,
    PrivateMessageEvent,
)
from nonebot.plugin import get_plugin_config

from .config import Config

# é…ç½®åŠ è½½
plugin_config = get_plugin_config(Config)
super_admins: List[int] = plugin_config.super_admins or []
logger.info(f"nonebot_plugin_bili2mp4 åˆå§‹åŒ–ï¼šè¶…ç®¡={super_admins}")

# FFmpeg è®¾ç½®
FFMPEG_DIR: Optional[str] = None


def _setup_ffmpeg() -> None:
    """è®¾ç½® FFmpeg è·¯å¾„"""
    global FFMPEG_DIR
    ffmpeg_path = shutil.which("ffmpeg")
    if ffmpeg_path:
        FFMPEG_DIR = os.path.dirname(ffmpeg_path)
        logger.info(f"[ffmpeg] ä½¿ç”¨ç³»ç»Ÿè·¯å¾„: {ffmpeg_path}")
        return
    logger.warning("[ffmpeg] æœªæ‰¾åˆ° ffmpegï¼Œè¯·ç¡®ä¿å·²å®‰è£…å¹¶é…ç½®æ­£ç¡®")


_setup_ffmpeg()

PLUGIN_NAME = "nonebot_plugin_bili2mp4"
DATA_DIR = store.get_plugin_data_dir()
STATE_PATH = store.get_plugin_data_file("state.json")
DOWNLOAD_DIR = store.get_plugin_data_dir() / "downloads"
COOKIE_FILE_PATH = store.get_plugin_data_file("bili_cookies.txt")
DOWNLOAD_DIR.mkdir(exist_ok=True)


enabled_groups: Set[int] = set()
bilibili_cookie: str = ""
max_height: int = 0  # 0 è¡¨ç¤ºä¸é™åˆ¶ï¼ˆç¤ºä¾‹ï¼š720/1080/2160ï¼‰
max_filesize_mb: int = 0  # 0 è¡¨ç¤ºä¸é™åˆ¶
_processing: Set[str] = set()


def _save_state() -> None:
    try:
        data = {
            "enabled_groups": sorted(list(enabled_groups)),
            "bilibili_cookie": bilibili_cookie or "",
            "max_height": int(max_height),
            "max_filesize_mb": int(max_filesize_mb),
        }
        with STATE_PATH.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.debug(f"çŠ¶æ€å·²ä¿å­˜: {STATE_PATH}")
    except Exception as e:
        logger.error(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")


def _load_state() -> None:
    global enabled_groups, bilibili_cookie, max_height, max_filesize_mb
    try:
        if STATE_PATH.exists():
            with STATE_PATH.open("r", encoding="utf-8") as f:
                data = json.load(f)
            enabled_groups = {int(g) for g in data.get("enabled_groups", [])}
            bilibili_cookie = str(data.get("bilibili_cookie", "") or "")
            max_height = int(data.get("max_height", 0) or 0)
            max_filesize_mb = int(data.get("max_filesize_mb", 0) or 0)
            logger.info(
                f"å·²åŠ è½½çŠ¶æ€: å¯ç”¨ç¾¤æ•°={len(enabled_groups)}ï¼ŒCookie={bool(bilibili_cookie)}"
            )
            return
    except Exception as e:
        logger.warning(f"è¯»å–çŠ¶æ€å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç©ºçŠ¶æ€: {e}")

    # é»˜è®¤å€¼
    enabled_groups = set()
    bilibili_cookie = ""
    max_height = 0
    max_filesize_mb = 0
    _save_state()


_load_state()

# åŸŸååŒ¹é…ï¼ˆå« m.bilibili.comã€t.bilibili.com ç­‰ï¼‰
BILI_URL_RE = re.compile(
    r"(https?://(?:[\w-]+\.)?(?:bilibili\.com|b23\.tv)/[^\s\"'<>]+)",
    flags=re.IGNORECASE,
)


def _find_urls_in_text(text: str) -> List[str]:
    urls = []
    for m in BILI_URL_RE.findall(text or ""):
        if m not in urls:
            urls.append(m)
    # å…¼å®¹ schema ä¸­çš„ url å‚æ•°
    try:
        parsed = urlparse(text)
        if parsed and parsed.query:
            qs = parse_qs(parsed.query)
            for key in ("url", "qqdocurl", "jumpUrl", "webpageUrl"):
                for v in qs.get(key, []):
                    v = unquote(v)
                    for u in BILI_URL_RE.findall(v):
                        if u not in urls:
                            urls.append(u)
    except Exception:
        pass
    return urls


def _walk_strings(obj) -> List[str]:
    out: List[str] = []
    try:
        if isinstance(obj, dict):
            for v in obj.values():
                out.extend(_walk_strings(v))
        elif isinstance(obj, list):
            for it in obj:
                out.extend(_walk_strings(it))
        elif isinstance(obj, str):
            out.append(obj)
    except Exception:
        pass
    return out


def _extract_bili_urls_from_event(event: GroupMessageEvent) -> List[str]:
    urls: List[str] = []
    try:
        for seg in event.message:
            # 1) çº¯æ–‡æœ¬
            if seg.type == "text":
                txt = seg.data.get("text", "")
                for u in _find_urls_in_text(txt):
                    if u not in urls:
                        urls.append(u)
            # 2) JSON å¡ç‰‡
            elif seg.type == "json":
                raw = seg.data.get("data") or seg.data.get("content") or ""
                for u in _find_urls_in_text(raw):
                    if u not in urls:
                        urls.append(u)
                try:
                    obj = json.loads(raw)
                    for s in _walk_strings(obj):
                        for u in _find_urls_in_text(s):
                            if u not in urls:
                                urls.append(u)
                except Exception:
                    pass
            # 3) XML å¡ç‰‡
            elif seg.type == "xml":
                raw = seg.data.get("data") or seg.data.get("content") or ""
                for u in _find_urls_in_text(raw):
                    if u not in urls:
                        urls.append(u)
            # 4) åˆ†äº«å¡ç‰‡
            elif seg.type == "share":
                u = seg.data.get("url") or ""
                for u2 in _find_urls_in_text(u):
                    if u2 not in urls:
                        urls.append(u2)
            else:
                s = str(seg)
                for u in _find_urls_in_text(s):
                    if u not in urls:
                        urls.append(u)
    except Exception as e:
        logger.debug(f"æå–é“¾æ¥å¼‚å¸¸: {e}")
    return urls


# ç¾¤æ¶ˆæ¯ç›‘å¬
group_listener = on_message(priority=100, block=False)


@group_listener.handle()
async def handle_group(bot: Bot, event: Event):
    if not isinstance(event, GroupMessageEvent):
        return

    group_id = int(event.group_id)
    if group_id not in enabled_groups:
        return  # æœªå¼€å¯è½¬æ¢çš„ç¾¤èŠé‡Œä¿æŒé™é»˜

    urls = _extract_bili_urls_from_event(event)
    if not urls:
        logger.debug(f"[bili2mp4] ç¾¤{group_id} æœªåœ¨è¯¥æ¶ˆæ¯ä¸­å‘ç°Bç«™é“¾æ¥")
        return

    url = urls[0]
    key = f"{group_id}|{url}"
    if key in _processing:
        logger.debug(f"[bili2mp4] å·²åœ¨å¤„ç†ä¸­ï¼Œå¿½ç•¥é‡å¤: {key}")
        return
    _processing.add(key)
    logger.info(f"[bili2mp4] æ£€æµ‹åˆ°Bç«™é“¾æ¥")

    async def work():
        try:
            await _download_and_send(bot, group_id, url)
        except Exception as e:
            # å¤±è´¥æ—¶é™é»˜ï¼ˆä»…æ—¥å¿—ï¼‰
            logger.error(f"[bili2mp4] å¤„ç†å¤±è´¥ï¼š{e}")
        finally:
            _processing.discard(key)

    asyncio.create_task(work())


# ç§èŠæ§åˆ¶
ctrl_listener = on_message(priority=50, block=False)

CMD_ENABLE_RE = re.compile(r"^è½¬æ¢\s*(\d+)$", flags=re.IGNORECASE)
CMD_DISABLE_RE = re.compile(r"^åœæ­¢è½¬æ¢\s*(\d+)$", flags=re.IGNORECASE)
CMD_SET_COOKIE_RE = re.compile(r"^è®¾ç½®Bç«™COOKIE\s+(.+)$", flags=re.S)
CMD_CLEAR_COOKIE = {"æ¸…é™¤Bç«™COOKIE", "åˆ é™¤Bç«™COOKIE"}


async def _handle_group_command(
    bot: Bot, event: PrivateMessageEvent, text: str
) -> bool:
    """å¤„ç†ç¾¤ç›¸å…³å‘½ä»¤"""
    global enabled_groups

    # å¼€å¯ç¾¤
    m = CMD_ENABLE_RE.fullmatch(text)
    if m:
        gid = int(m.group(1))
        if gid in enabled_groups:
            await bot.send(event, Message(f"â„¹ï¸ ç¾¤ {gid} å·²å¼€å¯è½¬æ¢"))
        else:
            enabled_groups.add(gid)
            _save_state()
            await bot.send(event, Message(f"âœ… å·²å¼€å¯ç¾¤ {gid} çš„Bç«™è§†é¢‘è½¬æ¢"))
        return True

    # å…³é—­ç¾¤
    m = CMD_DISABLE_RE.fullmatch(text)
    if m:
        gid = int(m.group(1))
        if gid in enabled_groups:
            enabled_groups.discard(gid)
            _save_state()
            await bot.send(event, Message(f"ğŸ›‘ å·²åœæ­¢ç¾¤ {gid} çš„Bç«™è§†é¢‘è½¬æ¢"))
        else:
            await bot.send(event, Message(f"â„¹ï¸ ç¾¤ {gid} æœªå¼€å¯è½¬æ¢"))
        return True

    # æŸ¥çœ‹åˆ—è¡¨
    if text in CMD_LIST:
        if enabled_groups:
            sorted_g = sorted(list(enabled_groups))
            await bot.send(
                event, Message("å½“å‰å·²å¼€å¯è½¬æ¢çš„ç¾¤ï¼š" + ", ".join(map(str, sorted_g)))
            )
        else:
            await bot.send(event, Message("æš‚æ— å¼€å¯è½¬æ¢çš„ç¾¤"))
        return True

    return False


async def _handle_config_command(
    bot: Bot, event: PrivateMessageEvent, text: str
) -> bool:
    """å¤„ç†é…ç½®ç›¸å…³å‘½ä»¤"""
    global bilibili_cookie, max_height, max_filesize_mb

    # è®¾ç½®Cookie
    m = CMD_SET_COOKIE_RE.fullmatch(text)
    if m:
        bilibili_cookie = m.group(1).strip()
        _save_state()
        await bot.send(event, Message("âœ… å·²è®¾ç½®Bç«™ Cookie"))
        return True

    # æ¸…é™¤Cookie
    if text in CMD_CLEAR_COOKIE:
        bilibili_cookie = ""
        _save_state()
        await bot.send(event, Message("ğŸ§¹ å·²æ¸…é™¤Bç«™ Cookie"))
        return True

    # è®¾ç½®æ¸…æ™°åº¦
    m = CMD_SET_HEIGHT_RE.fullmatch(text)
    if m:
        h = int(m.group(1))
        if h < 0:
            h = 0
        max_height = h
        _save_state()
        await bot.send(
            event, Message(f"â± æ¸…æ™°åº¦å·²è®¾ç½®ä¸º {'ä¸é™åˆ¶' if h == 0 else f'<= {h}p'}")
        )
        return True

    # è®¾ç½®æœ€å¤§å¤§å°ï¼ˆMBï¼‰
    m = CMD_SET_MAXSIZE_RE.fullmatch(text)
    if m:
        lim = int(m.group(1))
        if lim < 0:
            lim = 0
        max_filesize_mb = lim
        _save_state()
        await bot.send(
            event,
            Message(f"ğŸ“¦ æ–‡ä»¶å¤§å°é™åˆ¶ä¸º {'ä¸é™åˆ¶' if lim == 0 else f'<= {lim}MB'}"),
        )
        return True

    # æŸ¥çœ‹å‚æ•°
    if text in CMD_SHOW_PARAMS:
        await bot.send(
            event,
            Message(
                f"å‚æ•°ï¼šæ¸…æ™°åº¦<= {max_height or 'ä¸é™'}ï¼›å¤§å°<= {str(max_filesize_mb) + 'MB' if max_filesize_mb else 'ä¸é™'}ï¼›"
                f"Cookie={'å·²è®¾ç½®' if bool(bilibili_cookie) else 'æœªè®¾ç½®'}ï¼›å¯ç”¨ç¾¤æ•°={len(enabled_groups)}"
            ),
        )
        return True

    return False


@ctrl_listener.handle()
async def handle_private(bot: Bot, event: Event):
    if not isinstance(event, PrivateMessageEvent):
        return

    try:
        uid = int(event.user_id)
    except Exception:
        return
    if uid not in super_admins:
        return

    text = (event.get_message() or Message()).extract_plain_text().strip()
    if not text:
        return

    # å¸®åŠ©
    if text == "fhelp":
        await bot.send(event, Message(_get_help_message()))
        return

    # å¤„ç†ç¾¤ç›¸å…³å‘½ä»¤
    if await _handle_group_command(bot, event, text):
        return

    # å¤„ç†é…ç½®ç›¸å…³å‘½ä»¤
    if await _handle_config_command(bot, event, text):
        return

    # æœªåŒ¹é…å…¶ä»–å‘½ä»¤
    return


def _build_browser_like_headers() -> dict:
    # é¿å… 412ï¼šä½¿ç”¨å¸¸è§æµè§ˆå™¨å¤´ï¼Œå¹¶å›ºå®š Referer
    return {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Referer": "https://www.bilibili.com/",
        "Origin": "https://www.bilibili.com",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Site": "same-origin",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Dest": "document",
    }


def _expand_short_url(u: str, timeout: float = 8.0) -> str:
    try:
        host = urlparse(u).hostname or ""
        if host.lower() not in {"b23.tv", "www.b23.tv"}:
            return u
        # ä¼˜å…ˆ HEADï¼Œå¤±è´¥å† GET
        hdrs = {
            "User-Agent": _build_browser_like_headers()["User-Agent"],
            "Referer": "https://www.bilibili.com/",
        }
        try:
            req = urllib.request.Request(u, headers=hdrs, method="HEAD")
            with urllib.request.urlopen(req, timeout=timeout) as resp:
                final = resp.geturl()
                return final or u
        except Exception:
            req = urllib.request.Request(u, headers=hdrs, method="GET")
            with urllib.request.urlopen(req, timeout=timeout) as resp:
                final = resp.geturl()
                return final or u
    except Exception as e:
        logger.debug(f"çŸ­é“¾å±•å¼€å¤±è´¥ï¼Œä½¿ç”¨åŸé“¾æ¥ï¼ˆ{u}ï¼‰ï¼š{e}")
        return u


def _ensure_cookiefile(cookie_string: str) -> Optional[str]:
    """
    å°† Cookie å­—ç¬¦ä¸²è½¬ä¸º Netscape æ ¼å¼ï¼Œä¾› yt-dlp ä½¿ç”¨ã€‚
    """
    cookie_string = (cookie_string or "").strip().strip(";")
    if not cookie_string:
        # æ¸…é™¤æ—§æ–‡ä»¶
        if COOKIE_FILE_PATH.exists():
            try:
                if COOKIE_FILE_PATH.exists():
                    COOKIE_FILE_PATH.unlink()
            except Exception:
                pass
            return None

    # è§£æ Cookie é”®å€¼å¯¹
    pairs = []
    for part in cookie_string.split(";"):
        part = part.strip()
        if "=" not in part:
            continue
        k, v = part.split("=", 1)
        if k and v:
            pairs.append((k.strip(), v.strip()))

    if not pairs:
        return None

    # ç”Ÿæˆ Netscape æ ¼å¼ Cookie æ–‡ä»¶
    expiry = int(time.time()) + 180 * 24 * 3600  # 180 å¤©
    lines = [
        "# Netscape HTTP Cookie File",
        "# Generated by nonebot_plugin_bili2mp4",
        "",
    ]

    for k, v in pairs:
        # domain include_subdomains path secure expiry name value
        lines.append(f".bilibili.com\tTRUE\t/\tFALSE\t{expiry}\t{k}\t{v}")

    try:
        with COOKIE_FILE_PATH.open("w", encoding="utf-8") as f:
            f.write("\n".join(lines) + "\n")
        logger.info(f"Cookie å·²è®¾ç½®")
        return str(COOKIE_FILE_PATH)
    except Exception:
        return None


async def _download_and_send(bot: Bot, group_id: int, url: str) -> None:
    # æ‰§è¡Œä¸‹è½½
    try:
        path, title = await asyncio.to_thread(
            _download_with_ytdlp,
            url,
            bilibili_cookie,
            DOWNLOAD_DIR,
            max_height,
            max_filesize_mb,
        )
    except (ImportError, RuntimeError):
        return
    except Exception as e:
        logger.error(f"ä¸‹è½½å¼‚å¸¸ï¼š{e}")
        return

    # æ£€æŸ¥æ–‡ä»¶å¤§å°å’Œåˆ†è¾¨ç‡
    if not _check_video_file(path):
        return

    # å‘é€è§†é¢‘
    await _send_video_with_timeout(bot, group_id, path, title)


def _check_video_file(path: str) -> bool:
    """æ£€æŸ¥è§†é¢‘æ–‡ä»¶å¤§å°å’Œåˆ†è¾¨ç‡"""
    try:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        path_obj = Path(path)
        if max_filesize_mb and path_obj.exists():
            size_mb = path_obj.stat().st_size / (1024 * 1024)
            if size_mb > max_filesize_mb:
                if path_obj.exists():
                    path_obj.unlink()
                return False

        # æ£€æŸ¥è§†é¢‘åˆ†è¾¨ç‡
        if path_obj.exists():
            result = subprocess.run(
                [
                    "ffprobe",
                    "-v",
                    "error",
                    "-select_streams",
                    "v:0",
                    "-show_entries",
                    "stream=width,height",
                    "-of",
                    "csv=p=0",
                    path,
                ],
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                width, height = result.stdout.strip().split(",")
        return True
    except Exception:
        return False


async def _send_video_with_timeout(
    bot: Bot, group_id: int, path: str, title: str
) -> None:
    """å‘é€è§†é¢‘ï¼Œå¸¦è¶…æ—¶å¤„ç†"""
    try:
        await bot.send_group_msg(
            group_id=group_id,
            message=MessageSegment.video(file=path)
            + Message(f"\n{title or 'Bç«™è§†é¢‘'}"),
        )
        logger.info("è§†é¢‘å·²å‘é€åˆ°ç¾¤")
    except Exception as e:
        error_msg = str(e)
        if not ("timeout" in error_msg.lower() and "websocket" in error_msg.lower()):
            logger.error(f"å‘é€è§†é¢‘å¤±è´¥ï¼š{e}")
    finally:
        # æ¸…ç†æ–‡ä»¶
        try:
            path_obj = Path(path)
            if path_obj.exists():
                path_obj.unlink()
        except Exception:
            pass


def _build_format_candidates(height_limit: int, size_limit_mb: int) -> List[str]:
    """æ„å»ºæ ¼å¼å€™é€‰åˆ—è¡¨"""
    h = height_limit if height_limit and height_limit > 0 else None

    if not h:
        return ["bv*+ba/best"]

    # æ ¹æ®æ¸…æ™°åº¦é™åˆ¶æ„å»ºæ ¼å¼å€™é€‰
    format_map = {
        1080: [
            f"bv*[height>=1080]+ba/best",
            f"bv*[height>=720]+ba/best",
            "bv*+ba/best",
        ],
        720: [f"bv*[height>=720]+ba/best", f"bv*[height>=480]+ba/best", "bv*+ba/best"],
        480: [f"bv*[height>=480]+ba/best", "bv*+ba/best"],
    }

    # æ ¹æ®é«˜åº¦é€‰æ‹©æœ€é€‚åˆçš„æ ¼å¼åˆ—è¡¨
    for threshold, formats in sorted(format_map.items(), reverse=True):
        if h >= threshold:
            return formats

    # é»˜è®¤æ ¼å¼
    return ["bv*+ba/best"]


def _download_with_ytdlp(
    url: str, cookie: str, out_dir: str, height_limit: int, size_limit_mb: int
) -> Tuple[str, str]:
    try:
        from yt_dlp import YoutubeDL  # type: ignore
        from yt_dlp.utils import DownloadError  # type: ignore
    except Exception:
        raise ImportError("yt_dlp not installed")

    # å±•å¼€ b23 çŸ­é“¾ï¼Œç¡®ä¿é¦–ä¸ªè¯·æ±‚å‘½ä¸­ bilibili.com åŸŸ
    final_url = _expand_short_url(url)

    # æ„å»º Cookie æ–‡ä»¶
    cookiefile = _ensure_cookiefile(cookie)
    candidates = _build_format_candidates(height_limit, size_limit_mb)
    last_err: Optional[Exception] = None

    for i, fmt in enumerate(candidates):
        headers = _build_browser_like_headers()
        ydl_opts = {
            "format": fmt,
            "outtmpl": str(out_dir / "%(title).80s [%(id)s].%(ext)s"),
            "noplaylist": True,
            "merge_output_format": "mp4",
            "quiet": False,  # æ”¹ä¸ºFalseä»¥è·å–æ›´å¤šè°ƒè¯•ä¿¡æ¯
            "no_warnings": False,
            "http_headers": headers,
            # æ›´æ¢å®¢æˆ·ç«¯æœ‰åŠ©äºè¿‡æ£€ï¼›å¤±è´¥å¯å›é€€ä¸º web
            "extractor_args": {
                "bili": {
                    "player_client": ["android", "web"],  # æ·»åŠ webå®¢æˆ·ç«¯æé«˜å…¼å®¹æ€§
                    "lang": ["zh-CN"],
                }
            },
        }

        # å‘Šè¯‰ yt-dlp ffmpeg åœ¨å“ªé‡Œ
        if FFMPEG_DIR:
            ydl_opts["ffmpeg_location"] = FFMPEG_DIR

        # è®¾ç½® Cookie
        if cookiefile:
            ydl_opts["cookiefile"] = cookiefile
            logger.info(f"ä½¿ç”¨ cookiefile: {cookiefile}")
        elif cookie:
            headers["Cookie"] = cookie
            logger.info("ä½¿ç”¨ Cookie header")

        try:
            with YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(final_url, download=True)
                title = info.get("title") or "Bç«™è§†é¢‘"

                # è·å–ä¸‹è½½ä¿¡æ¯
                height = info.get("height", 0)
                logger.info(f"ä¸‹è½½å®Œæˆ: {title} ({height}p)")

                # å®šä½æ–‡ä»¶
                final_path = _locate_final_file(ydl, info)
                if not final_path or not Path(final_path).exists():
                    raise RuntimeError("æœªæ‰¾åˆ°å·²ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶ï¼Œå¯èƒ½æœªå®‰è£… ffmpeg")
                return final_path, title
        except DownloadError as e:
            last_err = e
            continue
        except Exception as e:
            last_err = e
            continue

    if last_err:
        raise RuntimeError(str(last_err))
    raise RuntimeError("æ— æ³•ä¸‹è½½è¯¥è§†é¢‘")


def _locate_final_file(ydl, info) -> Optional[str]:
    # ä¼˜å…ˆä»ä¸‹è½½é¡¹ä¸­å–
    for key in ("requested_downloads", "requested_formats"):
        arr = info.get(key)
        if isinstance(arr, list):
            for it in arr:
                fp = it.get("filepath")
                if fp and os.path.exists(fp):
                    return fp
    # å…¼å®¹å­—æ®µ
    for key in ("filepath", "_filename"):
        fp = info.get(key)
        if fp and os.path.exists(fp):
            return fp
    # é¢„æµ‹åˆå¹¶å mp4
    base = ydl.prepare_filename(info)
    root, _ = os.path.splitext(base)
    candidate = root + ".mp4"
    if os.path.exists(candidate):
        return candidate
    # å…œåº•ï¼šæŒ‰è§†é¢‘IDåœ¨ç›®å½•ä¸­æœ
    vid = info.get("id") or ""
    if vid:
        dirpath = os.path.dirname(base) or os.getcwd()
        try:
            files = [dirpath / f for f in os.listdir(dirpath) if vid in f]
            if files:
                files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
                return str(files[0])
        except Exception:
            pass
    return None
