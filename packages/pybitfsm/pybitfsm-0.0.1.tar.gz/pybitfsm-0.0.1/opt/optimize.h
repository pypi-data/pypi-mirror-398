#ifndef CARRYOPT_OPTIMIZE_H_
#define CARRYOPT_OPTIMIZE_H_

#include <cassert>
#include <climits>
#include <cstddef>
#include <cstdint>
#include <cstring>

#include <algorithm>
#include <array>
#include <bit>
#include <chrono>
#include <functional>
#include <iosfwd>
#include <limits>
#include <memory>
#include <mutex>
#include <stop_token>
#include <type_traits>
#include <unordered_map>
#include <vector>

#include <taskflow/taskflow.hpp>
#include <xxhash.h>

#include "bits.h"
#include "c_api.h"
#include "directives.h"
#include "opcode.h"
#include "options.h"
#include "pack.h"
#include "program.h"
#include "target_function.h"

namespace bitfsm_opt {

// Maximum number of constants that can be provided to the superoptimizer.
inline constexpr unsigned kMaxConstants = 3;

// Maximum number of parameters that a superoptimized function may have.
inline constexpr unsigned kMaxParameters = 3;

// Maximum number of carry-generating instructions that can appear in a program.
inline constexpr unsigned kMaxCarries = 5;

// Maximum number of operations that may be generated by the superoptimizer.
inline constexpr unsigned kMaxOperations = 10;

// Maximum number of length of a superoptimized program.
inline constexpr unsigned kMaxInstructions =
    kMaxConstants + kMaxParameters + kMaxOperations;

static_assert(kMaxInstructions <= BITFSM_OPT_MAX_INSTRUCTIONS,
              "BITFSM_OPT_MAX_INSTRUCTIONS cannot hold all possible programs");

template<typename Vec, typename Prune, typename...>
struct Optimizer {};

template<typename Vec, typename Prune, typename... Ops>
struct Optimizer<Vec, Prune, Pack<Ops...>> {
  static constexpr unsigned kNumOps = sizeof...(Ops);

  // kVariableOp is the op code for the Variable operation type.
  static constexpr unsigned kVariableOp = 0;
  using VariableOpCode = typename PackElement<kVariableOp, Pack<Ops...>>::type;
  static_assert(VariableOpCode::kCode == kVariableOp &&
                std::is_same_v<typename VariableOpCode::Impl, Variable>,
                "kVariableOp does not match the op code for the Variable operation");

  // kZeroOp is the op code for the Zero operation type.
  static constexpr unsigned kZeroOp = 1;
  using ZeroOpCode = typename PackElement<kZeroOp, Pack<Ops...>>::type;
  static_assert(ZeroOpCode::kCode == kZeroOp &&
                std::is_same_v<typename ZeroOpCode::Impl, Zero>,
                "kZeroOp does not match the op code for the Zero operation");

  // The first operation to generate. (Variable references and constants are not
  // generated.)
  static constexpr unsigned kStartOp = kZeroOp + 1;

  // The maximum operation to generate.
  static constexpr unsigned kEndOp = sizeof...(Ops) - 1;

  // PackedOpMask is the type of a bit mask that can represent all op codes.
  using PackedOpMask =
      typename std::conditional<
          kNumOps <= 16,
          typename std::conditional<
              kNumOps <= 8,
              std::uint8_t,
              std::uint16_t
          >::type,
          typename std::conditional<
              kNumOps <= 32,
              std::uint32_t,
              std::uint64_t
          >::type
      >::type;

  // FastOpMask is a bit mask that can represent all op codes.
  // It should be used instead of PackedOpMask for local variables or parameters.
  using FastOpMask =
      typename std::conditional<
          kNumOps <= 32,
          std::uint_fast32_t,
          std::uint_fast64_t
      >::type;
  static_assert(sizeof(FastOpMask) >= sizeof(PackedOpMask));

  // The maximum number of ops that can be represented by this class.
  // This is kNumOps rounded to the next power of two.
  static constexpr unsigned kMaxOps = std::popcount(PackedOpMask(-1));
  static_assert(std::has_single_bit(kMaxOps), "kMaxOps must be a power of 2");
  static_assert(kNumOps <= kMaxOps, "cannot pack op mask into one word");

  // The minimum number of bits required to encode an op code.
  static constexpr unsigned kOpBits = std::countr_zero(kMaxOps);
  static_assert(kNumOps <= (1 << kOpBits),
                "kOpBits is too small to represent all op codes");

  // kRegBits is the minimum number of bits requires to encode a register index.
  // It is fixed at 4, which is more than enough to encode all registers in the
  // longest program that can be generated exponentially.
  static constexpr unsigned kRegBits = 4;

  // kMaxRegs is the maximum number of registers that can be represented.
  static constexpr unsigned kMaxRegs = 1 << kRegBits;

  // PackedRegMask is the type of a bit mask that can represent all registers.
  using PackedRegMask = std::uint16_t;
  static_assert(kMaxRegs <= (unsigned) std::popcount(PackedRegMask(-1)),
                "RegMask is too small to represent all registers");

  // FastRegMask is a bit mask that can represent all registers.
  // It should be used instead of PackedRegMask for local variables or parameters.
  using FastRegMask = std::uint_fast16_t;
  static_assert(sizeof(FastRegMask) >= sizeof(PackedRegMask));

  struct alignas(4) Instruction {
    // Bit map, where each 1 bit corresponds to an unused register after
    // executing this instruction.
    //
    // The input variables are always considered used; otherwise, if the
    // function did not involve one of the variables, it would be uncomputable.
    //
    // TODO: Explain what this is used for.
    PackedRegMask unused;

    // Padding to round out the structure to a 64-bit word. This reduces the
    // amount of work we need to do in order to memcpy it.
    std::uint8_t padding[2];

    // The number of carry-generating instructions executed by the program
    // up to and including this instruction.
    std::uint8_t carries;

    // The instruction 'op r1, r2'.
    //
    // Instructions representing variables or constants are encoded with
    // op = kVariableOp, r1 = <instruction index>, r2 = <instruction index>.
    //
    // As a special case, the constant zero is encoded with op = kZeroOp.
    std::uint8_t r2;
    std::uint8_t r1;
    std::uint8_t op;

    using Ordinal = std::uint_fast32_t;

    BITFSM_ALWAYS_INLINE
    Ordinal ordinal() const {
      static_assert(std::endian::native == std::endian::little ||
                    std::endian::native == std::endian::big,
                    "unsupported endianness");

      std::uint32_t result;
      static_assert(((alignof(Instruction) + offsetof(Instruction, carries))
                     % alignof(decltype(result))) == 0,
                    "incorrect Instruction class layout");

      std::memcpy(&result, &this->carries, sizeof(result));
      if constexpr (std::endian::native == std::endian::little) {
        return result;
      } else {
        return ReverseBytes(result);
      }
    }
  };

  static_assert(sizeof(Instruction) == sizeof(uint64_t));

  // Returns true if the program in 'a[0...n]' is lexicographically greater than
  // the program in 'b[0...n]'.
  BITFSM_ALWAYS_INLINE
  static bool IsAfter(const Instruction* a, unsigned a_size,
                      const Instruction* b, unsigned b_size) {
    using Ordinal = Instruction::Ordinal;
    unsigned size = std::min(a_size, b_size);
    for (unsigned i = 0; i < size; i++) {
      Ordinal a_ordinal = a[i].ordinal();
      Ordinal b_ordinal = b[i].ordinal();
      if (a_ordinal != b_ordinal) {
        return a_ordinal > b_ordinal;
      }
    }
    return a_size > b_size;
  }

  // Container for a program that is equivalent to the target function.
  // If candidate.size() == 0, then the candidate does not contain a program.
  class ProgramCandidate {
    std::uint64_t version_;
    unsigned cost_;
    unsigned size_;
    Instruction isns_[kMaxRegs];

   public:
    // Using cost = UINT_MAX / 2 ensures that adding a small number to the
    // cost (e.g. the number of variables) does not cause an overflow, but
    // is still larger than any possible program's cost.
    ProgramCandidate() : version_(0), cost_(UINT_MAX / 2), size_(0) {}

    // Version number incremented whenever the program is modified.
    BITFSM_ALWAYS_INLINE
    std::uint64_t version() const { return version_; }

    // Total cost of the instructions in the program.
    BITFSM_ALWAYS_INLINE
    unsigned cost() const { return cost_; }

    // Number of instructions in the program.
    BITFSM_ALWAYS_INLINE
    unsigned size() const { return size_; }

    // Sequence of instructions in the program.
    BITFSM_ALWAYS_INLINE
    const Instruction* isns() const { return isns_; }

    // Returns true if this candidate has a lower cost than the provided
    // program, or if this candidate precedes it lexicographically.
    BITFSM_ALWAYS_INLINE
    bool BetterThan(unsigned cost, unsigned size,
                                  Instruction* isns) const {
      // TODO: Also consider the number of registers with >1 use in comparing.
      //       This increases register pressure (and moves on x86), so fewer
      //       reused registers is better.
      return cost_ < cost || (cost_ == cost && IsAfter(isns, size, isns_, size_));
    }

    // Replaces this candidate's current program with the provided one.
    BITFSM_ALWAYS_INLINE
    void Set(unsigned cost, unsigned size, const Instruction* isns) {
      cost_ = cost;
      size_ = size;
      std::memcpy(isns_, isns, sizeof(isns_));
      version_++;
    }

    // Replaces this candidate's program with 'other' if 'other' is better.
    BITFSM_ALWAYS_INLINE
    void Merge(const ProgramCandidate& other) {
      if (other.BetterThan(cost_, size_, isns_)) {
        Set(other.cost_, other.size_, other.isns_);
      }
    }
  };

  // A buffer that holds generated instructions and output before they are
  // merged into the instruction stream. Each instruction index in a context
  // has an associated buffer.
  struct Buf {
    // Values generated for each buffered instruction.
    Vec regs[kMaxOps];

    // Buffered instructions and metadata.
    Instruction isns[kMaxOps];

    // The offset of the next element to read from the buffer. The offset
    // is initialized to the last element in the buffer, and counts down.
    int next;

    // Registers to use to generate the next set of buffered instructions.
    // If r1 >= the instruction index, then no more instructions can be
    // generated without incrementing the previous instruction.
    unsigned r1;
    unsigned r2;
  };

  // Parameters and state for an instance of the superoptimizer.
  struct Context {
    // The values assigned to all registers. Each register is a vector that
    // contains the result of applying the current program to all elements
    // of the test vector.
    Vec regs[kMaxRegs];

    // The expected output of applying the function to the input variables.
    // This value is updated during evaluation as new test cases are discovered.
    Vec expected;

    // A mask with a '1' in each bit position that needs to be compared to
    // the expected result.
    Vec mask;

    // The number of constants provided to the program.
    // The constants are encoded as instructions starting at index 0.
    unsigned num_constants;

    // The number of parameters provided to the program.
    // The parameters are encoded as instructions following the last constant.
    unsigned num_parameters;

    // The number of instructions to generate. This includes the base set of
    // constants and variables, which are in the lowest indices.
    unsigned num_instructions;

    // Maximum number of carry-generating operations that may be performed.
    unsigned max_carries;

    // Token used for asynchronous cancellation.
    std::stop_token stop_token;

    // Number of runs of the exhaustive test suite that found a mismatch.
    std::size_t failed_checks;

    // Program with the minimum cost discovered so far.
    ProgramCandidate best;

    // Sequence of instructions in the current program.
    Instruction isns[kMaxRegs];

    // Instructions and registers that have been generated, but not consumed.
    Buf buf[kMaxRegs];

    Context() {}

    Context(const Options& options, const TargetFunction<Vec>& target,
            const std::stop_token stop_token_arg)
        : expected(target.Expected(0)),
          mask(target.Mask(0)),
          num_constants(options.num_constants),
          num_parameters(options.num_parameters),
          num_instructions(0),
          max_carries(options.max_carries),
          stop_token(stop_token_arg),
          failed_checks(0) {
      // Set up the constant values.
      for (unsigned i = 0; i < options.num_constants; i++) {
        std::uint8_t c = options.constants[i];
        regs[i] = Vec::Broadcast(c);
        Instruction& isn = isns[i];
        isn.op = c == 0 ? kZeroOp : kVariableOp;
        isn.r1 = i;
        isn.r2 = i;
      }

      // Set up the initial test cases.
      std::size_t offset = options.num_constants;
      const Vec* params = target.Parameters(0);
      for (unsigned i = 0; i < options.num_parameters; i++) {
        unsigned level = offset + i;
        regs[level] = params[i];
        Instruction& isn = isns[level];
        isn.op = kVariableOp;
        isn.r1 = level;
        isn.r2 = level;
      }

      // Ensure buffers are clear in case the context is used without
      // calling BeginSearch().
      ResetBuffers();
    }

    BITFSM_ALWAYS_INLINE
    void ResetBuffers() {
      for (unsigned i = 0; i < num_instructions; i++) {
        buf[i].next = -1;
        buf[i].r1 = UINT_MAX;
        buf[i].r2 = UINT_MAX;
      }
    }

    BITFSM_ALWAYS_INLINE
    void BeginSearch(unsigned num_instructions) {
      const unsigned num_variables = this->num_variables();
      this->num_instructions = num_instructions;
      ResetBuffers();

      // The initial unused mask has a one bit for all of the instruction slots
      // that will be generated.
      int num_unused = num_instructions - num_variables;
      FastRegMask unused = ((FastRegMask(1) << num_unused) - 1) << num_variables;
      for (unsigned i = 0; i < num_variables; i++) {
        isns[i].unused = unused;
        isns[i].carries = 0;
      }
    }

    BITFSM_ALWAYS_INLINE
    unsigned num_variables() const {
      return num_constants + num_parameters;
    }
  };

  // TODO: Document this
  // TODO: Move this to the driver
  struct ProgramPrefix {
    static constexpr unsigned kMaxLength = 3;
    Instruction isns[kMaxLength];

    ProgramPrefix(const Context& ctx, unsigned min_level, unsigned max_level) {
      unsigned count = max_level - min_level;
      memcpy(isns, ctx.isns + min_level, sizeof(isns[0]) * count);
    }
  };

  // TODO: Document this
  struct BufferArgs {
    Vec v1;
    Vec v2;
    FastOpMask op_mask;
    unsigned op_min;
    unsigned carries;
    Instruction& isn;
  };

  // TODO: Document this
  struct CheckArgs {
    Vec expected;
    Vec mask;
    Vec v1;
    Vec v2;
    unsigned r1;
    unsigned r2;
  };

  // TODO: Document this
  struct OpTable {
    struct Entry {
      const char* name;
      unsigned arity;
      bool flipped;
      bool carry;
    };

    Entry data[kNumOps];

    constexpr OpTable() {
      ((data[Ops::kCode].name = Ops::kName), ...);
      ((data[Ops::kCode].arity = Ops::kArity), ...);
      ((data[Ops::kCode].flipped = Ops::kFlipped), ...);
      ((data[Ops::kCode].carry = Ops::kCarry), ...);
    }
  };

  // The cost table maps each op code to the corresponding operation cost.
  struct CostTable {
    std::uint8_t data[kNumOps];

    constexpr CostTable() {
      ((data[Ops::kCode] = Ops::kCost), ...);
    }
  };

  // TODO: Document this
  struct PruneInfo {
    std::size_t index;
    PackedOpMask mask;
    bool may_prune;
  };

  // TODO: Document this
  struct PruneCandidate {
    XXH128_hash_t hash;
    Instruction isns[9];
    unsigned cost;
    std::uint8_t num_instructions;
    bool hashed;

    PruneCandidate(const Context& ctx) {
      num_instructions = ctx.num_instructions;
      assert(num_instructions > 0 && num_instructions <= 9);
      std::memcpy(isns, ctx.isns, sizeof(isns[0]) * num_instructions);
      cost = Cost(ctx.isns, num_instructions);
      hash = {};
      hashed = false;
    }

    bool operator<(const PruneCandidate& other) const {
      if (cost != other.cost) {
        return cost < other.cost;
      }
      return IsAfter(other.isns, other.num_instructions,
                     isns, num_instructions);
    }
  };

  constinit static inline const OpTable kOpTable;
  constinit static inline const CostTable kCostTable;

  // Returns the total cost of the program in 'isns[0...n]'.
  BITFSM_ALWAYS_INLINE
  static unsigned Cost(const Instruction* isns, unsigned n) {
    unsigned cost = 0;
    for (unsigned i = 0; i < n; i++) {
      cost += kCostTable.data[isns[i].op];
    }
    return cost;
  }

  // Evaluator<Ops...>::Eval(op, v1, v2) looks up the op code 'op' in Ops
  // and applies it, returning the result.
  //
  // This code relies on the compiler to turn the resulting chain of if-else
  // statements into a table switch on the op code.
  template<typename...>
  struct Evaluator {};

  template<typename Op, typename... Tail>
  struct Evaluator<Op, Tail...> {
    BITFSM_ALWAYS_INLINE
    static Vec Eval(unsigned op, Vec v1, Vec v2) {
      if (op == Op::kCode) {
        return Op::Apply(v1, v2);
      } else {
        return Evaluator<Tail...>::Eval(op, v1, v2);
      }
    }
  };

  template<typename Op>
  struct Evaluator<Op> {
    BITFSM_ALWAYS_INLINE
    static Vec Eval(unsigned op, Vec v1, Vec v2) {
      if (op == Op::kCode) {
        return Op::Apply(v1, v2);
      } else {
        abort();
      }
    }
  };

  // Evaluates 'isns[start:end]' using the registers 'regs' and returns the
  // output of the final instruction. 'regs' must already contain the desired
  // constant and parameter values.
  //
  // On completion, 'regs' will be overwritten with the results of the executed
  // instructions.
  BITFSM_ALWAYS_INLINE
  static Vec Eval(Instruction* isns, Vec* regs, unsigned start, unsigned end) {
    for (unsigned i = start; i < end; i++) {
      const Instruction& isn = isns[i];
      regs[i] = Evaluator<Ops...>::Eval(isn.op, regs[isn.r1], regs[isn.r2]);
    }
    return regs[end - 1];
  }

  // Rebuilds the register values in the current stack, plus all buffers.
  BITFSM_ALWAYS_INLINE
  static void RebuildRegs(Context& ctx) {
    // First, update the current register values for all instructions.
    Eval(ctx.isns, ctx.regs, ctx.num_variables(), ctx.num_instructions);

    // Then, rebuild the buffered registers for each instruction.
    const Vec* regs = ctx.regs;
    for (unsigned i = ctx.num_variables(); i < ctx.num_instructions; i++) {
      Buf& buf = ctx.buf[i];
      for (int j = 0; j <= buf.next; j++) {
        const Instruction& isn = buf.isns[j];
        buf.regs[j] = Evaluator<Ops...>::Eval(isn.op, regs[isn.r1], regs[isn.r2]);
      }
    }
  }

  BITFSM_ALWAYS_INLINE
  static void SetParameters(Context& ctx, Vec* regs, const Vec* params) {
    Vec* p = regs + ctx.num_constants;
    for (unsigned i = 0; i < ctx.num_parameters; i++) {
      p[i] = params[i];
    }
  }

  // Replaces the last instruction of the program in 'ctx' with 'op r1, r2'
  // and evaluates the resulting program on all of the inputs for 'target'.
  //
  // If the program matches the expected output for all of the inputs, and
  // the program's cost is lower than the previous best program, then the
  // program is saved.
  //
  // If the program does not match, then the expected output and inputs
  // are updated to include the failed case. After this function returns,
  // the caller must reload the expected output and any cached registers.
  BITFSM_NEVER_INLINE
  static void CheckExhaustive(Context& ctx, const TargetFunction<Vec>& target,
                              unsigned op, unsigned r1, unsigned r2) {
    Vec regs[kMaxRegs];
    unsigned num_instructions = ctx.num_instructions;
    Instruction& isn = ctx.isns[num_instructions - 1];
    isn.op = op;
    isn.r1 = r1;
    isn.r2 = r2;

    // Only evaluate programs with lower costs than the current one.
    unsigned new_cost = Cost(ctx.isns, num_instructions);
    if (ctx.best.BetterThan(new_cost, num_instructions, ctx.isns)) {
      return;
    }

    // TODO: Layout dependent!
    for (unsigned i = 0; i < ctx.num_constants; i++) {
      regs[i] = ctx.regs[i];
    }

    int mismatch = -1; // Index of first vec element which did not match, or -1.
    const std::size_t size = target.Size();
    std::size_t pos;
    for (pos = 0; pos < size; pos++) {
      SetParameters(ctx, regs, target.Parameters(pos));
      Vec actual = Eval(ctx.isns, regs, ctx.num_variables(), ctx.num_instructions);
      Vec masked = Vec::And(actual, target.Mask(pos));
      Vec expected = target.Expected(pos);
      if (!Vec::Equal(masked, expected)) {
        mismatch = Vec::Mismatch(masked, expected);
        assert(mismatch >= 0);
        break;
      }
    }

    if (mismatch < 0) {
      ctx.best.Set(new_cost, ctx.num_instructions, ctx.isns);
    } else {
      // Update the context with the failing test to reduce the chance
      // of a false-positive initial match in the future.
      unsigned dest = ctx.failed_checks++ % Vec::kNumElements;
      Vec expected = target.Expected(pos);
      ctx.expected = Vec::CopyElement(ctx.expected, dest, expected, mismatch);
      Vec mask = target.Mask(pos);
      ctx.mask = Vec::CopyElement(ctx.mask, dest, mask, mismatch);
      const Vec* params = target.Parameters(pos);
      Vec* to = ctx.regs + ctx.num_constants;
      for (unsigned i = 0; i < ctx.num_parameters; i++) {
        to[i] = Vec::CopyElement(to[i], dest, params[i], mismatch);
      }
      RebuildRegs(ctx);
    }
  }

  // Tries to generate the target by applying Op(args) to the current program.
  // If kEnable is false, then Op will not be evaluated.
  template<typename Op, bool kEnable, bool kMask>
  BITFSM_ALWAYS_INLINE
  static void Check(Context& ctx, const TargetFunction<Vec>& target,
                    CheckArgs& args) {
    if constexpr (kEnable) {
      Vec actual = Op::Apply(args.v1, args.v2);
      if constexpr (kMask) {
        actual = Vec::And(actual, args.mask);
      }
      if (Vec::Equal(actual, args.expected)) [[unlikely]] {
        CheckExhaustive(ctx, target, Op::kCode, args.r1, args.r2);
        args.expected = ctx.expected;
        args.mask = ctx.mask;
        args.v1 = ctx.regs[args.r1];
        args.v2 = ctx.regs[args.r2];
      }
    }
  }

  BITFSM_ALWAYS_INLINE
  static std::size_t PruneLeftIndex(Instruction* isns, unsigned r1,
                                    unsigned r2) {
    Instruction& i1 = isns[r1];
    unsigned r11_r2 = i1.r1 == r2;
    unsigned r12_r2 = i1.r2 == r2;
    return (i1.op << 2) | (r11_r2 << 1) | r12_r2;
  }

  static constexpr std::size_t kPruneLeftSize = kNumOps << 2;

  BITFSM_ALWAYS_INLINE
  static std::size_t PruneBothIndex(Instruction* isns, unsigned r1,
                                    unsigned r2) {
    Instruction& i1 = isns[r1];
    Instruction& i2 = isns[r2];
    std::size_t index = i1.op;
    index = (index << kOpBits) | i2.op;
    unsigned r11_r21 = i1.r1 == i2.r1;
    unsigned r11_r22 = i1.r1 == i2.r2;
    index = (index << 2) | (r11_r21 << 1) | r11_r22;
    unsigned r12_r21 = i1.r2 == i2.r1;
    unsigned r12_r22 = i1.r2 == i2.r2;
    index = (index << 2) | (r12_r21 << 1) | r12_r22;
    return index;
  }

  static constexpr std::size_t kPruneBothSize = kNumOps << (kOpBits + 4);

  BITFSM_ALWAYS_INLINE
  static FastOpMask GetOpMask(Context& ctx, unsigned op_min, unsigned r1,
                              unsigned r2) {
    FastOpMask mask = ~(FastOpMask(-1) << op_min);
    if constexpr (Prune::kEnabled) {
      // TODO: Switch this to Prune::kLeft(isns, r1, r2) and move all of
      //       the table and layout logic into the method (which is marked
      //       as BITFSM_ALWAYS_INLINE, of course.)
      //       Then, we can also eliminate kEnabled.
      mask |= Prune::kLeft[PruneLeftIndex(ctx.isns, r1, r2)];
      mask |= Prune::kBoth[PruneBothIndex(ctx.isns, r1, r2)];
    }
    return mask;
  }

  // TODO: Document this
  template<typename Op, bool kEnable>
  BITFSM_ALWAYS_INLINE
  static void Buffer(Buf& buf, int& next, BufferArgs& args) {
    if constexpr (kEnable) {
      if (!(args.op_mask & (FastOpMask(1) << Op::kCode))) [[likely]] {
        assert(Op::kCode >= args.op_min);
        next++;
        buf.regs[next] = Op::Apply(args.v1, args.v2);
        memcpy(&buf.isns[next], &args.isn, sizeof(args.isn));
        buf.isns[next].op = Op::kCode;
        if constexpr (Op::kCarry) {
          buf.isns[next].carries = args.carries + 1;
        }
      }
    }
  }

  // Advances the program in 'ctx' to the next instruction sequence by
  // updating instructions with indices in the range [min_level, max_level).
  //
  // The instruction sequence will be advanced to the next program in
  // canonical form that is known to compute a unique function.
  BITFSM_ALWAYS_INLINE
  static bool Advance(Context& ctx, unsigned min_level, unsigned max_level) {
    // Instructions representing variable references cannot be advanced.
    assert(min_level >= ctx.num_variables());
    assert(max_level <= ctx.num_instructions);

    const unsigned max_carries = ctx.max_carries;
    unsigned level = max_level - 1;
    bool refresh = false; // Is new data available in the lower instruction slot?

    while (level < max_level) {
      Buf& buf = ctx.buf[level];
      int next = buf.next;
      if (next < 0) [[unlikely]] { // The buffer is exhausted, we need to generate more data.
        const Instruction& lower = ctx.isns[level - 1];
        unsigned r1, r2, op_min;
        if (buf.r1 < level) { // Can continue at this level.
          r1 = buf.r1;
          r2 = buf.r2;
          op_min = 0;
        } else if (refresh) { // Pick up from the lower instruction.
          r1 = lower.r1;
          r2 = lower.r2;
          op_min = lower.op + 1;
          refresh = false;
        } else if (level > min_level) [[likely]] {
          if (ctx.stop_token.stop_requested()) {
            return false;
          }
          level--; // This level is exhausted, advance the lower level.
          continue;
        } else { // level == min_level
          if (buf.r1 < kMaxRegs) {
            // The buffer has already been filled once, so the
            // instruction stream has been exhausted.
            return false;
          }
          if (lower.op == kVariableOp) [[unlikely]] {
            // We're setting up to generate the first instruction.
            // Constants and parameters have r1 == r2 == level, so we must
            // start from zero instead of picking up from the lower level.
            r1 = 0;
            r2 = 0;
            op_min = 0;
          } else { // Pick up from the lower instruction.
            r1 = lower.r1;
            r2 = lower.r2;
            op_min = lower.op + 1;
          }
        }

        FastRegMask reg_mask = (FastRegMask(1) << r1) | (FastRegMask(1) << r2);
        FastRegMask unused = lower.unused & ~reg_mask;
        unsigned max_unused = 2 * (ctx.num_instructions - level);
        static_assert(std::popcount(PackedRegMask(-1)) <= 16,
                      "PopCount15() requires a 16-bit argument");
        if (PopCount15(unused) >= max_unused) {
          // There are more unused registers than can be consumed by the
          // remaining instructions, so advance r2 to the next unused register.
          // See Context::unused for more information about why this works.
          r2++;
          r2 += std::countr_zero(FastRegMask(unused >> r2));
        } else if (op_min >= kNumOps) {
          // If the minimum op exceeds the maximum op code, then it's impossible
          // to generate new instructions for this register pair.
          // (It could also cause op_min to overflow the maximum index in GetOpMask.)
          r2++;
        } else {
          // Evaluate all possible instructions for r1 and r2.
          assert(r2 <= r1 && r1 < level);
          unsigned carries = lower.carries;
          bool may_carry = carries < max_carries;
          FastOpMask op_mask = GetOpMask(ctx, op_min, r1, r2);
          Instruction isn;
          isn.unused = unused;
          isn.carries = carries;
          isn.r2 = r2;
          isn.r1 = r1;
          BufferArgs args = {ctx.regs[r1], Vec::Zero(), op_mask, op_min, carries, isn};
          if (r2 == r1) {
            (Buffer<Ops, (Ops::kArity == 1 && !Ops::kCarry)>(buf, next, args), ...);
            if (may_carry) {
              (Buffer<Ops, (Ops::kArity == 1 && Ops::kCarry)>(buf, next, args), ...);
            }
          } else if (r2 < r1) [[likely]] {
            args.v2 = ctx.regs[r2];
            (Buffer<Ops, (Ops::kArity == 2 && !Ops::kCarry)>(buf, next, args), ...);
            if (may_carry) {
              (Buffer<Ops, (Ops::kArity == 2 && Ops::kCarry)>(buf, next, args), ...);
            }
          }
          r2++;
        }

        // Save the registers to use the next time we visit this instruction.
        unsigned r2_overflow = r2 > r1;
        buf.r1 = r1 + r2_overflow;
        buf.r2 = r2_overflow ? 0 : r2;
      }
      if (next >= 0) {
        // Pop the top instruction candidate and insert it into the program.
        ctx.regs[level] = buf.regs[next];
        memcpy(&ctx.isns[level], &buf.isns[next], sizeof(ctx.isns[0]));
        next--;

        // The next instruction up the stack can be filled in now.
        level++;
        refresh = true;
      }
      buf.next = next;
    }
    return true;
  }

  // TrivialSearch checks if a function can be generated from a trivial program
  // like '0', 'x', 'y', or 'x + y'.
  //
  // This is necessary since our fast search strategy only works for programs
  // with at least two instructions. This is because the final instruction must
  // reference the results of the preceding instruction.
  BITFSM_NEVER_INLINE
  static void TrivialSearch(Context& ctx, const TargetFunction<Vec>& target) {
    assert(ctx.num_instructions == ctx.num_constants + ctx.num_parameters + 1);

    const unsigned r1_max = ctx.num_instructions - 2;
    for (unsigned r1 = 0; r1 <= r1_max; r1++) {
      CheckArgs args = {ctx.expected, ctx.mask, ctx.regs[r1], Vec::Zero(), r1, 0};
      (Check<Ops, (Ops::kArity == 1 || Ops::kCode == kVariableOp), true>(ctx, target, args), ...);
      for (unsigned r2 = 0; r2 <= r1; r2++) {
        args.r2 = r2;
        args.v2 = ctx.regs[r2];
        (Check<Ops, (Ops::kArity == 2), true>(ctx, target, args), ...);
      }
    }
  }

  // TODO: Document this
  BITFSM_ALWAYS_INLINE
  static void Restore(Context& ctx, ProgramPrefix& p, unsigned min_level,
                      unsigned max_level) {
    unsigned count = max_level - min_level;
    assert(count > 0 && count <= ProgramPrefix::kMaxLength);
    memcpy(ctx.isns + min_level, p.isns, sizeof(p.isns[0]) * count);

    // Clear the buffers to avoid reusing existing state.
    ctx.ResetBuffers();

    // Evaluate the program prefix to rebuild the register values.
    Eval(ctx.isns, ctx.regs, ctx.num_variables(), max_level);
  }

  // TODO: Document this
  template<bool kMask>
  BITFSM_NEVER_INLINE
  static void DoSearch(Context& ctx, const TargetFunction<Vec>& target,
                       unsigned min_level) {
    const unsigned max_carries = ctx.max_carries;

    // The first input to the topmost instruction is always the second
    // instruction from the top. Otherwise, the second instruction from the
    // top could be unused, and we would waste time evaluating programs
    // that are equivalent to shorter programs that were already evaluated.
    const unsigned r1 = ctx.num_instructions - 2;

    // 'unused & unused_mask' clears the status of the top two instructions
    // in 'unused'. The topmost instruction is never used, and the second
    // from the top is always used by the topmost instruction. (See r1.)
    //
    // Any unused instruction remaining after masking is genuinely unused,
    // and needs to be consumed by the topmost instruction.
    const FastRegMask unused_mask =
        ((FastRegMask(1) << (ctx.num_instructions - 2)) - 1);

    while (Advance(ctx, min_level, ctx.num_instructions - 1)) {
      const bool may_carry = ctx.isns[r1].carries < max_carries;
      FastRegMask unused = ctx.isns[r1].unused & unused_mask;
      assert(ClearLowBit(unused) == 0);
      unsigned r2 = unused ? std::countr_zero(unused) : 0;
      unsigned r2_end = unused ? (r2 + 1) : r1;
      CheckArgs args = {ctx.expected, ctx.mask, ctx.regs[r1], Vec::Zero(), r1, 0};
      // BITFSM_DO_NOT_UNROLL // TODO: Re-enable
      for (; r2 < r2_end; r2++) {
        args.r2 = r2;
        args.v2 = ctx.regs[r2];
        (Check<Ops, (Ops::kArity == 2 && !Ops::kCarry), kMask>(ctx, target, args), ...);
        if (may_carry) {
          (Check<Ops, (Ops::kArity == 2 && Ops::kCarry), kMask>(ctx, target, args), ...);
        }
      }
      // For unary operations, r2 == r1.
      args.r2 = r1;
      (Check<Ops, (Ops::kArity == 1 && !Ops::kCarry), kMask>(ctx, target, args), ...);
      if (may_carry) {
        (Check<Ops, (Ops::kArity == 1 && Ops::kCarry), kMask>(ctx, target, args), ...);
      }
    }
  }

  BITFSM_ALWAYS_INLINE
  static void Search(Context& ctx, const TargetFunction<Vec>& target,
                     unsigned min_level) {
    if (target.Masked()) {
      DoSearch<true>(ctx, target, min_level);
    } else {
      DoSearch<false>(ctx, target, min_level);
    }
  }

  // TODO: Document this
  BITFSM_NEVER_INLINE
  static void Translate(const Options& options, const Instruction* isns,
                        const unsigned num_instructions, Program& program) {
    // TODO: Factor this out, and use a separate cost type instead.
    assert(num_instructions > 0);
    const unsigned num_variables =
        options.num_constants + options.num_parameters;
    program.size = num_instructions;
    for (unsigned i = 0; i < num_instructions; i++) {
      const Instruction& isn = isns[i];
      const typename OpTable::Entry& opcode = kOpTable.data[isn.op];
      program.instructions[i].num_uses = 0;
      // TODO: Layout sensitive!
      if (i < options.num_constants) {
        assert(isn.op == kVariableOp || isn.op == kZeroOp);
        program.instructions[i].name = "const";
        program.instructions[i].num_args = 1;
        program.instructions[i].args[0] = options.constants[i];
        program.instructions[i].args[1] = -1;
      } else if (i < num_variables) {
        assert(isn.op == kVariableOp);
        program.instructions[i].name = "param";
        program.instructions[i].num_args = 1;
        program.instructions[i].args[0] = i - options.num_constants;
        program.instructions[i].args[1] = -1;
      } else if (isn.op == kVariableOp || isn.op == kZeroOp) {
        // TrivialSearch can generate constant and parameter references
        // in the final instruction slot.
        assert(i == num_instructions - 1);
        program.instructions[i] = program.instructions[isn.r1];
      } else {
        program.instructions[i].name = opcode.name;
        program.instructions[i].num_args = opcode.arity;
        unsigned char* args = program.instructions[i].args;
        program.instructions[isn.r1].num_uses++;
        if (opcode.arity > 1) {
          program.instructions[isn.r2].num_uses++;
        }
        if (opcode.flipped) {
          args[0] = isn.r2;
          args[1] = isn.r1;
        } else {
          args[0] = isn.r1;
          args[1] = isn.r2;
        }
      }
    }
    program.instructions[num_instructions - 1].num_uses = 1;
  }

  // TODO: Document this
  BITFSM_ALWAYS_INLINE
  static bool VarMatches(const PruneCandidate& alt, const PruneCandidate& elt,
                         const unsigned num_variables,
                         const unsigned alt_reg,
                         const unsigned elt_reg) {
    return (alt_reg < num_variables ?
            alt_reg == elt_reg :
            IsnMatches(alt, elt, num_variables, alt.isns[alt_reg],
                       elt.isns[elt_reg]));
  }

  // TODO: Document this
  static bool IsnMatches(const PruneCandidate& alt, const PruneCandidate& elt,
                         const unsigned num_variables,
                         const Instruction& alt_isn,
                         const Instruction& elt_isn) {
    return (alt_isn.op == elt_isn.op &&
            VarMatches(alt, elt, num_variables, alt_isn.r1, elt_isn.r2) &&
            VarMatches(alt, elt, num_variables, alt_isn.r2, elt_isn.r2));
  }

  // TODO: Document this
  static bool IsAlternative(const PruneCandidate& alt,
                            const PruneCandidate& elt,
                            const unsigned num_variables) {
    std::size_t alt_size = std::size_t(alt.num_instructions) - 1;
    std::size_t elt_size = std::size_t(elt.num_instructions) - 1;
    // To be an alternative program, each instruction except for the
    // topmost must also appear in the program it is replacing.
    for (unsigned i = num_variables; i < alt_size; i++) {
      bool found = false;
      for (unsigned j = num_variables; j < elt_size; j++) {
        if (IsnMatches(alt, elt, num_variables, alt.isns[i], elt.isns[j])) {
          found = true;
          break;
        }
      }
      if (!found) {
        return false;
      }
    }
    return true;
  }

  struct Xxh3Deleter {
    void operator()(XXH3_state_t* s) {
      XXH3_freeState(s);
    }
  };

  // TODO: Document this
  static XXH128_hash_t HashProgram(PruneCandidate& candidate,
                                   Context& ctx,
                                   const TargetFunction<Vec>& target) {
    if (candidate.hashed) {
      return candidate.hash;
    }
    std::unique_ptr<XXH3_state_t, Xxh3Deleter> hash(XXH3_createState());
    if (hash == nullptr || XXH3_128bits_reset(hash.get()) == XXH_ERROR) {
      abort();
    }
    const std::size_t size = target.Size();
    for (std::size_t i = 0; i < size; i++) {
      // TODO: Just using ctx.regs and ctx.num_variables() is weird!
      SetParameters(ctx, ctx.regs, target.Parameters(i));
      Vec v = Eval(candidate.isns, ctx.regs, ctx.num_variables(),
                   candidate.num_instructions);
      if (XXH3_128bits_update(hash.get(), &v, sizeof(v)) == XXH_ERROR) {
        abort();
      }
    }
    candidate.hashed = true;
    candidate.hash = XXH3_128bits_digest(hash.get());
    return candidate.hash;
  }

  // TODO: Document this
  BITFSM_NEVER_INLINE
  static void FindPrunable(unsigned num_operations, unsigned num_parameters,
                           PackedOpMask* table, std::size_t table_size,
                           std::function<PruneInfo(const Context&,Instruction*,unsigned)> prune_info) {
    std::unordered_map<Vec, std::vector<PruneCandidate>> candidates{};
    const std::uint8_t constants[] = {0};
    const Options options = {
      .func = [] (std::uint8_t*, std::uint8_t*) {
        return std::uint8_t(0);
      },
      .progress = nullptr,
      .progress_data = nullptr,
      .instruction_set = BITFSM_OPT_ALL_ISNS,
      .constants = constants,
      .num_constants = 1,
      .num_parameters = num_parameters,
      .max_carries = num_operations,
      .max_operations = num_operations,
      .max_sequential_size = 0,
    };
    TargetFunction<Vec> target(options);
    Context ctx(options, target, std::stop_token{});
    unsigned num_variables = options.num_constants + options.num_parameters;
    for (unsigned i = 0; i < num_variables; i++) {
      ctx.BeginSearch(num_variables + 1);
      Instruction& isn = ctx.isns[num_variables];
      isn.carries = 0;
      isn.op = kVariableOp;
      isn.r1 = i;
      isn.r2 = i;
      Eval(ctx.isns, ctx.regs, ctx.num_variables(), ctx.num_instructions);
      Vec fingerprint = ctx.regs[num_variables];
      std::get<0>(candidates.try_emplace(fingerprint))->second.emplace_back(ctx);
    }
    for (unsigned i = 1; i <= num_operations; i++) {
      unsigned num_instructions = num_variables + i;
      ctx.BeginSearch(num_instructions);
      const unsigned top = num_instructions - 1;
      while (Advance(ctx, num_variables, num_instructions)) {
        if (ClearLowBit(ctx.isns[top].unused) != 0) {
          // Don't bother adding instruction sequences that aren't minimal,
          // since they correspond to a shorter sequence.
          // (The top instruction's unused bit is always set.)
          continue;
        }
        std::get<0>(candidates.try_emplace(ctx.regs[top]))->second.emplace_back(ctx);
      }
    }
    std::vector<PackedOpMask> visited(table_size, PackedOpMask(0));
    for (auto it = candidates.begin(); it != candidates.end(); it++) {
      std::vector<PruneCandidate>& bin = it->second;
      std::sort(bin.begin(), bin.end());
      for (auto elt = bin.begin(); elt != bin.end(); elt++) {
        PruneInfo info = prune_info(ctx, elt->isns, elt->num_instructions);
        if (info.may_prune && (visited[info.index] & info.mask) == 0) {
          visited[info.index] |= info.mask;
          for (auto alt = bin.begin(); alt != elt; alt++) {
            PruneInfo alt_info =
                prune_info(ctx, alt->isns, alt->num_instructions);
            bool alt_pruned =
                alt_info.may_prune && (table[alt_info.index] & alt_info.mask);
            if (!alt_pruned &&
                IsAlternative(*alt, *elt, num_variables) &&
                XXH128_isEqual(HashProgram(*elt, ctx, target),
                               HashProgram(*alt, ctx, target))) {
              table[info.index] |= info.mask;
              break;
            }
          }
        }
      }
    }
  }

  static bool IsParameter(const Context& ctx, unsigned r) {
    return r >= ctx.num_constants && r < ctx.num_variables();
  }

  static bool IsRightLeaf(const Context& ctx, const Instruction& isn) {
    return isn.op >= kStartOp && IsParameter(ctx, isn.r2);
  }

  static bool IsLeaf(const Context& ctx, const Instruction& isn) {
    return (isn.op >= kStartOp &&
            IsParameter(ctx, isn.r1) &&
            IsParameter(ctx, isn.r2));
  }

  static bool IsLeafOrZero(const Context& ctx, const Instruction& isn) {
    return isn.op == kZeroOp || IsLeaf(ctx, isn);
  }

  static bool IsUnary(const Instruction& isn) {
    return kOpTable.data[isn.op].arity == 1;
  }

  // TODO: Document this
  static std::array<PackedOpMask, kPruneLeftSize> FindPruneLeft() {
    const unsigned num_operations = 2;
    const unsigned num_parameters = 3;

    std::array<PackedOpMask, kPruneLeftSize> table{};
    FindPrunable(
        num_operations,
        num_parameters,
        table.data(),
        table.size(),
        [](const Context& ctx, Instruction* isns, unsigned n) {
          // Program must match '(r11 op1 r12) op r2', where rXY is a parameter.
          PruneInfo info{};
          info.may_prune = (n == ctx.num_variables() + 2 &&
                            isns[n - 1].r1 == ctx.num_variables() &&
                            IsRightLeaf(ctx, isns[n - 1]) &&
                            IsLeaf(ctx, isns[n - 2]));
          if (info.may_prune) {
            Instruction& top = isns[n - 1];
            unsigned r1 = top.r1;
            unsigned r2 = top.r2;
            info.mask = PackedOpMask(1) << top.op;
            info.index = PruneLeftIndex(isns, r1, r2);
          }
          return info;
        });

    return table;
  }

  // TODO: Document this
  static std::array<PackedOpMask, kPruneBothSize> FindPruneBoth() {
    const unsigned num_operations = 3;
    const unsigned num_parameters = 3;

    std::array<PackedOpMask, kPruneBothSize> table{};
    FindPrunable(
        num_operations,
        num_parameters,
        table.data(),
        table.size(),
        [](const Context& ctx, Instruction* isns, unsigned n) {
          // Program must match '(r11 op1 r12) op (r21 op2 r22)',
          // where rXY is a parameter.
          PruneInfo info{};
          Instruction& top = isns[n - 1];
          info.may_prune = (top.op >= kStartOp &&
                            IsLeafOrZero(ctx, isns[top.r1]) &&
                            IsLeafOrZero(ctx, isns[top.r2]));
          if (info.may_prune) {
            Instruction& top = isns[n - 1];
            unsigned r1 = top.r1;
            unsigned r2 = top.r2;
            info.index = PruneBothIndex(isns, r1, r2);
            info.mask = PackedOpMask(1) << top.op;
          }
          return info;
        });

    return table;
  }
};

// Pruning table which does not eliminate any subexpressions.
//
// The 'PruneTable' template uses this no-op table in order to bootstrap
// itself during initialization.
struct PruneNone {
  static constexpr bool kEnabled = false;
  static constexpr unsigned kLeft[1] = {0};
  static constexpr unsigned kBoth[1] = {0};
};

// Pruning table constructed using the vector type 'Vec' for the op codes 'Ops'.
template<typename Vec, typename Ops>
struct PruneTable {
  using OptBase = Optimizer<Vec, PruneNone, Ops>;
  static constexpr bool kEnabled = true;
  static const inline std::array kLeft = OptBase::FindPruneLeft();
  static const inline std::array kBoth = OptBase::FindPruneBoth();
};

enum class ResultCode {
  kFound = BITFSM_OPT_FOUND,
  kNotFound = BITFSM_OPT_NOT_FOUND,
  kInvalid = BITFSM_OPT_INVALID,
};

// TODO: Need to turn this into a class so we can factor out methods.
// TODO: Apply constraints to all of these template parameters!
template<typename Vec, typename Operations>
BITFSM_NEVER_INLINE
static ResultCode Optimize(const Options& options, Program& program,
                           std::stop_token stop_token = {}) {
  // TODO: Optimize this method for size. Why is it huge? Translate?
  using Ops = OpCodes<Operations>::type;
  using Opt = Optimizer<Vec, PruneTable<Vec, Ops>, Ops>;
  using Context = Opt::Context;
  using ProgramCandidate = Opt::ProgramCandidate;
  using ProgramPrefix = Opt::ProgramPrefix;

  if (options.func == nullptr) {
    return ResultCode::kInvalid;
  } else if (options.constants == nullptr && options.num_constants > 0) {
    return ResultCode::kInvalid;
  } else if (options.num_constants > kMaxConstants) {
    return ResultCode::kInvalid;
  } else if (options.num_parameters > kMaxParameters) {
    return ResultCode::kInvalid;
  } else if (options.max_carries > kMaxCarries) {
    return ResultCode::kInvalid;
  } else if (options.max_operations > kMaxOperations) {
    return ResultCode::kInvalid;
  }

  const unsigned num_variables = options.num_constants + options.num_parameters;
  const TargetFunction<Vec> target(options);
  Context ctx(options, target, stop_token);
  ctx.BeginSearch(num_variables + 1);
  Opt::TrivialSearch(ctx, target);
  const unsigned max_instructions = num_variables + options.max_operations;
  const unsigned options_max_sequential_size =
      options.max_sequential_size == 0 ? 7 : options.max_sequential_size;
  const unsigned max_sequential_size =
      std::min(std::max(options_max_sequential_size,
                        num_variables + ProgramPrefix::kMaxLength),
               max_instructions);
  const unsigned min_level = ctx.num_variables();

  // TODO: Document this
  struct Worker {
    // Optimizer context. Owned and accessed exclusively by the worker.
    Context context;

    // Mutex that guards access to 'best'.
    //
    // The worker must hold the mutex while writing, while the progress task
    // must hold the mutex while reading.
    std::mutex best_mutex;

    // The best program discovered so far by this worker.
    ProgramCandidate best;

    // Number of work items (program prefixes) consumed by the worker.
    std::atomic<std::size_t> consumed;

    // Latest version of 'best' read by the progress task.
    // Read and written exclusively by the progress task.
    std::uint64_t progress_version;
  };
  unsigned num_instructions = ctx.num_instructions + 1;
  for (; num_instructions <= std::min(max_sequential_size, // TODO: Ugly!
                                      ctx.best.cost() + num_variables) &&
         !stop_token.stop_requested();
       num_instructions++) {
    ctx.BeginSearch(num_instructions);
    Opt::Search(ctx, target, ctx.num_variables());
  }

  if (num_instructions > std::min(max_instructions,
                                  ctx.best.cost() + num_variables) ||
      stop_token.stop_requested()) {
    // TODO: Factor this out.
    if (ctx.best.size() != 0) {
      Opt::Translate(options, ctx.best.isns(), ctx.best.size(), program);
    }
    if (options.progress != nullptr) {
      const unsigned num_operations = num_instructions - num_variables;
      options.progress(options.progress_data, num_operations - 1, 1.0f,
                       &program, ctx.best.version());
    }
    return ctx.best.size() == 0 ? ResultCode::kNotFound : ResultCode::kFound;
  }

  tf::Executor executor;
  tf::Taskflow taskflow;
  std::vector<Worker> workers(executor.num_workers());
  std::vector<ProgramPrefix> prefixes{};
  for (auto it = workers.begin(); it != workers.end(); it++) {
    // TODO: Is there a way to avoid default-initializing Context?
    it->context = ctx;
    it->best = ctx.best;
  }

  // The best candidate found so far. This variable is owned by progress_thread
  // until it returns, and must not be read or written by any other thread in
  // that time period.
  //
  // Note: 'best' is default-initialized so that the first iteration of the
  //       progress thread (or the final synchronous update) is guaranteed to
  //       print value of 'ctx.best', if one exists. If 'best' were initialized
  //       to 'ctx.best', then this value would never be printed.
  ProgramCandidate best;
  std::timed_mutex done;
  std::unique_lock done_lock(done);
  std::jthread progress_thread;
  std::atomic<unsigned> num_instructions_shared = num_instructions;
  std::atomic<std::size_t> num_prefixes_shared = 1;

  if (options.progress != nullptr) {
    std::jthread thread([&] () {
      Program progress_program;
      std::chrono::seconds update_interval{1};
      do {
        std::uint64_t old_version = best.version();
        std::size_t consumed = 0;
        for (auto it = workers.begin(); it != workers.end(); it++) {
          consumed += it->consumed.load(std::memory_order_relaxed);
          std::scoped_lock lock(it->best_mutex);
          if (it->best.version() != it->progress_version) {
            best.Merge(it->best);
            it->progress_version = it->best.version();
          }
        }
        if (best.version() != old_version) {
          Opt::Translate(options, best.isns(), best.size(), progress_program);
        }
        unsigned num_instructions =
            num_instructions_shared.load(std::memory_order_relaxed);
        unsigned level = num_instructions - num_variables;
        float frac = consumed /
            (float) num_prefixes_shared.load(std::memory_order_relaxed);
        options.progress(options.progress_data, level, frac, &progress_program,
                         best.version());
      } while (!done.try_lock_for(update_interval));
    });
    progress_thread = std::move(thread);
  }

  // It's hard to calculate the right prefix length to maximize parallelism,
  // without lopsided scheduling. Instead of trying, we simply aim to have
  // an order of magnitude more tasks than workers, and loop until we either
  // have enough tasks, or we can't make the prefix any longer without having
  // too little to do in each task.
  //
  // To avoid repeating the same check for each successive instruction length,
  // we reuse prefix_max_level between iterations.
  const std::size_t target_prefix_count = 20 * executor.num_workers();
  unsigned prefix_max_level = min_level + 1;
  unsigned min_cost = ctx.best.cost();
  for (; num_instructions <= std::min(max_instructions,
                                      min_cost + num_variables) &&
         !stop_token.stop_requested();
       num_instructions++) {
    num_instructions_shared.store(num_instructions, std::memory_order_relaxed);
    const unsigned prefix_max_level_max =
        std::min(min_level + ProgramPrefix::kMaxLength,
                 // Generate >1 instruction per task to minimize overhead.
                 num_instructions - 2);
    assert(prefix_max_level <= prefix_max_level_max);
    while (1) {
      taskflow.clear();
      prefixes.clear();
      ctx.BeginSearch(num_instructions);
      while (Opt::Advance(ctx, min_level, prefix_max_level)) {
        std::size_t prefix_index = prefixes.size();
        prefixes.emplace_back(ctx, min_level, prefix_max_level);
        taskflow.emplace([&workers, &executor, &prefixes, &target,
                          prefix_index, min_level, prefix_max_level] () {
          Worker& w = workers[executor.this_worker_id()];
          ProgramPrefix& prefix = prefixes[prefix_index];
          std::uint64_t old_version = w.context.best.version();
          Opt::Restore(w.context, prefix, min_level, prefix_max_level);
          Opt::Search(w.context, target, prefix_max_level);
          if (w.context.best.version() != old_version) {
            std::scoped_lock lock(w.best_mutex);
            w.best = w.context.best;
          }
          w.consumed.store(1 + w.consumed.load(std::memory_order_relaxed),
                           std::memory_order_relaxed);
        });
      }
      if (prefixes.size() >= target_prefix_count ||
          prefix_max_level == prefix_max_level_max) {
        break;
      }
      prefix_max_level++;
    }
    num_prefixes_shared.store(prefixes.size(), std::memory_order_relaxed);
    for (auto it = workers.begin(); it != workers.end(); it++) {
      it->context.BeginSearch(num_instructions);
      it->consumed.store(0, std::memory_order_relaxed);
    }
    tf::Future<void> future = executor.run(taskflow);
    std::stop_callback stop(stop_token, [&] { future.cancel(); });
    future.wait();
    for (auto it = workers.begin(); it != workers.end(); it++) {
      min_cost = std::min(min_cost, it->context.best.cost());
    }
  }

  // Let the progress thread terminate.
  done_lock.unlock();

  // Wait for the progress thread to complete. This is required for it to
  // surrender ownership of 'best' so that the progress callback can be
  // triggered synchronously with the final results.
  if (progress_thread.joinable()) {
    progress_thread.join();
  }
  for (auto it = workers.begin(); it != workers.end(); it++) {
    best.Merge(it->context.best);
  }
  if (best.size() != 0) {
    Opt::Translate(options, best.isns(), best.size(), program);
  }
  if (options.progress != nullptr) {
    const unsigned num_operations = num_instructions - num_variables;
    options.progress(options.progress_data, num_operations - 1, 1.0f,
                     &program, best.version());
  }
  return best.size() == 0 ? ResultCode::kNotFound : ResultCode::kFound;
}

} // namespace bitfsm_opt

#endif // BITFSM_OPT_OPTIMIZE_H_
