from typing import Annotated, Any, Dict, List, Literal, Optional, Union

from pydantic import BaseModel, Field, StringConstraints

# This is necessary because pydantic prefers Annotated types outside classes
IDsStringPattern = Annotated[str, StringConstraints(pattern="^[0-9a-z_]{1,32}$")]

# ----------------------------------------------------------------------------
# Schemas for manifest.yaml (Embedded in Test Containers)
# ----------------------------------------------------------------------------


class SystemInput(BaseModel):
    """Defines a system input that the container requires."""

    name: str = Field(
        ...,
        description="The system input name, e.g., 'system_under_test', 'simulator_system', 'evaluator_system'.",
    )
    type: str = Field(
        ...,
        description="The system type, e.g., 'llm_api','rest_api', 'rag_api', 'image_generation_api', 'image_editing_api', or 'vlm_api'.",
    )
    required: bool = Field(True, description="Whether this system input is required.")
    description: Optional[str] = Field(
        None, description="Description of the system's role in the test."
    )


class InputParameter(BaseModel):
    """Defines a parameter that can be passed to the test container."""

    name: str
    type: Literal["string", "integer", "float", "boolean", "list", "object"]
    required: bool = False
    description: Optional[str] = None


class OutputMetric(BaseModel):
    """Defines a metric that will be present in the test container's output."""

    name: str
    type: Literal["string", "integer", "float", "boolean", "list", "object"]
    description: Optional[str] = None


class OutputArtifact(BaseModel):
    """Defines a file artifact generated by the test container."""

    name: str
    path: str
    description: Optional[str] = None


class EnvironmentVariable(BaseModel):
    """Defines an environment variable required by the test container."""

    name: str = Field(
        ...,
        description="Environment variable name (e.g., 'OPENAI_API_KEY', 'HF_TOKEN').",
    )
    required: bool = Field(
        True,
        description="Whether this environment variable is mandatory for execution.",
    )
    description: Optional[str] = Field(
        None, description="Explanation of what this variable is used for."
    )


class Manifest(BaseModel):
    """Schema for the manifest.yaml file inside a test container."""

    name: str = Field(..., description="The canonical name for the test framework.")
    version: str
    description: Optional[str] = None
    host_access: bool = Field(
        False,
        description="Whether the container requires host access (e.g., for Docker-in-Docker).",
    )
    input_systems: List[SystemInput] = Field(
        ...,
        description="Systems required as input. Should minimally include a system_under_test",
    )
    input_schema: List[InputParameter] = Field(
        [], description="Defines the schema for the user-provided 'params' object."
    )
    output_metrics: Union[List[str], List[OutputMetric]] = Field(
        [],
        description="Defines expected high-level metrics in the final JSON output. Can be a simple list of strings or detailed metric definitions.",
    )
    output_artifacts: Optional[List[OutputArtifact]] = None
    environment_variables: List[EnvironmentVariable] = Field(
        [],
        description="Environment variables required by this test container. Used for validation and documentation.",
    )


# ----------------------------------------------------------------------------
# Schema for systems.yaml (User-provided)
# ----------------------------------------------------------------------------


class SystemDefinition(BaseModel):
    """Base system definition."""

    description: Optional[str] = Field(
        None,
        description="Description of the system being evaluated.",
    )

    provider: Optional[str] = Field(
        None,
        description="Name of the provider of the system, either 'custom' for internal systems or 'openai, aws-bedrock...' for external systems.",
    )


# LLM API system


class LLMAPIParams(BaseModel):
    """Parameters for the LLM API systems."""

    base_url: str = Field(
        ...,
        description="Base URL for the OpenAI-compatible API (e.g., 'http://localhost:4000/v1', 'https://api.openai.com/v1')",
    )
    model: str = Field(
        ...,
        description="Model name to use with the API",
    )
    env_file: Optional[str] = Field(
        None,
        description="Path to .env file containing environment variables for authentication",
    )
    api_key: Optional[str] = Field(
        None,
        description="Direct API key for authentication (alternative to env_file)",
    )


class VLMAPIParams(LLMAPIParams):
    """Parameters for Vision Language Model API systems."""

    supports_vision: Literal[True] = Field(
        True,
        description="Whether the VLM system supports vision. Forced to True for vlm_api type.",
    )


class LLMAPIConfig(SystemDefinition):
    """Configuration for LLM API systems."""

    type: Literal["llm_api"] = Field(
        ...,
        description="LLM API system: llm_api",
    )
    params: LLMAPIParams = Field(
        ...,
        description="Parameters specific to the LLM API system (e.g., base url, model name, API key and env file).",
    )


# RAG API system


class RAGAPIConfig(SystemDefinition):
    """Configuration for RAG API systems."""

    type: Literal["rag_api"] = Field(
        ...,
        description="RAG API system: rag_api",
    )
    params: LLMAPIParams = Field(
        ...,
        description="Parameters specific to the RAG API system (e.g., base url, model name, API key and env file).",
    )


# Image Generation API system


class ImageGenerationAPIConfig(SystemDefinition):
    """Configuration for Image Generation API systems."""

    type: Literal["image_generation_api"] = Field(
        ...,
        description="Image Generation API system: image_generation_api",
    )
    params: LLMAPIParams = Field(
        ...,
        description="Parameters specific to the Image Generation API system (e.g., base url, model name, API key and env file).",
    )


# Image Editing API system


class ImageEditingAPIConfig(SystemDefinition):
    """Configuration for Image Editing API systems."""

    type: Literal["image_editing_api"] = Field(
        ...,
        description="Image Editing API system: image_editing_api",
    )
    params: LLMAPIParams = Field(
        ...,
        description="Parameters specific to the Image Editing API system (e.g., base url, model name, API key and env file).",
    )


# VLM API system


class VLMAPIConfig(SystemDefinition):
    """Configuration for Vision Language Model API systems."""

    type: Literal["vlm_api"] = Field(
        ...,
        description="Vision Language Model API system: vlm_api",
    )
    params: VLMAPIParams = Field(
        ...,
        description="Parameters specific to the VLM API system (e.g., base url, model name, API key, env file, and vision support).",
    )


# Generic system


class GenericSystemConfig(SystemDefinition):
    """Generic system configuration for system types without specific validation.

    This allows backward compatibility and support for system types that don't have dedicated config classes yet.
    """

    type: str = Field(
        ...,
        description="System type, e.g., 'rest_api', 'custom_api', etc.",
    )
    params: Dict[str, Any] = Field(
        ...,
        description="Parameters specific to the system type.",
    )


SystemConfig = Union[
    LLMAPIConfig,
    RAGAPIConfig,
    ImageGenerationAPIConfig,
    ImageEditingAPIConfig,
    VLMAPIConfig,
    GenericSystemConfig,
]


class SystemsConfig(BaseModel):
    """Schema for the top-level systems configuration file.

    Extension Guide:
        1. Create a new XXXConfig class inheriting from SystemDefinition. e.g, RESTAPIConfig
        2. Create a new XXXParam class for the parameters of the system. e.g. RESTAPIParams
        2. Add the new system definition (XXXConfig) to the SystemConfig union type
            e.g. SystemConfig = Union[LLMAPIConfig, XXXConfig, ..., GenericSystemConfig]

    """

    systems: Dict[str, SystemConfig] = Field(
        ..., description="Dictionary of system definitions."
    )


# ----------------------------------------------------------------------------
# Schema for test_suite.yaml (User-provided)
# ----------------------------------------------------------------------------


class TestDefinitionBase(BaseModel):
    """Base class for test configuration fields shared between TestDefinition and TestSuiteDefault."""

    systems_under_test: Optional[List[str]] = Field(
        None,
        description="A list of system names (from systems.yaml) to run this test against. Can be inherited from test_suite_default.",
    )
    systems: Optional[Dict[str, str]] = Field(
        None,
        description="Optional additional systems for the test (e.g., simulator_system, evaluator_system).",
    )
    tags: Optional[List[str]] = Field(
        None, description="Optional tags for filtering and reporting."
    )
    params: Optional[Dict[str, Any]] = Field(
        None, description="Parameters to be passed to the test container's entrypoint."
    )
    volumes: Optional[Dict[str, Any]] = Field(
        None, description="Optional input/output mounts."
    )
    env_file: Optional[str] = Field(
        None,
        description="Path to .env file containing environment variables for this test's container execution (e.g., '.env', 'test.env').",
    )
    environment: Optional[Dict[str, str]] = Field(
        None,
        description="Dictionary of environment variables to pass to the test container. Supports interpolation syntax (e.g., {'OPENAI_API_KEY': '${OPENAI_API_KEY}'}).",
    )


class TestDefinition(TestDefinitionBase):
    """A single test to be executed."""

    id: IDsStringPattern = Field(
        ...,
        description="A unique, human-readable ID (up to 32 characters) for this test instance. Can include lowercase letters (a–z), digits (0–9) and underscore (_).",
    )
    name: Optional[str] = Field(
        None,
        description="A descriptive, human-friendly name for this test instance.",
    )
    description: Optional[str] = Field(
        None,
        description="A short summary of the purpose of the test and what it aims to validate.",
    )
    image: str = Field(
        ...,
        description="The Docker image to run for this test, e.g., 'my-registry/garak:latest'.",
    )


class TestSuiteDefault(TestDefinitionBase):
    """Default values that apply to all tests in the suite unless overridden."""

    pass


class SuiteConfig(BaseModel):
    """Schema for the top-level Test Suite configuration file."""

    suite_name: str = Field(..., description="Name of this test suite.")
    test_suite_default: Optional[TestSuiteDefault] = Field(
        None,
        description="Default values that apply to all tests in the suite unless overridden",
    )
    description: Optional[str] = Field(
        None,
        description="A short summary of the test suite and what it aims to evaluate.",
    )
    test_suite: List[TestDefinition] = Field(
        ..., description="List of individual focused tests."
    )


# ----------------------------------------------------------------------------
# Schema for Score Card (User-provided)
# ----------------------------------------------------------------------------


class ScoreCardFilter(BaseModel):
    """Defines which test results an indicator applies to."""

    test_id: str = Field(
        ...,
        description="Test id to filter by, e.g., 'run_mock_on_compatible_sut'",
    )


class AssessmentRule(BaseModel):
    """Individual assessment outcome with condition."""

    outcome: str = Field(
        ..., description="Assessment outcome, e.g., 'PASS', 'FAIL', 'A', 'F'"
    )
    condition: Literal[
        "equal_to",
        "greater_than",
        "less_than",
        "greater_equal",
        "less_equal",
        "all_true",
        "any_false",
        "count_equals",
    ] = Field(..., description="Condition to evaluate against the metric value")
    threshold: Optional[Union[int, float, bool]] = Field(
        None, description="Threshold value for comparison conditions"
    )
    description: Optional[str] = Field(
        None, description="Human-readable description for this assessment outcome"
    )


class MetricExpression(BaseModel):
    """Expression-based metric evaluation with explicit value declarations."""

    expression: str = Field(
        ...,
        description="Mathematical formula combining metrics. Variable names in the expression must match keys in the 'values' dict.",
    )
    values: Dict[str, str] = Field(
        ...,
        description="Mapping from expression variable names to metric paths. Keys are used in the expression, values are paths to extract from test results.",
    )


class ScoreCardIndicator(BaseModel):
    """Individual score card indicator with filtering and assessment."""

    id: IDsStringPattern = Field(
        ...,
        description="A unique, human-readable ID (up to 32 characters) for this score card indicator. Can include lowercase letters (a–z), digits (0–9) and underscore (_).",
    )
    name: Optional[str] = Field(
        None, description="Human-readable name for this score card indicator"
    )
    apply_to: ScoreCardFilter = Field(
        ...,
        description="Filter criteria for which test results this indicator applies to",
    )
    metric: Union[str, MetricExpression] = Field(
        ...,
        description=(
            "Metric to evaluate. Can be either:\n"
            "1. Simple path (string): 'average_answer_relevance' or 'stats.pass_rate'\n"
            "2. Expression object with 'expression' and 'values' fields:\n"
            "   metric:\n"
            "     expression: '0.7 * accuracy + 0.3 * relevance'\n"
            "     values:\n"
            "       accuracy: 'average_answer_accuracy'\n"
            "       relevance: 'average_answer_relevance'\n"
            "   Supports arithmetic operators (+, -, *, /), functions (min, max, avg), and parentheses.\n"
            "   Variable names in expression are mapped to metric paths via the 'values' dict."
        ),
    )
    assessment: List[AssessmentRule] = Field(
        ..., description="List of assessment rules to evaluate against the metric"
    )


# ----------------------------------------------------------------------------
# Schema for Score Card with Audit Indicators (User-provided)
# ----------------------------------------------------------------------------


class AuditAssessmentRule(BaseModel):
    """Assessment outcome for audit indicators."""

    outcome: str = Field(
        ..., description="Assessment outcome, e.g., 'A', 'B', 'C', 'PASS', 'FAIL'."
    )
    description: Optional[str] = Field(
        None,
        description="Human-readable description for this audit outcome",
    )


class AuditScoreCardIndicator(BaseModel):
    """Manual audit indicator.

    Outcome is provided via an external audit responses file.
    """

    id: IDsStringPattern = Field(
        ...,
        description=(
            "A unique, human-readable ID (up to 32 characters) for this audit "
            "indicator. Can include lowercase letters (a–z), digits (0–9) and underscore (_)."
        ),
    )
    type: Literal["audit"] = Field(
        "audit",
        description="Indicator type. Must be 'audit' for manual audit indicators.",
    )
    name: Optional[str] = Field(
        None,
        description="Human-readable name for this audit indicator",
    )
    assessment: List[AuditAssessmentRule] = Field(
        ...,
        description="List of possible audit outcomes (A–E, PASS/FAIL, etc.).",
    )


class ScoreCard(BaseModel):
    """Complete grading score card configuration."""

    score_card_name: str = Field(..., description="Name of the grading score card")
    indicators: List[Union[ScoreCardIndicator, AuditScoreCardIndicator]] = Field(
        ...,
        description="List of score card indicators to evaluate (non-audit and audit).",
    )


# ----------------------------------------------------------------------------
# Schema for Audit Response (User-provided)
# ----------------------------------------------------------------------------
class AuditResponse(BaseModel):
    indicator_id: str = Field(..., description="ID of the audit indicator")
    sut_name: Optional[str] = Field(
        None,
        description="Name of the system under test this response applies to. If omitted, applies globally.",
    )
    selected_outcome: str = Field(
        ..., description="Letter grade or label (A–E, PASS/FAIL, etc.)."
    )
    notes: Optional[str] = Field(None, description="Optional free text notes")


class AuditResponses(BaseModel):
    responses: List[AuditResponse]
