"""LLM-Assisted DICOM Fuzzing Module.

This module provides intelligent fuzzing capabilities using Large Language Models
to generate protocol-compliant test cases and analyze DICOM specifications.

Features:
- DICOM specification parsing and rule extraction
- Intelligent mutation generation based on protocol semantics
- Anomaly detection using LLM reasoning
- Protocol state machine inference
- Test case prioritization based on coverage predictions

Based on research from:
- "NetworkFuzzer" (ARES 2025) - Response-aware network fuzzing
- "LLM-Guided Protocol Conformance Testing" (IEEE S&P 2024)
- DICOM PS3.7 Message Exchange specification

Note: This module requires an LLM backend (local or API-based).
Supported backends:
- OpenAI API (GPT-4, GPT-4o)
- Anthropic API (Claude)
- Local Ollama (llama3, mixtral, etc.)
- Azure OpenAI

"""

from __future__ import annotations

import json
import logging
import os
import re
import threading
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any

logger = logging.getLogger(__name__)


class LLMBackend(Enum):
    """Supported LLM backend types."""

    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    OLLAMA = "ollama"
    AZURE_OPENAI = "azure_openai"
    MOCK = "mock"  # For testing without API calls


class MutationCategory(Enum):
    """Categories of mutations that can be generated."""

    BOUNDARY_VALUE = "boundary_value"
    FORMAT_STRING = "format_string"
    TYPE_CONFUSION = "type_confusion"
    LENGTH_MANIPULATION = "length_manipulation"
    ENCODING_ERROR = "encoding_error"
    SEMANTIC_VIOLATION = "semantic_violation"
    STATE_MACHINE = "state_machine"
    PROTOCOL_SPECIFIC = "protocol_specific"


@dataclass
class DICOMProtocolRule:
    """A rule extracted from DICOM specification.

    Attributes:
        rule_id: Unique identifier for the rule
        description: Human-readable description
        element_type: Type of DICOM element (tag, VR, sequence, etc.)
        constraint: Constraint specification (range, pattern, enum, etc.)
        mandatory: Whether this is a mandatory field
        iod_module: IOD module this rule belongs to
        security_relevant: Whether violation could be security-relevant

    """

    rule_id: str
    description: str
    element_type: str
    constraint: dict[str, Any]
    mandatory: bool = False
    iod_module: str = ""
    security_relevant: bool = False
    source: str = ""  # Reference to DICOM standard section

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "rule_id": self.rule_id,
            "description": self.description,
            "element_type": self.element_type,
            "constraint": self.constraint,
            "mandatory": self.mandatory,
            "iod_module": self.iod_module,
            "security_relevant": self.security_relevant,
            "source": self.source,
        }


@dataclass
class GeneratedMutation:
    """A mutation generated by the LLM.

    Attributes:
        mutation_id: Unique identifier
        category: Mutation category
        target_element: DICOM element being mutated
        original_value: Original value (if applicable)
        mutated_value: Mutated value
        rationale: LLM's reasoning for this mutation
        expected_behavior: What behavior might be triggered
        priority: Estimated priority (0-100)
        cve_reference: Related CVE if applicable

    """

    mutation_id: str
    category: MutationCategory
    target_element: str
    original_value: Any
    mutated_value: Any
    rationale: str
    expected_behavior: str
    priority: int = 50
    cve_reference: str = ""

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "mutation_id": self.mutation_id,
            "category": self.category.value,
            "target_element": self.target_element,
            "original_value": str(self.original_value),
            "mutated_value": str(self.mutated_value),
            "rationale": self.rationale,
            "expected_behavior": self.expected_behavior,
            "priority": self.priority,
            "cve_reference": self.cve_reference,
        }


@dataclass
class LLMFuzzerConfig:
    """Configuration for LLM-assisted fuzzing.

    Attributes:
        backend: LLM backend to use
        model: Model name/identifier
        api_key: API key (from env if not provided)
        api_base: API base URL (for self-hosted)
        temperature: Sampling temperature
        max_tokens: Maximum tokens per response
        timeout: Request timeout in seconds
        cache_responses: Whether to cache LLM responses
        cache_dir: Directory for caching
        rules_file: Path to extracted rules JSON

    """

    backend: LLMBackend = LLMBackend.MOCK
    model: str = "gpt-4"
    api_key: str = ""
    api_base: str = ""
    temperature: float = 0.7
    max_tokens: int = 2000
    timeout: float = 30.0
    cache_responses: bool = True
    cache_dir: Path = field(default_factory=lambda: Path(".llm_cache"))
    rules_file: Path | None = None


class LLMClient(ABC):
    """Abstract base class for LLM clients."""

    @abstractmethod
    def complete(self, prompt: str, system: str = "") -> str:
        """Generate a completion from the LLM.

        Args:
            prompt: User prompt
            system: System prompt

        Returns:
            Generated text

        """

    @abstractmethod
    def is_available(self) -> bool:
        """Check if the LLM backend is available."""


class MockLLMClient(LLMClient):
    """Mock LLM client for testing without API calls."""

    def complete(self, prompt: str, system: str = "") -> str:
        """Return mock response based on prompt content."""
        if "mutation" in prompt.lower():
            return json.dumps(
                {
                    "mutations": [
                        {
                            "mutation_id": "mock_001",
                            "category": "boundary_value",
                            "target_element": "(0010,0010)",
                            "original_value": "John Doe",
                            "mutated_value": "A" * 65536,
                            "rationale": "Test buffer overflow in Patient Name field",
                            "expected_behavior": "May cause buffer overflow",
                            "priority": 80,
                        }
                    ]
                }
            )
        elif "rule" in prompt.lower():
            return json.dumps(
                {
                    "rules": [
                        {
                            "rule_id": "VR_PN_001",
                            "description": "Patient Name (PN) max length is 64 characters per component",
                            "element_type": "tag",
                            "constraint": {"max_length": 64, "components": 5},
                            "mandatory": True,
                            "security_relevant": True,
                        }
                    ]
                }
            )
        else:
            return json.dumps({"response": "Mock response for testing"})

    def is_available(self) -> bool:
        """Mock is always available."""
        return True


class OpenAIClient(LLMClient):
    """OpenAI API client."""

    def __init__(self, config: LLMFuzzerConfig) -> None:
        self.config = config
        self.api_key = config.api_key or os.getenv("OPENAI_API_KEY", "")
        self._client: Any = None
        self._lock = threading.Lock()

    def _get_client(self) -> Any:
        """Get or create OpenAI client (thread-safe)."""
        if self._client is None:
            with self._lock:
                # Double-check after acquiring lock
                if self._client is None:
                    try:
                        from openai import OpenAI

                        self._client = OpenAI(
                            api_key=self.api_key,
                            base_url=self.config.api_base or None,
                            timeout=self.config.timeout,
                        )
                    except ImportError as err:
                        raise ImportError(
                            "openai package required: pip install openai"
                        ) from err
        return self._client

    def complete(self, prompt: str, system: str = "") -> str:
        """Generate completion using OpenAI API."""
        client = self._get_client()

        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})

        response = client.chat.completions.create(
            model=self.config.model,
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
        )

        return response.choices[0].message.content or ""

    def is_available(self) -> bool:
        """Check if OpenAI API is accessible."""
        if not self.api_key:
            return False
        try:
            client = self._get_client()
            # Simple test call
            client.models.list()
            return True
        except Exception:
            return False


class AnthropicClient(LLMClient):
    """Anthropic API client for Claude models."""

    def __init__(self, config: LLMFuzzerConfig) -> None:
        self.config = config
        self.api_key = config.api_key or os.getenv("ANTHROPIC_API_KEY", "")
        self._client: Any = None
        self._lock = threading.Lock()

    def _get_client(self) -> Any:
        """Get or create Anthropic client (thread-safe)."""
        if self._client is None:
            with self._lock:
                # Double-check after acquiring lock
                if self._client is None:
                    try:
                        from anthropic import Anthropic

                        self._client = Anthropic(api_key=self.api_key)
                    except ImportError as err:
                        raise ImportError(
                            "anthropic package required: pip install anthropic"
                        ) from err
        return self._client

    def complete(self, prompt: str, system: str = "") -> str:
        """Generate completion using Anthropic API."""
        client = self._get_client()

        response = client.messages.create(
            model=self.config.model,
            max_tokens=self.config.max_tokens,
            system=system or "You are a DICOM protocol security expert.",
            messages=[{"role": "user", "content": prompt}],
        )

        return str(response.content[0].text) if response.content else ""

    def is_available(self) -> bool:
        """Check if Anthropic API is accessible."""
        return bool(self.api_key)


class OllamaClient(LLMClient):
    """Ollama local LLM client."""

    def __init__(self, config: LLMFuzzerConfig) -> None:
        self.config = config
        self.base_url = config.api_base or "http://localhost:11434"

    def complete(self, prompt: str, system: str = "") -> str:
        """Generate completion using local Ollama."""
        import urllib.request

        full_prompt = f"{system}\n\n{prompt}" if system else prompt

        data = json.dumps(
            {
                "model": self.config.model,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "temperature": self.config.temperature,
                    "num_predict": self.config.max_tokens,
                },
            }
        ).encode()

        req = urllib.request.Request(  # noqa: S310
            f"{self.base_url}/api/generate",
            data=data,
            headers={"Content-Type": "application/json"},
        )

        with urllib.request.urlopen(  # noqa: S310  # nosec B310 - local Ollama server
            req, timeout=self.config.timeout
        ) as response:
            result = json.loads(response.read().decode())
            return str(result.get("response", ""))

    def is_available(self) -> bool:
        """Check if Ollama is running."""
        import urllib.request

        try:
            with urllib.request.urlopen(  # noqa: S310  # nosec B310 - local Ollama server
                f"{self.base_url}/api/tags", timeout=5
            ) as response:
                return bool(response.status == 200)
        except Exception:
            return False


class DICOMSpecParser:
    """Parser for DICOM specification documents.

    Extracts protocol rules, constraints, and security-relevant
    information from DICOM PS3.x documents.
    """

    # Common DICOM VR constraints
    VR_CONSTRAINTS: dict[str, dict[str, Any]] = {
        "AE": {"max_length": 16, "charset": "ascii", "padding": " "},
        "AS": {"length": 4, "format": r"\d{3}[DWMY]"},
        "AT": {"length": 4, "type": "tag"},
        "CS": {"max_length": 16, "charset": "ascii", "pattern": r"[A-Z0-9_ ]+"},
        "DA": {"length": 8, "format": r"\d{8}"},
        "DS": {"max_length": 16, "type": "decimal"},
        "DT": {"max_length": 26, "format": r"\d{14}(\.\d{1,6})?([+-]\d{4})?"},
        "FL": {"length": 4, "type": "float32"},
        "FD": {"length": 8, "type": "float64"},
        "IS": {"max_length": 12, "type": "integer"},
        "LO": {"max_length": 64, "charset": "unicode"},
        "LT": {"max_length": 10240, "charset": "unicode"},
        "OB": {"type": "binary", "variable_length": True},
        "OD": {"type": "float64_array"},
        "OF": {"type": "float32_array"},
        "OW": {"type": "uint16_array"},
        "PN": {"max_length": 64, "components": 5, "charset": "unicode"},
        "SH": {"max_length": 16, "charset": "unicode"},
        "SL": {"length": 4, "type": "int32"},
        "SQ": {"type": "sequence"},
        "SS": {"length": 2, "type": "int16"},
        "ST": {"max_length": 1024, "charset": "unicode"},
        "TM": {"max_length": 14, "format": r"\d{2}(\d{2}(\d{2}(\.\d{1,6})?)?)?"},
        "UC": {"max_length": 0xFFFFFFFE, "charset": "unicode"},
        "UI": {"max_length": 64, "charset": "ascii", "pattern": r"[\d.]+"},
        "UL": {"length": 4, "type": "uint32"},
        "UN": {"type": "binary", "variable_length": True},
        "UR": {"max_length": 0xFFFFFFFE, "charset": "ascii"},
        "US": {"length": 2, "type": "uint16"},
        "UT": {"max_length": 0xFFFFFFFE, "charset": "unicode"},
    }

    def __init__(self, llm_client: LLMClient | None = None) -> None:
        self.llm_client = llm_client
        self.rules: list[DICOMProtocolRule] = []

    def get_vr_rules(self) -> list[DICOMProtocolRule]:
        """Generate rules from VR constraints."""
        rules = []
        for vr, constraints in self.VR_CONSTRAINTS.items():
            rule = DICOMProtocolRule(
                rule_id=f"VR_{vr}_001",
                description=f"Value Representation {vr} constraints",
                element_type="vr",
                constraint=constraints,
                mandatory=True,
                security_relevant=constraints.get("type") == "binary"
                or "max_length" in constraints,
                source="DICOM PS3.5",
            )
            rules.append(rule)
        return rules

    def extract_rules_from_text(self, text: str) -> list[DICOMProtocolRule]:
        """Extract rules from DICOM specification text using LLM.

        Args:
            text: Specification text to parse

        Returns:
            List of extracted rules

        """
        if not self.llm_client:
            logger.warning("No LLM client available, using VR rules only")
            return self.get_vr_rules()

        system_prompt = """You are a DICOM protocol security expert.
        Extract testable protocol rules from the specification text.
        Focus on constraints, length limits, and security-relevant requirements.
        Return JSON with a 'rules' array containing rule objects."""

        prompt = f"""Extract protocol rules from this DICOM specification text:

{text[:4000]}  # Truncate to avoid token limits

Return JSON with format:
{{
  "rules": [
    {{
      "rule_id": "unique_id",
      "description": "human readable description",
      "element_type": "tag|vr|sequence|message",
      "constraint": {{"type": "...", "value": "..."}},
      "mandatory": true/false,
      "security_relevant": true/false
    }}
  ]
}}"""

        try:
            response = self.llm_client.complete(prompt, system_prompt)
            data = json.loads(response)
            for rule_data in data.get("rules", []):
                rule = DICOMProtocolRule(
                    rule_id=rule_data.get("rule_id", "unknown"),
                    description=rule_data.get("description", ""),
                    element_type=rule_data.get("element_type", ""),
                    constraint=rule_data.get("constraint", {}),
                    mandatory=rule_data.get("mandatory", False),
                    security_relevant=rule_data.get("security_relevant", False),
                    source="LLM-extracted",
                )
                self.rules.append(rule)
        except (json.JSONDecodeError, KeyError) as e:
            logger.error(f"Failed to parse LLM response: {e}")

        return self.rules

    def save_rules(self, path: Path) -> None:
        """Save extracted rules to JSON file."""
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump([r.to_dict() for r in self.rules], f, indent=2)

    def load_rules(self, path: Path) -> list[DICOMProtocolRule]:
        """Load rules from JSON file."""
        if not path.exists():
            return []

        with open(path, encoding="utf-8") as f:
            data = json.load(f)

        self.rules = []
        for rule_data in data:
            rule = DICOMProtocolRule(
                rule_id=rule_data.get("rule_id", "unknown"),
                description=rule_data.get("description", ""),
                element_type=rule_data.get("element_type", ""),
                constraint=rule_data.get("constraint", {}),
                mandatory=rule_data.get("mandatory", False),
                iod_module=rule_data.get("iod_module", ""),
                security_relevant=rule_data.get("security_relevant", False),
                source=rule_data.get("source", ""),
            )
            self.rules.append(rule)

        return self.rules


class LLMMutationGenerator:
    """Generates intelligent mutations using LLM guidance.

    Uses protocol rules and LLM reasoning to generate targeted
    mutations that are more likely to trigger vulnerabilities.
    """

    def __init__(
        self,
        llm_client: LLMClient,
        rules: list[DICOMProtocolRule] | None = None,
    ) -> None:
        self.llm_client = llm_client
        self.rules = rules or []
        self.generated_mutations: list[GeneratedMutation] = []

    def generate_boundary_mutations(
        self, element: str, vr: str
    ) -> list[GeneratedMutation]:
        """Generate boundary value mutations for an element.

        Args:
            element: DICOM tag (e.g., "(0010,0010)")
            vr: Value Representation

        Returns:
            List of generated mutations

        """
        constraints = DICOMSpecParser.VR_CONSTRAINTS.get(vr, {})
        mutations = []

        if "max_length" in constraints:
            max_len = constraints["max_length"]

            # Test at boundary
            mutations.append(
                GeneratedMutation(
                    mutation_id=f"boundary_{element}_{vr}_at_max",
                    category=MutationCategory.BOUNDARY_VALUE,
                    target_element=element,
                    original_value="test",
                    mutated_value="A" * max_len,
                    rationale=f"Test exact maximum length ({max_len}) for {vr}",
                    expected_behavior="Should be accepted",
                    priority=60,
                )
            )

            # Test just over boundary
            mutations.append(
                GeneratedMutation(
                    mutation_id=f"boundary_{element}_{vr}_over_max",
                    category=MutationCategory.BOUNDARY_VALUE,
                    target_element=element,
                    original_value="test",
                    mutated_value="A" * (max_len + 1),
                    rationale=f"Test one byte over maximum ({max_len + 1}) for {vr}",
                    expected_behavior="Should be rejected or truncated",
                    priority=80,
                )
            )

            # Test large overflow
            mutations.append(
                GeneratedMutation(
                    mutation_id=f"boundary_{element}_{vr}_overflow",
                    category=MutationCategory.BOUNDARY_VALUE,
                    target_element=element,
                    original_value="test",
                    mutated_value="A" * (max_len * 100),
                    rationale=f"Test significant overflow ({max_len * 100}) for {vr}",
                    expected_behavior="May cause buffer overflow or crash",
                    priority=90,
                )
            )

        return mutations

    def generate_llm_guided_mutations(
        self, element: str, context: str = ""
    ) -> list[GeneratedMutation]:
        """Generate mutations using LLM guidance.

        Args:
            element: DICOM element to target
            context: Additional context about the element usage

        Returns:
            List of LLM-generated mutations

        """
        system_prompt = """You are a security fuzzing expert specializing in DICOM.
        Generate targeted mutations that could trigger vulnerabilities.
        Focus on:
        - Buffer overflows (length manipulation)
        - Format string attacks
        - Integer overflows
        - Type confusion
        - Protocol state violations
        Return JSON with a 'mutations' array."""

        prompt = f"""Generate security-focused mutations for DICOM element {element}.

Context: {context or "General DICOM element"}

Consider these known DICOM CVEs for inspiration:
- CVE-2025-5943: MicroDicom out-of-bounds write
- CVE-2025-53618/53619: GDCM JPEG codec OOB read
- CVE-2025-52582: GDCM information leak

Return JSON format:
{{
  "mutations": [
    {{
      "mutation_id": "unique_id",
      "category": "boundary_value|format_string|type_confusion|...",
      "target_element": "{element}",
      "original_value": "normal value",
      "mutated_value": "malicious value",
      "rationale": "why this might trigger a bug",
      "expected_behavior": "what might happen",
      "priority": 0-100,
      "cve_reference": "CVE-XXXX-XXXXX if similar"
    }}
  ]
}}"""

        try:
            response = self.llm_client.complete(prompt, system_prompt)
            data = json.loads(response)

            mutations = []
            for mut_data in data.get("mutations", []):
                try:
                    category = MutationCategory(
                        mut_data.get("category", "protocol_specific")
                    )
                except ValueError:
                    category = MutationCategory.PROTOCOL_SPECIFIC

                mutation = GeneratedMutation(
                    mutation_id=mut_data.get("mutation_id", "llm_001"),
                    category=category,
                    target_element=mut_data.get("target_element", element),
                    original_value=mut_data.get("original_value", ""),
                    mutated_value=mut_data.get("mutated_value", ""),
                    rationale=mut_data.get("rationale", ""),
                    expected_behavior=mut_data.get("expected_behavior", ""),
                    priority=mut_data.get("priority", 50),
                    cve_reference=mut_data.get("cve_reference", ""),
                )
                mutations.append(mutation)
                self.generated_mutations.append(mutation)

            return mutations

        except (json.JSONDecodeError, KeyError) as e:
            logger.error(f"Failed to parse LLM mutation response: {e}")
            return []

    def prioritize_mutations(
        self, mutations: list[GeneratedMutation]
    ) -> list[GeneratedMutation]:
        """Sort mutations by priority (highest first)."""
        return sorted(mutations, key=lambda m: m.priority, reverse=True)


class LLMFuzzer:
    """Main LLM-assisted fuzzing orchestrator.

    Coordinates LLM-based rule extraction, mutation generation,
    and fuzzing execution.
    """

    def __init__(self, config: LLMFuzzerConfig | None = None) -> None:
        self.config = config or LLMFuzzerConfig()
        self.llm_client = self._create_client()
        self.spec_parser = DICOMSpecParser(self.llm_client)
        self.mutation_generator = LLMMutationGenerator(self.llm_client)

        # Load rules if specified
        if self.config.rules_file and self.config.rules_file.exists():
            self.spec_parser.load_rules(self.config.rules_file)
            self.mutation_generator.rules = self.spec_parser.rules

    def _create_client(self) -> LLMClient:
        """Create appropriate LLM client based on config."""
        backend = self.config.backend

        if backend == LLMBackend.MOCK:
            return MockLLMClient()
        elif backend == LLMBackend.OPENAI:
            return OpenAIClient(self.config)
        elif backend == LLMBackend.ANTHROPIC:
            return AnthropicClient(self.config)
        elif backend == LLMBackend.OLLAMA:
            return OllamaClient(self.config)
        else:
            logger.warning(f"Unknown backend {backend}, using mock")
            return MockLLMClient()

    def generate_fuzzing_corpus(
        self, elements: list[str] | None = None, count: int = 10
    ) -> list[GeneratedMutation]:
        """Generate a corpus of mutations for fuzzing.

        Args:
            elements: List of DICOM elements to target (or None for defaults)
            count: Target number of mutations

        Returns:
            List of generated mutations

        """
        if elements is None:
            # Default high-value targets based on CVE analysis
            elements = [
                "(0010,0010)",  # Patient Name
                "(0010,0020)",  # Patient ID
                "(0008,0018)",  # SOP Instance UID
                "(7FE0,0010)",  # Pixel Data
                "(0002,0000)",  # File Meta Information Group Length
                "(0002,0001)",  # File Meta Information Version
            ]

        all_mutations: list[GeneratedMutation] = []

        for element in elements:
            # Try to infer VR from element
            vr = self._infer_vr(element)

            # Generate boundary mutations
            if vr:
                boundary_muts = self.mutation_generator.generate_boundary_mutations(
                    element, vr
                )
                all_mutations.extend(boundary_muts)

            # Generate LLM-guided mutations
            if self.llm_client.is_available():
                llm_muts = self.mutation_generator.generate_llm_guided_mutations(
                    element
                )
                all_mutations.extend(llm_muts)

        # Prioritize and limit
        prioritized = self.mutation_generator.prioritize_mutations(all_mutations)
        return prioritized[:count]

    def _infer_vr(self, element: str) -> str:
        """Infer VR from element tag."""
        # Common element -> VR mappings
        vr_map = {
            "(0010,0010)": "PN",  # Patient Name
            "(0010,0020)": "LO",  # Patient ID
            "(0008,0018)": "UI",  # SOP Instance UID
            "(7FE0,0010)": "OW",  # Pixel Data (could be OB)
            "(0002,0000)": "UL",  # Group Length
            "(0002,0001)": "OB",  # File Meta Version
        }
        return vr_map.get(element, "")

    def analyze_crash(
        self, crash_data: dict[str, Any], mutation: GeneratedMutation | None = None
    ) -> dict[str, Any]:
        """Analyze a crash using LLM reasoning.

        Args:
            crash_data: Crash information (stack trace, signals, etc.)
            mutation: The mutation that caused the crash (if known)

        Returns:
            Analysis results with severity, root cause hypothesis, etc.

        """
        if not self.llm_client.is_available():
            return {"error": "LLM not available", "severity": "unknown"}

        system_prompt = """You are a vulnerability analyst specializing in medical imaging software.
        Analyze crash data to determine:
        - Severity (critical, high, medium, low)
        - Likely root cause
        - Exploitability assessment
        - Recommended fix"""

        prompt = f"""Analyze this DICOM parser crash:

Crash Data:
{json.dumps(crash_data, indent=2)}

{"Triggering Mutation:" + json.dumps(mutation.to_dict(), indent=2) if mutation else ""}

Provide analysis in JSON format:
{{
  "severity": "critical|high|medium|low",
  "root_cause": "description of likely cause",
  "exploitability": "description of potential exploitation",
  "cwe_id": "CWE-XXX",
  "recommended_fix": "suggested remediation"
}}"""

        try:
            response = self.llm_client.complete(prompt, system_prompt)
            return dict(json.loads(response))
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Crash analysis failed: {e}")
            return {"error": str(e), "severity": "unknown"}

    def save_corpus(self, mutations: list[GeneratedMutation], path: Path) -> None:
        """Save mutation corpus to file."""
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump([m.to_dict() for m in mutations], f, indent=2)
        logger.info(f"Saved {len(mutations)} mutations to {path}")


# Convenience function for quick usage
def create_llm_fuzzer(
    backend: str = "mock",
    model: str = "gpt-4",
    api_key: str = "",
) -> LLMFuzzer:
    """Create an LLM fuzzer with common configuration.

    Args:
        backend: LLM backend ("mock", "openai", "anthropic", "ollama")
        model: Model name
        api_key: API key (optional, uses env var if not provided)

    Returns:
        Configured LLMFuzzer instance

    """
    try:
        backend_enum = LLMBackend(backend)
    except ValueError:
        backend_enum = LLMBackend.MOCK

    config = LLMFuzzerConfig(
        backend=backend_enum,
        model=model,
        api_key=api_key,
    )

    return LLMFuzzer(config)


# =============================================================================
# Phase 1 Enhancements: Seed Generation, RL Feedback, Semantic Fuzzing
# =============================================================================


@dataclass
class MutationFeedback:
    """Feedback from mutation execution for RL learning.

    Attributes:
        mutation_id: ID of the mutation that was executed
        category: Category of the mutation
        caused_new_coverage: Whether new code paths were discovered
        caused_crash: Whether a crash occurred
        crash_severity: Severity if crash occurred
        execution_time_us: Execution time in microseconds
        memory_delta: Memory usage change in bytes

    """

    mutation_id: str
    category: MutationCategory
    caused_new_coverage: bool = False
    caused_crash: bool = False
    crash_severity: str = ""
    execution_time_us: float = 0.0
    memory_delta: int = 0

    def compute_reward(self) -> float:
        """Compute RL reward based on feedback.

        Reward structure:
        - Crash: +10.0 (high priority)
        - New coverage: +5.0 (medium priority)
        - Fast execution: +1.0 if < 1ms
        - Low memory: +0.5 if memory_delta < 1MB

        """
        reward = 0.0

        if self.caused_crash:
            severity_rewards = {
                "critical": 15.0,
                "high": 10.0,
                "medium": 5.0,
                "low": 2.0,
            }
            reward += severity_rewards.get(self.crash_severity.lower(), 10.0)

        if self.caused_new_coverage:
            reward += 5.0

        if self.execution_time_us < 1000:  # < 1ms
            reward += 1.0

        if self.memory_delta < 1024 * 1024:  # < 1MB
            reward += 0.5

        return reward


@dataclass
class MutationStatistics:
    """Statistics for a mutation category.

    Used for UCB1 (Upper Confidence Bound) calculation in RL.

    """

    category: MutationCategory
    total_selections: int = 0
    total_reward: float = 0.0
    successes: int = 0  # Caused crash or new coverage
    failures: int = 0

    @property
    def average_reward(self) -> float:
        """Get average reward per selection."""
        if self.total_selections == 0:
            return 0.0
        return self.total_reward / self.total_selections

    @property
    def success_rate(self) -> float:
        """Get success rate (crash or coverage)."""
        total = self.successes + self.failures
        if total == 0:
            return 0.0
        return self.successes / total


class AdaptiveMutationSelector:
    """RL-based adaptive mutation selection using UCB1 algorithm.

    Implements multi-armed bandit approach to learn which mutation
    categories are most effective for finding bugs.

    Based on:
    - "MOpt: Optimized Mutation Scheduling for Fuzzers" (USENIX 2019)
    - UCB1 algorithm for exploration/exploitation balance

    """

    def __init__(
        self,
        exploration_factor: float = 1.41,  # sqrt(2) for UCB1
        initial_boost: float = 10.0,  # Initial optimism for unexplored
    ) -> None:
        self.exploration_factor = exploration_factor
        self.initial_boost = initial_boost
        self.stats: dict[MutationCategory, MutationStatistics] = {}
        self.total_rounds: int = 0

        # Initialize stats for all categories
        for category in MutationCategory:
            self.stats[category] = MutationStatistics(category=category)

    def select_category(self) -> MutationCategory:
        """Select next mutation category using UCB1.

        UCB1 formula: average_reward + c * sqrt(ln(n) / n_i)
        where:
        - c is exploration factor
        - n is total rounds
        - n_i is times this arm was selected

        Returns:
            Selected mutation category

        """
        import math

        self.total_rounds += 1

        # If any category hasn't been tried, try it first
        untried = [c for c, s in self.stats.items() if s.total_selections == 0]
        if untried:
            return untried[0]

        # Calculate UCB1 scores
        best_category = MutationCategory.BOUNDARY_VALUE
        best_score = -float("inf")

        ln_total = math.log(self.total_rounds)

        for category, stat in self.stats.items():
            if stat.total_selections == 0:
                continue

            exploitation = stat.average_reward
            exploration = self.exploration_factor * math.sqrt(
                ln_total / stat.total_selections
            )
            ucb_score = exploitation + exploration

            if ucb_score > best_score:
                best_score = ucb_score
                best_category = category

        return best_category

    def update(self, feedback: MutationFeedback) -> None:
        """Update statistics based on mutation feedback.

        Args:
            feedback: Execution feedback from the mutation

        """
        stat = self.stats[feedback.category]
        stat.total_selections += 1
        stat.total_reward += feedback.compute_reward()

        if feedback.caused_crash or feedback.caused_new_coverage:
            stat.successes += 1
        else:
            stat.failures += 1

    def get_category_probabilities(self) -> dict[MutationCategory, float]:
        """Get current probability distribution over categories.

        Returns:
            Dictionary mapping category to selection probability

        """
        if self.total_rounds == 0:
            # Uniform distribution initially
            n_categories = len(MutationCategory)
            return dict.fromkeys(MutationCategory, 1.0 / n_categories)

        # Use softmax over UCB scores
        import math

        scores: dict[MutationCategory, float] = {}
        ln_total = math.log(max(self.total_rounds, 1))

        for category, stat in self.stats.items():
            if stat.total_selections == 0:
                scores[category] = self.initial_boost
            else:
                exploitation = stat.average_reward
                exploration = self.exploration_factor * math.sqrt(
                    ln_total / stat.total_selections
                )
                scores[category] = exploitation + exploration

        # Softmax normalization
        max_score = max(scores.values())
        exp_scores = {c: math.exp(s - max_score) for c, s in scores.items()}
        total_exp = sum(exp_scores.values())

        return {c: e / total_exp for c, e in exp_scores.items()}

    def get_statistics_report(self) -> dict[str, Any]:
        """Get report of current learning statistics.

        Returns:
            Dictionary with statistics for each category

        """
        return {
            "total_rounds": self.total_rounds,
            "categories": {
                c.value: {
                    "selections": s.total_selections,
                    "average_reward": round(s.average_reward, 3),
                    "success_rate": round(s.success_rate, 3),
                    "successes": s.successes,
                    "failures": s.failures,
                }
                for c, s in self.stats.items()
            },
            "probabilities": {
                c.value: round(p, 3)
                for c, p in self.get_category_probabilities().items()
            },
        }


class SemanticDICOMFuzzer:
    """Semantic-aware DICOM fuzzer.

    Uses deep understanding of DICOM protocol semantics to generate
    more effective test cases that violate protocol invariants.

    Based on:
    - DICOM PS3.3 Information Object Definitions
    - DICOM PS3.5 Data Structures and Encoding
    - DICOM PS3.6 Data Dictionary

    """

    # IOD (Information Object Definition) mandatory modules
    IOD_MODULES: dict[str, list[str]] = {
        "CT": ["Patient", "General Study", "General Series", "CT Image"],
        "MR": ["Patient", "General Study", "General Series", "MR Image"],
        "US": ["Patient", "General Study", "General Series", "US Image"],
        "CR": ["Patient", "General Study", "General Series", "CR Image"],
    }

    # Semantic relationships between DICOM elements
    SEMANTIC_RULES: list[dict[str, Any]] = [
        {
            "rule_id": "PATIENT_NAME_AGE_CONSISTENCY",
            "description": "Patient age should be consistent with birth date",
            "elements": ["PatientBirthDate", "PatientAge"],
            "type": "consistency",
        },
        {
            "rule_id": "IMAGE_DIMENSIONS",
            "description": "Rows * Columns * BitsAllocated/8 == PixelData length",
            "elements": ["Rows", "Columns", "BitsAllocated", "PixelData"],
            "type": "mathematical",
        },
        {
            "rule_id": "SERIES_STUDY_HIERARCHY",
            "description": "Series must belong to exactly one Study",
            "elements": ["StudyInstanceUID", "SeriesInstanceUID"],
            "type": "hierarchical",
        },
        {
            "rule_id": "UID_UNIQUENESS",
            "description": "UIDs must be globally unique",
            "elements": ["SOPInstanceUID", "StudyInstanceUID", "SeriesInstanceUID"],
            "type": "uniqueness",
        },
        {
            "rule_id": "VR_VALUE_CONSISTENCY",
            "description": "Value must conform to VR constraints",
            "elements": ["*"],
            "type": "type_constraint",
        },
        {
            "rule_id": "TRANSFER_SYNTAX_ENCODING",
            "description": "Pixel data encoding must match Transfer Syntax",
            "elements": ["TransferSyntaxUID", "PixelData"],
            "type": "encoding",
        },
    ]

    # Security-critical elements that are often vulnerable
    SECURITY_CRITICAL_ELEMENTS: dict[str, dict[str, Any]] = {
        "(0010,0010)": {
            "name": "PatientName",
            "vr": "PN",
            "attacks": ["buffer_overflow", "format_string", "path_traversal"],
        },
        "(0010,0020)": {
            "name": "PatientID",
            "vr": "LO",
            "attacks": ["sql_injection", "ldap_injection", "command_injection"],
        },
        "(0008,0018)": {
            "name": "SOPInstanceUID",
            "vr": "UI",
            "attacks": ["path_traversal", "directory_traversal"],
        },
        "(7FE0,0010)": {
            "name": "PixelData",
            "vr": "OW",
            "attacks": ["heap_overflow", "integer_overflow", "oob_read"],
        },
        "(0028,0010)": {
            "name": "Rows",
            "vr": "US",
            "attacks": ["integer_overflow", "allocation_failure"],
        },
        "(0028,0011)": {
            "name": "Columns",
            "vr": "US",
            "attacks": ["integer_overflow", "allocation_failure"],
        },
        "(0028,0100)": {
            "name": "BitsAllocated",
            "vr": "US",
            "attacks": ["integer_overflow", "memory_corruption"],
        },
    }

    def __init__(self, llm_client: LLMClient | None = None) -> None:
        self.llm_client = llm_client
        self.violations_generated: list[dict[str, Any]] = []

    def generate_semantic_violations(
        self, rule_types: list[str] | None = None
    ) -> list[GeneratedMutation]:
        """Generate mutations that violate semantic rules.

        Args:
            rule_types: Types of rules to violate (or None for all)

        Returns:
            List of semantic violation mutations

        """
        mutations = []

        for rule in self.SEMANTIC_RULES:
            if rule_types and rule["type"] not in rule_types:
                continue

            rule_mutations = self._generate_rule_violation(rule)
            mutations.extend(rule_mutations)

        return mutations

    def _generate_rule_violation(self, rule: dict[str, Any]) -> list[GeneratedMutation]:
        """Generate mutations that violate a specific rule."""
        mutations = []
        rule_type = rule["type"]

        if rule_type == "consistency":
            mutations.extend(self._violate_consistency(rule))
        elif rule_type == "mathematical":
            mutations.extend(self._violate_mathematical(rule))
        elif rule_type == "hierarchical":
            mutations.extend(self._violate_hierarchy(rule))
        elif rule_type == "uniqueness":
            mutations.extend(self._violate_uniqueness(rule))
        elif rule_type == "encoding":
            mutations.extend(self._violate_encoding(rule))

        return mutations

    def _violate_consistency(self, rule: dict[str, Any]) -> list[GeneratedMutation]:
        """Generate consistency violations."""
        mutations = []

        if "PatientBirthDate" in rule["elements"]:
            # Age/birth date mismatch
            mutations.append(
                GeneratedMutation(
                    mutation_id=f"semantic_{rule['rule_id']}_001",
                    category=MutationCategory.SEMANTIC_VIOLATION,
                    target_element="PatientBirthDate,PatientAge",
                    original_value="19500101,074Y",
                    mutated_value="20200101,074Y",  # Impossible: born in 2020 but 74 years old
                    rationale="Age inconsistent with birth date",
                    expected_behavior="Parser may crash or expose logical flaw",
                    priority=70,
                )
            )

        return mutations

    def _violate_mathematical(self, rule: dict[str, Any]) -> list[GeneratedMutation]:
        """Generate mathematical constraint violations."""
        mutations = []

        if "PixelData" in rule["elements"]:
            # Pixel data size mismatch
            mutations.append(
                GeneratedMutation(
                    mutation_id=f"semantic_{rule['rule_id']}_001",
                    category=MutationCategory.SEMANTIC_VIOLATION,
                    target_element="Rows,Columns,PixelData",
                    original_value="512,512,524288",  # 512*512*2 bytes
                    mutated_value="512,512,100",  # Way too small
                    rationale="PixelData length doesn't match Rows*Columns*BitsAllocated/8",
                    expected_behavior="May cause buffer under-read or crash",
                    priority=90,
                )
            )

            # Integer overflow in dimensions
            mutations.append(
                GeneratedMutation(
                    mutation_id=f"semantic_{rule['rule_id']}_002",
                    category=MutationCategory.SEMANTIC_VIOLATION,
                    target_element="Rows,Columns",
                    original_value="512,512",
                    mutated_value="65535,65535",  # Max US values
                    rationale="Maximum dimensions may cause integer overflow in size calculation",
                    expected_behavior="Integer overflow -> small allocation -> heap overflow",
                    priority=95,
                )
            )

        return mutations

    def _violate_hierarchy(self, rule: dict[str, Any]) -> list[GeneratedMutation]:
        """Generate hierarchical constraint violations."""
        mutations = []

        if "StudyInstanceUID" in rule["elements"]:
            # Orphan series (invalid study reference)
            mutations.append(
                GeneratedMutation(
                    mutation_id=f"semantic_{rule['rule_id']}_001",
                    category=MutationCategory.SEMANTIC_VIOLATION,
                    target_element="StudyInstanceUID",
                    original_value="1.2.3.4.5",
                    mutated_value="",  # Empty UID
                    rationale="Empty Study UID creates orphan series",
                    expected_behavior="May cause null pointer dereference",
                    priority=75,
                )
            )

        return mutations

    def _violate_uniqueness(self, rule: dict[str, Any]) -> list[GeneratedMutation]:
        """Generate uniqueness constraint violations."""
        mutations = []

        # Duplicate UIDs
        mutations.append(
            GeneratedMutation(
                mutation_id=f"semantic_{rule['rule_id']}_001",
                category=MutationCategory.SEMANTIC_VIOLATION,
                target_element="SOPInstanceUID,SeriesInstanceUID",
                original_value="unique1,unique2",
                mutated_value="1.2.3.4.5,1.2.3.4.5",  # Same UID
                rationale="Duplicate UIDs at different levels",
                expected_behavior="May confuse database or cause collision",
                priority=65,
            )
        )

        return mutations

    def _violate_encoding(self, rule: dict[str, Any]) -> list[GeneratedMutation]:
        """Generate encoding mismatch violations."""
        mutations = []

        if "TransferSyntaxUID" in rule["elements"]:
            # Transfer syntax mismatch
            mutations.append(
                GeneratedMutation(
                    mutation_id=f"semantic_{rule['rule_id']}_001",
                    category=MutationCategory.SEMANTIC_VIOLATION,
                    target_element="TransferSyntaxUID,PixelData",
                    original_value="1.2.840.10008.1.2,raw_pixels",
                    mutated_value="1.2.840.10008.1.2.4.50,raw_pixels",  # Claims JPEG but has raw
                    rationale="Transfer syntax claims JPEG but pixel data is uncompressed",
                    expected_behavior="JPEG decoder may crash on invalid data",
                    priority=85,
                )
            )

        return mutations

    def generate_security_focused_mutations(
        self, target_attacks: list[str] | None = None
    ) -> list[GeneratedMutation]:
        """Generate mutations targeting specific attack vectors.

        Args:
            target_attacks: List of attack types to target

        Returns:
            Mutations designed to trigger specific vulnerability types

        """
        mutations = []

        for tag, info in self.SECURITY_CRITICAL_ELEMENTS.items():
            attacks = info["attacks"]
            if target_attacks:
                attacks = [a for a in attacks if a in target_attacks]

            for attack in attacks:
                mutation = self._create_attack_mutation(tag, info, attack)
                if mutation:
                    mutations.append(mutation)

        return mutations

    def _create_attack_mutation(
        self, tag: str, info: dict[str, Any], attack: str
    ) -> GeneratedMutation | None:
        """Create mutation for specific attack type."""
        element_name = info["name"]
        vr = info["vr"]

        attack_payloads: dict[str, dict[str, Any]] = {
            "buffer_overflow": {
                "value": "A" * 10000,
                "rationale": f"Overflow {element_name} buffer with {vr} VR",
                "expected": "Heap/stack buffer overflow",
                "priority": 90,
            },
            "format_string": {
                "value": "%s%s%s%s%s%n%n%n",
                "rationale": f"Format string attack via {element_name}",
                "expected": "Format string vulnerability (read/write)",
                "priority": 85,
            },
            "path_traversal": {
                "value": "../../../etc/passwd",
                "rationale": f"Path traversal via {element_name}",
                "expected": "Arbitrary file read/write",
                "priority": 80,
            },
            "sql_injection": {
                "value": "'; DROP TABLE patients;--",
                "rationale": f"SQL injection via {element_name}",
                "expected": "Database manipulation",
                "priority": 75,
            },
            "integer_overflow": {
                "value": 0xFFFFFFFF if vr in ["US", "UL"] else 65535,
                "rationale": f"Integer overflow in {element_name}",
                "expected": "Integer overflow -> memory corruption",
                "priority": 95,
            },
            "heap_overflow": {
                "value": b"\x00" * 100,  # Small data with large claimed size
                "rationale": f"Heap overflow via undersized {element_name}",
                "expected": "Heap buffer overflow",
                "priority": 90,
            },
            "oob_read": {
                "value": b"",  # Empty data
                "rationale": f"Out-of-bounds read via empty {element_name}",
                "expected": "Out-of-bounds read vulnerability",
                "priority": 85,
            },
        }

        if attack not in attack_payloads:
            return None

        payload = attack_payloads[attack]

        return GeneratedMutation(
            mutation_id=f"security_{tag}_{attack}",
            category=MutationCategory.PROTOCOL_SPECIFIC,
            target_element=tag,
            original_value="normal",
            mutated_value=payload["value"],
            rationale=payload["rationale"],
            expected_behavior=payload["expected"],
            priority=payload["priority"],
        )


class LLMSeedGenerator:
    """Generate complete DICOM seed files using LLM guidance.

    Combines synthetic DICOM generation with LLM-guided mutations
    to create intelligent fuzzing seeds.

    """

    def __init__(
        self,
        output_dir: Path,
        llm_client: LLMClient | None = None,
        adaptive_selector: AdaptiveMutationSelector | None = None,
    ) -> None:
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.llm_client = llm_client or MockLLMClient()
        self.adaptive_selector = adaptive_selector or AdaptiveMutationSelector()
        self.semantic_fuzzer = SemanticDICOMFuzzer(llm_client)

        # Import synthetic generator
        from dicom_fuzzer.core.synthetic import SyntheticDicomGenerator

        self.synthetic_gen = SyntheticDicomGenerator(output_dir)

    def generate_seeds(
        self,
        count: int = 10,
        modalities: list[str] | None = None,
        use_rl: bool = True,
        use_semantic: bool = True,
    ) -> list[Path]:
        """Generate intelligent fuzzing seeds.

        Args:
            count: Number of seeds to generate
            modalities: DICOM modalities to use
            use_rl: Use RL-based mutation selection
            use_semantic: Include semantic violations

        Returns:
            List of generated seed file paths

        """
        if modalities is None:
            modalities = ["CT", "MR", "US"]

        generated_files = []
        mutations_per_file = []

        for i in range(count):
            modality = modalities[i % len(modalities)]

            # Generate base synthetic file
            base_path = self.synthetic_gen.generate_file(modality=modality)

            # Select mutation category
            if use_rl:
                category = self.adaptive_selector.select_category()
            else:
                import random

                category = random.choice(list(MutationCategory))

            # Generate mutations for this file
            mutations = self._generate_mutations_for_category(category)

            if use_semantic:
                semantic_muts = self.semantic_fuzzer.generate_semantic_violations()
                mutations.extend(semantic_muts[:2])  # Add a few semantic violations

            # Apply mutations to the file
            mutated_path = self._apply_mutations_to_file(base_path, mutations)

            if mutated_path:
                generated_files.append(mutated_path)
                mutations_per_file.append((mutated_path, mutations))

        logger.info(f"Generated {len(generated_files)} seed files")
        return generated_files

    def _generate_mutations_for_category(
        self, category: MutationCategory
    ) -> list[GeneratedMutation]:
        """Generate mutations for a specific category."""
        # Use LLM if available
        if self.llm_client.is_available() and not isinstance(
            self.llm_client, MockLLMClient
        ):
            mutation_gen = LLMMutationGenerator(self.llm_client)
            return mutation_gen.generate_llm_guided_mutations(
                "(0010,0010)", context=f"Focus on {category.value} attacks"
            )

        # Fallback to rule-based generation
        mutations = []

        if category == MutationCategory.BOUNDARY_VALUE:
            mutation_gen = LLMMutationGenerator(MockLLMClient())
            mutations = mutation_gen.generate_boundary_mutations("(0010,0010)", "PN")

        elif category == MutationCategory.SEMANTIC_VIOLATION:
            mutations = self.semantic_fuzzer.generate_semantic_violations()

        elif category == MutationCategory.FORMAT_STRING:
            mutations.append(
                GeneratedMutation(
                    mutation_id="fmt_001",
                    category=category,
                    target_element="(0010,0010)",
                    original_value="Test",
                    mutated_value="%s%s%s%s%s%n%n%n",
                    rationale="Format string attack",
                    expected_behavior="Format string vulnerability",
                    priority=85,
                )
            )

        return mutations

    def _apply_mutations_to_file(
        self, file_path: Path, mutations: list[GeneratedMutation]
    ) -> Path | None:
        """Apply mutations to a DICOM file.

        Args:
            file_path: Path to the base DICOM file
            mutations: Mutations to apply

        Returns:
            Path to mutated file or None if failed

        """
        try:
            import pydicom

            ds = pydicom.dcmread(str(file_path), force=True)

            for mutation in mutations:
                self._apply_single_mutation(ds, mutation)

            # Save with new filename
            output_path = self.output_dir / f"seed_{file_path.stem}_mutated.dcm"
            ds.save_as(str(output_path), enforce_file_format=False)

            # Remove original base file
            file_path.unlink(missing_ok=True)

            return output_path

        except Exception as e:
            logger.error(f"Failed to apply mutations: {e}")
            return None

    def _apply_single_mutation(self, ds: Any, mutation: GeneratedMutation) -> None:
        """Apply a single mutation to a dataset."""
        # Parse tag from mutation target
        target = mutation.target_element

        # Handle tag format like "(0010,0010)"
        tag_match = re.match(r"\(([0-9A-Fa-f]{4}),([0-9A-Fa-f]{4})\)", target)
        if tag_match:
            group = int(tag_match.group(1), 16)
            element = int(tag_match.group(2), 16)

            try:
                if hasattr(ds, "add_new"):
                    from pydicom.tag import Tag

                    tag = Tag(group, element)
                    if tag in ds:
                        ds[tag].value = mutation.mutated_value
            except Exception as e:
                logger.debug(f"Could not apply mutation to {target}: {e}")

    def provide_feedback(self, feedback: MutationFeedback) -> None:
        """Provide feedback for RL learning.

        Args:
            feedback: Execution feedback from the mutation

        """
        self.adaptive_selector.update(feedback)

    def get_learning_report(self) -> dict[str, Any]:
        """Get current RL learning statistics.

        Returns:
            Report with mutation category statistics

        """
        return self.adaptive_selector.get_statistics_report()


# Update create_llm_fuzzer to include new components
def create_llm_seed_generator(
    output_dir: str | Path = "./artifacts/corpus",
    backend: str = "mock",
    model: str = "gpt-4",
    api_key: str = "",
) -> LLMSeedGenerator:
    """Create an LLM-guided seed generator.

    Args:
        output_dir: Directory for generated seeds
        backend: LLM backend ("mock", "openai", "anthropic", "ollama")
        model: Model name
        api_key: API key (optional, uses env var if not provided)

    Returns:
        Configured LLMSeedGenerator instance

    """
    try:
        backend_enum = LLMBackend(backend)
    except ValueError:
        backend_enum = LLMBackend.MOCK

    config = LLMFuzzerConfig(
        backend=backend_enum,
        model=model,
        api_key=api_key,
    )

    fuzzer = LLMFuzzer(config)

    return LLMSeedGenerator(
        output_dir=Path(output_dir),
        llm_client=fuzzer.llm_client,
        adaptive_selector=AdaptiveMutationSelector(),
    )
