"""End-to-End Tests for Analytics and CLI Modules

This module provides comprehensive e2e tests for:
- Analytics visualization (FuzzingVisualizer)
- CLI realtime_monitor
- CLI generate_report and create_html_report
- CLI coverage_fuzz

These tests improve coverage for modules with low coverage percentages.
"""

import json
from datetime import datetime, timedelta
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

from dicom_fuzzer.analytics.campaign_analytics import (
    CoverageCorrelation,
    PerformanceMetrics,
    TrendAnalysis,
)
from dicom_fuzzer.analytics.visualization import FuzzingVisualizer


class TestFuzzingVisualizerE2E:
    """End-to-end tests for FuzzingVisualizer."""

    @pytest.fixture
    def temp_output_dir(self, tmp_path):
        """Create temporary output directory."""
        output_dir = tmp_path / "charts"
        output_dir.mkdir()
        return output_dir

    @pytest.fixture
    def visualizer(self, temp_output_dir):
        """Create FuzzingVisualizer instance."""
        return FuzzingVisualizer(output_dir=str(temp_output_dir))

    @pytest.fixture
    def sample_effectiveness_data(self):
        """Sample strategy effectiveness data."""
        return {
            "metadata_fuzzer": {"effectiveness_score": 0.85, "usage_count": 100},
            "header_fuzzer": {"effectiveness_score": 0.72, "usage_count": 80},
            "pixel_fuzzer": {"effectiveness_score": 0.65, "usage_count": 50},
            "structure_fuzzer": {"effectiveness_score": 0.58, "usage_count": 40},
            "dictionary_fuzzer": {"effectiveness_score": 0.45, "usage_count": 30},
        }

    @pytest.fixture
    def sample_trend_data(self):
        """Sample trend analysis data."""
        base_time = datetime.now()
        start_time = base_time - timedelta(hours=5)
        end_time = base_time

        trend = TrendAnalysis(
            campaign_name="test_campaign",
            start_time=start_time,
            end_time=end_time,
            total_duration=end_time - start_time,
            crashes_over_time=[
                (base_time - timedelta(hours=5), 2),
                (base_time - timedelta(hours=4), 1),
                (base_time - timedelta(hours=3), 3),
                (base_time - timedelta(hours=2), 2),
                (base_time - timedelta(hours=1), 4),
            ],
        )
        return trend

    @pytest.fixture
    def sample_coverage_data(self):
        """Sample coverage correlation data."""
        return {
            "metadata_fuzzer": CoverageCorrelation(
                strategy="metadata_fuzzer",
                coverage_increase=15.5,
                unique_paths=45,
                crash_correlation=0.3,
                sample_size=100,
            ),
            "header_fuzzer": CoverageCorrelation(
                strategy="header_fuzzer",
                coverage_increase=12.0,
                unique_paths=38,
                crash_correlation=0.5,
                sample_size=80,
            ),
            "pixel_fuzzer": CoverageCorrelation(
                strategy="pixel_fuzzer",
                coverage_increase=8.5,
                unique_paths=25,
                crash_correlation=0.7,
                sample_size=50,
            ),
        }

    @pytest.fixture
    def sample_performance_data(self):
        """Sample performance metrics."""
        return PerformanceMetrics(
            mutations_per_second=125.5,
            peak_memory_mb=512.0,
            avg_memory_mb=256.0,
            cpu_utilization=75.5,
            disk_io_mb_per_sec=50.0,
            cache_hit_rate=92.3,
        )

    def test_plot_strategy_effectiveness_png(
        self, visualizer, sample_effectiveness_data
    ):
        """Test strategy effectiveness chart generation (PNG)."""
        output_path = visualizer.plot_strategy_effectiveness(
            sample_effectiveness_data, output_format="png"
        )

        assert output_path.exists()
        assert output_path.suffix == ".png"
        assert output_path.stat().st_size > 0

    def test_plot_strategy_effectiveness_svg(
        self, visualizer, sample_effectiveness_data
    ):
        """Test strategy effectiveness chart generation (SVG)."""
        output_path = visualizer.plot_strategy_effectiveness(
            sample_effectiveness_data, output_format="svg"
        )

        assert output_path.exists()
        assert output_path.suffix == ".svg"
        assert output_path.stat().st_size > 0

    def test_plot_strategy_effectiveness_html(
        self, visualizer, sample_effectiveness_data
    ):
        """Test strategy effectiveness interactive chart (HTML)."""
        output_path = visualizer.plot_strategy_effectiveness(
            sample_effectiveness_data, output_format="html"
        )

        assert output_path.exists()
        assert output_path.suffix == ".html"
        # Verify HTML content - use utf-8 encoding
        content = output_path.read_text(encoding="utf-8")
        assert "plotly" in content.lower() or "script" in content.lower()

    def test_plot_crash_trend_png(self, visualizer, sample_trend_data):
        """Test crash trend chart generation (PNG)."""
        output_path = visualizer.plot_crash_trend(
            sample_trend_data, output_format="png"
        )

        assert output_path.exists()
        assert output_path.suffix == ".png"
        assert output_path.stat().st_size > 0

    def test_plot_crash_trend_html(self, visualizer, sample_trend_data):
        """Test crash trend interactive chart (HTML)."""
        output_path = visualizer.plot_crash_trend(
            sample_trend_data, output_format="html"
        )

        assert output_path.exists()
        assert output_path.suffix == ".html"

    def test_plot_crash_trend_empty_data(self, visualizer):
        """Test crash trend with empty data."""
        base_time = datetime.now()
        empty_trend = TrendAnalysis(
            campaign_name="empty_test",
            start_time=base_time - timedelta(hours=1),
            end_time=base_time,
            total_duration=timedelta(hours=1),
            crashes_over_time=[],
        )

        output_path = visualizer.plot_crash_trend(empty_trend, output_format="png")
        assert output_path.exists()

        output_path_html = visualizer.plot_crash_trend(
            empty_trend, output_format="html"
        )
        assert output_path_html.exists()

    def test_plot_coverage_heatmap_png(self, visualizer, sample_coverage_data):
        """Test coverage heatmap generation (PNG)."""
        output_path = visualizer.plot_coverage_heatmap(
            sample_coverage_data, output_format="png"
        )

        assert output_path.exists()
        assert output_path.suffix == ".png"
        assert output_path.stat().st_size > 0

    def test_plot_coverage_heatmap_html(self, visualizer, sample_coverage_data):
        """Test coverage heatmap interactive chart (HTML)."""
        output_path = visualizer.plot_coverage_heatmap(
            sample_coverage_data, output_format="html"
        )

        assert output_path.exists()
        assert output_path.suffix == ".html"

    def test_plot_performance_dashboard_png(self, visualizer, sample_performance_data):
        """Test performance dashboard generation (PNG)."""
        output_path = visualizer.plot_performance_dashboard(
            sample_performance_data, output_format="png"
        )

        assert output_path.exists()
        assert output_path.suffix == ".png"
        assert output_path.stat().st_size > 0

    def test_plot_performance_dashboard_html(self, visualizer, sample_performance_data):
        """Test performance dashboard interactive chart (HTML)."""
        output_path = visualizer.plot_performance_dashboard(
            sample_performance_data, output_format="html"
        )

        assert output_path.exists()
        assert output_path.suffix == ".html"

    def test_create_summary_report_html(
        self,
        visualizer,
        sample_effectiveness_data,
        sample_trend_data,
        sample_coverage_data,
        sample_performance_data,
    ):
        """Test complete summary report HTML generation."""
        # Generate all charts
        strategy_chart = visualizer.plot_strategy_effectiveness(
            sample_effectiveness_data, output_format="png"
        )
        trend_chart = visualizer.plot_crash_trend(
            sample_trend_data, output_format="png"
        )
        coverage_chart = visualizer.plot_coverage_heatmap(
            sample_coverage_data, output_format="png"
        )
        performance_chart = visualizer.plot_performance_dashboard(
            sample_performance_data, output_format="png"
        )

        # Generate summary HTML
        html = visualizer.create_summary_report_html(
            strategy_chart, trend_chart, coverage_chart, performance_chart
        )

        # Verify HTML content
        assert "charts-container" in html
        assert "Strategy Effectiveness" in html
        assert "Crash Discovery Trend" in html
        assert "Coverage Correlation" in html
        assert "Performance Metrics" in html
        assert "<style>" in html

    def test_visualizer_creates_output_directory(self, tmp_path):
        """Test that visualizer creates output directory if missing."""
        new_dir = tmp_path / "new_charts_dir"
        assert not new_dir.exists()

        visualizer = FuzzingVisualizer(output_dir=str(new_dir))
        assert new_dir.exists()


class TestRealtimeMonitorE2E:
    """End-to-end tests for realtime_monitor CLI module."""

    @pytest.fixture
    def temp_session_dir(self, tmp_path):
        """Create temporary session directory with reports."""
        session_dir = tmp_path / "output"
        session_dir.mkdir()

        reports_dir = tmp_path / "reports" / "json"
        reports_dir.mkdir(parents=True)

        # Create sample session JSON
        session_data = {
            "session_info": {
                "session_name": "test_session",
                "start_time": datetime.now().isoformat(),
            },
            "statistics": {
                "files_fuzzed": 50,
                "mutations_applied": 250,
                "crashes": 3,
                "hangs": 1,
                "successes": 46,
            },
            "crashes": [
                {
                    "crash_id": "crash_001",
                    "crash_type": "segfault",
                    "severity": "high",
                },
                {
                    "crash_id": "crash_002",
                    "crash_type": "memory",
                    "severity": "critical",
                },
            ],
        }

        session_file = reports_dir / "session_001.json"
        session_file.write_text(json.dumps(session_data))

        return tmp_path

    def test_realtime_monitor_initialization(self, temp_session_dir):
        """Test RealtimeMonitor initialization."""
        from dicom_fuzzer.cli.realtime_monitor import RealtimeMonitor

        monitor = RealtimeMonitor(
            session_dir=temp_session_dir / "output",
            refresh_interval=2,
            session_id="test_session",
        )

        assert monitor.session_dir == temp_session_dir / "output"
        assert monitor.refresh_interval == 2
        assert monitor.session_id == "test_session"

    def test_display_stats_with_rich(self):
        """Test display_stats function with rich console."""
        from dicom_fuzzer.cli.realtime_monitor import display_stats

        stats = {
            "iterations": 100,
            "crashes": 5,
            "coverage": 0.75,
            "exec_speed": 50.5,
        }

        # Should not raise exception
        mock_console = MagicMock()
        display_stats(stats, console=mock_console)

    def test_display_stats_fallback(self):
        """Test display_stats fallback when rich not available."""
        from dicom_fuzzer.cli.realtime_monitor import display_stats

        stats = {"iterations": 100, "crashes": 5}

        # Mock HAS_RICH to False
        with patch("dicom_fuzzer.cli.realtime_monitor.HAS_RICH", False):
            display_stats(stats)  # Should use print fallback

    def test_get_session_stats(self):
        """Test get_session_stats function."""
        from dicom_fuzzer.cli.realtime_monitor import get_session_stats

        stats = get_session_stats("test_session")

        assert "iterations" in stats
        assert "crashes" in stats
        assert "coverage" in stats
        assert "exec_speed" in stats

    def test_monitor_loop_keyboard_interrupt(self):
        """Test monitor_loop handles KeyboardInterrupt."""
        from dicom_fuzzer.cli.realtime_monitor import monitor_loop

        with patch("dicom_fuzzer.cli.realtime_monitor.get_session_stats") as mock_stats:
            with patch(
                "dicom_fuzzer.cli.realtime_monitor.display_stats"
            ) as mock_display:
                with patch("time.sleep", side_effect=KeyboardInterrupt):
                    mock_stats.return_value = {"iterations": 0}

                    with pytest.raises(KeyboardInterrupt):
                        monitor_loop("test_session", update_interval=1)

    def test_realtime_monitor_main(self):
        """Test realtime_monitor main function."""
        from dicom_fuzzer.cli.realtime_monitor import main

        with patch("sys.argv", ["realtime_monitor", "--session-dir", "./test"]):
            with patch(
                "dicom_fuzzer.cli.realtime_monitor.RealtimeMonitor"
            ) as mock_class:
                mock_monitor = MagicMock()
                mock_class.return_value = mock_monitor

                # Mock monitor to raise KeyboardInterrupt
                mock_monitor.monitor.side_effect = KeyboardInterrupt

                try:
                    main()
                except KeyboardInterrupt:
                    pass

                mock_class.assert_called_once()


class TestGenerateReportE2E:
    """End-to-end tests for generate_report CLI module."""

    @pytest.fixture
    def temp_session_json(self, tmp_path):
        """Create temporary session JSON file for report generation."""
        # Create sample session data matching the expected format
        session_data = {
            "session_info": {
                "session_name": "test_session",
                "start_time": datetime.now().isoformat(),
            },
            "statistics": {
                "files_fuzzed": 50,
                "mutations_applied": 250,
                "crashes": 3,
                "hangs": 1,
                "successes": 46,
            },
            "crashes": [
                {
                    "crash_id": "crash_001",
                    "crash_type": "segfault",
                    "preserved_sample_path": "/tmp/crash_001.dcm",
                    "crash_log_path": "/tmp/crash_001.log",
                },
            ],
            "configuration": {
                "viewer_path": "/usr/bin/test_viewer",
                "input_dir": "/tmp/input",
                "output_dir": "/tmp/output",
                "timeout": 30,
            },
        }

        session_file = tmp_path / "session_test.json"
        session_file.write_text(json.dumps(session_data))

        return session_file

    def test_generate_json_report(self, tmp_path):
        """Test JSON report generation using generate_json_report."""
        from dicom_fuzzer.cli.generate_report import generate_json_report

        output_path = tmp_path / "output_report.json"

        data = {
            "campaign_id": "test",
            "iterations": 100,
            "crashes": 5,
        }

        generate_json_report(data, str(output_path))

        assert output_path.exists()
        loaded = json.loads(output_path.read_text())
        assert loaded["campaign_id"] == "test"
        assert loaded["iterations"] == 100

    def test_generate_csv_report(self, tmp_path):
        """Test CSV report generation."""
        from dicom_fuzzer.cli.generate_report import generate_csv_report

        output_path = tmp_path / "crashes.csv"

        crashes = [
            {"id": "crash_001", "type": "segfault", "severity": "high"},
            {"id": "crash_002", "type": "memory", "severity": "critical"},
        ]

        generate_csv_report(crashes, str(output_path))

        assert output_path.exists()
        content = output_path.read_text()
        assert "crash_001" in content
        assert "crash_002" in content

    def test_generate_markdown_report(self, tmp_path):
        """Test Markdown report generation."""
        from dicom_fuzzer.cli.generate_report import generate_markdown_report

        output_path = tmp_path / "report.md"

        data = {
            "title": "Test Report",
            "summary": {"Total Tests": 100, "Crashes": 5},
            "findings": [
                {"severity": "HIGH", "description": "Buffer overflow detected"},
            ],
        }

        generate_markdown_report(data, str(output_path))

        assert output_path.exists()
        content = output_path.read_text()
        assert "# Test Report" in content
        assert "HIGH" in content

    def test_generate_reports_function(self, temp_session_json, tmp_path):
        """Test main generate_reports function."""
        from dicom_fuzzer.cli.generate_report import generate_reports

        output_html = tmp_path / "report.html"

        # Mock the EnhancedReportGenerator to avoid file dependencies
        with patch(
            "dicom_fuzzer.cli.generate_report.EnhancedReportGenerator"
        ) as mock_reporter_class:
            mock_reporter = MagicMock()
            mock_reporter.generate_html_report.return_value = output_html
            mock_reporter_class.return_value = mock_reporter

            # Create output file to satisfy exists() check
            output_html.write_text("<html>test</html>")

            result = generate_reports(
                session_json_path=temp_session_json,
                output_html=output_html,
                keep_json=False,
            )

            assert result == output_html
            mock_reporter.generate_html_report.assert_called_once()


class TestCreateHtmlReportE2E:
    """End-to-end tests for create_html_report CLI module."""

    @pytest.fixture
    def sample_json_report(self, tmp_path):
        """Create sample JSON report file for HTML generation."""
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "statistics": {
                "files_processed": 100,
                "files_fuzzed": 80,
                "files_generated": 200,
                "viewer_crashes": 3,
                "viewer_hangs": 5,
                "viewer_success": 72,
                "hang_rate": 5.0,
            },
            "configuration": {
                "viewer_path": "/usr/bin/test_viewer",
                "input_dir": "/tmp/input",
                "output_dir": "/tmp/output",
                "timeout": 30,
            },
        }

        json_path = tmp_path / "report.json"
        json_path.write_text(json.dumps(report_data))
        return json_path

    def test_create_html_report(self, sample_json_report, tmp_path):
        """Test HTML report creation from JSON."""
        from dicom_fuzzer.cli.create_html_report import create_html_report

        html_path = tmp_path / "report.html"

        result = create_html_report(str(sample_json_report), str(html_path))

        assert result == str(html_path)
        assert html_path.exists()
        content = html_path.read_text(encoding="utf-8")
        assert "DICOM Viewer Security Assessment" in content
        assert "Test Results" in content

    def test_create_html_report_auto_path(self, sample_json_report):
        """Test HTML report with auto-generated output path."""
        from dicom_fuzzer.cli.create_html_report import create_html_report

        result = create_html_report(str(sample_json_report))

        expected_html = str(sample_json_report).replace(".json", ".html")
        assert result == expected_html
        assert Path(result).exists()

        # Cleanup
        Path(result).unlink()

    def test_load_template(self, tmp_path):
        """Test load_template function."""
        from dicom_fuzzer.cli.create_html_report import load_template

        template_content = "<html>{{ title }}</html>"
        template_file = tmp_path / "template.html"
        template_file.write_text(template_content)

        result = load_template(str(template_file))
        assert result == template_content

    def test_render_report(self):
        """Test render_report function."""
        from dicom_fuzzer.cli.create_html_report import render_report

        template = "<html><title>{{ title }}</title></html>"
        data = {"title": "Test Report"}

        result = render_report(template, data)
        assert "Test Report" in result

    def test_save_report(self, tmp_path):
        """Test save_report function."""
        from dicom_fuzzer.cli.create_html_report import save_report

        content = "<html>Test Content</html>"
        output_file = tmp_path / "output.html"

        save_report(content, str(output_file))

        assert output_file.exists()
        assert output_file.read_text() == content

    def test_create_report_with_charts(self):
        """Test create_report_with_charts function."""
        from dicom_fuzzer.cli.create_html_report import create_report_with_charts

        data = {"crashes": [], "coverage": 0.5}

        result = create_report_with_charts(data, "/tmp/output")

        assert "data" in result
        assert "charts" in result
        assert "output_dir" in result

    def test_generate_charts(self):
        """Test generate_charts function."""
        from dicom_fuzzer.cli.create_html_report import generate_charts

        data = {"metrics": {"coverage": 0.75}}

        result = generate_charts(data)

        assert "coverage_chart" in result
        assert "crash_chart" in result


class TestCoverageFuzzE2E:
    """End-to-end tests for coverage_fuzz CLI module."""

    @pytest.fixture
    def sample_dicom_file(self, tmp_path):
        """Create sample DICOM file."""
        from pydicom.dataset import FileDataset, FileMetaDataset
        from pydicom.uid import generate_uid

        file_meta = FileMetaDataset()
        file_meta.MediaStorageSOPClassUID = "1.2.840.10008.5.1.4.1.1.2"
        file_meta.MediaStorageSOPInstanceUID = generate_uid()
        file_meta.TransferSyntaxUID = "1.2.840.10008.1.2"
        file_meta.ImplementationClassUID = generate_uid()

        file_path = tmp_path / "sample.dcm"
        ds = FileDataset(
            str(file_path), {}, file_meta=file_meta, preamble=b"\x00" * 128
        )

        ds.PatientName = "TEST^PATIENT"
        ds.PatientID = "12345"
        ds.StudyInstanceUID = generate_uid()
        ds.SeriesInstanceUID = generate_uid()
        ds.SOPInstanceUID = file_meta.MediaStorageSOPInstanceUID
        ds.SOPClassUID = file_meta.MediaStorageSOPClassUID
        ds.Modality = "CT"

        ds.save_as(str(file_path), write_like_original=False)
        return file_path

    def test_coverage_fuzz_basic(self, sample_dicom_file, tmp_path):
        """Test basic coverage-guided fuzzing."""
        from dicom_fuzzer.cli.coverage_fuzz import main

        output_dir = tmp_path / "output"
        output_dir.mkdir()

        args = [
            "coverage_fuzz",
            str(sample_dicom_file),
            "--output",
            str(output_dir),
            "--iterations",
            "10",
        ]

        with patch("sys.argv", args):
            with patch(
                "dicom_fuzzer.cli.coverage_fuzz.CoverageGuidedFuzzer"
            ) as mock_fuzzer_class:
                mock_fuzzer = MagicMock()
                mock_fuzzer.fuzz.return_value = MagicMock(
                    total_iterations=10,
                    unique_crashes=0,
                    corpus_size=5,
                    total_coverage=100,
                )
                mock_fuzzer.get_report.return_value = "Test Report"
                mock_fuzzer_class.return_value = mock_fuzzer

                try:
                    main()
                except SystemExit:
                    pass  # Expected exit

    def test_coverage_fuzz_with_target(self, sample_dicom_file, tmp_path):
        """Test coverage-guided fuzzing with target executable."""
        import sys

        from dicom_fuzzer.cli.coverage_fuzz import main as coverage_fuzz_main

        output_dir = tmp_path / "output"
        output_dir.mkdir()

        # Create mock target
        if sys.platform == "win32":
            target = tmp_path / "target.bat"
            target.write_text("@echo off\nexit 0")
        else:
            target = tmp_path / "target.sh"
            target.write_text("#!/bin/bash\nexit 0")
            target.chmod(0o755)

        args = [
            "coverage_fuzz",
            str(sample_dicom_file),
            "--output",
            str(output_dir),
            "--iterations",
            "5",
            "--target",
            str(target),
        ]

        with patch("sys.argv", args):
            with patch(
                "dicom_fuzzer.cli.coverage_fuzz.CoverageGuidedFuzzer"
            ) as mock_fuzzer_class:
                mock_fuzzer = MagicMock()
                mock_fuzzer.fuzz.return_value = MagicMock(
                    total_iterations=5,
                    unique_crashes=0,
                    corpus_size=3,
                    total_coverage=50,
                )
                mock_fuzzer.get_report.return_value = "Test Report"
                mock_fuzzer_class.return_value = mock_fuzzer

                try:
                    coverage_fuzz_main()
                except SystemExit:
                    pass


class TestCampaignAnalyticsDataclasses:
    """Test CoverageCorrelation and PerformanceMetrics dataclasses."""

    def test_coverage_correlation_score(self):
        """Test CoverageCorrelation.correlation_score method."""
        corr = CoverageCorrelation(
            strategy="test_strategy",
            coverage_increase=20.0,
            unique_paths=50,
            crash_correlation=0.6,
            sample_size=100,
        )

        score = corr.correlation_score()
        assert 0 <= score <= 1
        assert isinstance(score, float)

    def test_performance_metrics_throughput_score(self):
        """Test PerformanceMetrics.throughput_score method."""
        metrics = PerformanceMetrics(
            mutations_per_second=150.0,
            peak_memory_mb=1024.0,
            avg_memory_mb=512.0,
            cpu_utilization=80.0,
            disk_io_mb_per_sec=100.0,
            cache_hit_rate=95.0,
        )

        score = metrics.throughput_score()
        assert 0 <= score <= 1
        assert isinstance(score, float)

    def test_trend_analysis_with_data(self):
        """Test TrendAnalysis with crash data."""
        base_time = datetime.now()
        trend = TrendAnalysis(
            campaign_name="test",
            start_time=base_time - timedelta(hours=1),
            end_time=base_time,
            total_duration=timedelta(hours=1),
            crashes_over_time=[
                (base_time - timedelta(minutes=30), 2),
                (base_time - timedelta(minutes=15), 1),
            ],
        )

        assert trend.campaign_name == "test"
        rate = trend.crash_discovery_rate()
        assert rate >= 0

    def test_coverage_correlation_edge_cases(self):
        """Test CoverageCorrelation with edge case values."""
        # Zero values
        corr_zero = CoverageCorrelation(
            strategy="zero_test",
            coverage_increase=0.0,
            unique_paths=0,
            crash_correlation=0.0,
            sample_size=0,
        )
        score_zero = corr_zero.correlation_score()
        assert score_zero >= 0

        # High values
        corr_high = CoverageCorrelation(
            strategy="high_test",
            coverage_increase=100.0,
            unique_paths=1000,
            crash_correlation=1.0,
            sample_size=10000,
        )
        score_high = corr_high.correlation_score()
        assert score_high <= 1


class TestIntegrationWorkflowE2E:
    """Integration tests combining multiple modules."""

    def test_full_analytics_workflow(self, tmp_path):
        """Test complete analytics workflow from data to visualization."""
        # 1. Create analytics data
        effectiveness_data = {
            "strategy_a": {"effectiveness_score": 0.8, "usage_count": 100},
            "strategy_b": {"effectiveness_score": 0.6, "usage_count": 80},
        }

        base_time = datetime.now()
        trend_data = TrendAnalysis(
            campaign_name="integration_test",
            start_time=base_time - timedelta(hours=2),
            end_time=base_time,
            total_duration=timedelta(hours=2),
            crashes_over_time=[
                (base_time - timedelta(hours=1), 1),
                (base_time, 2),
            ],
        )

        coverage_data = {
            "strategy_a": CoverageCorrelation(
                strategy="strategy_a",
                coverage_increase=10.0,
                unique_paths=30,
                crash_correlation=0.4,
                sample_size=50,
            ),
        }

        performance_data = PerformanceMetrics(
            mutations_per_second=100.0,
            peak_memory_mb=512.0,
            avg_memory_mb=256.0,
            cpu_utilization=70.0,
            disk_io_mb_per_sec=50.0,
            cache_hit_rate=90.0,
        )

        # 2. Create visualizer and generate all charts
        visualizer = FuzzingVisualizer(output_dir=str(tmp_path / "charts"))

        charts = []
        charts.append(visualizer.plot_strategy_effectiveness(effectiveness_data, "png"))
        charts.append(visualizer.plot_crash_trend(trend_data, "png"))
        charts.append(visualizer.plot_coverage_heatmap(coverage_data, "png"))
        charts.append(visualizer.plot_performance_dashboard(performance_data, "png"))

        # 3. Verify all charts created
        for chart_path in charts:
            assert chart_path.exists()
            assert chart_path.stat().st_size > 0

        # 4. Generate summary HTML
        html = visualizer.create_summary_report_html(*charts)
        assert len(html) > 0
        assert "charts-container" in html

    def test_realtime_monitor_with_session_data(self, tmp_path):
        """Test realtime monitor integration with session data."""
        from dicom_fuzzer.cli.realtime_monitor import RealtimeMonitor

        # Create session directory structure
        reports_dir = tmp_path / "reports" / "json"
        reports_dir.mkdir(parents=True)

        session_data = {
            "session_info": {"session_name": "integration_test"},
            "statistics": {
                "files_fuzzed": 100,
                "mutations_applied": 500,
                "crashes": 5,
                "hangs": 2,
                "successes": 93,
            },
            "crashes": [],
        }

        (reports_dir / "session_test.json").write_text(json.dumps(session_data))

        # Create monitor
        monitor = RealtimeMonitor(session_dir=tmp_path, refresh_interval=1)

        # Test internal methods
        with patch.object(monitor, "_display_stats") as mock_display:
            with patch(
                "dicom_fuzzer.cli.realtime_monitor.Path", return_value=reports_dir
            ):
                # Manually test refresh display
                monitor._refresh_display()


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
