#+PROPERTY: header-args:jupyter-python  :session db :kernel reddit
#+PROPERTY: header-args    :pandoc t

* Imports and initialization
We start with some imports
#+begin_src jupyter-python
  from pathlib import Path
  import pandas as pd
  import numpy as np
  import os
  import re
  import sqlite3
  import matplotlib.pyplot as plt
  from rcounting import side_threads, counters, analysis, thread_navigation as tn
  from rcounting.reddit_interface import reddit

  plt.style.use("seaborn")
  plot_directory = Path('../../plots')
  data_directory = Path('../../data')
#+end_src

And connect to the sqlite database
#+begin_src jupyter-python
  db = sqlite3.connect("counting.sqlite")
#+end_src

* Loading Data
Then we load some data, both the counts and the gets. We convert the timestamp to a date column, and add a "replying to" column, since some of what we'll be doing later needs it.

#+begin_src jupyter-python
  counts = pd.read_sql("select comments.* "
                       "from comments join submissions "
                       "on comments.submission_id = submissions.submission_id "
                       "where comments.position > 0 "
                       "order by submissions.integer_id, comments.position", db)
  counts['date'] = pd.to_datetime(counts['timestamp'], unit='s')
  counts["username"] = counts["username"].apply(counters.apply_alias)
  counts.drop('timestamp', inplace=True, axis=1)
  counts["replying_to"] = counts["username"].shift(1)
  print(len(counts))
  gets = counts.groupby("submission_id").last().sort_values('date').reset_index()
  gets['basecount'] = (gets.index + 15) * 1000
  gets.loc[[0, 1], ['basecount']] = [0, 16691]

#+end_src

* Validation
** Validating side threads
Using various pieces of functionality, we can validate side threads. Here, we confirm that an old side thread is valid:
#+begin_src jupyter-python
  comment = reddit.comment("gja0ehe")
  comments = pd.DataFrame(tn.fetch_comments(comment))
  side_thread = side_threads.get_side_thread('slowestest')
  side_thread.is_valid_thread(comments)
#+end_src

** Finding errors in the data
Some of comments must be errors. For example, there are some counts which were made before their predecessors. How can that make sense?
#+begin_src jupyter-python
weird_timestamps = counts.loc[counts['date'].diff() < pd.Timedelta('0s')]
weird_timestamps[["position", "username", "comment_id", "submission_id", "body", "date"]]
#+end_src

* Analysis
** Progress over time
A first bit of analysis is to visualize the progress of r/counting over time
#+begin_src jupyter-python
  data = gets.set_index('date')
  resampled = data.resample('30d').mean()
  resampled['basecount'] /= 1e6
  ax = resampled[['basecount']].plot(ylabel='Current count [millions]', xlabel='Date')
  h, l = ax.get_legend_handles_labels()
  ax.legend(h[:1],['count'])
  ax.set_title('Counting progress over time', fontsize=20)
  ax.xaxis.label.set_size(18)
  ax.yaxis.label.set_size(18)
  plt.savefig(plot_directory / 'counts.png', bbox_inches='tight', dpi=300)
#+end_src

The plot looks as follows

[[assets/img/counts.png]]

You can see that the counting rate varies quite a bit over time, with signifcant peaks and lulls in activity. Whether or not there are active runners really changes how fast the count is progressing!

** Total counts vs k_parts
We can try plotting thread participation vs total counts. The expectation is that generally, people who've made more total counts will also have counted in more threads. However, some users might have periods where they make a count every now and then but never do any runs, leading to comparatively more k_parts. On the other hand, some counters might only do runs, giving a counts/thread of up to 500.

We'll start by extracting the number of counts and the threads participated in, using the groupby functionality of `pandas`
#+begin_src jupyter-python
  groups = counts.groupby("username")["submission_id"]
  k_parts = groups.nunique()
  hoc = groups.count()
  combined = pd.concat([k_parts, hoc], axis=1)
  combined.columns = ["k_parts", "total_counts"]
  combined = combined.query("k_parts >= 10")
#+end_src

We can make a polynomial fit of this (well, a linear fit of the log-log quantities), and use matplotlib to plot that
#+begin_src jupyter-python
  linear_model = np.polyfit(np.log10(combined.k_parts), np.log10(combined.total_counts), 1)
  print(linear_model)
  axis = np.linspace(1, combined.k_parts.max(), endpoint=True)
  fig, ax = plt.subplots(1, figsize=(8,5))
  ax.scatter(combined.k_parts, combined.total_counts, alpha=0.7)
  ax.plot(axis, 10**(np.poly1d(linear_model)(np.log10(axis))), linestyle="--", color="0.3",
           lw=2)
  ax.set_xlabel("Threads participated in ")
  ax.set_ylabel("Total counts made")
  ax.set_yscale("log")
  ax.set_xscale("log")
  ax.set_xlim(left=10)
  ax.set_ylim(bottom=10)
  plt.savefig(plot_directory / "parts_vs_counts.png", dpi=300, bbox_inches="tight")

#+end_src

The plot looks as follows:

[[assets/img/parts_vs_counts.png]]

The dashed line is a linear fit on the log-log plot, and it has a slope of 1.3. In this model, that means that if you double the total number of threads participated in by a user, you would expect to multiply their total counts by 2.5

** Number of partners and effective number of partners
As with the number of counts vs threads participated in, we can expect that different counters might have qualitatively different behaviour when it comes to how many counting partners they have, and how often they've counted with each one. Some counters might count a little bit with everybody, while others might run with only a few partners, and drop a count with others every now and then.

To quantify how uneven the counting distribution is we can look at the [[https://en.wikipedia.org/wiki/Effective_number_of_parties][effective number of partners]] of each counter, and compare with the actual number of partners.

#+begin_src jupyter-python
  sorted_counters = counts.groupby("username").size().sort_values(ascending=False)
  top_counters = [x for x in sorted_counters.index[:35] if not counters.is_banned_counter(x)][:30]
  top = sorted_counters.filter(items=top_counters)
  df = counts.loc[counts["username"].isin(top_counters)].groupby(["username", "replying_to"]).size()
  effective_partners = df.groupby(level=0).apply(analysis.effective_number_of_counters).to_frame()
  partners = df.groupby(level=0).count()
  combined = pd.concat([top, effective_partners, partners], axis=1)
  combined["HOC rank"] = range(1, len(combined) + 1)
  combined.columns = ["counts", "c_eff", "c", "rank"]
  combined = combined[["rank", "c", "c_eff"]]
  combined.c_eff = combined.c_eff.round()
  combined.columns = ["HOC rank", "N", "N_(effective)"]
  combined.index.name = "Username"
  print(combined.to_markdown())
#+end_src

We can also get the replying-to and replied-by stats for a single user
#+begin_src jupyter-python
  counter = "thephilsblogbar2"
  nick = "phil"
  subset = counts.loc[counts["username"] == counter].copy()
  replied_by = counts['username'].loc[subset.index + 1]
  replied_by.index -= 1
  subset['replied_by'] = replied_by
  result = pd.concat([subset.groupby("replied_by").count().iloc[:, 0].sort_values(ascending=False),
                      subset.groupby("replying_to").count().iloc[:, 0].sort_values(ascending=False)], axis=1).head(10)
  print(result.to_markdown(headers=['Counting partner', f'No. of replies by {nick}', f'No. of replies to {nick}']))
#+end_src

** Oldest counters
We can see who the oldest still-active counters are
#+begin_src jupyter-python
  cutoff_date = pd.to_datetime('today') - pd.Timedelta('180d')
  active_counters = counts.loc[counts['date'] > cutoff_date].groupby("username").groups.keys()
  counts.loc[counts['username'].isin(active_counters)].groupby("username")["date"].min().sort_values().head(30)
#+end_src

** Gets and streaks
Similarly to the oldest counters, we can see what the longest difference between a counter's first and last get is:
#+begin_src jupyter-python
  gets.groupby('username').agg(lambda x: x.index[-1] - x.index[0]).iloc[:, 0].sort_values(ascending=False).head(10)
#+end_src

We can also calculate what the longest get streaks are. The core of the extraction is the line that says `groups = gets.groupby((y != y.shift()).cumsum())`. Let's unpack it:

- `y != y.shift()` assigns a value of True to all threads with a username that's different from their predecessor
- `.cumsum()` sums up all these True values. The net result is that each get streak is given its own unique number
- `.groupby()` extracts these groups for later use

The groups are then sorted according to size, and prepared for pretty printing.
#+begin_src jupyter-python
  y = gets['username']
  groups = gets.groupby((y != y.shift()).cumsum())
  columns = ['username', 'submission_id', 'comment_id', 'basecount']
  length = 10

  indices = (-groups.size()).sort_values(kind='mergesort').index
  old = groups.first().loc[indices, columns]
  new = groups.last().loc[indices, columns]
  combined = old.join(new, rsuffix='_new')
  combined = combined.loc[~combined['username'].apply(counters.is_banned_counter)].head(length).reset_index(drop=True)
  combined['old_link'] = combined.apply(lambda x: f'[{int(x.basecount / 1000) + 1}K](https://reddit.com/comments/{x.submission_id}/_/{x.comment_id})', axis=1)
  combined['new_link'] = combined.apply(lambda x: f'[{int(x.basecount_new / 1000) + 1}K](https://reddit.com/comments/{x.submission_id_new}/_/{x.comment_id_new})', axis=1)
  combined['streak'] = 1 + (combined['basecount_new'] - combined['basecount']) // 1000
  combined.index += 1
  combined.index.name = "Rank"
  print(combined[['username', 'old_link', 'new_link', 'streak']].to_markdown(headers=['**Rank**', '**username**', '**First Get**', '**Last Get**', '**Streak Length**']))
#+end_src

** Comment bodies
We have access to the body of each comment, so it's possible to do a bit of analysis on those.

To start with, we can just try and find the longest comment bodies in the data. Now, this is possible do do in pure sql, so we don't actually need to load all the millions of rows into memory if we are only interested in the top few:

#+begin_src jupyter-python
  df = pd.read_sql_query('select submission_id, comment_id, LENGTH(body) as length from comments '
                         'order by LENGTH(body) desc limit 6', db)
  df.sort_values('length', ascending=False, inplace=True)
  for row in df.itertuples():
      print(f'https://www.reddit.com/comments/{row.submission_id}/_/{row.comment_id}')
#+end_src

In a more advanced example, we can determine whether a count is comma separated, space separated or has no separator by using regular expressions.

The rules are as follows:

- Comma separated counts look like [digit]*{1-3}(,[digit]*3)*
- Space separated counts are the same, with the comma replaced by a space
- No separated counts are defined as one of
  - Counts with only one digit
  - Counts with no separators between their first and last digit, with separators defined fairly broadly.

Now in this case, we do need access to every comment (since we are determining the type of every comment), and so we'll just proceed with the counts dataframe defined earlier. This gets fairly memory intensive, so care should be used.

#+begin_src jupyter-python
  data = counts.set_index('date')

  data['body'] = data['body'].apply(rct.parsing.strip_markdown_links)
  comma_regex = re.compile(r'\d{1,3}(?:,\d{3})+')
  data['is_comma_separated'] = data['body'].apply(lambda x: bool(re.search(comma_regex, x)))
  space_regex = re.compile(r'\d{1,3}(?: \d{3})+')
  data['is_space_separated'] = data['body'].apply(lambda x: bool(re.search(space_regex, x)))
  def no_separators(body):
      body = body.split('\n')[0]
      separators = re.escape("'â€¯, .*/")
      regex = (rf"(?:^[^\d]*\d[^\d]*$)|"
               rf"(?:^[^\d]*\d[^{separators}]*\d[^\d]*$)")
      regex = re.compile(regex)
      result = re.search(regex, body)
      return bool(result)

  data['no_separators'] = data['body'].apply(no_separators)
  data.sort_index(inplace=True)
#+end_src

Once we have the data, we can get a 14-day rolling average, and resample the points to nice 6h intervals. The resampling makes plotting with pandas look nicer, since it can more easily deal with the x-axis.
#+begin_src jupyter-python
  resampled = (data[['is_comma_separated', 'is_space_separated', 'no_separators']].rolling('14d').mean().resample('6h').mean() * 100)
  fig, ax = plt.subplots(1, figsize = (12, 8))
  resampled.plot(ax=ax, ylabel='Percentage of counts', lw=2)
  h, l = ax.get_legend_handles_labels()
  ax.legend(h[:3],["commas", "spaces", "no separator"])
  ax.set_ylim([0, 100])
  ax.set_title('Separators used on r/counting over time', fontsize=20)
  ax.set_xlabel('')
  ax.yaxis.label.set_size(18)
  ax.tick_params(axis='y', labelsize=16)
  ax.tick_params(axis='x', labelsize=16)

  plt.savefig(plot_directory / 'separators.png', bbox_inches='tight', dpi=300)
#+end_src

That gives the following graph:

[[assets/img/separators.png]]

Notice you can clearly see when the count crossed 100k: that's when the red 'no separators' line drops from being the majority to being a clear minority of counts. That was followed by the era of commas, when the default format was clearly just to use commas as separators. The last two years have been significantly more noisy and muddy, with spaces as separators sometimes overtaking commas. Pretty neat!

** Network analysis
We can do some network analysis. This snippet will generate the (comment, replying to, weight) graph for the top 250 counters. The heavy lifting is done by the [[file:../rcounting/analysis.py::def response_graph(df, n=250, username_column="username"):][response_graph]] function in analysis.py.
#+begin_src jupyter-python
  n = 250
  graph = analysis.response_graph(counts, n, username_column="username")
  graph.to_csv(data_directory / f"graph_{n}.csv", index=False)
#+end_src
