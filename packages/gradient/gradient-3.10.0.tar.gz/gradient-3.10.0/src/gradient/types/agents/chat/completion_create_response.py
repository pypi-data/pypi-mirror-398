# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import List, Optional
from typing_extensions import Literal

from ...._models import BaseModel
from ...shared.completion_usage import CompletionUsage
from ...shared.chat_completion_token_logprob import ChatCompletionTokenLogprob

__all__ = [
    "CompletionCreateResponse",
    "Choice",
    "ChoiceLogprobs",
    "ChoiceMessage",
    "ChoiceMessageToolCall",
    "ChoiceMessageToolCallFunction",
]


class ChoiceLogprobs(BaseModel):
    """Log probability information for the choice."""

    content: Optional[List[ChatCompletionTokenLogprob]] = None
    """A list of message content tokens with log probability information."""

    refusal: Optional[List[ChatCompletionTokenLogprob]] = None
    """A list of message refusal tokens with log probability information."""


class ChoiceMessageToolCallFunction(BaseModel):
    """The function that the model called."""

    arguments: str
    """
    The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
    hallucinate parameters not defined by your function schema. Validate the
    arguments in your code before calling your function.
    """

    name: str
    """The name of the function to call."""


class ChoiceMessageToolCall(BaseModel):
    id: str
    """The ID of the tool call."""

    function: ChoiceMessageToolCallFunction
    """The function that the model called."""

    type: Literal["function"]
    """The type of the tool. Currently, only `function` is supported."""


class ChoiceMessage(BaseModel):
    """A chat completion message generated by the model."""

    content: Optional[str] = None
    """The contents of the message."""

    reasoning_content: Optional[str] = None
    """The reasoning content generated by the model."""

    refusal: Optional[str] = None
    """The refusal message generated by the model."""

    role: Literal["assistant"]
    """The role of the author of this message."""

    tool_calls: Optional[List[ChoiceMessageToolCall]] = None
    """The tool calls generated by the model, such as function calls."""


class Choice(BaseModel):
    finish_reason: Literal["stop", "length", "tool_calls", "content_filter"]
    """The reason the model stopped generating tokens.

    This will be `stop` if the model hit a natural stop point or a provided stop
    sequence, or `length` if the maximum number of tokens specified in the request
    was reached, `tool_calls` if the model called a tool.
    """

    index: int
    """The index of the choice in the list of choices."""

    logprobs: Optional[ChoiceLogprobs] = None
    """Log probability information for the choice."""

    message: ChoiceMessage
    """A chat completion message generated by the model."""


class CompletionCreateResponse(BaseModel):
    """
    Represents a chat completion response returned by model, based on the provided input.
    """

    id: str
    """A unique identifier for the chat completion."""

    choices: List[Choice]
    """A list of chat completion choices.

    Can be more than one if `n` is greater than 1.
    """

    created: int
    """The Unix timestamp (in seconds) of when the chat completion was created."""

    model: str
    """The model used for the chat completion."""

    object: Literal["chat.completion"]
    """The object type, which is always `chat.completion`."""

    usage: Optional[CompletionUsage] = None
    """Usage statistics for the completion request."""
