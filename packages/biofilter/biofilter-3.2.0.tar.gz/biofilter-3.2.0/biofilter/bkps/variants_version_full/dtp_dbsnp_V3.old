import os
import re
import ast
import bz2
import json
import pandas as pd
import __main__
import sys
from concurrent.futures import ProcessPoolExecutor, as_completed
from sqlalchemy.exc import IntegrityError
from sqlalchemy import text


from biofilter.etl.conflict_manager import ConflictManager
from biofilter.etl.mixins.base_dtp import DTPBase
from biofilter.etl.mixins.variant_query_mixin import VariantQueryMixin

from biofilter.db.models.variants_models import (
    # VariantType,
    # AlleleType,
    GenomeAssembly,
    Variant,
    # VariantLocation,
    GeneVariantLink,
)


def transform_batch(lines_batch):
    results = []

    for line in lines_batch:
        record = json.loads(line)
        rs_id = f"{record['refsnp_id']}"
        last_build_id = record.get("last_update_build_id", None)
        primary_data = record.get("primary_snapshot_data", {})
        variant_type = primary_data.get("variant_type", None)

        if variant_type != "snv":
            continue
        # Apos reuniao com a Molly, ela solicitou que apenas as snv fosse carregadas no sistama

        # Run only last build
        placements = primary_data.get("placements_with_allele", [])
        ptlp_placement = next(
            (p for p in placements if p.get("is_ptlp", False)), None
        )  # noqa: E501

        # Get Genes ID
        gene_ids = set()
        for allele_annot in primary_data.get("allele_annotations", []):
            for assembly in allele_annot.get("assembly_annotation", []):
                for gene in assembly.get("genes", []):
                    gene_id = gene.get("id")
                    if gene_id:
                        gene_ids.add(gene_id)

        # Get Allele Info
        if ptlp_placement:
            for allele_info in ptlp_placement.get("alleles", []):

                # Get Data
                hgvs = allele_info.get("hgvs")
                spdi = allele_info.get("allele", {}).get("spdi", {})
                seq_id = spdi.get("seq_id")
                spdi_position = spdi.get("position")
                position_base_1 = int(spdi_position + 1)
                alt_seq = spdi.get("inserted_sequence")

                match = re.match(r"^(.*?):g\.([\d_]+)(.*)$", hgvs)
                pos_raw = match.group(2)
                suffix = match.group(3)

                # Positions
                if "_" in pos_raw:
                    pos_start, pos_end = map(int, pos_raw.split("_"))
                else:
                    pos_start = pos_end = int(pos_raw)

                # Type
                if suffix == "=":
                    allele_type = "ref"
                elif "del" in suffix:
                    allele_type = "del"
                elif "dup" in suffix:
                    allele_type = "dup"
                elif re.search(r"\[\d+\]$", suffix):
                    allele_type = "rep"
                elif re.match(r"[ACGT]>[ACGT]", suffix):
                    allele_type = "sub"
                else:
                    allele_type = "oth"

                results.append({
                    "rs_id": rs_id,
                    "build_id": last_build_id,
                    "seq_id": seq_id,
                    "var_type": variant_type,
                    "hgvs": hgvs,
                    "position_base_1": position_base_1,
                    "position_start": pos_start,
                    "position_end": pos_end,
                    "allele_type": allele_type,
                    "allele": alt_seq,
                    "gene_ids": list(gene_ids),
                })

    return results

# TODO: Ajustar o source_url


class DTP(DTPBase, VariantQueryMixin):
    def __init__(
        self,
        logger=None,
        datasource=None,
        etl_process=None,
        session=None,
        use_conflict_csv=False,
    ):  # noqa: E501
        self.logger = logger
        self.datasource = datasource
        self.etl_process = etl_process
        self.session = session
        self.use_conflict_csv = use_conflict_csv
        self.conflict_mgr = ConflictManager(session, logger)

    def extract(self, raw_dir: str, force_steps: bool):
        """
        Downloads the file from the dbSNP JSON release and stores it locally
        only if it doesn't exist or if the MD5 has changed.
        """

        msg = ""
        source_url = self.datasource.source_url
        if force_steps:
            last_hash = ""
            msg = "Ignoring hash check."
            self.logger.log(msg, "WARNING")
        else:
            last_hash = self.etl_process.raw_data_hash

        try:
            # Landing path
            landing_path = os.path.join(
                raw_dir,
                self.datasource.source_system.name,
                self.datasource.name,
            )

            # Get hash from current md5 file
            url_md5 = f"{source_url}.md5"
            current_hash = self.get_md5_from_url_file(url_md5)

            if not current_hash:
                msg = f"Failed to retrieve MD5 from {url_md5}"
                self.logger.log(msg, "WARNING")
                return False, msg, None

            # Compare current hash and last processed hash
            if current_hash == last_hash:
                msg = f"No change detected in {source_url}"
                self.logger.log(msg, "INFO")
                return False, msg, current_hash

            # Download the file
            status, msg = self.http_download(source_url, landing_path)

            if not status:
                self.logger.log(msg, "ERROR")
                return False, msg, current_hash

            # Finish block
            msg = f"‚úÖ {self.datasource.name} file downloaded to {landing_path}"
            self.logger.log(msg, "INFO")
            return True, msg, current_hash

        except Exception as e:
            msg = f"‚ùå ETL extract failed: {str(e)}"
            self.logger.log(msg, "ERROR")
            return False, msg, None







    def transform(self, raw_path, processed_path):

        # INPUT DATA
        input_file = self.get_raw_file(raw_path)
        if not input_file.exists():
            msg = f"‚ùå Input file not found: {input_file}."
            msg += " Consider running the extract() step or checking the source URL."  # noqa: E501
            self.logger.log(msg, "ERROR")
            return None, False, msg

        # OUTPUT DATA
        output_dir = self.get_path(processed_path)
        output_dir.mkdir(parents=True, exist_ok=True)

        # VARIABLES
        batch_size: int = 1000
        max_workers: int = 10
        # assembly: str = "GRCh38"
        status = False

        results = []
        futures = []
        batch = []

        try:
            with bz2.open(input_file, "rt", encoding="utf-8") as f:
                # if hasattr(__main__, "__file__"):
                if __name__ == "__main__" or (hasattr(__main__, "__file__") and not hasattr(sys, "ps1")):
                    with ProcessPoolExecutor(max_workers=max_workers) as executor:

                        for line in f:
                            batch.append(line)
                            if len(batch) >= batch_size:
                                futures.append(
                                    executor.submit(transform_batch, batch)
                                )  # noqa: E501
                                batch = []

                        # Runs the last batch
                        if batch:
                            futures.append(executor.submit(transform_batch, batch))

                        # Rescues the results from the futures
                        for future in as_completed(futures):
                            try:
                                batch_result = future.result()
                                results.extend(batch_result)
                            except Exception as e:
                                msg = f"‚ö†Ô∏è Worker failed during batch transform: {e}"
                                self.logger.log(msg, "WARNING")

                else:
                    msg = "‚ö†Ô∏è Skipping multiprocessing: not in __main__ context."
                    self.logger.log(msg, "WARNING")

            # Save the results to a CSV file
            transform_df = pd.DataFrame(results)

            # # Buscar os valores do banco
            # variant_type_map = {v.name: str(v.id) for v in self.session.query(VariantType)}
            # allele_type_map = {a.name: str(a.id) for a in self.session.query(AlleleType)}
            assembly_map = {a.accession: str(a.id) for a in self.session.query(GenomeAssembly)}

            # # Mapear as colunas
            # transform_df["variant_type_id"] = transform_df["var_type"].map(variant_type_map)
            # transform_df["allele_type_id"] = transform_df["allele_type"].map(allele_type_map)
            transform_df["assembly_id"] = transform_df["seq_id"].map(assembly_map)

            column_order = [
                "build_id",
                "rs_id",
                "seq_id",
                "assembly_id",
                "var_type",
                # "variant_type_id",
                "hgvs",
                "position_base_1",
                "position_start",
                "position_end",
                "allele_type",
                # "allele_type_id",
                "allele",
                "gene_ids",
            ]

            # Reorganiza o DataFrame (ignora colunas faltantes)
            transform_df = transform_df[[col for col in column_order if col in transform_df.columns]]

            transform_df.to_csv(output_dir / "processed_data.csv", index=False)

            msg = f"‚úÖ Transform completed with {len(transform_df)} records."
            self.logger.log(msg, "INFO")
            status = True
            return transform_df, status, msg

        except Exception as e:
            msg = f"‚ùå ETL transform failed: {str(e)}"
            self.logger.log(msg, "ERROR")
            return None, False, msg

    # # üöß üöú In developing
    def load(self, df=None, processed_path=None, chunk_size=100_000):
        total_variants = 0
        load_status = False
        message = ""

        # üö® Garante que self.datasource √© v√°lido na sess√£o atual
        self.datasource = self.session.merge(self.datasource)
        data_source_id = self.datasource.id

        if df is None:
            if not processed_path:
                msg = "Either 'df' or 'processed_path' must be provided."
                self.logger.log(msg, "ERROR")
                return total_variants, load_status, msg

            processed_path = self.get_path(processed_path)
            processed_data = str(processed_path / "processed_data.csv")  # TODO: change to Msater_data

            if not os.path.exists(processed_data):
                msg = f"File not found: {processed_data}"
                self.logger.log(msg, "ERROR")
                return total_variants, load_status, msg

            self.logger.log(f"üì• Reading data in chunks from {processed_data}", "INFO")

            df = pd.read_csv(processed_data, dtype=str)

        # Apaga os dados da tabela de links
        self.session.query(GeneVariantLink).filter_by(data_source_id=self.datasource.id).delete()

        # Opcional: apagar tamb√©m os variants, se desejar
        self.session.query(Variant).filter_by(data_source_id=self.datasource.id).delete()

        self.session.commit()
        self.logger.log("üóëÔ∏è Previous records deleted for this data source", "INFO")

        # üßº Limpeza de dados
        # df["position"] = df["position_base_1"].astype(int)
        # df["assembly_id"] = df["assembly_id"].astype(int)
        # df["chromosome"] = df["assembly_id"].astype(str) # Adicionar o Cromossmo do CSV
        df["ref"] = ''
        df["alt"] = ''
        # df["rs_id"] = df["rs_id"].astype(str)

        # Isola os registros do tipo 'ref' para obter o alelo de refer√™ncia
        df_ref = df[df["allele_type"] == "ref"].copy()
        df_ref = df_ref[["rs_id", "position_base_1", "assembly_id", "allele"]].drop_duplicates("rs_id")

        # Agrupa os alternativos por rs_id e junta os diferentes ALT
        df_alt = (
            df[df["allele_type"] == "sub"]
            .groupby("rs_id")["allele"]
            .agg(lambda alleles: "/".join(sorted(set(alleles))))
            .reset_index()
            .rename(columns={"allele": "alt"})
        )

        # üîÑ Certifique-se de que os tipos s√£o iguais para merge
        df_ref["rs_id"] = df_ref["rs_id"].astype(str)
        df_alt["rs_id"] = df_alt["rs_id"].astype(str)

        # Junta ref + alt
        df_variants = df_ref.merge(df_alt, on="rs_id", how="left")
        df_variants["alt"] = df_variants["alt"].fillna("")  # pode haver variante sem alt

        # Vou apagar quem nao tem assenmbly_id
        df_variants = df_variants.dropna(subset=["assembly_id", "position_base_1"])

        df_variants["assembly_id"] = df_variants["assembly_id"].astype(int)
        df_variants["position_base_1"] = df_variants["position_base_1"].astype(int)

        # üîÑ Inser√ß√£o em batch
        variants_to_insert = []
        for _, row in df_variants.iterrows():
            variant = Variant(
                rs_id=row["rs_id"],
                position=row["position_base_1"],
                assembly_id=row["assembly_id"],
                chromosome=row["assembly_id"],
                ref=row["allele"],
                alt=row["alt"],
                data_source_id=data_source_id
            )
            variants_to_insert.append(variant)

        try:
            self.session.bulk_save_objects(variants_to_insert)
            self.session.commit()
            total_inserted = len(variants_to_insert)
            load_status = True
            message = f"‚úÖ Loaded {total_inserted} variants into database."
            self.logger.log(message, "SUCCESS")

        except IntegrityError as e:
            self.session.rollback()
            message = f"‚ùå Integrity error during load: {str(e)}"
            self.logger.log(message, "ERROR")

        # Processar os genes variantes links
        # 1. Remove registros sem genes
        df_links = df[df["gene_ids"].notna() & (df["gene_ids"] != "[]")].copy()

        df_links = df_links[["rs_id", "gene_ids"]].drop_duplicates("rs_id")

        # 2. Converte string para lista (se necess√°rio)
        df_links["gene_ids"] = df_links["gene_ids"].apply(
            lambda x: ast.literal_eval(x) if isinstance(x, str) else x
        )

        # 3. Explode o campo gene_ids
        df_links = df_links.explode("gene_ids")

        # 5. (Opcional) Converte tipos para seguran√ßa
        df_links["gene_ids"] = df_links["gene_ids"].astype(int)
        df_links["rs_id"] = df_links["rs_id"].astype(str)

        links_to_insert = []

        for _, row in df_links.iterrows():
            link = GeneVariantLink(
                gene_id=row["gene_ids"],
                variant_id=row["rs_id"],
                data_source_id=data_source_id
            )
            links_to_insert.append(link)

        self.session.bulk_save_objects(links_to_insert)
        self.session.commit()
        self.logger.log(f"‚úÖ Inserted {len(links_to_insert)} gene-variant links", "INFO")

        # Manutencao:
        self.session.execute(text("VACUUM"))
        self.session.commit()

        self.session.execute(text("DROP INDEX IF EXISTS uq_gene_variant"))
        self.session.execute(text("""
            CREATE UNIQUE INDEX uq_gene_variant 
            ON gene_variant_links (gene_id, variant_id)
        """))
        self.session.commit()
        # TODO criar um methodo optimize_database() para essas tarefas

        return total_inserted, load_status, message
