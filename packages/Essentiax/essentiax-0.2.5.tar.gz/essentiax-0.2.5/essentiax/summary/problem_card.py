"""
problem_card.py
====================================
Essentiax ‚Äî Dataset Problem Card (with Model Recommender)

Generates an ML-focused diagnostic report:
‚úî dataset health score
‚úî missing/duplicate summary
‚úî high-cardinality detection
‚úî target analysis + imbalance check
‚úî ML problem type inference
‚úî data leakage warnings
‚úî model recommendations (baseline + advanced)
"""

import pandas as pd
import numpy as np


def _infer_problem_type(df: pd.DataFrame, target: str):
    """Infer ML problem type based on target column."""
    if target is None or target not in df.columns:
        return None, "no_target"

    y = df[target]
    n_unique = y.nunique()

    # Pure numeric target
    if np.issubdtype(y.dtype, np.number):
        if n_unique <= 1:
            return None, "invalid"
        if n_unique <= 20:
            return "classification", "numeric_low_card"
        return "regression", "numeric_regression"

    # Non-numeric ‚Üí could be classification or NLP
    avg_len = y.astype(str).str.len().mean()
    if n_unique <= 1:
        return None, "invalid"
    if n_unique <= 30 and avg_len < 50:
        return "classification", "categorical_labels"
    if avg_len >= 50:
        return "nlp", "long_text"
    return "classification", "categorical_high_card"


def _check_imbalance(y: pd.Series):
    """Return imbalance flag + ratio of majority class if classification."""
    if y is None:
        return False, None

    if np.issubdtype(y.dtype, np.number) and y.nunique() > 20:
        return False, None  # regression, ignore

    counts = y.value_counts(normalize=True)
    if len(counts) < 2:
        return False, None

    top = counts.iloc[0]
    return bool(top > 0.8), float(top)


def _model_recommendations(problem_type, df: pd.DataFrame, target: str, imbalance_flag: bool):
    """
    Return a dict of recommended models/approaches based on:
    - problem_type: classification / regression / nlp / None
    - dataset size / feature count
    - imbalance
    """
    if problem_type is None:
        return {
            "type": "unknown",
            "baseline": [],
            "advanced": [],
            "notes": ["No target or invalid target ‚Äî cannot recommend models."]
        }

    rows, cols = df.shape
    feature_cols = [c for c in df.columns if c != target]
    n_features = len(feature_cols)

    small_data = rows < 10_000
    wide_data = n_features > 100

    rec = {
        "type": problem_type,
        "baseline": [],
        "advanced": [],
        "notes": []
    }

    if problem_type == "classification":
        # Baseline
        rec["baseline"].append("LogisticRegression (with scaling)")
        rec["baseline"].append("RandomForestClassifier")

        # Advanced
        if wide_data:
            rec["advanced"].append("LinearSVC / SGDClassifier (handles many features)")
        else:
            rec["advanced"].append("GradientBoostingClassifier / XGBoost (if available)")

        if imbalance_flag:
            rec["notes"].append("Use class_weight='balanced' or resampling (SMOTE/undersampling).")

    elif problem_type == "regression":
        rec["baseline"].append("LinearRegression (check assumptions)")
        rec["baseline"].append("RandomForestRegressor")

        if wide_data:
            rec["advanced"].append("Lasso / ElasticNet (for feature selection)")
        else:
            rec["advanced"].append("GradientBoostingRegressor / XGBoostRegressor (if available)")

        if rows < 1000:
            rec["notes"].append("Small data ‚Äî prefer simpler models, strong regularization, cross-validation.")

    elif problem_type == "nlp":
        rec["baseline"].append("TF-IDF + LogisticRegression / LinearSVC")
        rec["baseline"].append("TF-IDF + NaiveBayes (for text classification)")

        rec["advanced"].append("Pretrained transformers (e.g., BERT) via HuggingFace")
        rec["notes"].append("Do proper train/val/test split by time or document source to avoid leakage.")

    else:
        rec["notes"].append("Unknown problem type ‚Äî treat as EDA-only for now.")

    if rows > 200_000:
        rec["notes"].append("Large dataset ‚Äî consider subsampling for prototyping, then scale with efficient models.")

    return rec


def problem_card(df: pd.DataFrame, target: str = None):
    print("\nüß† ESSENTIAX ‚Äî PROBLEM CARD (ML READY + MODEL RECOMMENDER)")
    print("=" * 70)

    ROWS, COLS = df.shape

    # =========================================================
    # 1Ô∏è‚É£ BASIC METADATA
    # =========================================================
    print(f"üì¶ Dataset Shape: {ROWS:,} rows √ó {COLS} columns")
    mem = df.memory_usage(deep=True).sum() / 1024**2
    print(f"üß† Memory Usage: {mem:.2f} MB")
    dup = df.duplicated().sum()
    print(f"üîÅ Duplicate Rows: {dup:,}")

    # =========================================================
    # 2Ô∏è‚É£ MISSING VALUES
    # =========================================================
    missing = df.isnull().sum()
    missing_pct = (missing / ROWS) * 100
    missing_df = missing_pct[missing_pct > 0].sort_values(ascending=False)

    print("\n‚ö† Missing Values Summary:")
    if missing_df.empty:
        print("   ‚úì No missing values detected.")
    else:
        for col, pct in missing_df.head(10).items():
            print(f"   ‚Ä¢ {col:<20} ‚Üí {pct:6.2f}% missing")
        if len(missing_df) > 10:
            print(f"   ‚Ä¢ ... and {len(missing_df) - 10} more columns with missing values.")

    # =========================================================
    # 3Ô∏è‚É£ HIGH CARDINALITY CHECK
    # =========================================================
    print("\nüî¢ High-Cardinality Categorical Columns (unique > 50% of rows):")
    high_card = []
    for col in df.select_dtypes(include=['object', 'category']):
        nunq = df[col].nunique()
        if nunq > ROWS * 0.5:
            high_card.append(col)
            print(f"   ‚Ä¢ {col} ({nunq:,} unique)")

    if not high_card:
        print("   ‚úì No high-cardinality categorical issues detected.")

    # =========================================================
    # 4Ô∏è‚É£ NUMERIC SUMMARY
    # =========================================================
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    print("\nüìà Numeric Columns:", len(num_cols))

    if num_cols:
        desc = df[num_cols].describe().T[['mean', 'std', 'min', 'max']]
        print(desc.to_string())
    else:
        print("   ‚ö† Dataset has NO numeric columns.")

    # =========================================================
    # 5Ô∏è‚É£ TARGET ANALYSIS + PROBLEM TYPE
    # =========================================================
    y = None
    problem_type, reason = _infer_problem_type(df, target)

    if target is None:
        print("\nüéØ Target: NOT PROVIDED ‚Äî cannot infer ML problem type.")
    elif target not in df.columns:
        print(f"\nüéØ Target: '{target}' ‚ùå (column not found)")
    else:
        y = df[target]
        print(f"\nüéØ Target Column: {target}")
        print(f"   ‚Ä¢ Dtype: {y.dtype}")
        print(f"   ‚Ä¢ Unique Values: {y.nunique()}")

        if problem_type is None:
            print("   ‚ùå Invalid or constant target ‚Äî cannot define a problem.")
        else:
            print(f"   ‚úî Inferred Problem Type: {problem_type.upper()} ({reason})")

    # Imbalance check
    imbalance_flag, majority_ratio = _check_imbalance(y) if y is not None else (False, None)
    if y is not None and problem_type == "classification":
        if imbalance_flag:
            print(f"   ‚ö† Class Imbalance Detected ‚Äî majority class = {majority_ratio*100:.1f}%")
        else:
            print("   ‚úì No severe class imbalance detected.")

    # =========================================================
    # 6Ô∏è‚É£ DATA LEAKAGE CHECK
    # =========================================================
    print("\nüõë Leakage Scan:")
    potential_leaks = []
    keywords = ['id', 'identifier', 'code', 'uuid', 'ssn', 'phone', 'email']

    for col in df.columns:
        lcol = col.lower()
        for key in keywords:
            if key in lcol:
                potential_leaks.append(col)
                break

    if potential_leaks:
        for col in potential_leaks:
            print(f"   ‚Ä¢ Potential ID/leakage column: {col}")
    else:
        print("   ‚úì No obvious ID/leakage columns detected.")

    # =========================================================
    # 7Ô∏è‚É£ DATASET HEALTH SCORE
    # =========================================================
    score = 100

    missing_pct_total = (df.isnull().sum().sum() / (ROWS * COLS)) * 100 if ROWS * COLS > 0 else 0
    if missing_pct_total > 10:
        score -= 20
    if len(high_card) > 2:
        score -= 10
    if dup > 0:
        score -= 5
    if len(num_cols) == 0:
        score -= 20
    if mem > 200:
        score -= 5
    if problem_type is None and target is not None:
        score -= 10

    score = max(int(score), 1)
    print("\nüíØ Dataset Quality Score:", score, "/ 100")

    # =========================================================
    # 8Ô∏è‚É£ MODEL RECOMMENDATIONS
    # =========================================================
    print("\nü§ñ Model Recommendations:")
    rec = _model_recommendations(problem_type, df, target, imbalance_flag)

    print(f"   ‚Ä¢ Problem Type: {rec['type']}")
    if rec["baseline"]:
        print("   ‚Ä¢ Baseline Models:")
        for m in rec["baseline"]:
            print(f"       - {m}")
    if rec["advanced"]:
        print("   ‚Ä¢ Advanced Models:")
        for m in rec["advanced"]:
            print(f"       - {m}")
    if rec["notes"]:
        print("   ‚Ä¢ Notes:")
        for n in rec["notes"]:
            print(f"       - {n}")

    # =========================================================
    # 9Ô∏è‚É£ RECOMMENDATIONS SUMMARY
    # =========================================================
    print("\nüìù General Recommendations:")
    if not missing_df.empty:
        print("   ‚Ä¢ Handle missing values (use smart_clean).")
    if high_card:
        print("   ‚Ä¢ High-cardinality columns ‚Üí avoid naive one-hot; use target/hashing encoding.")
    if num_cols:
        print("   ‚Ä¢ Scale numeric features before training linear models.")
    if problem_type in ["classification", "regression"]:
        print("   ‚Ä¢ Do proper train/val/test split; avoid leakage from time or IDs.")

    print("\n" + "=" * 70 + "\n")
