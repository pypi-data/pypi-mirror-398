[project]
name = "ebt_flash_attention"
version = "0.2.0"
description = "Triton implemention of the Energy Based Transformer attention mechanism"
authors = [
    { name = "Emile Dugelay", email = "dugelayemile@gmail.com" }
]
readme = "README.md"
requires-python = ">=3.10"
license = "MIT"
keywords = [
    'artificial intelligence',
    'deep learning',
    'triton',
    'transformer',
]
classifiers=[
    'Development Status :: 3 - Alpha',
    'Intended Audience :: Developers',
    'Topic :: Scientific/Engineering :: Artificial Intelligence',
    'License :: OSI Approved :: MIT License',
    'Programming Language :: Python :: 3.10',
]
dependencies = [
    "torch>=2.9.1",
    "triton>=3.5.1",
]

[project.urls]
"Homepage" = "https://pypi.org/project/ebt_flash_attention/"
"Repository" = "https://github.com/emiledgl/ebt_flash_attention"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["ebt_flash_attention"]
