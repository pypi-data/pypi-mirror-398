jinx_name: paper_search
description: Search for academic papers across multiple sources (Semantic Scholar, arXiv, local datasets)
inputs:
  - query: ""
  - limit: 10
  - source: "all"
steps:
  - name: search_papers
    engine: python
    code: |
      import os
      import time
      import requests
      import urllib.request
      import urllib.parse
      import xml.etree.ElementTree as ET

      query = context.get('query', '')
      limit = int(context.get('limit', 10))
      source = context.get('source', 'all').lower()

      if not query:
          context['output'] = """Usage: /paper_search <query> [--limit N] [--source SOURCE]

      Sources:
        all    - Search all available sources (default)
        s2     - Semantic Scholar only (requires S2_API_KEY)
        arxiv  - arXiv only
      """
          exit()

      all_results = []

      # Semantic Scholar
      if source in ['all', 's2']:
          api_key = os.environ.get('S2_API_KEY')
          if api_key:
              try:
                  url = "https://api.semanticscholar.org/graph/v1/paper/search"
                  headers = {"x-api-key": api_key}
                  params = {"query": query, "limit": limit, "fields": "title,abstract,authors,year,citationCount,url"}
                  response = requests.get(url, headers=headers, params=params, timeout=30)
                  response.raise_for_status()
                  for paper in response.json().get('data', []):
                      all_results.append({
                          'source': 'Semantic Scholar',
                          'title': paper.get('title', ''),
                          'year': paper.get('year'),
                          'citations': paper.get('citationCount', 0),
                          'authors': [a.get('name', '') for a in paper.get('authors', [])],
                          'abstract': paper.get('abstract', '')[:300] if paper.get('abstract') else '',
                          'url': paper.get('url', '')
                      })
              except Exception as e:
                  print(f"S2 error: {e}")

      # arXiv
      if source in ['all', 'arxiv']:
          try:
              base_url = "http://export.arxiv.org/api/query"
              params = {"search_query": f"all:{query}", "max_results": limit}
              url = f"{base_url}?{urllib.parse.urlencode(params)}"
              with urllib.request.urlopen(url, timeout=30) as response:
                  data = response.read().decode('utf-8')
              root = ET.fromstring(data)
              ns = {'atom': 'http://www.w3.org/2005/Atom'}
              for entry in root.findall('atom:entry', ns):
                  all_results.append({
                      'source': 'arXiv',
                      'title': entry.find('atom:title', ns).text.strip().replace('\n', ' '),
                      'year': entry.find('atom:published', ns).text[:4],
                      'citations': None,
                      'authors': [a.find('atom:name', ns).text for a in entry.findall('atom:author', ns)],
                      'abstract': entry.find('atom:summary', ns).text.strip()[:300],
                      'url': entry.find('atom:id', ns).text
                  })
          except Exception as e:
              print(f"arXiv error: {e}")

      if not all_results:
          context['output'] = f"No papers found for: {query}"
          exit()

      # Format output
      results = []
      for i, paper in enumerate(all_results[:limit], 1):
          authors = ', '.join(paper['authors'][:3])
          if len(paper['authors']) > 3:
              authors += ' et al.'
          year = paper.get('year', '?')
          citations = f", {paper['citations']} citations" if paper.get('citations') else ""

          results.append(f"{i}. [{paper['source']}] {paper['title']} ({year}{citations})")
          results.append(f"   Authors: {authors}")
          if paper['abstract']:
              results.append(f"   Abstract: {paper['abstract']}...")
          results.append(f"   URL: {paper['url']}")
          results.append("")

      context['output'] = f"Found {len(all_results)} papers:\n\n" + "\n".join(results)
      context['papers'] = all_results
