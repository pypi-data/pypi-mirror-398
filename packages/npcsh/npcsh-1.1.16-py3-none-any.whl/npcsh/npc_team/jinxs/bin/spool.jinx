jinx_name: spool
description: Interactive chat mode - simple conversational interface with an NPC
inputs:
  - model: null
  - provider: null
  - attachments: null
  - stream: true

steps:
  - name: spool_repl
    engine: python
    code: |
      import os
      import sys
      from termcolor import colored

      from npcpy.llm_funcs import get_llm_response
      from npcpy.npc_sysenv import get_system_message, render_markdown
      from npcpy.data.load import load_file_contents
      from npcpy.data.text import rag_search

      npc = context.get('npc')
      team = context.get('team')
      messages = context.get('messages', [])
      stream = context.get('stream', True)
      attachments = context.get('attachments')

      # Use NPC's model/provider or fallback
      model = context.get('model') or (npc.model if npc else None)
      provider = context.get('provider') or (npc.provider if npc else None)

      # ASCII art
      print("""
       _____ ____   ____   ____  _
      / ___/|  _ \ / __ \ / __ \| |
      \___ \| |_) | |  | | |  | | |
       ___) |  __/| |  | | |  | | |___
      |____/|_|    \____/ \____/|_____|
      """)

      npc_name = npc.name if npc else "chat"
      print(f"Entering spool mode (NPC: {npc_name}). Type '/sq' to exit.")

      # Load attachments if provided
      loaded_chunks = {}
      if attachments:
          if isinstance(attachments, str):
              attachments = [f.strip() for f in attachments.split(',')]
          for file_path in attachments:
              file_path = os.path.expanduser(file_path)
              if os.path.exists(file_path):
                  try:
                      chunks = load_file_contents(file_path)
                      loaded_chunks[file_path] = chunks
                      print(colored(f"Loaded {len(chunks)} chunks from: {file_path}", "green"))
                  except Exception as e:
                      print(colored(f"Error loading {file_path}: {e}", "red"))

      # Ensure system message
      if not messages or messages[0].get("role") != "system":
          sys_msg = get_system_message(npc) if npc else "You are a helpful assistant."
          messages.insert(0, {"role": "system", "content": sys_msg})

      # REPL loop
      while True:
          try:
              prompt_str = f"{npc_name}> "
              user_input = input(prompt_str).strip()

              if not user_input:
                  continue

              if user_input.lower() == "/sq":
                  print("Exiting spool mode.")
                  break

              # Handle /ots for screenshots inline
              if user_input.startswith("/ots"):
                  from npcpy.data.image import capture_screenshot
                  parts = user_input.split()
                  image_paths = []
                  if len(parts) > 1:
                      for p in parts[1:]:
                          fp = os.path.expanduser(p)
                          if os.path.exists(fp):
                              image_paths.append(fp)
                  else:
                      ss = capture_screenshot()
                      if ss and "file_path" in ss:
                          image_paths.append(ss["file_path"])
                          print(colored(f"Screenshot: {ss['filename']}", "green"))

                  if image_paths:
                      vision_prompt = input("Prompt for image(s): ").strip() or "Describe these images."
                      resp = get_llm_response(
                          vision_prompt,
                          model=npc.vision_model if hasattr(npc, 'vision_model') else model,
                          provider=npc.vision_provider if hasattr(npc, 'vision_provider') else provider,
                          messages=messages,
                          images=image_paths,
                          stream=stream,
                          npc=npc
                      )
                      messages = resp.get('messages', messages)
                      render_markdown(str(resp.get('response', '')))
                  continue

              # Add RAG context if files loaded
              current_prompt = user_input
              if loaded_chunks:
                  context_content = ""
                  for filename, chunks in loaded_chunks.items():
                      full_text = "\n".join(chunks)
                      retrieved = rag_search(user_input, full_text, similarity_threshold=0.3)
                      if retrieved:
                          context_content += f"\n\nContext from {filename}:\n{retrieved}\n"
                  if context_content:
                      current_prompt += f"\n\n--- Relevant context ---{context_content}"

              # Get response
              resp = get_llm_response(
                  current_prompt,
                  model=model,
                  provider=provider,
                  messages=messages,
                  stream=stream,
                  npc=npc
              )

              messages = resp.get('messages', messages)
              response_text = resp.get('response', '')

              # Handle streaming vs non-streaming
              if hasattr(response_text, '__iter__') and not isinstance(response_text, str):
                  full_response = ""
                  for chunk in response_text:
                      if hasattr(chunk, 'choices') and chunk.choices:
                          delta = chunk.choices[0].delta
                          if hasattr(delta, 'content') and delta.content:
                              print(delta.content, end='', flush=True)
                              full_response += delta.content
                  print()
              else:
                  render_markdown(str(response_text))

              # Track usage if available
              if 'usage' in resp and npc and hasattr(npc, 'shared_context'):
                  usage = resp['usage']
                  npc.shared_context['session_input_tokens'] += usage.get('input_tokens', 0)
                  npc.shared_context['session_output_tokens'] += usage.get('output_tokens', 0)
                  npc.shared_context['turn_count'] += 1

          except KeyboardInterrupt:
              print("\nUse '/sq' to exit or continue.")
              continue
          except EOFError:
              print("\nExiting spool mode.")
              break

      context['output'] = "Exited spool mode."
      context['messages'] = messages
