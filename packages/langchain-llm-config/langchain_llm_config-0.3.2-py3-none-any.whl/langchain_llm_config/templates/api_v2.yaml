# Configuration template for langchain-llm-config (v2 format)
# This is the new flexible model-centric configuration format

# Default models to use for different capabilities
default:
  chat_provider: gpt-3.5-turbo
  embedding_provider: text-embedding-ada-002

# Model definitions - each model is independently configured
models:
  # OpenAI Chat Models
  gpt-3.5-turbo:
    model_type: chat
    provider_type: openai
    model_config:
      api_base: https://api.openai.com/v1
      api_key: ${OPENAI_API_KEY}
      model_name: gpt-3.5-turbo
      temperature: 0.7
      max_tokens: 8192
      connect_timeout: 30
      read_timeout: 60

  gpt-4:
    model_type: chat
    provider_type: openai
    model_config:
      api_base: https://api.openai.com/v1
      api_key: ${OPENAI_API_KEY}
      model_name: gpt-4
      temperature: 0.7
      max_tokens: 8192
      connect_timeout: 30
      read_timeout: 60

  # VLLM Chat Models
  llama-2-7b:
    model_type: chat
    provider_type: vllm
    model_config:
      api_base: http://localhost:8000/v1
      api_key: ${OPENAI_API_KEY}
      model_name: meta-llama/Llama-2-7b-chat-hf
      temperature: 0.6
      top_p: 0.8
      max_tokens: 8192
      connect_timeout: 30
      read_timeout: 60
      extra_body:
        return_reasoning: false

  # Gemini Chat Models
  gemini-pro:
    model_type: chat
    provider_type: gemini
    model_config:
      api_key: ${GEMINI_API_KEY}
      model_name: gemini-pro
      temperature: 0.7
      max_tokens: 8192
      top_p: 1.0
      connect_timeout: 30
      model_kwargs: {}

  # OpenAI Embeddings
  text-embedding-ada-002:
    model_type: embedding
    provider_type: openai
    model_config:
      api_base: https://api.openai.com/v1
      api_key: ${OPENAI_API_KEY}
      model_name: text-embedding-ada-002
      timeout: 30

  # VLLM Embeddings
  bge-m3:
    model_type: embedding
    provider_type: vllm
    model_config:
      api_base: http://localhost:8000/v1
      api_key: ${OPENAI_API_KEY}
      model_name: bge-m3
      dimensions: 1024
      timeout: 30

  # Infinity Embeddings
  bge-m3-infinity:
    model_type: embedding
    provider_type: infinity
    model_config:
      api_base: http://localhost:7997/v1
      model_name: models/bge-m3

  # Gemini Embeddings
  gemini-embedding:
    model_type: embedding
    provider_type: gemini
    model_config:
      api_key: ${GEMINI_API_KEY}
      model_name: embedding-001
      timeout: 30

  # Kunlun Chat Models (OpenAI-compatible with bearer token)
  kunlun-qwen3-235b:
    model_type: chat
    provider_type: kunlun
    model_config:
      api_base: ${KUNLUN_QWEN3_235B_API_BASE}  # Your Kunlun API endpoint URL
      bearer_token: ${KUNLUN_BEARER_TOKEN}
      model_name: Qwen3-235B-A22B
      temperature: 0.7
      max_tokens: 8000
      connect_timeout: 60
      read_timeout: 60
      extra_body:
        chat_template_kwargs:
          enable_thinking: true  # Enable reasoning mode

  kunlun-qwen3-32b:
    model_type: chat
    provider_type: kunlun
    model_config:
      api_base: ${KUNLUN_QWEN3_32B_API_BASE}  # Your Kunlun API endpoint URL
      bearer_token: ${KUNLUN_BEARER_TOKEN}
      model_name: Qwen3-32B
      temperature: 0.7
      max_tokens: 32000
      connect_timeout: 60
      read_timeout: 60

  # Kunlun Embeddings (OpenAI-compatible with bearer token)
  kunlun-bge-m3:
    model_type: embedding
    provider_type: kunlun
    model_config:
      api_base: ${KUNLUN_BGE_M3_API_BASE}  # Your Kunlun API endpoint URL
      bearer_token: ${KUNLUN_BEARER_TOKEN}
      model_name: embedding
      dimensions: 1024
      timeout: 60

  kunlun-qwen3-embedding:
    model_type: embedding
    provider_type: kunlun
    model_config:
      api_base: ${KUNLUN_QWEN3_EMBEDDING_API_BASE}  # Your Kunlun API endpoint URL
      bearer_token: ${KUNLUN_BEARER_TOKEN}
      model_name: Qwen3-Embedding-4B
      timeout: 60

