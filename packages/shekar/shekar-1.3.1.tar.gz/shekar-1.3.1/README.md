
![Shekar](https://amirivojdan.io/wp-content/uploads/2025/01/shekar-lib.png)

<p align="center">
<a href="https://pypi.python.org/pypi/shekar" target="_blank"><img alt="PyPI - Version" src="https://img.shields.io/pypi/v/shekar?color=00A693"></a>
<a href="https://pypi.python.org/pypi/shekar" target="_blank"><img alt="GitHub Actions Workflow Status" src="https://img.shields.io/github/actions/workflow/status/amirivojdan/shekar/test.yml?color=00A693"></a>
<a href="https://pypi.python.org/pypi/shekar" target="_blank"><img alt="Codecov" src="https://img.shields.io/codecov/c/github/amirivojdan/shekar?color=00A693"></a>
<a href="https://pypi.python.org/pypi/shekar" target="_blank"><img alt="PyPI - License" src="https://img.shields.io/pypi/l/shekar?color=00A693"></a>
<a href="https://pypi.python.org/pypi/shekar" target="_blank"><img alt="PyPI - Python Version" src="https://img.shields.io/pypi/pyversions/shekar?color=00A693"></a>
<a href="https://doi.org/10.21105/joss.09128" target="_blank">
<img alt="Static Badge" src="https://img.shields.io/badge/JOSS-10.21105%2Fjoss.09128-00A693"></a>
</p>

<p align="center">
    <em>Simplifying Persian NLP for Modern Applications</em>
</p>

**Shekar** (meaning 'sugar' in Persian) is an open-source Python library for Persian natural language processing, named after the influential satirical story *"ÙØ§Ø±Ø³ÛŒ Ø´Ú©Ø± Ø§Ø³Øª"* (Persian is Sugar) published in 1921 by Mohammad Ali Jamalzadeh. The story became a cornerstone of Iran's literary renaissance, advocating for accessible yet eloquent expression. Shekar embodies this philosophy in its design and development.

It provides tools for text preprocessing, tokenization, part-of-speech(POS) tagging, named entity recognition(NER), embeddings, spell checking, and more. With its modular pipeline design, Shekar makes it easy to build reproducible workflows for both research and production applications.

<div dir="rtl">
<b>Ø´Ú©Ø±</b> ÛŒÚ© Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡Ù” Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª Ú©Ù‡ Ù†Ø§Ù… Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø² Ø¯Ø§Ø³ØªØ§Ù† Ø·Ù†Ø² <b>Â«ÙØ§Ø±Ø³ÛŒ Ø´Ú©Ø± Ø§Ø³ØªÂ»</b> ÙˆØ§Ù… Ú¯Ø±ÙØªÙ‡ Ø§Ø³ØªØ› Ø§Ø«Ø±ÛŒ Ù…Ø§Ù†Ø¯Ú¯Ø§Ø± Ú©Ù‡ Ø¯Ø± Ø³Ø§Ù„ Û±Û¹Û²Û± Ø¨Ù‡ Ù‚Ù„Ù… Ù…Ø­Ù…Ø¯Ø¹Ù„ÛŒ Ø¬Ù…Ø§Ù„Ø²Ø§Ø¯Ù‡ Ù…Ù†ØªØ´Ø± Ø´Ø¯. Ø§ÛŒÙ† Ø¯Ø§Ø³ØªØ§Ù† Ø¨Ù‡ ÛŒÚ©ÛŒ Ø§Ø² Ø§Ø±Ú©Ø§Ù† Ù†ÙˆØ²Ø§ÛŒÛŒ Ø§Ø¯Ø¨ÛŒ Ø§ÛŒØ±Ø§Ù† Ø¨Ø¯Ù„ Ø´Ø¯ Ú©Ù‡ Ø¨Ø§ ØªØ±ÙˆÛŒØ¬ Ø²Ø¨Ø§Ù†ÛŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ø±ÙˆØ§Ù†ØŒ Ù…Ø³ÛŒØ± ØªØ§Ø²Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø§Ø¯Ø¨ÛŒØ§Øª Ù…Ø¹Ø§ØµØ± Ú¯Ø´ÙˆØ¯. 
Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡Ù” <b>Ø´Ú©Ø±</b> Ù†ÛŒØ² Ø¨Ø§ Ø§Ù„Ù‡Ø§Ù… Ø§Ø² Ù‡Ù…ÛŒÙ† Ù†Ú¯Ø±Ø´ØŒ ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒØŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ø¯Ø± Ø¹ÛŒÙ† Ø­Ø§Ù„ Ø¯Ù‚ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ ÙØ±Ø§Ù‡Ù… Ú©Ù†Ø¯ ØªØ§ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø±Ø§Ù†ØŒ ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù† Ùˆ Ø¹Ù„Ø§Ù‚Ù‡â€ŒÙ…Ù†Ø¯Ø§Ù† Ø¨ØªÙˆØ§Ù†Ù†Ø¯ Ø¨Ù‡â€ŒØ±Ø§Ø­ØªÛŒ Ø§Ø² Ø¢Ù† Ø¯Ø± Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù†Ø¯.
</div>


Documentation: https://lib.shekar.io/

### Table of Contents

- [Installation](#installation)
  - [CPU Installation (All Platforms)](#cpu-installation-all-platforms)
  - [GPU Acceleration (NVIDIA CUDA)](#gpu-acceleration-nvidia-cuda)
- [Preprocessing](#preprocessing)
  - [Normalizer](#normalizer)
  - [Customization](#customization)
- [Tokenization](#tokenization)
  - [WordTokenizer](#wordtokenizer)
  - [SentenceTokenizer](#sentencetokenizer)
- [Embeddings](#embeddings)
  - [Word Embeddings](#word-embeddings)
  - [Contextual Embeddings](#contextual-embeddings)
- [Stemming](#stemming)
- [Lemmatization](#lemmatization)
- [Part-of-Speech Tagging](#part-of-speech-tagging)
- [Named Entity Recognition (NER)](#named-entity-recognition-ner)
- [Sentiment Analysis](#sentiment-analysis)
- [Toxicity Detection](#toxicity-detection)
- [Keyword Extraction](#keyword-extraction)
- [Spell Checking](#spell-checking)
- [WordCloud](#wordcloud)
- [Command-Line Interface (CLI)](#command-line-interface-cli)
- [Download Models](#download-models)
- [Citation](#citation)


## Installation

You can install Shekar with pip. By default, the `CPU` runtime of ONNX is included, which works on all platforms.

### CPU Installation (All Platforms)

<!-- termynal -->
```bash
$ pip install shekar
```
This works on **Windows**, **Linux**, and **macOS** (including Apple Silicon M1/M2/M3).

### GPU Acceleration (NVIDIA CUDA)
If you have an NVIDIA GPU and want hardware acceleration, you need to replace the CPU runtime with the GPU version.

**Prerequisites**

- NVIDIA GPU with CUDA support
- Appropriate CUDA Toolkit installed
- Compatible NVIDIA drivers

<!-- termynal -->
```bash
$ pip install shekar && pip uninstall -y onnxruntime && pip install onnxruntime-gpu
```

## Preprocessing

[![Notebook](https://img.shields.io/badge/Notebook-Jupyter-00A693.svg)](examples/preprocessing.ipynb)  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amirivojdan/shekar/blob/main/examples/preprocessing.ipynb)

### Normalizer

The built-in `Normalizer` class provides a ready-to-use, opinionated normalization pipeline for Persian text. It combines the most common and error-prone normalization steps into a single component, covering the majority of real-world use cases such as web text, social media, OCR output, and mixed informalâ€“formal writing.

Most importantly, the normalization rules in Shekar strictly follow the official guidelines of **Academy of Persian Language and Literature** (ÙØ±Ù‡Ù†Ú¯Ø³ØªØ§Ù† Ø²Ø¨Ø§Ù† Ùˆ Ø§Ø¯Ø¨ ÙØ§Ø±Ø³ÛŒ) published on **apll.ir**. This makes the output suitable not only for NLP pipelines, but also for linguistically correct and publishable Persian text.

```python
from shekar import Normalizer

normalizer = Normalizer()

text = "Â«ÙØ§Ø±Ø³ÛŒ Ø´ÙÚ©ÙØ± Ø§Ø³ØªÂ» Ù†Ø§Ù… Ø¯Ø§Ø³ØªØ§Ù† ÚªÙˆØªØ§Ù‡ Ø·Ù†Ø²    Ø¢Ù…ÛØ²ÛŒ Ø§Ø² Ù…Ø­Ù…Ø¯ Ø¹Ù„ÛŒ Ø¬Ù…Ø§Ù„Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ø²Ø§Ø¯Ù‡ ÛŒ Ú¯Ø±Ø§Ù…ÛŒ Ù…ÛŒ   Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ø¯Ø± Ø³Ø§Ù„ 1921 Ù…Ù†ØªØ´Ø±  Ø´Ø¯Ù‡ Ø§Ø³Øª Ùˆ Ø¢ØºØ§Ø²   Ú±Ø± ØªØ­ÙˆÙ„ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯Ø± Ø§Ø¯ÙØ¨ÛØ§Øª Ù…Ø¹Ø§ØµØ± Ø§ÛŒØ±Ø§Ù† ğŸ‡®ğŸ‡· Ø¨Ûƒ Ø´Ù…Ø§Ø± Ù…ÛŒØ±ÙˆØ¯."
print(normalizer(text))

# Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯ÙØªØ§Ø±ÛŒ Ùˆ Ø±ÙˆØ²Ù…Ø±Ù‡
text = normalizer("Ù…ÛŒ Ø¯ÙˆÙ†ÛŒ Ú©Ù‡ Ù†Ù…ÛŒØ®Ø§Ø³ØªÙ… Ù†Ø§Ø±Ø§Ø­ØªØª Ú©Ù†Ù….Ø§Ù…Ø§ Ø®ÙˆÙ†Ù‡ Ù‡Ø§Ø´ÙˆÙ† Ø®ÛŒÙ„ÛŒ Ú¯Ø±ÙˆÙ† ØªØ± Ø´Ø¯Ù‡")
print(text)

# Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ú©Ø¨ Ùˆ Ø§ÙØ¹Ø§Ù„ Ù¾ÛŒØ´ÙˆÙ†Ø¯ÛŒ! 
text = normalizer("ÛŒÚ© Ú©Ø§Ø± Ø¢ÙØ±ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡ Ùˆ Ø³Ø®Øª Ú©ÙˆØ´ ØŒ Ù¾ÛŒØ±ÙˆØ² Ù…Ù†Ø¯Ø§Ù†Ù‡ Ø§Ø² Ù¾Ø³ Ø¯Ø´ÙˆØ§Ø±ÛŒ Ù‡Ø§ Ø¨Ø±Ø®ÙˆØ§Ù‡Ø¯Ø¢Ù…Ø¯.")
print(text) 

```

```shell
Â«ÙØ§Ø±Ø³ÛŒ Ø´Ú©Ø± Ø§Ø³ØªÂ» Ù†Ø§Ù… Ø¯Ø§Ø³ØªØ§Ù† Ú©ÙˆØªØ§Ù‡ Ø·Ù†Ø²Ø¢Ù…ÛŒØ²ÛŒ Ø§Ø² Ù…Ø­Ù…Ø¯â€ŒØ¹Ù„ÛŒ Ø¬Ù…Ø§Ù„Ø²Ø§Ø¯Ù‡â€ŒÛŒ Ú¯Ø±Ø§Ù…ÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ Ú©Ù‡ Ø¯Ø± Ø³Ø§Ù„ Û±Û¹Û²Û± Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡â€ŒØ§Ø³Øª Ùˆ Ø¢ØºØ§Ø²Ú¯Ø± ØªØ­ÙˆÙ„ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯Ø± Ø§Ø¯Ø¨ÛŒØ§Øª Ù…Ø¹Ø§ØµØ± Ø§ÛŒØ±Ø§Ù† Ø¨Ù‡ Ø´Ù…Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯.

Ù…ÛŒâ€ŒØ¯ÙˆÙ†ÛŒ Ú©Ù‡ Ù†Ù…ÛŒâ€ŒØ®Ø§Ø³ØªÙ… Ù†Ø§Ø±Ø§Ø­ØªØª Ú©Ù†Ù…. Ø§Ù…Ø§ Ø®ÙˆÙ†Ù‡â€ŒÙ‡Ø§Ø´ÙˆÙ† Ø®ÛŒÙ„ÛŒ Ú¯Ø±ÙˆÙ†â€ŒØªØ± Ø´Ø¯Ù‡

ÛŒÚ© Ú©Ø§Ø±Ø¢ÙØ±ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡ Ùˆ Ø³Ø®Øªâ€ŒÚ©ÙˆØ´ØŒ Ù¾ÛŒØ±ÙˆØ²Ù…Ù†Ø¯Ø§Ù†Ù‡ Ø§Ø² Ù¾Ø³ Ø¯Ø´ÙˆØ§Ø±ÛŒâ€ŒÙ‡Ø§ Ø¨Ø± Ø®ÙˆØ§Ù‡Ø¯ Ø¢Ù…Ø¯.
```

### Customization

For advanced customization, Shekar offers a modular and composable framework for text preprocessing. It includes components such as `filters`, `normalizers`, and `maskers`, which can be applied individually or flexibly combined using the `Pipeline` class with the `|` operator.
A comprehensive list of operators is available at https://lib.shekar.io/tutorials/preprocessing/

You can combine any of the preprocessing components using the `|` operator:

```python
from shekar.preprocessing import EmojiRemover, PunctuationRemover

text = "Ø² Ø§ÛŒØ±Ø§Ù† Ø¯Ù„Ø´ ÛŒØ§Ø¯ Ú©Ø±Ø¯ Ùˆ Ø¨Ø³ÙˆØ®Øª! ğŸŒğŸ‡®ğŸ‡·"
pipeline = EmojiRemover() | PunctuationRemover()
output = pipeline(text)
print(output)
```

```shell
Ø² Ø§ÛŒØ±Ø§Ù† Ø¯Ù„Ø´ ÛŒØ§Ø¯ Ú©Ø±Ø¯ Ùˆ Ø¨Ø³ÙˆØ®Øª
```

## Tokenization

### WordTokenizer
The WordTokenizer class in Shekar is a simple, rule-based tokenizer for Persian that splits text based on punctuation and whitespace using Unicode-aware regular expressions.

```python
from shekar import WordTokenizer

tokenizer = WordTokenizer()

text = "Ú†Ù‡ Ø³ÛŒØ¨â€ŒÙ‡Ø§ÛŒ Ù‚Ø´Ù†Ú¯ÛŒ! Ø­ÛŒØ§Øª Ù†Ø´Ø¦Ù‡Ù” ØªÙ†Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª."
tokens = list(tokenizer(text))
print(tokens)
```

```shell
["Ú†Ù‡", "Ø³ÛŒØ¨â€ŒÙ‡Ø§ÛŒ", "Ù‚Ø´Ù†Ú¯ÛŒ", "!", "Ø­ÛŒØ§Øª", "Ù†Ø´Ø¦Ù‡Ù”", "ØªÙ†Ù‡Ø§ÛŒÛŒ", "Ø§Ø³Øª", "."]
```

### SentenceTokenizer

The `SentenceTokenizer` class is designed to split a given text into individual sentences. This class is particularly useful in natural language processing tasks where understanding the structure and meaning of sentences is important. The `SentenceTokenizer` class can handle various punctuation marks and language-specific rules to accurately identify sentence boundaries.

Below is an example of how to use the `SentenceTokenizer`:

```python
from shekar.tokenization import SentenceTokenizer

text = "Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª! Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ù†ÛŒÙ…."
tokenizer = SentenceTokenizer()
sentences = tokenizer(text)

for sentence in sentences:
    print(sentence)
```

```output
Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª!
Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ù†ÛŒÙ….
```

## Embeddings

[![Notebook](https://img.shields.io/badge/Notebook-Jupyter-00A693.svg)](examples/embeddings.ipynb)  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amirivojdan/shekar/blob/main/examples/embeddings.ipynb)

**Shekar** offers two main embedding classes:

- **`WordEmbedder`**: Provides static word embeddings using pre-trained FastText models.
- **`ContextualEmbedder`**: Provides contextual embeddings using a fine-tuned ALBERT model.

Both classes share a consistent interface:

- `embed(text)` returns a NumPy vector.
- `transform(text)` is an alias for `embed(text)` to integrate with pipelines.

### Word Embeddings

`WordEmbedder` supports two static FastText models:

- **`fasttext-d100`**: A 100-dimensional CBOW model trained on [Persian Wikipedia](https://huggingface.co/datasets/codersan/Persian-Wikipedia-Corpus).
- **`fasttext-d300`**: A 300-dimensional CBOW model trained on the large-scale [Naab dataset](https://huggingface.co/datasets/SLPL/naab).


```python
from shekar.embeddings import WordEmbedder

embedder = WordEmbedder(model="fasttext-d100")

embedding = embedder("Ú©ØªØ§Ø¨")
print(embedding.shape)

similar_words = embedder.most_similar("Ú©ØªØ§Ø¨", top_n=5)
print(similar_words)
```

### Contextual Embeddings

`ContextualEmbedder` uses an ALBERT model trained with Masked Language Modeling (MLM) on the Naab dataset to generate high-quality contextual embeddings.
The resulting embeddings are 768-dimensional vectors representing the semantic meaning of entire phrases or sentences.

```python
from shekar.embeddings import ContextualEmbedder

embedder = ContextualEmbedder(model="albert")

sentence = "Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ Ø¯Ø±ÛŒÚ†Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ Ø¬Ù‡Ø§Ù† Ø¯Ø§Ù†Ø´ Ù‡Ø³ØªÙ†Ø¯."
embedding = embedder(sentence)
print(embedding.shape)  # (768,)
```

## Stemming

The `Stemmer` is a lightweight, rule-based reducer for Persian word forms. It trims common suffixes while respecting Persian orthography and Zero Width Non-Joiner usage. The goal is to produce stable stems for search, indexing, and simple text analysis without requiring a full morphological analyzer.

```python
from shekar import Stemmer

stemmer = Stemmer()

print(stemmer("Ù†ÙˆÙ‡â€ŒØ§Ù…"))
print(stemmer("Ú©ØªØ§Ø¨â€ŒÙ‡Ø§"))
print(stemmer("Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ"))
print(stemmer("Ø®ÙˆÙ†Ù‡â€ŒÙ‡Ø§Ù…ÙˆÙ†"))
```

```output
Ù†ÙˆÙ‡
Ú©ØªØ§Ø¨
Ø®Ø§Ù†Ù‡
Ø®Ø§Ù†Ù‡
```

## Lemmatization

The `Lemmatizer` maps Persian words to their base dictionary form. Unlike stemming, which only trims affixes, lemmatization uses explicit verb conjugation rules, vocabulary lookups, and a stemmer fallback to ensure valid lemmas. This makes it more accurate for tasks like part-of-speech tagging, text normalization, and linguistic analysis where the canonical form of a word is required.

```python
from shekar import Lemmatizer

lemmatizer = Lemmatizer()

# Ø±ÛŒØ´Ù‡â€ŒÛŒØ§Ø¨ÛŒ Ø§ÙØ¹Ø§Ù„
print(lemmatizer("Ø±ÙØªÙ†Ø¯"))
print(lemmatizer("Ú¯ÙØªÙ‡ Ø¨ÙˆØ¯Ù‡â€ŒØ§ÛŒÙ…"))

# Ø±ÛŒØ´Ù‡â€ŒÛŒØ§Ø¨ÛŒ ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§
print(lemmatizer("Ú©ØªØ§Ø¨â€ŒÙ‡Ø§"))
print(lemmatizer("Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ"))
print(lemmatizer("Ø®ÙˆÙ†Ù‡â€ŒÙ‡Ø§Ù…ÙˆÙ†"))

# Ø±ÛŒØ´Ù‡â€ŒÛŒØ§Ø¨ÛŒ Ø§ÙØ¹Ø§Ù„ Ù¾ÛŒØ´ÙˆÙ†Ø¯ÛŒ
print(lemmatizer("Ø¨Ø± Ù†Ø®ÙˆØ§Ù‡Ù… Ú¯Ø´Øª"))
print(lemmatizer("Ø¨Ø±Ù†Ù…ÛŒâ€ŒØ¯Ø§Ø±Ù…"))
```

```output
Ø±ÙØª/Ø±Ùˆ
Ú¯ÙØª/Ú¯Ùˆ
Ú©ØªØ§Ø¨
Ø®Ø§Ù†Ù‡
Ø®Ø§Ù†Ù‡
Ø¨Ø±Ú¯Ø´Øª/Ø¨Ø±Ú¯Ø±Ø¯
Ø¨Ø±Ø¯Ø§Ø´Øª/Ø¨Ø±Ø¯Ø§Ø±
```

## Part-of-Speech Tagging

[![Notebook](https://img.shields.io/badge/Notebook-Jupyter-00A693.svg)](examples/pos_tagging.ipynb)  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amirivojdan/shekar/blob/main/examples/pos_tagging.ipynb)

The POSTagger class provides part-of-speech tagging for Persian text using a transformer-based model (default: ALBERT). It returns one tag per word based on Universal POS tags (following the Universal Dependencies standard).

Example usage:

```python
from shekar import POSTagger

pos_tagger = POSTagger()
text = "Ù†ÙˆØ±ÙˆØ²ØŒ Ø¬Ø´Ù† Ø³Ø§Ù„ Ù†Ùˆ Ø§ÛŒØ±Ø§Ù†ÛŒØŒ Ø¨ÛŒØ´ Ø§Ø² Ø³Ù‡ Ù‡Ø²Ø§Ø± Ø³Ø§Ù„ Ù‚Ø¯Ù…Øª Ø¯Ø§Ø±Ø¯ Ùˆ Ø¯Ø± Ú©Ø´ÙˆØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¬Ø´Ù† Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯."

result = pos_tagger(text)
for word, tag in result:
    print(f"{word}: {tag}")
```

```output
Ù†ÙˆØ±ÙˆØ²: PROPN
ØŒ: PUNCT
Ø¬Ø´Ù†: NOUN
Ø³Ø§Ù„: NOUN
Ù†Ùˆ: ADJ
Ø§ÛŒØ±Ø§Ù†ÛŒ: ADJ
ØŒ: PUNCT
Ø¨ÛŒØ´: ADJ
Ø§Ø²: ADP
Ø³Ù‡: NUM
Ù‡Ø²Ø§Ø±: NUM
Ø³Ø§Ù„: NOUN
Ù‚Ø¯Ù…Øª: NOUN
Ø¯Ø§Ø±Ø¯: VERB
Ùˆ: CCONJ
Ø¯Ø±: ADP
Ú©Ø´ÙˆØ±Ù‡Ø§ÛŒ: NOUN
Ù…Ø®ØªÙ„Ù: ADJ
Ø¬Ø´Ù†: NOUN
Ú¯Ø±ÙØªÙ‡: VERB
Ù…ÛŒâ€ŒØ´ÙˆØ¯: VERB
.: PUNCT
```

## Named Entity Recognition (NER)

[![Notebook](https://img.shields.io/badge/Notebook-Jupyter-00A693.svg)](examples/ner.ipynb)  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amirivojdan/shekar/blob/main/examples/ner.ipynb)

The `NER` module offers a fast, quantized Named Entity Recognition pipeline using a fine-tuned ALBERT model. It detects common Persian entities such as persons, locations, organizations, and dates. This model is designed for efficient inference and can be easily combined with other preprocessing steps.

Example usage:

```python
from shekar import NER
from shekar import Normalizer

input_text = (
    "Ø´Ø§Ù‡Ø±Ø® Ù…Ø³Ú©ÙˆØ¨ Ø¨Ù‡ Ø³Ø§Ù„Ù Û±Û³Û°Û´ Ø¯Ø± Ø¨Ø§Ø¨Ù„ Ø²Ø§Ø¯Ù‡ Ø´Ø¯ Ùˆ Ø¯ÙˆØ±Ù‡ Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ Ø±Ø§ Ø¯Ø± ØªÙ‡Ø±Ø§Ù† Ùˆ Ø¯Ø± Ù…Ø¯Ø±Ø³Ù‡ Ø¹Ù„Ù…ÛŒÙ‡ Ù¾Ø´Øª "
    "Ù…Ø³Ø¬Ø¯ Ø³Ù¾Ù‡Ø³Ø§Ù„Ø§Ø± Ú¯Ø°Ø±Ø§Ù†Ø¯. Ø§Ø² Ú©Ù„Ø§Ø³ Ù¾Ù†Ø¬Ù… Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø±Ù…Ø§Ù† Ùˆ Ø¢Ø«Ø§Ø± Ø§Ø¯Ø¨ÛŒ Ø±Ø§ Ø´Ø±ÙˆØ¹ Ú©Ø±Ø¯. Ø§Ø² Ù‡Ù…Ø§Ù† Ø²Ù…Ø§Ù† "
    "Ø¯Ø± Ø¯Ø¨ÛŒØ±Ø³ØªØ§Ù† Ø§Ø¯Ø¨ Ø§ØµÙÙ‡Ø§Ù† Ø§Ø¯Ø§Ù…Ù‡ ØªØ­ØµÛŒÙ„ Ø¯Ø§Ø¯. Ù¾Ø³ Ø§Ø² Ù¾Ø§ÛŒØ§Ù† ØªØ­ØµÛŒÙ„Ø§Øª Ø¯Ø¨ÛŒØ±Ø³ØªØ§Ù† Ø¯Ø± Ø³Ø§Ù„ Û±Û³Û²Û´ Ø§Ø² Ø§ØµÙÙ‡Ø§Ù† Ø¨Ù‡ ØªÙ‡Ø±Ø§Ù† Ø±ÙØª Ùˆ "
    "Ø¯Ø± Ø±Ø´ØªÙ‡ Ø­Ù‚ÙˆÙ‚ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ù…Ø´ØºÙˆÙ„ Ø¨Ù‡ ØªØ­ØµÛŒÙ„ Ø´Ø¯."
)

normalizer = Normalizer()
normalized_text = normalizer(input_text)

albert_ner = NER()
entities = albert_ner(normalized_text)

for text, label in entities:
    print(f"{text} â†’ {label}")
```

```output
Ø´Ø§Ù‡Ø±Ø® Ù…Ø³Ú©ÙˆØ¨ â†’ PER
Ø³Ø§Ù„ Û±Û³Û°Û´ â†’ DAT
Ø¨Ø§Ø¨Ù„ â†’ LOC
Ø¯ÙˆØ±Ù‡ Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ â†’ DAT
ØªÙ‡Ø±Ø§Ù† â†’ LOC
Ù…Ø¯Ø±Ø³Ù‡ Ø¹Ù„Ù…ÛŒÙ‡ â†’ LOC
Ù…Ø³Ø¬Ø¯ Ø³Ù¾Ù‡Ø³Ø§Ù„Ø§Ø± â†’ LOC
Ø¯Ø¨ÛŒØ±Ø³ØªØ§Ù† Ø§Ø¯Ø¨ Ø§ØµÙÙ‡Ø§Ù† â†’ LOC
Ø¯Ø± Ø³Ø§Ù„ Û±Û³Û²Û´ â†’ DAT
Ø§ØµÙÙ‡Ø§Ù† â†’ LOC
ØªÙ‡Ø±Ø§Ù† â†’ LOC
Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† â†’ ORG
ÙØ±Ø§Ù†Ø³Ù‡ â†’ LOC
```

## Sentiment Analysis

The `SentimentClassifier` module enables automatic sentiment analysis of Persian text using transformer-based models. It currently supports the `AlbertBinarySentimentClassifier`, a lightweight ALBERT model fine-tuned on Snapfood dataset to classify text as **positive** or **negative**, returning both the predicted label and its confidence score.

**Example usage:**

```python
from shekar import SentimentClassifier

sentiment_classifier = SentimentClassifier()

print(sentiment_classifier("Ø³Ø±ÛŒØ§Ù„ Ù‚ØµÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¬ÛŒØ¯ Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯!"))
print(sentiment_classifier("ÙÛŒÙ„Ù… Û³Û°Û° Ø§ÙØªØ¶Ø§Ø­ Ø¨ÙˆØ¯!"))
```

```output
('positive', 0.9923112988471985)
('negative', 0.9330866932868958)
```

## Toxicity Detection

The `toxicity` module currently includes a Logistic Regression classifier trained on TF-IDF features extracted from the [Naseza (Ù†Ø§Ø³Ø²Ø§) dataset](https://github.com/amirivojdan/naseza), a large-scale collection of Persian text labeled for offensive and neutral language. The `OffensiveLanguageClassifier` processes input text to determine whether it is neutral or offensive, returning both the predicted label and its confidence score.

```python
from shekar.toxicity import OffensiveLanguageClassifier

offensive_classifier = OffensiveLanguageClassifier()

print(offensive_classifier("Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù…ÛŒÙ‡Ù† Ù…Ù† Ø§Ø³Øª!"))
print(offensive_classifier("ØªÙˆ Ø®ÛŒÙ„ÛŒ Ø§Ø­Ù…Ù‚ Ùˆ Ø¨ÛŒâ€ŒØ´Ø±ÙÛŒ!"))
```

```output
('neutral', 0.7651197910308838)
('offensive', 0.7607775330543518)
```

## Keyword Extraction

[![Notebook](https://img.shields.io/badge/Notebook-Jupyter-00A693.svg)](examples/keyword_extraction.ipynb)  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amirivojdan/shekar/blob/main/examples/keyword_extraction.ipynb)

The **shekar.keyword_extraction** module provides tools for automatically identifying and extracting key terms and phrases from Persian text. These algorithms help identify the most important concepts and topics within documents.

```python
from shekar import KeywordExtractor

extractor = KeywordExtractor(max_length=2, top_n=10)

input_text = (
    "Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ ÛŒÚ©ÛŒ Ø§Ø² Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ù…Ù†Ø·Ù‚Ù‡ Ùˆ Ø¬Ù‡Ø§Ù† Ø§Ø³Øª Ú©Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡â€ŒØ§ÛŒ Ú©Ù‡Ù† Ø¯Ø§Ø±Ø¯. "
    "Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ø¯Ø§Ø´ØªÙ† Ø§Ø¯Ø¨ÛŒØ§ØªÛŒ ØºÙ†ÛŒ Ùˆ Ø´Ø§Ø¹Ø±Ø§Ù†ÛŒ Ø¨Ø±Ø¬Ø³ØªÙ‡ØŒ Ù†Ù‚Ø´ÛŒ Ø¨ÛŒâ€ŒØ¨Ø¯ÛŒÙ„ Ø¯Ø± Ú¯Ø³ØªØ±Ø´ ÙØ±Ù‡Ù†Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø§ÛŒÙØ§ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. "
    "Ø§Ø² Ø¯ÙˆØ±Ø§Ù† ÙØ±Ø¯ÙˆØ³ÛŒ Ùˆ Ø´Ø§Ù‡Ù†Ø§Ù…Ù‡ ØªØ§ Ø¯ÙˆØ±Ø§Ù† Ù…Ø¹Ø§ØµØ±ØŒ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù‡Ù…ÙˆØ§Ø±Ù‡ Ø§Ø¨Ø²Ø§Ø± Ø¨ÛŒØ§Ù† Ø§Ù†Ø¯ÛŒØ´Ù‡ØŒ Ø§Ø­Ø³Ø§Ø³ Ùˆ Ù‡Ù†Ø± Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª. "
)

keywords = extractor(input_text)

for kw in keywords:
    print(kw)
```

```output
ÙØ±Ù‡Ù†Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ
Ú¯Ø³ØªØ±Ø´ ÙØ±Ù‡Ù†Ú¯
Ø§ÛŒØ±Ø§Ù†ÛŒ Ø§ÛŒÙØ§
Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ
ØªØ§Ø±ÛŒØ®Ú†Ù‡â€ŒØ§ÛŒ Ú©Ù‡Ù†
```

## Spell Checking

The `SpellChecker` class provides simple and effective spelling correction for Persian text. It can automatically detect and fix common errors such as extra characters, spacing mistakes, or misspelled words. You can use it directly as a callable on a sentence to clean up the text, or call `suggest()` to get a ranked list of correction candidates for a single word.

```python
from shekar import SpellChecker

spell_checker = SpellChecker()
print(spell_checker("Ø³Ø³Ù„Ø§Ù… Ø¨Ø± Ø´Ø´Ù…Ø§ Ø¯Ø¯ÙˆØ³Øª Ù…Ù†"))
print(spell_checker.suggest("Ø¯Ø±ÙˆØ¯"))
```

```output
Ø³Ù„Ø§Ù… Ø¨Ø± Ø´Ù…Ø§ Ø¯ÙˆØ³Øª Ù…Ù†
['Ø¯Ø±ÙˆØ¯', 'Ø¯Ø±ØµØ¯', 'ÙˆØ±ÙˆØ¯', 'Ø¯Ø±Ø¯', 'Ø¯Ø±ÙˆÙ†']
```

## WordCloud

[![Notebook](https://img.shields.io/badge/Notebook-Jupyter-00A693.svg)](examples/word_cloud.ipynb)  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amirivojdan/shekar/blob/main/examples/word_cloud.ipynb)

The `WordCloud` class provides a convenient interface for generating Persian word clouds with correct shaping, directionality, and typography. It is specifically designed to work with right-to-left Persian text and integrates seamlessly with Shekarâ€™s normalization utilities to produce visually accurate and linguistically correct results.

The WordCloud functionality depends on visualization libraries that are not installed by default. To enable this feature, install Shekar with the optional visualization dependencies:

<!-- termynal -->
```bash
$ pip install 'shekar[viz]'
```
**Example usage:**

```python
import requests
from collections import Counter

from shekar.visualization import WordCloud
from shekar import WordTokenizer
from shekar.preprocessing import (
  HTMLTagRemover,
  PunctuationRemover,
  StopWordRemover,
  NonPersianRemover,
)
preprocessing_pipeline = HTMLTagRemover() | PunctuationRemover() | StopWordRemover() | NonPersianRemover()


url = f"https://shahnameh.me/p.php?id=F82F6CED"
response = requests.get(url)
html_content = response.text
clean_text = preprocessing_pipeline(html_content)

word_tokenizer = WordTokenizer()
tokens = word_tokenizer(clean_text)

word_freqs = Counter(tokens)

wordCloud = WordCloud(
        mask="Iran",
        width=640,
        height=480,
        max_font_size=220,
        min_font_size=6,
        bg_color="white",
        contour_color="black",
        contour_width=5,
        color_map="greens",
    )

# if shows disconnect words, try again with bidi_reshape=True
image = wordCloud.generate(word_freqs, bidi_reshape=False)
image.show()
```

![](https://raw.githubusercontent.com/amirivojdan/shekar/main/assets/wordcloud_example.png)

## Command-Line Interface (CLI)

Shekar includes a command-line interface (CLI) for quick text processing and visualization.  
You can normalize Persian text or generate wordclouds directly from files or inline strings.

**Usage**

```console
shekar [COMMAND] [OPTIONS]
```

**Examples**

```console
# Normalize a text file and save output
shekar normalize -i ./corpus.txt -o ./normalized_corpus.txt

# Normalize inline text
shekar normalize -t "Ø¯Ø±ÙˆØ¯ Ù¾Ø±ÙˆØ¯Ú¯Ø§Ø± Ø¨Ø± Ø§ÛŒØ±Ø§Ù† Ùˆ Ø§ÛŒØ±Ø§Ù†ÛŒ"
```

## Download Models

If Shekar Hub is unavailable, you can manually download the models and place them in the cache directory at `home/[username]/.shekar/` 

| Model Name                | Download Link |
|----------------------------|---------------|
| FastText Embedding d100    | [Download](https://drive.google.com/file/d/1qgd0slGA3Ar7A2ShViA3v8UTM4qXIEN6/view?usp=drive_link) (50MB)|
| FastText Embedding d300    | [Download](https://drive.google.com/file/d/1yeAg5otGpgoeD-3-E_W9ZwLyTvNKTlCa/view?usp=drive_link) (500MB)|
| SentenceEmbedding    | [Download](https://drive.google.com/file/d/1PftSG2QD2M9qzhAltWk_S38eQLljPUiG/view?usp=drive_link) (60MB)|
| POS Tagger  | [Download](https://drive.google.com/file/d/1d80TJn7moO31nMXT4WEatAaTEUirx2Ju/view?usp=drive_link) (38MB)|
| NER       | [Download](https://drive.google.com/file/d/1DLoMJt8TWlNnGGbHDWjwNGsD7qzlLHfu/view?usp=drive_link) (38MB)|
| Sentiment Classifier       | [Download](https://drive.google.com/file/d/17gTip7RwipEkA7Rf3-Cv1W8XNHTdaS4c/view?usp=drive_link) (38MB)|
| Offensive Language Classifier       | [Download](https://drive.google.com/file/d/1ZLiFI6nzpQ2rYjJTKxOYKTfD9IqHZ5tc/view?usp=drive_link) (8MB)|
| AlbertTokenizer   | [Download](https://drive.google.com/file/d/1w-oe53F0nPePMcoor5FgXRwRMwkYqDqM/view?usp=drive_link) (2MB)|

-----

## Citation

If you find **Shekar** useful in your research, please consider citing the following paper:

```
@article{Amirivojdan_Shekar,
author = {Amirivojdan, Ahmad},
doi = {10.21105/joss.09128},
journal = {Journal of Open Source Software},
month = oct,
number = {114},
pages = {9128},
title = {{Shekar: A Python Toolkit for Persian Natural Language Processing}},
url = {https://joss.theoj.org/papers/10.21105/joss.09128},
volume = {10},
year = {2025}
}
```

<p align="center"><em>With â¤ï¸ for <strong>IRAN</strong></em></p>


