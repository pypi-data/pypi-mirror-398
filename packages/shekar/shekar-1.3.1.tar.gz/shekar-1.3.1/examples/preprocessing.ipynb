{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88000902",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Persian text preprocessing presents unique challenges in natural language processing (NLP), due to issues like inconsistent orthography, pseudo-spaces, complex morphology, and limited standardized tools. Handling these nuances properly is essential for building accurate and reliable language models and downstream NLP applications.\n",
    "\n",
    "**[Shekar](https://github.com/amirivojdan/shekar)** is an open-source Python library designed to simplify and enhance Persian text preprocessing. It offers a modular and efficient pipeline for a variety of tasks including normalization, punctuation and stopword removal, stemming and lemmatization, spell correction, and word embedding generation.\n",
    "\n",
    "This notebook demonstrates practical examples of how to use Shekar for preprocessing Persian text. By the end, youâ€™ll be able to integrate Shekar into your own NLP workflows with ease and clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31697924",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "To install Shekar, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba69c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shekar\n",
      "  Downloading shekar-0.1.21-py3-none-any.whl (930 kB)\n",
      "     ------------------------------------ 930.3/930.3 kB 503.3 kB/s eta 0:00:00\n",
      "Collecting onnxruntime>=1.22.1\n",
      "  Downloading onnxruntime-1.22.1-cp310-cp310-win_amd64.whl (12.7 MB)\n",
      "     -------------------------------------- 12.7/12.7 MB 486.7 kB/s eta 0:00:00\n",
      "Collecting regex>=2024.11.6\n",
      "  Downloading regex-2025.7.34-cp310-cp310-win_amd64.whl (276 kB)\n",
      "     -------------------------------------- 276.0/276.0 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\amiri\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from shekar) (2.2.5)\n",
      "Collecting python-bidi>=0.6.6\n",
      "  Using cached python_bidi-0.6.6-cp310-cp310-win_amd64.whl (160 kB)\n",
      "Collecting tokenizers>=0.21.2\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 558.8 kB/s eta 0:00:00\n",
      "Collecting arabic-reshaper>=3.0.0\n",
      "  Using cached arabic_reshaper-3.0.0-py3-none-any.whl (20 kB)\n",
      "Collecting pillow>=11.2.1\n",
      "  Downloading pillow-11.3.0-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 616.6 kB/s eta 0:00:00\n",
      "Collecting emoji>=2.14.1\n",
      "  Using cached emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "Collecting wordcloud>=1.9.4\n",
      "  Using cached wordcloud-1.9.4-cp310-cp310-win_amd64.whl (299 kB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-6.32.0-cp310-abi3-win_amd64.whl (435 kB)\n",
      "     -------------------------------------- 435.7/435.7 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Collecting packaging\n",
      "  Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "     -------------------------------------- 561.5/561.5 kB 3.9 MB/s eta 0:00:00\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "     ---------------------------------------- 8.1/8.1 MB 543.4 kB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "     -------------------------------------- 199.6/199.6 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting humanfriendly>=9.1\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting python-dateutil>=2.7\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.59.1-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "     ---------------------------------------- 2.3/2.3 MB 2.2 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "     ---------------------------------------- 73.7/73.7 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting pyreadline3\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amiri\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud>=1.9.4->shekar) (1.17.0)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.3-cp310-cp310-win_amd64.whl (107 kB)\n",
      "     -------------------------------------- 107.5/107.5 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "     ------------------------------------- 161.2/161.2 kB 10.1 MB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Installing collected packages: python-bidi, mpmath, flatbuffers, arabic-reshaper, urllib3, typing-extensions, sympy, regex, pyyaml, python-dateutil, pyreadline3, pyparsing, protobuf, pillow, packaging, kiwisolver, idna, fsspec, fonttools, filelock, emoji, cycler, contourpy, colorama, charset_normalizer, certifi, tqdm, requests, matplotlib, humanfriendly, wordcloud, huggingface-hub, coloredlogs, tokenizers, onnxruntime, shekar\n",
      "Successfully installed arabic-reshaper-3.0.0 certifi-2025.8.3 charset_normalizer-3.4.3 colorama-0.4.6 coloredlogs-15.0.1 contourpy-1.3.2 cycler-0.12.1 emoji-2.14.1 filelock-3.19.1 flatbuffers-25.2.10 fonttools-4.59.1 fsspec-2025.7.0 huggingface-hub-0.34.4 humanfriendly-10.0 idna-3.10 kiwisolver-1.4.9 matplotlib-3.10.5 mpmath-1.3.0 onnxruntime-1.22.1 packaging-25.0 pillow-11.3.0 protobuf-6.32.0 pyparsing-3.2.3 pyreadline3-3.5.4 python-bidi-0.6.6 python-dateutil-2.9.0.post0 pyyaml-6.0.2 regex-2025.7.34 requests-2.32.4 shekar-0.1.21 sympy-1.14.0 tokenizers-0.21.4 tqdm-4.67.1 typing-extensions-4.14.1 urllib3-2.5.0 wordcloud-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/shekar/\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\amiri\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install shekar -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffd2f4",
   "metadata": {},
   "source": [
    "### Preprocessing with Shekar\n",
    "\n",
    "The `shekar.preprocessing` module provides a rich set of building blocks for cleaning, normalizing, and transforming Persian text. These classes form the foundation of text preprocessing workflows and can be used independently or combined in a `Pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3fd497",
   "metadata": {},
   "source": [
    "Here are some of the key text transformers available in the module:\n",
    "\n",
    "- **`SpacingStandardizer`**: Removes extra spaces and adjusts spacing around punctuation.\n",
    "- **`AlphabetNormalizer`**: Converts Arabic characters to standard Persian forms.\n",
    "- **`NumericNormalizer`**: Converts English and Arabic numerals into Persian digits.\n",
    "- **`PunctuationNormalizer`**: Standardizes punctuation symbols.\n",
    "- **`EmojiRemover`**: Removes emojis.\n",
    "- **`EmailMasker` / `URLMasker`**: Mask or remove emails and URLs.\n",
    "- **`DiacriticsRemover`**: Removes Persian/Arabic diacritics.\n",
    "- **`PunctuationRemover`**: Removes all punctuation characters.\n",
    "- **`RedundantCharacterRemover`**: Shrinks repeated characters like \"Ø³Ø³Ø³Ù„Ø§Ù…\".\n",
    "- **`ArabicUnicodeNormalizer`**: Converts Arabic presentation forms (e.g., ï·½) into Persian equivalents.\n",
    "- **`StopWordsRemover`**: Removes frequent Persian stopwords.\n",
    "- **`NonPersianRemover`**: Removes all non-Persian content (optionally keeps English).\n",
    "- **`HTMLTagRemover`**: Cleans HTML tags but retains content.\n",
    "- **`SpacingNormalizer`**: Standardizes the spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df924aab",
   "metadata": {},
   "source": [
    "##### Example 1: Remove Emojis and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f22b699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª\n"
     ]
    }
   ],
   "source": [
    "from shekar.preprocessing import EmojiRemover, PunctuationRemover\n",
    "\n",
    "emoji_remover = EmojiRemover()\n",
    "punct_remover = PunctuationRemover()\n",
    "\n",
    "text = \"Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª! ğŸŒğŸ˜Š\"\n",
    "text = emoji_remover.fit_transform(text)\n",
    "text = punct_remover.fit_transform(text)\n",
    "text = text.strip()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8892226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before standardization: Ø´Ø±Ú©Øª Â« Ú¯ÙˆÚ¯Ù„ Â»Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯ .\n",
      "after standardization: Ø´Ø±Ú©Øª Â«Ú¯ÙˆÚ¯Ù„Â» Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯.\n"
     ]
    }
   ],
   "source": [
    "from shekar.preprocessing import SpacingNormalizer\n",
    "\n",
    "punct_spacing_standardizer = SpacingNormalizer()\n",
    "text = \"Ø´Ø±Ú©Øª Â« Ú¯ÙˆÚ¯Ù„ Â»Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯ .\"\n",
    "print(\"before standardization:\", text)\n",
    "text = punct_spacing_standardizer.fit_transform(text).strip()\n",
    "print(\"after standardization:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0ff6c",
   "metadata": {},
   "source": [
    "In Shekar, all preprocessing transformers implement both the **fit_transform()** method and the **__call__()** method. This allows you to use them like functions. Calling a transformer directly is the same as calling .fit_transform().\n",
    "\n",
    "So we could rewrite the previous cell as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f31122a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª\n"
     ]
    }
   ],
   "source": [
    "from shekar.preprocessing import EmojiRemover, PunctuationRemover\n",
    "\n",
    "emoji_remover = EmojiRemover()\n",
    "punct_remover = PunctuationRemover()\n",
    "\n",
    "text = \"Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª! ğŸŒğŸ˜Š\"\n",
    "text = punct_remover(emoji_remover(text)).strip()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a87ef",
   "metadata": {},
   "source": [
    "This version is more concise and produces the exact same output!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae7bdef",
   "metadata": {},
   "source": [
    "##### Example 2: Normalize Persian Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c037e00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù‚Ø§Ø¦Ø¯Ù‡\n"
     ]
    }
   ],
   "source": [
    "from shekar.preprocessing import AlphabetNormalizer\n",
    "\n",
    "alphabet_normalizer = AlphabetNormalizer()\n",
    "text = \"Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ø© Ù‚Ø§Ø¦Ø¯Ø©\"\n",
    "normalized = alphabet_normalizer(text)\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3be24",
   "metadata": {},
   "source": [
    "##### Example 3: Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "258d4ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¬Ù…Ù„Ù‡Ù” Ù†Ù…ÙˆÙ†Ù‡\n"
     ]
    }
   ],
   "source": [
    "from shekar.preprocessing import StopWordRemover\n",
    "\n",
    "stopword_remover = StopWordRemover()\n",
    "text = \"Ø§ÛŒÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡Ù” Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª\"\n",
    "cleaned = stopword_remover(text)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e6758",
   "metadata": {},
   "source": [
    "#### Creating Custom Transformers\n",
    "\n",
    "In Shekar, you can easily define your own text transformation logic by subclassing `BaseTextTransformer`. This allows you to integrate any custom rule-based or pattern-based transformation into the Shekar pipeline system.\n",
    "\n",
    "All you need to do is implement the `_function(self, text: str) -> str` method, which takes a string and returns the transformed version.\n",
    "\n",
    "Note that the _function() method is automatically invoked by the class when you call the transformer. In most cases, defining this method is sufficient. However, if you need more control over the transformation logic (such as managing state, performing setup, or handling input types differently), you can also override the __init__(), fit(), transform(), and fit_transform() methods directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6151bd",
   "metadata": {},
   "source": [
    "##### Example: WhitespaceStripper\n",
    "\n",
    "This custom transformer removes leading and trailing whitespace from input strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7061ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shekar.base import BaseTextTransform\n",
    "\n",
    "\n",
    "class WhitespaceStripper(BaseTextTransform):\n",
    "    def _function(self, text: str) -> str:\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377af5af",
   "metadata": {},
   "source": [
    "You can now use it like any other Shekar component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f326d970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø³Ù„Ø§Ù… Ø¯Ù†ÛŒØ§!\n"
     ]
    }
   ],
   "source": [
    "text = \"   Ø³Ù„Ø§Ù… Ø¯Ù†ÛŒØ§!   \"\n",
    "whitespace_stripper = WhitespaceStripper()\n",
    "\n",
    "print(whitespace_stripper(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1686724",
   "metadata": {},
   "source": [
    "#### Pipelines: Chaining Text Transformations\n",
    "\n",
    "Shekar's `Pipeline` class allows you to chain multiple text preprocessing steps together into a seamless and reusable workflow. Inspired by Unix-style piping, Shekar also supports the `|` operator for combining transformers, making your code not only more readable but also expressive and modular.\n",
    "\n",
    "##### Why Pipelines?\n",
    "\n",
    "Text preprocessing often involves applying several transformations in sequence. Instead of writing nested function calls or multiple intermediate steps, Shekarâ€™s `Pipeline` lets you define a clean and testable chain of operations.\n",
    "\n",
    "For example, instead of writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d331371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª\n"
     ]
    }
   ],
   "source": [
    "text = \"Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª! ğŸŒğŸ˜Š\"\n",
    "text = whitespace_stripper(punct_remover(emoji_remover(text)))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec85a5",
   "metadata": {},
   "source": [
    "The same sequence of transformations can be constructed using the | operator, creating a concise and expressive pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f64b3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª\n"
     ]
    }
   ],
   "source": [
    "text = \"Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª! ğŸŒğŸ˜Š\"\n",
    "pipeline = EmojiRemover() | PunctuationRemover() | WhitespaceStripper()\n",
    "output = pipeline(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec89b9d",
   "metadata": {},
   "source": [
    "This approach clearly shows the order of transformations: first remove emojis, then punctuation, and finally trim whitespace. It reads naturally and makes the preprocessing flow easy to understand at a glance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb42367",
   "metadata": {},
   "source": [
    "The same transformation chain can also be written explicitly using the Pipeline class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fe9eeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª\n"
     ]
    }
   ],
   "source": [
    "from shekar import Pipeline\n",
    "from shekar.preprocessing import EmojiRemover, PunctuationRemover\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"emoji\", EmojiRemover()),\n",
    "        (\"punct\", PunctuationRemover()),\n",
    "        (\"strip\", WhitespaceStripper()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "text = \"Ø§ÛŒØ±Ø§Ù† Ø³Ø±Ø§ÛŒ Ù…Ù† Ø§Ø³Øª! ğŸŒğŸ˜Š\"\n",
    "output = pipeline(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d127b",
   "metadata": {},
   "source": [
    "##### Batch Processing with Pipelines\n",
    "\n",
    "Note that Pipelines also support batch processing. You can pass a list (or any iterable) of strings to the pipeline, and it will apply the transformations to each item in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9cf63fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Pipeline.fit_transform.<locals>.generator at 0x0000029271C58200>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"Ø¯Ø±ÙˆØ¯! ğŸŒŸ\", \"Ú†Ø·ÙˆØ±ÛŒØŸ! ğŸ˜„\"]\n",
    "cleaned_texts = pipeline(texts)\n",
    "cleaned_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf16ae",
   "metadata": {},
   "source": [
    "Keep in mind that the result is a generator, not a list. This makes the pipeline more memory-efficient, especially when processing large datasets. You can convert the output to a list if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d926d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø¯Ø±ÙˆØ¯', 'Ú†Ø·ÙˆØ±ÛŒ']\n"
     ]
    }
   ],
   "source": [
    "print(list(cleaned_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0204e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Ø¯Ø±ÙˆØ¯! ğŸŒŸ\", \"Ú†Ø·ÙˆØ±ÛŒØŸ! ğŸ˜„\"]\n",
    "cleaned_texts = pipeline(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b627a83",
   "metadata": {},
   "source": [
    "##### Using Pipelines as Decorators\n",
    "You can apply a pipeline to specific arguments in a function using the `.on_args()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "620708e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¹Ù„ÛŒ Ø§Ø­Ù…Ø¯ÛŒ\n"
     ]
    }
   ],
   "source": [
    "@pipeline.on_args([\"first_name\", \"last_name\"])\n",
    "def process(first_name: str, last_name: str) -> str:\n",
    "    return f\"{first_name} {last_name}\"\n",
    "\n",
    "\n",
    "processed_texts = process(first_name=\"ğŸŒŸØ¹Ù„ÛŒ\", last_name=\"!Ø§Ø­Ù…Ø¯ÛŒ\")\n",
    "print(processed_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3a60a",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "- Pipelines let you chain transformations cleanly.\n",
    "- You can build them explicitly or using the `|` operator.\n",
    "- Pipelines support strings, lists, and even decorators.\n",
    "- The result is more modular, testable, and elegant preprocessing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8761df8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª!\n",
      "Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ù†ÛŒÙ….\n"
     ]
    }
   ],
   "source": [
    "from shekar import SentenceTokenizer\n",
    "\n",
    "text = \"Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª! Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ù†ÛŒÙ….\"\n",
    "sentence_tokenizer = SentenceTokenizer()\n",
    "sentences = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdc66fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object WordTokenizer._function.<locals>.<genexpr> at 0x0000029271C58970>\n"
     ]
    }
   ],
   "source": [
    "from shekar import WordTokenizer\n",
    "\n",
    "tokenizer = WordTokenizer()\n",
    "\n",
    "text = \"Ú†Ù‡ Ø³ÛŒØ¨â€ŒÙ‡Ø§ÛŒ Ù‚Ø´Ù†Ú¯ÛŒ! Ø­ÛŒØ§Øª Ù†Ø´Ø¦Ù‡Ù” ØªÙ†Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a2b9174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ø³Ù„Ø§Ù…', 'Ø¯Ù†ÛŒØ§', 'Ø§ÛŒÙ†', 'ÛŒÚ©', 'Ø¬Ù…Ù„Ù‡']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shekar.transforms import Flatten\n",
    "\n",
    "flatten = Flatten()\n",
    "text = [[\"Ø³Ù„Ø§Ù…\", \"Ø¯Ù†ÛŒØ§\"], [\"Ø§ÛŒÙ†\", \"ÛŒÚ©\", \"Ø¬Ù…Ù„Ù‡\"]]\n",
    "list(flatten(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73f765c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ù‡Ø¯Ù', 'Ù…Ø§', 'Ú©Ù…Ú©', 'Ø¨Ù‡', 'ÛŒÚ©Ø¯ÛŒÚ¯Ø±', 'Ø§Ø³Øª', '!', 'Ù…Ø§', 'Ù…ÛŒ\\u200cØªÙˆØ§Ù†ÛŒÙ…', 'Ø¨Ø§', 'Ù‡Ù…', 'Ú©Ø§Ø±', 'Ú©Ù†ÛŒÙ…', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª! Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ù†ÛŒÙ….\"\n",
    "\n",
    "pipeline = SentenceTokenizer() | WordTokenizer() | Flatten()\n",
    "output = pipeline(text)\n",
    "print(list(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shekar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
