import pytest

from shekar.preprocessing import (
    PunctuationNormalizer,
    AlphabetNormalizer,
    DigitNormalizer,
    SpacingNormalizer,
    YaNormalizer,
    EmojiMasker,
    EmailMasker,
    URLMasker,
    DiacriticRemover,
    NonPersianLetterMasker,
    HTMLTagMasker,
    RepeatedLetterNormalizer,
    ArabicUnicodeNormalizer,
    StopWordRemover,
    PunctuationRemover,
    DigitRemover,
    MentionMasker,
    HashtagMasker,
    OffensiveWordMasker,
)

from shekar.transforms import (
    NGramExtractor,
    Flatten,
)

from shekar.data import ZWNJ


def test_correct_spacings():
    spacing_normalizer = SpacingNormalizer()

    input_text = (
        "Ù…ÛŒØ±ÙˆÛŒÙ… Ø¨Ù‡ Ø®Ø§Ù†Ù‡Ù‡Ø§ÛŒ Ø®Ø§Ú© Ø¢Ù„ÙˆØ¯ Ú©Ù‡ Ú¯ÙØªÙ‡ Ø§Ù†Ø¯ ØªØ§ Ú†Ù†Ø¯ Ø³Ø§Ù„ Ø¨Ø¹Ø¯ ØªØ± ÙˆÛŒØ±Ø§Ù† Ù†Ù…ÛŒ Ø´ÙˆÙ†Ø¯ !"
    )
    expected_output = (
        "Ù…ÛŒâ€ŒØ±ÙˆÛŒÙ… Ø¨Ù‡ Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ú©â€ŒØ¢Ù„ÙˆØ¯ Ú©Ù‡ Ú¯ÙØªÙ‡â€ŒØ§Ù†Ø¯ ØªØ§ Ú†Ù†Ø¯ Ø³Ø§Ù„ Ø¨Ø¹Ø¯ØªØ± ÙˆÛŒØ±Ø§Ù† Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯!"
    )
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ø®ÙˆÙ†Ù‡ Ù‡Ø§Ø´ÙˆÙ† Ø®ÛŒÙ„ÛŒ Ú¯Ø±ÙˆÙ† ØªØ± Ø´Ø¯Ù‡"
    expected_output = "Ø®ÙˆÙ†Ù‡â€ŒÙ‡Ø§Ø´ÙˆÙ† Ø®ÛŒÙ„ÛŒ Ú¯Ø±ÙˆÙ†â€ŒØªØ± Ø´Ø¯Ù‡"
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ø¯ÙˆÙ‚Ù„Ùˆ Ù‡Ø§ÛŒ Ù‡Ù… Ø®ÙˆÙ†"
    expected_output = "Ø¯ÙˆÙ‚Ù„ÙˆÙ‡Ø§ÛŒ Ù‡Ù…â€ŒØ®ÙˆÙ†"
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ø¨Ù†ÛŒØ§Ù† Ú¯Ø°Ø§Ø± Ø®Ø§Ù†Ù‡ Ù‡Ø§ÛŒÙ…Ø§Ù†"
    expected_output = "Ø¨Ù†ÛŒØ§Ù†â€ŒÚ¯Ø°Ø§Ø± Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒÙ…Ø§Ù†"
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ù‡Ù… Ø´Ø§ÛŒØ¯"
    expected_output = "Ù‡Ù… Ø´Ø§ÛŒØ¯"
    assert spacing_normalizer(input_text) == expected_output

    input_text = "   Ø§ÛŒÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡   Ù†Ù…ÙˆÙ†Ù‡   Ø§Ø³Øª. "
    expected_output = "Ø§ÛŒÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª."
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ø§ÛŒÙ†Ø¬Ø§ Ú©Ø¬Ø§Ø³ØªØŸØªÙˆ Ù…ÛŒØ¯Ø§Ù†ÛŒØŸÙ†Ù…ÛŒØ¯Ø§Ù†Ù…!"
    expected_output = "Ø§ÛŒÙ†Ø¬Ø§ Ú©Ø¬Ø§Ø³ØªØŸ ØªÙˆ Ù…ÛŒâ€ŒØ¯Ø§Ù†ÛŒØŸ Ù†Ù…ÛŒâ€ŒØ¯Ø§Ù†Ù…!"
    assert spacing_normalizer.fit_transform(input_text) == expected_output

    input_text = "Ù†Ø§ØµØ± Ú¯ÙØª:Â«Ù…Ù† Ù…ÛŒâ€ŒØ±ÙˆÙ….Â»"
    expected_output = "Ù†Ø§ØµØ± Ú¯ÙØª: Â«Ù…Ù† Ù…ÛŒâ€ŒØ±ÙˆÙ….Â»"
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ø¨Ø§ Ú©ÛŒ Ø¯Ø§Ø±ÛŒ Ø­Ø±Ù Ù…ÛŒ Ø²Ù†ÛŒØŸ"
    expected_output = "Ø¨Ø§ Ú©ÛŒ Ø¯Ø§Ø±ÛŒ Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ù†ÛŒØŸ"
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ù…Ù† Ù…ÛŒâ€ŒØ±ÙˆÙ….ØªÙˆ Ù†Ù…ÛŒâ€ŒØ¢ÛŒÛŒØŸ"
    expected_output = "Ù…Ù† Ù…ÛŒâ€ŒØ±ÙˆÙ…. ØªÙˆ Ù†Ù…ÛŒâ€ŒØ¢ÛŒÛŒØŸ"
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ø¨Ù‡ Ù†Ú©ØªÙ‡ Ø±ÛŒØ²ÛŒ Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯ÛŒ!"
    expected_output = "Ø¨Ù‡ Ù†Ú©ØªÙ‡ Ø±ÛŒØ²ÛŒ Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯ÛŒ!"
    assert spacing_normalizer.fit_transform(input_text) == expected_output

    sentences = ["   Ø§ÛŒÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡   Ù†Ù…ÙˆÙ†Ù‡   Ø§Ø³Øª. ", "Ø¨Ø§ Ú©ÛŒ Ø¯Ø§Ø±ÛŒ Ø­Ø±Ù Ù…ÛŒ Ø²Ù†ÛŒØŸ"]
    expected_output = ["Ø§ÛŒÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª.", "Ø¨Ø§ Ú©ÛŒ Ø¯Ø§Ø±ÛŒ Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ù†ÛŒØŸ"]
    assert list(spacing_normalizer(sentences)) == expected_output
    assert list(spacing_normalizer.fit_transform(sentences)) == expected_output

    input_text = 13.4
    expected_output = "Input must be a string or a Iterable of strings."
    with pytest.raises(ValueError, match=expected_output):
        spacing_normalizer(input_text)


def test_prefixed_verbs_packed():
    spacing_normalizer = SpacingNormalizer()

    input_text = "Ù…ÛŒ Ø±ÙˆÙ… Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒÙ… Ø±Ø§ Ø¨Ø± Ù…ÛŒ Ø¯Ø§Ø±Ù…."
    expected_output = "Ù…ÛŒâ€ŒØ±ÙˆÙ… Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ… Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒØ¯Ø§Ø±Ù…."
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ù…ÛŒ Ø±ÙˆÙ… Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒÙ… Ø±Ø§ Ø¨Ø± Ù…ÛŒâ€ŒØ¯Ø§Ø±Ù…."
    expected_output = "Ù…ÛŒâ€ŒØ±ÙˆÙ… Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ… Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒØ¯Ø§Ø±Ù…."
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ù…ÛŒ Ø±ÙˆÙ… Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒÙ… Ø±Ø§ Ø¨Ø± Ù…ÛŒØ¯Ø§Ø±Ù…."
    expected_output = "Ù…ÛŒâ€ŒØ±ÙˆÙ… Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ… Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒØ¯Ø§Ø±Ù…."
    assert spacing_normalizer(input_text) == expected_output

    input_text = f"Ù…ÛŒ Ø±ÙˆÙ… Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒÙ… Ø±Ø§ Ø¨Ø±{ZWNJ}Ù…ÛŒØ¯Ø§Ø±Ù…."
    expected_output = "Ù…ÛŒâ€ŒØ±ÙˆÙ… Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ… Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒØ¯Ø§Ø±Ù…."
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒÙ… Ø±Ø§ Ù¾Ø³ Ø¨Ø¯Ù‡!"
    expected_output = "Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ… Ø±Ø§ Ù¾Ø³â€ŒØ¨Ø¯Ù‡!"
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒÙ… Ø±Ø§ Ù¾Ø³â€ŒØ¨Ø¯Ù‡!"
    expected_output = "Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ… Ø±Ø§ Ù¾Ø³â€ŒØ¨Ø¯Ù‡!"
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒÙ… Ø±Ø§ Ù¾Ø³Ø¨Ø¯Ù‡!"
    expected_output = "Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ… Ø±Ø§ Ù¾Ø³â€ŒØ¨Ø¯Ù‡!"
    assert spacing_normalizer(input_text) == expected_output


def test_prefixed_verbs_spacing():
    spacing_normalizer = SpacingNormalizer()

    input_text = "Ø§Ùˆ Ø¨Ø± Ø®ÙˆØ§Ù‡Ø¯ Ú¯Ø´Øª."
    expected_output = "Ø§Ùˆ Ø¨Ø± Ø®ÙˆØ§Ù‡Ø¯ Ú¯Ø´Øª."
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ø§Ùˆ Ø¨Ø±Ø®ÙˆØ§Ù‡Ø¯ Ú¯Ø´Øª."
    expected_output = "Ø§Ùˆ Ø¨Ø± Ø®ÙˆØ§Ù‡Ø¯ Ú¯Ø´Øª."
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ø§Ùˆ Ø¨Ø± Ø®ÙˆØ§Ù‡Ø¯Ú¯Ø´Øª."
    expected_output = "Ø§Ùˆ Ø¨Ø± Ø®ÙˆØ§Ù‡Ø¯ Ú¯Ø´Øª."
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ø§Ùˆ Ø¨Ø± Ù†Ø®ÙˆØ§Ù‡Ø¯Ú¯Ø´Øª."
    expected_output = "Ø§Ùˆ Ø¨Ø± Ù†Ø®ÙˆØ§Ù‡Ø¯ Ú¯Ø´Øª."
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ø§Ùˆ Ø¨Ø±Ù†Ø®ÙˆØ§Ù‡Ø¯Ú¯Ø´Øª."
    expected_output = "Ø§Ùˆ Ø¨Ø± Ù†Ø®ÙˆØ§Ù‡Ø¯ Ú¯Ø´Øª."
    assert spacing_normalizer(input_text) == expected_output

    input_text = f"Ø§Ùˆ Ø¨Ø±{ZWNJ}Ù†Ø®ÙˆØ§Ù‡Ø¯{ZWNJ}Ú¯Ø´Øª."
    expected_output = "Ø§Ùˆ Ø¨Ø± Ù†Ø®ÙˆØ§Ù‡Ø¯ Ú¯Ø´Øª."
    assert spacing_normalizer(input_text) == expected_output


def test_remove_extra_spaces():
    spacing_normalizer = SpacingNormalizer()

    input_text = "Ø§ÛŒÙ†  ÛŒÚ©  Ø¢Ø²Ù…ÙˆÙ†  Ø§Ø³Øª"
    expected_output = "Ø§ÛŒÙ† ÛŒÚ© Ø¢Ø²Ù…ÙˆÙ† Ø§Ø³Øª"
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ø§ÛŒÙ†\u200cÛŒÚ©\u200cØ¢Ø²Ù…ÙˆÙ†\u200cØ§Ø³Øª"
    expected_output = "Ø§ÛŒÙ†\u200cÛŒÚ©\u200cØ¢Ø²Ù…ÙˆÙ†\u200cØ§Ø³Øª"
    assert spacing_normalizer.fit_transform(input_text) == expected_output

    input_text = "Ø§ÛŒÙ†\u200c ÛŒÚ©\u200c Ø¢Ø²Ù…ÙˆÙ†\u200c Ø§Ø³Øª"
    expected_output = "Ø§ÛŒÙ† ÛŒÚ© Ø¢Ø²Ù…ÙˆÙ† Ø§Ø³Øª"
    assert spacing_normalizer(input_text) == expected_output

    # test ZWNJ after non-left joiner letters!
    input_text = "Ú†Ù‡Ø§Ø±â€ŒÙ„Ø§Ú†Ù†Ú¯"
    expected_output = "Ú†Ù‡Ø§Ø±Ù„Ø§Ú†Ù†Ú¯"
    assert spacing_normalizer(input_text) == expected_output

    input_text = "Ø§ÛŒÙ†  ÛŒÚ©  Ø¢Ø²Ù…ÙˆÙ†  Ø§Ø³Øª  "
    expected_output = "Ø§ÛŒÙ† ÛŒÚ© Ø¢Ø²Ù…ÙˆÙ† Ø§Ø³Øª"
    assert spacing_normalizer.fit_transform(input_text) == expected_output

    input_text = "Ø§ÛŒÙ†  ÛŒÚ©  Ø¢Ø²Ù…ÙˆÙ†  Ø§Ø³Øª\n\n\n\n"
    expected_output = "Ø§ÛŒÙ† ÛŒÚ© Ø¢Ø²Ù…ÙˆÙ† Ø§Ø³Øª"
    assert spacing_normalizer(input_text) == expected_output


def test_ya_normalizer():
    ya_normalizer = YaNormalizer(style="standard")

    input_text = "Ø®Ø§Ù†Ù‡â€ŒÛŒ Ù…Ø§"
    expected_output = "Ø®Ø§Ù†Û€ Ù…Ø§"
    assert ya_normalizer(input_text) == expected_output

    ya_normalizer = YaNormalizer()
    input_text = "Ø®Ø§Ù†Û€ Ù…Ø§"
    expected_output = "Ø®Ø§Ù†Ù‡â€ŒÛŒ Ù…Ø§"
    assert ya_normalizer(input_text) == expected_output


def test_non_left_joiner_compound_words():
    space_normalizer = SpacingNormalizer()

    input_text = "Ø¯ÛŒØ¯Ù† Ø¢Ù† ØµØ­Ù†Ù‡ Ù…Ù†Ø²Ø¬Ø± Ú©Ù†Ù†Ø¯Ù‡ Ø¨ÙˆØ¯!"
    expected_output = "Ø¯ÛŒØ¯Ù† Ø¢Ù† ØµØ­Ù†Ù‡ Ù…Ù†Ø²Ø¬Ø±Ú©Ù†Ù†Ø¯Ù‡ Ø¨ÙˆØ¯!"
    assert space_normalizer(input_text) == expected_output

    input_text = "Ú©Ø§Ø± Ø¢ÙØ±ÛŒÙ†ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø§Ø±Ø²Ø´Ù…Ù†Ø¯ Ø§Ø³Øª."
    expected_output = "Ú©Ø§Ø±Ø¢ÙØ±ÛŒÙ†ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø§Ø±Ø²Ø´Ù…Ù†Ø¯ Ø§Ø³Øª."
    assert space_normalizer(input_text) == expected_output

    input_text = "ÛŒÚ© Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø®ÙˆØ¨ Ø¨Ø§ÛŒØ¯ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø³Ù†Ø¯ Ø¨Ø§Ø´Ø¯!"
    expected_output = "ÛŒÚ© Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø®ÙˆØ¨ Ø¨Ø§ÛŒØ¯ Ú©Ø§Ø±Ø¨Ø±Ù¾Ø³Ù†Ø¯ Ø¨Ø§Ø´Ø¯!"
    assert space_normalizer(input_text) == expected_output


def test_mask_email():
    email_masker = EmailMasker(mask_token="")

    input_text = "Ø§ÛŒÙ…ÛŒÙ„ Ù…Ù†: she.kar@shekar.panir.io"
    expected_output = "Ø§ÛŒÙ…ÛŒÙ„ Ù…Ù†:"
    assert email_masker(input_text) == expected_output

    input_text = "Ø§ÛŒÙ…ÛŒÙ„ Ù…Ù†: she+kar@she-kar.io"
    expected_output = "Ø§ÛŒÙ…ÛŒÙ„ Ù…Ù†:"
    assert email_masker.fit_transform(input_text) == expected_output


def test_mask_url():
    url_masker = URLMasker(mask_token="")

    input_text = "Ù„ÛŒÙ†Ú©: https://shekar.parsi-shekar.com"
    expected_output = "Ù„ÛŒÙ†Ú©:"
    assert url_masker(input_text) == expected_output

    input_text = "Ù„ÛŒÙ†Ú©: http://shekar2qand.com/id=2"
    expected_output = "Ù„ÛŒÙ†Ú©:"
    assert url_masker.fit_transform(input_text) == expected_output


def test_normalize_numbers():
    numeric_normalizer = DigitNormalizer()
    input_text = "Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù© â’•34"
    expected_output = "Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹ Û±Û´Û³Û´"
    assert numeric_normalizer(input_text) == expected_output

    input_text = "Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹Û°"
    expected_output = "Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹Û°"
    assert numeric_normalizer.fit_transform(input_text) == expected_output


def test_unify_characters():
    alphabet_normalizer = AlphabetNormalizer()

    input_text = "Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ø©"
    expected_output = "Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ø¯Ø±Ø¨Ø§Ø±Û€ Ù…Ø§"
    expected_output = "Ø¯Ø±Ø¨Ø§Ø±Û€ Ù…Ø§"
    assert alphabet_normalizer.fit_transform(input_text) == expected_output

    input_text = "Ù†Ø§Ù…Û€ ÙØ±Ù‡Ù†Ú¯Ø³ØªØ§Ù†"
    expected_output = "Ù†Ø§Ù…Û€ ÙØ±Ù‡Ù†Ú¯Ø³ØªØ§Ù†"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ø±Ø¦Ø§Ù„ÛŒØ³Ù… Ø±Ø¦ÛŒØ³ Ù„Ø¦ÛŒÙ…"
    expected_output = "Ø±Ø¦Ø§Ù„ÛŒØ³Ù… Ø±Ø¦ÛŒØ³ Ù„Ø¦ÛŒÙ…"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ø±Ø£Ø³ Ù…ØªÙ„Ø£Ù„Ø¦ Ù…Ø£Ø®Ø°"
    expected_output = "Ø±Ø£Ø³ Ù…ØªÙ„Ø£Ù„Ø¦ Ù…Ø£Ø®Ø°"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ù…Ø¤Ù„Ù Ù…Ø¤Ù…Ù† Ù…Ø¤Ø³Ø³Ù‡"
    expected_output = "Ù…Ø¤Ù„Ù Ù…Ø¤Ù…Ù† Ù…Ø¤Ø³Ø³Ù‡"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ø¬Ø²Ø¡"
    expected_output = "Ø¬Ø²Ø¡"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ø³Ø§ÛŒØ©"
    expected_output = "Ø³Ø§ÛŒÙ‡"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Û¿Ø¯Ù Ù…Ø§ Ø»Ù…Ú« Ø¨Ûƒ ÛÚªÚ‰ÙŠÚ±Ú• Ø¥ÚšÙ¼"
    expected_output = "Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª"
    assert alphabet_normalizer.fit_transform(input_text) == expected_output

    input_text = "Ú©Ø§Ø±ØªÙˆÙ†"
    expected_output = "Ú©Ø§Ø±ØªÙˆÙ†"
    assert alphabet_normalizer(input_text) == expected_output

    input_text = "Ù‡Ù…Ù‡ Ø¨Ø§ Ù‡Ù… Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ù¾Ù„ÛŒØ¯ÛŒ Ùˆ Ø³ØªÙ… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø§ÛŒØ³ØªØ§Ø¯"
    expected_output = "Ù‡Ù…Ù‡ Ø¨Ø§ Ù‡Ù… Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ù¾Ù„ÛŒØ¯ÛŒ Ùˆ Ø³ØªÙ… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø§ÛŒØ³ØªØ§Ø¯"
    assert alphabet_normalizer(input_text) == expected_output


def test_unify_punctuations():
    punct_normalizer = PunctuationNormalizer()

    input_text = "ØŸ?ØŒÙ¬!%:Â«Â»Ø›"
    expected_output = "ØŸØŸØŒØŒ!Ùª:Â«Â»Ø›"
    assert punct_normalizer(input_text) == expected_output

    input_text = "Ø³Ù„Ø§Ù…!Ú†Ø·ÙˆØ±ÛŒ?"
    expected_output = "Ø³Ù„Ø§Ù…!Ú†Ø·ÙˆØ±ÛŒØŸ"
    assert punct_normalizer.fit_transform(input_text) == expected_output


def test_unify_arabic_unicode():
    arabic_unicode_normalizer = ArabicUnicodeNormalizer()

    input_text = "ï·½"
    expected_output = "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÛŒÙ…"
    assert arabic_unicode_normalizer(input_text) == expected_output

    input_text = "Ù¾Ù†Ø¬Ø§Ù‡ Ù‡Ø²Ø§Ø± ï·¼"
    expected_output = "Ù¾Ù†Ø¬Ø§Ù‡ Ù‡Ø²Ø§Ø± Ø±ÛŒØ§Ù„"
    assert arabic_unicode_normalizer(input_text) == expected_output

    input_text = "ï·² Ø§Ø¹Ù„Ù…"
    expected_output = "Ø§Ù„Ù„Ù‡ Ø§Ø¹Ù„Ù…"
    assert arabic_unicode_normalizer.fit_transform(input_text) == expected_output

    input_text = "ï·² ï·³"
    expected_output = "Ø§Ù„Ù„Ù‡ Ø§Ú©Ø¨Ø±"
    assert arabic_unicode_normalizer(input_text) == expected_output

    input_text = "ï·´"
    expected_output = "Ù…Ø­Ù…Ø¯"
    assert arabic_unicode_normalizer.fit_transform(input_text) == expected_output


def test_remove_punctuations():
    punc_Filter = PunctuationRemover()

    input_text = "Ø§ØµÙÙ‡Ø§Ù†ØŒ Ù†ØµÙ Ø¬Ù‡Ø§Ù†!"
    expected_output = "Ø§ØµÙÙ‡Ø§Ù† Ù†ØµÙ Ø¬Ù‡Ø§Ù†"
    assert punc_Filter(input_text) == expected_output

    input_text = "ÙØ±Ø¯ÙˆØ³ÛŒØŒ Ø´Ø§Ø¹Ø± Ø¨Ø²Ø±Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø§Ø³Øª."
    expected_output = "ÙØ±Ø¯ÙˆØ³ÛŒ Ø´Ø§Ø¹Ø± Ø¨Ø²Ø±Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø§Ø³Øª"
    assert punc_Filter.fit_transform(input_text) == expected_output


def test_remove_redundant_characters():
    redundant_character_Filter = RepeatedLetterNormalizer()
    input_text = "Ø³Ù„Ø§Ù…Ù…"
    expected_output = "Ø³Ù„Ø§Ù…Ù…"
    assert redundant_character_Filter(input_text) == expected_output

    input_text = "Ø³Ù„Ø§Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…"
    expected_output = "Ø³Ù„Ø§Ù…Ù…"
    assert redundant_character_Filter.fit_transform(input_text) == expected_output

    input_text = "Ø±ÙˆØ²ÛŒ Ø¨Ø§Øº Ø³Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ø¨Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ø² Ø¨ÙˆØ¯"
    expected_output = "Ø±ÙˆØ²ÛŒ Ø¨Ø§Øº Ø³Ø¨Ø² Ø¨ÙˆØ¯"
    assert redundant_character_Filter(input_text) == expected_output


def test_remove_emojis():
    emoji_Filter = EmojiMasker()
    input_text = "ğŸ˜ŠğŸ‡®ğŸ‡·Ø³Ù„Ø§Ù… Ú¯Ù„Ø§ÛŒ ØªÙˆ Ø®ÙˆÙ†Ù‡!ğŸ‰ğŸ‰ğŸŠğŸˆ"
    expected_output = "Ø³Ù„Ø§Ù… Ú¯Ù„Ø§ÛŒ ØªÙˆ Ø®ÙˆÙ†Ù‡!"
    assert emoji_Filter(input_text) == expected_output

    input_text = "ğŸŒ¹Ø¨Ø§Ø² Ù‡Ù… Ù…Ø±Øº Ø³Ø­Ø±ğŸ” Ø¨Ø± Ø³Ø± Ù…Ù†Ø¨Ø± Ú¯Ù„"
    expected_output = "Ø¨Ø§Ø² Ù‡Ù… Ù…Ø±Øº Ø³Ø­Ø± Ø¨Ø± Ø³Ø± Ù…Ù†Ø¨Ø± Ú¯Ù„"

    assert emoji_Filter.fit_transform(input_text) == expected_output


def test_remove_diacritics():
    diacritics_Filter = DiacriticRemover()
    input_text = "Ù…ÙÙ†Ù’"
    expected_output = "Ù…Ù†"
    assert diacritics_Filter(input_text) == expected_output

    input_text = "Ú©ÙØ¬Ø§ Ù†ÙØ´Ø§Ù†Ù Ù‚ÙØ¯ÙÙ… Ù†Ø§ØªÙÙ…Ø§Ù… Ø®ÙˆØ§Ù‡ÙØ¯ Ù…Ø§Ù†Ø¯ØŸ"
    expected_output = "Ú©Ø¬Ø§ Ù†Ø´Ø§Ù† Ù‚Ø¯Ù… Ù†Ø§ØªÙ…Ø§Ù… Ø®ÙˆØ§Ù‡Ø¯ Ù…Ø§Ù†Ø¯ØŸ"
    assert diacritics_Filter.fit_transform(input_text) == expected_output


def test_remove_stopwords():
    stopword_Filter = StopWordRemover()
    input_text = "Ø³Ù„Ø§Ù… Ø¨Ø± ØªÙˆ"
    expected_output = "Ø³Ù„Ø§Ù…"
    assert stopword_Filter(input_text) == expected_output

    input_text = "ÙˆÛŒ Ø®Ø§Ø·Ø±Ù†Ø´Ø§Ù† Ú©Ø±Ø¯"
    expected_output = "Ø®Ø§Ø·Ø±Ù†Ø´Ø§Ù† Ú©Ø±Ø¯"
    assert stopword_Filter(input_text) == expected_output

    input_text = "Ø¯Ø±ÙˆØ¯ Ø¨Ø± Ø§ÛŒØ±Ø§Ù†"
    expected_output = "Ø¯Ø±ÙˆØ¯  Ø§ÛŒØ±Ø§Ù†"
    assert stopword_Filter(input_text) == expected_output

    stopword_Filter = StopWordRemover(mask_token="|")
    input_text = "Ø¯Ø±ÙˆØ¯ Ø¨Ø± Ø§ÛŒØ±Ø§Ù†"
    expected_output = "Ø¯Ø±ÙˆØ¯ | Ø§ÛŒØ±Ø§Ù†"
    assert stopword_Filter(input_text) == expected_output


def test_remove_non_persian():
    non_persian_Filter = NonPersianLetterMasker()
    input_text = "Ø¨Ø§ ÛŒÙ‡ Ú¯Ù„ Ø¨Ù‡Ø§Ø± Ù†Ù…ÛŒâ€ŒØ´Ù‡"
    expected_output = "Ø¨Ø§ ÛŒÙ‡ Ú¯Ù„ Ø¨Ù‡Ø§Ø± Ù†Ù…ÛŒâ€ŒØ´Ù‡"
    assert non_persian_Filter(input_text) == expected_output

    input_text = "What you seek is seeking you!"
    expected_output = "!"
    assert non_persian_Filter(input_text) == expected_output

    input_text = "ØµØ¨Ø­ Ø§Ø² Ø®ÙˆØ§Ø¨ Ù¾Ø§Ø´Ø¯Ù… Ø¯ÛŒØ¯Ù… Ø§ÛŒÙ†ØªØ±Ù†Øª Ù†Ø¯Ø§Ø±Ù…ØŒ Ø±Ø³Ù…Ø§Ù‹ panic attack Ú©Ø±Ø¯Ù…!"
    expected_output = "ØµØ¨Ø­ Ø§Ø² Ø®ÙˆØ§Ø¨ Ù¾Ø§Ø´Ø¯Ù… Ø¯ÛŒØ¯Ù… Ø§ÛŒÙ†ØªØ±Ù†Øª Ù†Ø¯Ø§Ø±Ù…ØŒ Ø±Ø³Ù…Ø§   Ú©Ø±Ø¯Ù…!"
    assert non_persian_Filter(input_text) == expected_output

    non_persian_Filter = NonPersianLetterMasker(keep_english=True)

    input_text = "ØµØ¨Ø­ Ø§Ø² Ø®ÙˆØ§Ø¨ Ù¾Ø§Ø´Ø¯Ù… Ø¯ÛŒØ¯Ù… Ø§ÛŒÙ†ØªØ±Ù†Øª Ù†Ø¯Ø§Ø±Ù…ØŒ Ø±Ø³Ù…Ø§Ù‹ panic attack Ú©Ø±Ø¯Ù…!"
    expected_output = "ØµØ¨Ø­ Ø§Ø² Ø®ÙˆØ§Ø¨ Ù¾Ø§Ø´Ø¯Ù… Ø¯ÛŒØ¯Ù… Ø§ÛŒÙ†ØªØ±Ù†Øª Ù†Ø¯Ø§Ø±Ù…ØŒ Ø±Ø³Ù…Ø§ panic attack Ú©Ø±Ø¯Ù…!"
    assert non_persian_Filter(input_text) == expected_output

    input_text = "100 Ø³Ø§Ù„ Ø¨Ù‡ Ø§ÛŒÙ† Ø³Ø§Ù„â€ŒÙ‡Ø§"
    expected_output = "100 Ø³Ø§Ù„ Ø¨Ù‡ Ø§ÛŒÙ† Ø³Ø§Ù„â€ŒÙ‡Ø§"
    assert non_persian_Filter(input_text) == expected_output

    non_persian_Filter = NonPersianLetterMasker(keep_diacritics=True)
    input_text = "Ú¯ÙÙ„Ù Ù…ÙÙ†Ùˆ Ø§ÙØ°ÛŒÙØª Ù†ÙÚ©ÙÙ†ÛŒÙ†!"
    expected_output = "Ú¯ÙÙ„Ù Ù…ÙÙ†Ùˆ Ø§ÙØ°ÛŒÙØª Ù†ÙÚ©ÙÙ†ÛŒÙ†!"
    assert non_persian_Filter(input_text) == expected_output


def test_remove_html_tags():
    html_tag_Filter = HTMLTagMasker(mask_token="")
    input_text = "<p>Ú¯Ù„ ØµØ¯Ø¨Ø±Ú¯ Ø¨Ù‡ Ù¾ÛŒØ´ ØªÙˆ ÙØ±Ùˆ Ø±ÛŒØ®Øª Ø² Ø®Ø¬Ù„Øª!</p>"
    expected_output = "Ú¯Ù„ ØµØ¯Ø¨Ø±Ú¯ Ø¨Ù‡ Ù¾ÛŒØ´ ØªÙˆ ÙØ±Ùˆ Ø±ÛŒØ®Øª Ø² Ø®Ø¬Ù„Øª!"
    assert html_tag_Filter(input_text) == expected_output

    input_text = "<div>Ø¢Ù†Ø¬Ø§ Ø¨Ø¨Ø± Ù…Ø±Ø§ Ú©Ù‡ Ø´Ø±Ø§Ø¨Ù… Ù†Ù…ÛŒâ€ŒØ¨Ø±Ø¯!</div>"
    expected_output = "Ø¢Ù†Ø¬Ø§ Ø¨Ø¨Ø± Ù…Ø±Ø§ Ú©Ù‡ Ø´Ø±Ø§Ø¨Ù… Ù†Ù…ÛŒâ€ŒØ¨Ø±Ø¯!"
    assert html_tag_Filter.fit_transform(input_text) == expected_output

    input_text = "<a href='https://example.com'>Example</a>"
    expected_output = "Example"
    assert html_tag_Filter(input_text) == expected_output

    input_text = "Ø®Ø¯Ø§ÛŒØ§! Ø®Ø¯Ø§ÛŒØ§ØŒ <b>Ú©ÙˆÛŒØ±Ù…!</b>"
    result = html_tag_Filter(input_text)
    assert result == "Ø®Ø¯Ø§ÛŒØ§! Ø®Ø¯Ø§ÛŒØ§ØŒ Ú©ÙˆÛŒØ±Ù…!"


def test_punctuation_spacings():
    batch_input = []
    batch_expected_output = []
    punct_space_normalizer = SpacingNormalizer()
    input_text = "Ø³Ù„Ø§Ù…!Ú†Ø·ÙˆØ±ÛŒØŸ"
    expected_output = "Ø³Ù„Ø§Ù…! Ú†Ø·ÙˆØ±ÛŒØŸ"
    assert punct_space_normalizer(input_text) == expected_output

    batch_input.append(input_text)
    batch_expected_output.append(expected_output)

    input_text = "Ø´Ø±Ú©Øª Â« Ú¯ÙˆÚ¯Ù„ Â»Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯ ."
    expected_output = "Ø´Ø±Ú©Øª Â«Ú¯ÙˆÚ¯Ù„Â» Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯."

    assert punct_space_normalizer.fit_transform(input_text) == expected_output

    batch_input.append(input_text)
    batch_expected_output.append(expected_output)

    assert list(punct_space_normalizer(batch_input)) == batch_expected_output
    assert (
        list(punct_space_normalizer.fit_transform(batch_input)) == batch_expected_output
    )


def test_mention_masker():
    mention_masker = MentionMasker(mask_token="")
    input_text = "@user Ø´Ù…Ø§ Ø®Ø¨Ø± Ø¯Ø§Ø±ÛŒØ¯ØŸ"
    expected_output = "Ø´Ù…Ø§ Ø®Ø¨Ø± Ø¯Ø§Ø±ÛŒØ¯ØŸ"
    assert mention_masker(input_text) == expected_output

    input_text = "@user Ø³Ù„Ø§Ù… Ø±ÙÙ‚Ø§ @user"
    expected_output = "Ø³Ù„Ø§Ù… Ø±ÙÙ‚Ø§"
    assert mention_masker.fit_transform(input_text) == expected_output


def test_hashtag_masker():
    hashtag_masker = HashtagMasker(mask_token="")
    input_text = "#Ù¾ÛŒØ´Ø±ÙØª_Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ø±Ø§Ø³ØªØ§ÛŒ ØªÙˆØ³Ø¹Ù‡"
    expected_output = "Ø¯Ø± Ø±Ø§Ø³ØªØ§ÛŒ ØªÙˆØ³Ø¹Ù‡"
    assert hashtag_masker(input_text) == expected_output

    input_text = "Ø±ÙˆØ² #Ú©ÙˆØ¯Ú© Ø´Ø§Ø¯ Ø¨Ø§Ø¯."
    expected_output = "Ø±ÙˆØ²  Ø´Ø§Ø¯ Ø¨Ø§Ø¯."
    assert hashtag_masker.fit_transform(input_text) == expected_output


def test_ngram_extractor():
    ngram_extractor = NGramExtractor(range=(1, 2))
    input_text = "Ù‡Ù…Ø§Ù† Ø´Ù‡Ø± Ø§ÛŒØ±Ø§Ù†Ø´ Ø¢Ù…Ø¯ Ø¨Ù‡ ÛŒØ§Ø¯"
    expected_output = [
        "Ù‡Ù…Ø§Ù†",
        "Ø´Ù‡Ø±",
        "Ø§ÛŒØ±Ø§Ù†Ø´",
        "Ø¢Ù…Ø¯",
        "Ø¨Ù‡",
        "ÛŒØ§Ø¯",
        "Ù‡Ù…Ø§Ù† Ø´Ù‡Ø±",
        "Ø´Ù‡Ø± Ø§ÛŒØ±Ø§Ù†Ø´",
        "Ø§ÛŒØ±Ø§Ù†Ø´ Ø¢Ù…Ø¯",
        "Ø¢Ù…Ø¯ Ø¨Ù‡",
        "Ø¨Ù‡ ÛŒØ§Ø¯",
    ]
    assert ngram_extractor(input_text) == expected_output
    assert ngram_extractor.fit_transform(input_text) == expected_output

    ngram_extractor = NGramExtractor(range=(1, 1))
    input_text = "Ù‡ÛŒÚ† Ø¬Ø§ÛŒ Ø¯Ù†ÛŒØ§ ØªØ± Ùˆ Ø®Ø´Ú© Ø±Ø§ Ù…Ø«Ù„ Ø§ÛŒØ±Ø§Ù† Ø¨Ø§ Ù‡Ù… Ù†Ù…ÛŒâ€ŒØ³ÙˆØ²Ø§Ù†Ù†Ø¯."
    expected_output = [
        "Ù‡ÛŒÚ†",
        "Ø¬Ø§ÛŒ",
        "Ø¯Ù†ÛŒØ§",
        "ØªØ±",
        "Ùˆ",
        "Ø®Ø´Ú©",
        "Ø±Ø§",
        "Ù…Ø«Ù„",
        "Ø§ÛŒØ±Ø§Ù†",
        "Ø¨Ø§",
        "Ù‡Ù…",
        "Ù†Ù…ÛŒâ€ŒØ³ÙˆØ²Ø§Ù†Ù†Ø¯",
        ".",
    ]
    assert ngram_extractor(input_text) == expected_output
    assert ngram_extractor.fit_transform(input_text) == expected_output

    ngram_extractor = NGramExtractor(range=(3, 3))
    input_text = ""
    assert ngram_extractor(input_text) == []

    input_text = "Ø¯Ø±ÙˆØ¯"
    assert ngram_extractor(input_text) == []

    input_text = "Ø³Ù„Ø§Ù… Ø¯ÙˆØ³Øª"
    assert ngram_extractor(input_text) == []

    ngram_extractor = NGramExtractor(range=(3, 3))
    input_text = "Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª"
    expected_output = [
        "Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ†",
        "ÛŒÚ© Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡",
        "Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª",
    ]
    assert ngram_extractor(input_text) == expected_output

    ngram_extractor = NGramExtractor(range=(2, 2))
    input_text = [
        "Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ†",
        "ÛŒÚ© Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡",
        "Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª",
    ]
    expected_output = [
        ["Ø§ÛŒÙ† ÛŒÚ©", "ÛŒÚ© Ù…ØªÙ†"],
        ["ÛŒÚ© Ù…ØªÙ†", "Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡"],
        ["Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡", "Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª"],
    ]
    assert list(ngram_extractor(input_text)) == expected_output
    assert list(ngram_extractor.fit_transform(input_text)) == expected_output


def test_ngram_extractor_invalid_inputs():
    with pytest.raises(
        TypeError, match="N-gram range must be a tuple tuple of integers."
    ):
        NGramExtractor(range="invalid")

    with pytest.raises(ValueError, match="N-gram range must be a tuple of length 2."):
        NGramExtractor(range=(1, 2, 3))

    with pytest.raises(ValueError, match="N-gram range must be greater than 0."):
        NGramExtractor(range=(0, 2))

    with pytest.raises(
        ValueError, match="N-gram range must be in the form of \\(min, max\\)."
    ):
        NGramExtractor(range=(3, 1))


def test_flatten():
    flatten = Flatten()
    input_text = [
        ["Ø³Ù„Ø§Ù…", "Ø¯ÙˆØ³Øª"],
        ["Ø®ÙˆØ¨ÛŒØŸ", "Ú†Ø·ÙˆØ±ÛŒØŸ"],
    ]
    expected_output = ["Ø³Ù„Ø§Ù…", "Ø¯ÙˆØ³Øª", "Ø®ÙˆØ¨ÛŒØŸ", "Ú†Ø·ÙˆØ±ÛŒØŸ"]
    assert list(flatten(input_text)) == expected_output

    input_text = [
        ["Ø³Ù„Ø§Ù…", "Ø¯ÙˆØ³Øª"],
        ["Ø®ÙˆØ¨ÛŒØŸ", "Ú†Ø·ÙˆØ±ÛŒØŸ"],
        ["Ù…Ù† Ø®ÙˆØ¨Ù…", "Ø´Ù…Ø§ Ú†Ø·ÙˆØ±ÛŒØ¯ØŸ"],
    ]
    expected_output = ["Ø³Ù„Ø§Ù…", "Ø¯ÙˆØ³Øª", "Ø®ÙˆØ¨ÛŒØŸ", "Ú†Ø·ÙˆØ±ÛŒØŸ", "Ù…Ù† Ø®ÙˆØ¨Ù…", "Ø´Ù…Ø§ Ú†Ø·ÙˆØ±ÛŒØ¯ØŸ"]
    assert list(flatten(input_text)) == expected_output


def test_digit_remover():
    digit_remover = DigitRemover()

    input_text = "Ù‚ÛŒÙ…Øª Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ Û±Û²Û³Û´Ûµ ØªÙˆÙ…Ø§Ù† Ø§Ø³Øª"
    expected_output = "Ù‚ÛŒÙ…Øª Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„  ØªÙˆÙ…Ø§Ù† Ø§Ø³Øª"
    assert digit_remover(input_text) == expected_output

    input_text = "Ø³ÙØ§Ø±Ø´ Ø´Ù…Ø§ Ø¨Ø§ Ú©Ø¯ 98765 Ø«Ø¨Øª Ø´Ø¯"
    expected_output = "Ø³ÙØ§Ø±Ø´ Ø´Ù…Ø§ Ø¨Ø§ Ú©Ø¯  Ø«Ø¨Øª Ø´Ø¯"
    assert digit_remover(input_text) == expected_output

    input_text = "Ú©Ø¯ Ù¾Ø³ØªÛŒ Û±Û°Û´ÛµÛ¶-32901 Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯"
    expected_output = "Ú©Ø¯ Ù¾Ø³ØªÛŒ - Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯"
    assert digit_remover.fit_transform(input_text) == expected_output

    input_text = "Ø³Ù„Ø§Ù…ØŒ Ú†Ø·ÙˆØ±ÛŒ Ø¯ÙˆØ³Øª Ù…Ù†ØŸ"
    expected_output = "Ø³Ù„Ø§Ù…ØŒ Ú†Ø·ÙˆØ±ÛŒ Ø¯ÙˆØ³Øª Ù…Ù†ØŸ"
    assert digit_remover(input_text) == expected_output

    digit_remover_custom = DigitRemover(mask_token="X")
    input_text = "Ø³Ø§Ù„ Û±Û´Û°Û² Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯"
    expected_output = "Ø³Ø§Ù„ XXXX Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯"
    assert digit_remover_custom(input_text) == expected_output

    input_texts = ["Ø´Ù…Ø§Ø±Ù‡ Û±Û²Û³Û´", "Ú©Ø¯ 5678", "Ø¨Ø¯ÙˆÙ† Ø¹Ø¯Ø¯"]
    expected_outputs = ["Ø´Ù…Ø§Ø±Ù‡", "Ú©Ø¯", "Ø¨Ø¯ÙˆÙ† Ø¹Ø¯Ø¯"]
    assert list(digit_remover(input_texts)) == expected_outputs
    assert list(digit_remover.fit_transform(input_texts)) == expected_outputs

    input_text = "Ù†Ø±Ø® ØªÙˆØ±Ù… Û²Û´.Ûµ Ø¯Ø±ØµØ¯ Ø§Ø¹Ù„Ø§Ù… Ø´Ø¯"
    expected_output = "Ù†Ø±Ø® ØªÙˆØ±Ù… . Ø¯Ø±ØµØ¯ Ø§Ø¹Ù„Ø§Ù… Ø´Ø¯"
    assert digit_remover(input_text) == expected_output

    input_text = 12345
    expected_output = "Input must be a string or a Iterable of strings."
    with pytest.raises(ValueError, match=expected_output):
        digit_remover(input_text)


def test_offensive_word_masker():
    offensive_word_masker = OffensiveWordMasker(
        words=["ØªØ§Ù¾Ø§Ù„Ù‡", "ÙØ­Ø´", "Ø¨Ø¯", "Ø²Ø´Øª"], mask_token="[Ø¨ÙˆÙ‚]"
    )

    input_text = "Ø¹Ø¬Ø¨ Ø¢Ø¯Ù… ØªØ§Ù¾Ø§Ù„Ù‡ Ø§ÛŒ Ù‡Ø³ØªÛŒ!"
    expected_output = "Ø¹Ø¬Ø¨ Ø¢Ø¯Ù… [Ø¨ÙˆÙ‚] Ø§ÛŒ Ù‡Ø³ØªÛŒ!"
    assert offensive_word_masker(input_text) == expected_output

    input_text = "Ø§ÛŒÙ† ÙØ­Ø´ Ø¨Ø¯ Ùˆ Ø²Ø´Øª Ø§Ø³Øª"
    expected_output = "Ø§ÛŒÙ† [Ø¨ÙˆÙ‚] [Ø¨ÙˆÙ‚] Ùˆ [Ø¨ÙˆÙ‚] Ø§Ø³Øª"
    assert offensive_word_masker.fit_transform(input_text) == expected_output


def test_offensive_word_masker_default_words():
    offensive_word_masker = OffensiveWordMasker()

    # Test with default offensive words from data.offensive_words
    input_text = "Ø§ÛŒÙ† Ù…ØªÙ† Ø¹Ø§Ø¯ÛŒ Ø§Ø³Øª"
    expected_output = "Ø§ÛŒÙ† Ù…ØªÙ† Ø¹Ø§Ø¯ÛŒ Ø§Ø³Øª"
    assert offensive_word_masker(input_text) == expected_output

    # Test empty mask token behavior
    offensive_word_masker = OffensiveWordMasker(words=["Ø¨Ø¯", "Ø²Ø´Øª"], mask_token="")
    input_text = "Ú©Ù„Ù…Ù‡ Ø¨Ø¯ Ùˆ Ø²Ø´Øª Ø±Ø§ Ø­Ø°Ù Ú©Ù†"
    expected_output = "Ú©Ù„Ù…Ù‡  Ùˆ  Ø±Ø§ Ø­Ø°Ù Ú©Ù†"
    assert offensive_word_masker(input_text) == expected_output

    # Test with iterable input
    input_texts = ["ÙØ­Ø´ Ù†Ú¯Ùˆ", "Ú©Ù„Ø§Ù… Ø²ÛŒØ¨Ø§ Ø¨Ú¯Ùˆ"]
    offensive_word_masker = OffensiveWordMasker(words=["ÙØ­Ø´"], mask_token="***")
    expected_outputs = ["*** Ù†Ú¯Ùˆ", "Ú©Ù„Ø§Ù… Ø²ÛŒØ¨Ø§ Ø¨Ú¯Ùˆ"]
    assert list(offensive_word_masker(input_texts)) == expected_outputs
