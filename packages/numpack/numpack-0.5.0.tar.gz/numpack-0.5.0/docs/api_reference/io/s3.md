# S3 Cloud Storage API Reference

Functions for reading from and writing to Amazon S3.

## Dependencies

```bash
pip install s3fs
```

---

## Functions

### `from_s3(s3_path, output_path, format='auto', drop_if_exists=False, chunk_size=DEFAULT_CHUNK_SIZE, **s3_kwargs)`

Download an object from S3 and import it into NumPack.

#### Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `s3_path` | `str` | *required* | S3 URL (`s3://bucket/path/to/file`) |
| `output_path` | `str` or `Path` | *required* | Output NumPack directory path |
| `format` | `str` | `'auto'` | Input format (see below) |
| `drop_if_exists` | `bool` | `False` | Overwrite if exists |
| `chunk_size` | `int` | `DEFAULT_CHUNK_SIZE` | Chunk size for streaming |
| `**s3_kwargs` | | | Keyword arguments for `s3fs.S3FileSystem` |

#### Supported Formats

| Format | Extensions | Value |
|--------|------------|-------|
| NumPy | `.npy`, `.npz` | `'numpy'` |
| CSV | `.csv` | `'csv'` |
| Text | `.txt` | `'txt'` |
| Parquet | `.parquet` | `'parquet'` |
| Feather | `.feather` | `'feather'` |
| HDF5 | `.h5`, `.hdf5` | `'hdf5'` |
| Auto-detect | | `'auto'` |

#### Returns

- `None`

#### Raises

| Exception | Condition |
|-----------|-----------|
| `DependencyError` | If s3fs is not installed |
| `ValueError` | If format is not supported |

#### Example

```python
from numpack.io import from_s3

# Using default AWS credentials
from_s3('s3://my-bucket/data.npy', 'output.npk')

# Public bucket (anonymous access)
from_s3('s3://public-bucket/data.csv', 'output.npk', anon=True)

# Explicit format
from_s3('s3://bucket/data', 'output.npk', format='parquet')

# Custom S3 endpoint (e.g., MinIO)
from_s3(
    's3://bucket/data.npy',
    'output.npk',
    client_kwargs={'endpoint_url': 'http://localhost:9000'}
)
```

---

### `to_s3(input_path, s3_path, format='auto', array_name=None, chunk_size=DEFAULT_CHUNK_SIZE, **s3_kwargs)`

Export a NumPack array and upload it to S3.

#### Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `input_path` | `str` or `Path` | *required* | Input NumPack directory path |
| `s3_path` | `str` | *required* | S3 URL (`s3://bucket/path/to/file`) |
| `format` | `str` | `'auto'` | Output format |
| `array_name` | `str` or `None` | `None` | Array to export |
| `chunk_size` | `int` | `DEFAULT_CHUNK_SIZE` | Chunk size for streaming |
| `**s3_kwargs` | | | Keyword arguments for `s3fs.S3FileSystem` |

#### Returns

- `None`

#### Raises

| Exception | Condition |
|-----------|-----------|
| `DependencyError` | If s3fs is not installed |
| `ValueError` | If format is not supported |

#### Example

```python
from numpack.io import to_s3

# Upload as Parquet
to_s3('input.npk', 's3://my-bucket/output.parquet')

# Upload as CSV
to_s3('input.npk', 's3://my-bucket/output.csv', format='csv')

# Upload specific array
to_s3('input.npk', 's3://bucket/features.npy', array_name='features')
```

---

## S3 Authentication

### Default Credentials

Uses AWS credential chain (environment variables, `~/.aws/credentials`, IAM role):

```python
from_s3('s3://bucket/data.npy', 'output.npk')
```

### Anonymous Access

For public buckets:

```python
from_s3('s3://public-bucket/data.csv', 'output.npk', anon=True)
```

### Explicit Credentials

```python
from_s3(
    's3://bucket/data.npy',
    'output.npk',
    key='AKIAIOSFODNN7EXAMPLE',
    secret='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'
)
```

### Custom Endpoint (MinIO, etc.)

```python
from_s3(
    's3://bucket/data.npy',
    'output.npk',
    client_kwargs={
        'endpoint_url': 'http://localhost:9000'
    },
    key='minioadmin',
    secret='minioadmin'
)
```

---

## Usage Examples

### Cloud Data Pipeline

```python
from numpack.io import from_s3, to_s3
from numpack import NumPack

# Download from S3
from_s3('s3://data-lake/raw/features.parquet', 'local.npk')

# Process locally
with NumPack('local.npk') as npk:
    features = npk.load('features')
    processed = transform(features)
    npk.save({'processed': processed})

# Upload results
to_s3('local.npk', 's3://data-lake/processed/features.parquet')
```

### Multi-Format Workflow

```python
from numpack.io import from_s3, to_s3

# Download CSV, process, upload as Parquet
from_s3('s3://bucket/raw.csv', 'data.npk')

# ... process with NumPack ...

to_s3('data.npk', 's3://bucket/processed.parquet')
```
