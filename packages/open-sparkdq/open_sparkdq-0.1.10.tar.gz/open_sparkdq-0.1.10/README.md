# open-spark-dlh-dq

Open source Plug-and-play Data Quality for Apache Spark (Batch + Streaming) with YAML checks, profiling, and OpenTelemetry.

---

## ðŸ“Œ Project Overview
`open-spark-dlh-dq` is an open-source Python library providing a **Data Quality (DQ) framework for Apache Spark**.

It supports:

- âœ… **Batch & Streaming DQ** with declarative YAML suites
- âœ… **Custom checks** via Python (`dq_check`, `unit_test`)
- âœ… **CLI execution** for datasets in directories or Spark DataFrames
- âœ… **Inline checks** in PySpark scripts
- âœ… **Format support**: Parquet, CSV, Iceberg, Delta, JSON, ORC
- âœ… **Profiler & OpenTelemetry** for observability

Built on **PySpark**, **PyDeequ**, and **Chispa**, this library enables robust data validation pipelines.

---

## âœ… Features

- **Batch DQ**: Validate static datasets using YAML or inline rules.
- **Streaming DQ**: Apply checks on micro-batches via `foreachBatch`.
- **Custom Checks**: Extend with Python functions in `user_checks/`.
- **CLI Tool**: Run suites via `sparkdq run --yaml <suite.yml>`.
- **Profiler**: Generate summary stats and quantiles.
- **OpenTelemetry**: Capture spans and traces for test cases.

---

## ðŸ“‚ Repository Structure

<pre>

open-spark-dlh-dq/
â”œâ”€ pyproject.toml
â”œâ”€ README.md
â”œâ”€ LICENSE
â”‚
â”œâ”€ sparkdq/
â”‚  â”œâ”€ cli/main.py                          # CLI entry point
â”‚  â”œâ”€ config/                              # YAML loader, env vars, schema binding
â”‚  â”œâ”€ core/                                # Models, registry, Spark session, runner
â”‚  â”‚  â””â”€ validators/                       # Built-in + custom validator classes
â”‚  â”œâ”€ profiling/profiler.py                # Profiling utilities
â”‚  â”œâ”€ resources/open_spark_dlh_dq.yml      # Default YAML suite
â”‚  â”œâ”€ observability/otel.py                # OpenTelemetry integration
â”‚  â””â”€ integrations/streaming.py            # foreachBatch wrapper
â”‚
â”œâ”€ user_checks/                            # User-defined checks
â”‚  â””â”€ example_checks.py
â”‚
â”œâ”€ examples/                               # Usage examples
â”‚  â”œâ”€ suites/orders_dq.yml
â”‚  â”œâ”€ batch_example.py
â”‚  â””â”€ streaming_example.py
â”‚
â””â”€ tests/                                  # Unit tests
   â”œâ”€ test_yaml_loader.py
   â”œâ”€ test_chispa_integration.py
   â”œâ”€ test_pydeequ_integration.py
   â”œâ”€ test_runner.py
   â”œâ”€ test_validators.py
   â”œâ”€ test_validator_contracts.py
   â””â”€ test_cli.py

</pre>

---

## ðŸ›  Usage

### **Run CLI with YAML suite**
```bash
sparkdq run --yaml ./sparkdq/resources/open_spark_dlh_dq.yml --suite-name orders_dq --format text
```

### **Inline checks in PySpark**
```python
from sparkdq.core.runner import run_suite
from sparkdq.config.loader import load_yaml_suite

suite = load_yaml_suite("./sparkdq/resources/open_spark_dlh_dq.yml")
df = spark.read.parquet("./data/orders")
run_suite(df, suite)
```

### **Streaming example**
```bash
python examples/streaming_example.py
```

---

## ðŸ§© Custom Checks
Add Python methods in `user_checks/example_checks.py`:
```python
from sparkdq.core.registry import dq_check, unit_test

@dq_check("amount_positive")
def amount_positive(df):
    return df.filter(df.amount > 0).count() == df.count()
```
Reference them in YAML:
```yaml
test_cases:
  - name: amount_positive
    type: dq_check
```

---

## ðŸ“Š Profiler & OpenTelemetry
Enable profiling and observability in your pipeline:
```python
from sparkdq.profiling.profiler import profile_df
profile_df(df)
```

OpenTelemetry spans can be enabled via `sparkdq/observability/otel.py`.

---

## ðŸ”¨ Build & Publish

### **Build for PyPI (Windows)**
```powershell
./build.ps1
```

### **Build for PyPI (Linux)**
```bash
./build.sh
```

### **Example Repository to understand how to use**

https://github.com/aashish72it/spark-test


