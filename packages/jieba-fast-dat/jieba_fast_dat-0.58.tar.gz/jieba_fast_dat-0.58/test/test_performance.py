import cProfile
import importlib
import io
import logging
import os
import pstats
import shutil
import signal
import sys
import tempfile
import time
import warnings
from collections.abc import Callable
from multiprocessing import Process, Queue
from pathlib import Path
from types import FrameType
from typing import Any

import pytest


# --- Timeout Implementation ---
class TimeoutException(Exception):
    pass


def timeout_handler(signum: int, frame: FrameType | None):
    raise TimeoutException("Test timed out!")


# Suppress specific warnings from jieba or related packages
warnings.filterwarnings(
    "ignore",
    message="pkg_resources is deprecated as an API.*",
    category=UserWarning,
    module="jieba",
)
warnings.filterwarnings(
    "ignore",
    message="Can't find any available. Use default.",
    category=UserWarning,
)


def measure_time(
    func: Callable[..., Any], repetitions: int = 1, *args: Any, **kwargs: Any
) -> tuple[float, Any]:
    """
    Measures the execution time of a function.
    Handles iterators by consuming them to ensure all computation is done
    """
    total_time = 0.0
    result: Any = None
    for _ in range(repetitions):
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        # If the result is an iterator, consume it to ensure all computation is done
        if hasattr(result, "__iter__") and not isinstance(
            result, (list, tuple, str, dict, bytes)
        ):
            _ = list(result)
        end_time = time.perf_counter()
        total_time += end_time - start_time
    return total_time / repetitions, result


def clear_all_caches(silent: bool = False):
    """
    Clears all cache files generated by jieba_fast_dat and original jieba.
    """
    if not silent:
        print("Clearing all caches...")

    tmp_dir = tempfile.gettempdir()
    for f in os.listdir(tmp_dir):
        path = os.path.join(tmp_dir, f)

        # Check for unified cache patterns
        is_jieba_cache = f.startswith("jieba.") and (
            f.endswith(".cache") or f.endswith(".cache.dat")
        )
        is_jieba_fast_dat_cache = f.startswith("jieba_fast_dat.") and (
            f.endswith(".cache")
            or f.endswith(".dat")
            or f.endswith(".idf.dat")
            or f.endswith("hmm_models.bin")
        )

        if is_jieba_cache or is_jieba_fast_dat_cache:
            try:
                os.remove(path)
            except OSError:
                pass  # Ignore errors
        # Check for directory caches (like those created by jieba)
        elif f.startswith("jieba.") and os.path.isdir(path):
            shutil.rmtree(path, ignore_errors=True)

    if not silent:
        print("All caches cleared.")


def _run_library_tests(
    lib_name: str,
    num_repetitions: int,
    custom_dict_path: str,
    user_dict_path: str,
    test_text: str,
    results_queue: Queue,
    timeout_seconds: int = 60,
) -> None:
    """
    Runs the full benchmark suite for a single library.
    Results are put into the results_queue.
    """
    # Each child process creates its own temporary log file
    temp_log_fd, temp_log_path = tempfile.mkstemp(
        prefix=f"benchmark_log_{lib_name}_", suffix=".log"
    )
    temp_log_file = os.fdopen(temp_log_fd, "w", encoding="utf-8", buffering=1)

    # Redirect stdout and stderr to this temporary log file
    original_stdout_fd = os.dup(1)
    original_stderr_fd = os.dup(2)
    os.dup2(temp_log_file.fileno(), 1)  # Redirect stdout
    os.dup2(temp_log_file.fileno(), 2)  # Redirect stderr

    # Reconfigure logging for this child process
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        stream=sys.stdout,
    )

    profiler = cProfile.Profile()
    profiler.enable()

    print(f"\n--- Starting tests for: {lib_name} ---")

    lib_results: dict[str, Any] = {}

    try:
        lib_module = importlib.import_module(lib_name)
        Tokenizer = lib_module.Tokenizer
    except ImportError as e:
        print(f"Error loading {lib_name}: {e}", file=sys.stderr)
        results_queue.put((lib_name, {"error": str(e)}, temp_log_path))
        return

    # --- Test 1: Cold Start Dictionary Initialization ---
    def cold_init():
        clear_all_caches(silent=True)
        tokenizer = Tokenizer(dictionary=custom_dict_path)
        if lib_name == "jieba_fast_dat":
            tokenizer.initialize(force_rebuild=True)
        else:
            tokenizer.initialize()
        _ = list(tokenizer.lcut(test_text))
        return tokenizer

    try:
        print("Measuring 1. Cold Init...")
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout_seconds)
        first_init_time, _ = measure_time(cold_init, 1)
        signal.alarm(0)
        lib_results["first_init_time"] = first_init_time
    except TimeoutException:
        signal.alarm(0)
        lib_results["first_init_time"] = float("inf")
        logging.error(f"❌ {lib_name} Cold Init timed out.")
    except Exception as e:
        signal.alarm(0)
        lib_results["first_init_time"] = float("inf")
        logging.error(f"⚠️ {lib_name} Cold Init Error: {e}", exc_info=True)

    # --- Test 2: Warm Start Dictionary Initialization ---
    def warm_init():
        tokenizer = Tokenizer(dictionary=custom_dict_path)
        tokenizer.initialize()
        _ = list(tokenizer.lcut(test_text))
        return tokenizer

    try:
        print("Measuring 2. Warm Init...")
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout_seconds)
        second_init_time, final_tokenizer = measure_time(warm_init, num_repetitions)
        signal.alarm(0)
        lib_results["second_init_time"] = second_init_time
    except TimeoutException:
        signal.alarm(0)
        lib_results["second_init_time"] = float("inf")
        logging.error(f"❌ {lib_name} Warm Init timed out.")
        results_queue.put((lib_name, lib_results, temp_log_path))
        return
    except Exception as e:
        signal.alarm(0)
        lib_results["second_init_time"] = float("inf")
        logging.error(f"⚠️ {lib_name} Warm Init Error: {e}", exc_info=True)
        results_queue.put((lib_name, lib_results, temp_log_path))
        return

    # --- Test X: HMM Model Load ---
    lib_posseg_module = None
    try:
        print("Measuring X. HMM Model Load...")
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout_seconds)
        _lib_posseg_module = importlib.import_module(f"{lib_name}.posseg")
        hmm_load_time, _ = measure_time(_lib_posseg_module.load_model, 1)
        lib_posseg_module = _lib_posseg_module
        signal.alarm(0)
        lib_results["HMM_model_load_time"] = hmm_load_time
    except TimeoutException:
        signal.alarm(0)
        lib_results["HMM_model_load_time"] = float("inf")
        logging.error(f"❌ {lib_name} HMM Model Load timed out.")
    except Exception as e:
        signal.alarm(0)
        lib_results["HMM_model_load_time"] = float("inf")
        logging.error(f"⚠️ {lib_name} HMM Model Load Error: {e}", exc_info=True)

    # --- Test 3: User Dictionary Loading Performance ---
    try:
        print("Measuring 3. User Dict Load (No Cache)...")
        if lib_name == "jieba_fast_dat":
            # For jieba_fast_dat, we manually invalidate the cache to measure
            # "No Cache" time
            from jieba_fast_dat.core.tokenizer import USER_DICT_CACHE_PREFIX
            from jieba_fast_dat.utils import CacheManager

            user_cache_file = CacheManager.get_cache_path(
                user_dict_path, prefix=USER_DICT_CACHE_PREFIX
            )
            user_cache_file.unlink(missing_ok=True)

        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout_seconds)
        user_dict_load_time_no_cache, _ = measure_time(
            final_tokenizer.load_userdict, 1, user_dict_path
        )
        signal.alarm(0)
        lib_results["user_dict_load_time_no_cache"] = user_dict_load_time_no_cache

        print("Measuring 3.1. User Dict Load (With Cache)...")
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout_seconds)
        user_dict_load_time_with_cache, _ = measure_time(
            final_tokenizer.load_userdict, num_repetitions, user_dict_path
        )
        signal.alarm(0)
        lib_results["user_dict_load_time_with_cache"] = user_dict_load_time_with_cache
    except TimeoutException:
        signal.alarm(0)
        lib_results["user_dict_load_time_no_cache"] = float("inf")
        lib_results["user_dict_load_time_with_cache"] = float("inf")
        logging.error(f"❌ {lib_name} User Dict Load timed out.")
    except Exception as e:
        signal.alarm(0)
        lib_results["user_dict_load_time_no_cache"] = float("inf")
        lib_results["user_dict_load_time_with_cache"] = float("inf")
        logging.error(f"⚠️ {lib_name} User Dict Load Error: {e}", exc_info=True)

    # Set up POSSEG
    try:
        if lib_posseg_module is None:
            lib_posseg_module = importlib.import_module(f"{lib_name}.posseg")
        posseg_tokenizer = lib_posseg_module.dt
        posseg_tokenizer.tokenizer = final_tokenizer
    except Exception as e:
        print(f"⚠️ {lib_name} POSSEG setup Error: {e}", file=sys.stderr)

    # --- Test 4-7: Cut and POS Performance ---
    perf_tests = [
        (
            "4. Cut (HMM=False)",
            "cut_time_HMM_False",
            final_tokenizer.cut,
            {"HMM": False},
        ),
        (
            "5. POS (HMM=False)",
            "pos_time_HMM_False",
            posseg_tokenizer.cut,
            {"HMM": False},
        ),
        ("6. Cut (HMM=True)", "cut_time_HMM_True", final_tokenizer.cut, {"HMM": True}),
        ("7. POS (HMM=True)", "pos_time_HMM_True", posseg_tokenizer.cut, {"HMM": True}),
    ]

    for label, key, func, kwargs in perf_tests:
        try:
            print(f"Measuring {label}...")
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout_seconds)
            t, _ = measure_time(func, num_repetitions, test_text, **kwargs)
            signal.alarm(0)
            lib_results[key] = t
        except TimeoutException:
            signal.alarm(0)
            lib_results[key] = float("inf")
            logging.error(f"❌ {lib_name} {label} timed out.")
        except Exception as e:
            signal.alarm(0)
            lib_results[key] = float("inf")
            logging.error(f"⚠️ {lib_name} {label} Error: {e}", exc_info=True)

    profiler.disable()
    print(f"\n--- cProfile Stats for {lib_name} ---\n")
    s = io.StringIO()
    sortby = pstats.SortKey.CUMULATIVE
    ps = pstats.Stats(profiler, stream=s).sort_stats(sortby)
    ps.print_stats(500)
    print(s.getvalue())

    del final_tokenizer
    if "posseg_tokenizer" in locals():
        del posseg_tokenizer

    sys.stdout.flush()
    sys.stderr.flush()
    os.dup2(original_stdout_fd, 1)
    os.dup2(original_stderr_fd, 2)
    os.close(original_stdout_fd)
    os.close(original_stderr_fd)
    temp_log_file.close()

    results_queue.put((lib_name, lib_results, temp_log_path))


def run_performance_test(log_file_path: str):
    # Define file paths relative to this file
    base_dir = Path(__file__).parent.parent
    custom_dict_path = str(base_dir / "extra_dict" / "dict.txt.big")
    user_dict_path = str(base_dir / "extra_dict" / "dict.txt.big.tw_nerd.txt")
    test_text_path = str(base_dir / "extra_dict" / "profile_test")

    # Check if files exist
    for path in [custom_dict_path, user_dict_path, test_text_path]:
        if not os.path.exists(path):
            pytest.fail(f"Required file not found: {path}")

    with open(test_text_path, encoding="utf-8") as f:
        long_text = f.read()

    NUM_REPETITIONS = 5
    DEFAULT_TIMEOUT_SECONDS = 600

    results: dict[str, dict[str, Any]] = {}
    libraries = ["jieba", "jieba_fast_dat"]
    results_queue = Queue()
    processes = []

    # Ensure the directory for log file exists
    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)

    # Redirect stdout/stderr to the log file in the main process
    with open(log_file_path, "w", encoding="utf-8") as log_f:
        original_stdout = sys.stdout
        original_stderr = sys.stderr
        sys.stdout = log_f
        sys.stderr = log_f

        try:
            print("Starting performance comparison (jieba vs. jieba_fast_dat)...")

            for lib_name in libraries:
                p = Process(
                    target=_run_library_tests,
                    args=(
                        lib_name,
                        NUM_REPETITIONS,
                        custom_dict_path,
                        user_dict_path,
                        long_text,
                        results_queue,
                        DEFAULT_TIMEOUT_SECONDS,
                    ),
                )
                processes.append(p)
                p.start()

            for p in processes:
                p.join()

            child_log_files: list[str] = []
            for _ in libraries:
                lib_name, lib_results, temp_log_path = results_queue.get()
                results[lib_name] = lib_results
                child_log_files.append(temp_log_path)

            for temp_log_path in child_log_files:
                try:
                    with open(temp_log_path, encoding="utf-8") as temp_f:
                        lib_name_from_path = os.path.basename(temp_log_path).split("_")[
                            2
                        ]
                        print(
                            f"\n--- Log from {os.path.basename(temp_log_path)} "
                            f"for {lib_name_from_path} ---"
                        )
                        print(temp_f.read())
                        print(f"--- End Log from {os.path.basename(temp_log_path)} ---")
                finally:
                    if os.path.exists(temp_log_path):
                        os.remove(temp_log_path)

            # --- Summary ---
            print("\n" + "=" * 92)
            print("--- Final Summary: Performance Comparison (Time in seconds) ---")
            print("=" * 92)

            metrics = [
                ("Initial Main Dict Load (No Cache)", "first_init_time"),
                ("Initial Main Dict Load (With Cache)", "second_init_time"),
                ("HMM Model Load", "HMM_model_load_time"),
                ("User Dict Load (No Cache)", "user_dict_load_time_no_cache"),
                ("User Dict Load (With Cache)", "user_dict_load_time_with_cache"),
                ("Word Segmentation (HMM=False)", "cut_time_HMM_False"),
                ("POS Tagging (HMM=False)", "pos_time_HMM_False"),
                ("Word Segmentation (HMM=True)", "cut_time_HMM_True"),
                ("POS Tagging (HMM=True)", "pos_time_HMM_True"),
            ]

            header = (
                f"{'Metric':<40} {'Jieba':>15} {'Jieba_fast_dat':>20} {'Speedup':>15}"
            )
            print(header)
            print("-" * 92)

            for desc, key in metrics:
                jieba_time = results.get("jieba", {}).get(key, float("inf"))
                fast_dat_time = results.get("jieba_fast_dat", {}).get(key, float("inf"))

                if jieba_time == float("inf") or fast_dat_time == float("inf"):
                    speedup_str, j_str, f_str = "N/A", "N/A", "N/A"
                else:
                    j_str, f_str = f"{jieba_time:.7f}s", f"{fast_dat_time:.7f}s"
                    if fast_dat_time < jieba_time:
                        speedup_str = f"**{jieba_time / fast_dat_time:.2f}x** faster"
                    else:
                        speedup_str = f"{fast_dat_time / jieba_time:.2f}x slower"
                print(f"{desc:<40} {j_str:>15} {f_str:>20} {speedup_str:>15}")

        finally:
            sys.stdout = original_stdout
            sys.stderr = original_stderr


@pytest.mark.timeout(600)
def test_comprehensive_performance_comparison():
    """
    Test case that runs the full performance comparison benchmark.
    """
    log_path = "tmp/test_performance.log"
    # Set start method for multiprocessing if not already set
    import multiprocessing

    try:
        multiprocessing.set_start_method("spawn", force=True)
    except RuntimeError:
        pass

    run_performance_test(log_path)

    # Assertions to ensure it ran successfully
    assert os.path.exists(log_path)
    assert os.path.getsize(log_path) > 0
