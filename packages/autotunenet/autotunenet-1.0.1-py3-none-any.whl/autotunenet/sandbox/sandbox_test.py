from autotunenet.parameters import ParameterSpace
# from autotunenet.core.optimizer_test import DummyOptimizer
from autotunenet.bayesian_optimizer import BayesianOptimizer
# import logging
from autotunenet.logging.global_exception import install_global_exception_handler
install_global_exception_handler()

def test_dummy_optimizer(params):
    lr = params["lr"]
    #for testing, simulate occasional instability
    if lr > 0.88:
        return -100.0 # Penalize high learning rates
    return -(lr - 0.01) ** 2

def main():
    space = ParameterSpace({
        "lr": (0.001, 0.1)
    })
    
    optimizer = BayesianOptimizer(space, seed=42, smoothing_window=3)
    
    for _ in range(25):
        params = optimizer.suggest()
        score = test_dummy_optimizer(params)
        optimizer.observe(params, score)
    
    # for step in range(1, 21):
    #     params = optimizer.suggest()
    #     score = test_dummy_optimizer(params)
    #     optimizer.observe(params, score)
        
    #     best_lr = optimizer.best_params()["lr"]
    #     best_score = optimizer.best_score()
        
    #     print(
    #         f"step: {step:02d} | "  
    #         f"lr: {params['lr']:.5f} | "  
    #         f"best_lr: {best_lr:.5f} | "  
    #         f"best_score: {best_score:.5f} | "  
    #     )
        
if __name__ == "__main__":
    main()
        