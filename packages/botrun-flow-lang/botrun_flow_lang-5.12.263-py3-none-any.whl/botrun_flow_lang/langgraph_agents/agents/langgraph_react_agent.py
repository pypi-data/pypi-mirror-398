import os
import asyncio
import json
from datetime import datetime
from typing import ClassVar, Dict, List, Optional, Any

from langchain_core.messages import SystemMessage

from botrun_flow_lang.constants import LANG_EN, LANG_ZH_TW

from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

from langchain_core.tools import BaseTool

from langchain_core.tools import tool

from botrun_flow_lang.utils.botrun_logger import get_default_botrun_logger

# All tools now provided by MCP server - no local tool imports needed

from botrun_flow_lang.langgraph_agents.agents.checkpointer.firestore_checkpointer import (
    AsyncFirestoreCheckpointer,
)

from langgraph.prebuilt import create_react_agent

from dotenv import load_dotenv

import copy  # ç”¨æ–¼æ·±æ‹·è² schemaï¼Œé¿å…æ„å¤–ä¿®æ”¹åŸå§‹å°è±¡

# Removed DALL-E and rate limiting imports - tools now provided by MCP server

# =========
# ğŸ“‹ STAGE 4 REFACTORING COMPLETED (MCP Integration)
#
# This file has been refactored to integrate with MCP (Model Context Protocol):
#
# âœ… REMOVED (~600 lines):
#   - Language-specific system prompts (zh_tw_system_prompt, en_system_prompt)
#   - Local tool definitions: scrape, chat_with_pdf, chat_with_imgs, generate_image,
#     generate_tmp_public_url, create_html_page, compare_date_time
#   - Complex conditional logic (if botrun_flow_lang_url and user_id)
#   - Rate limiting exception and related imports
#   - Unused utility imports
#
# âœ… SIMPLIFIED:
#   - Direct system_prompt usage (no concatenation)
#   - Streamlined tools list (only language-specific tools)
#   - Clean MCP integration via mcp_config parameter
#   - Maintained backward compatibility for all parameters
#
# ğŸ¯ RESULT:
#   - Reduced complexity while maintaining full functionality
#   - All tools available via MCP server at /mcp/default/mcp/
#   - Ready for Phase 2: language-specific tools migration
# =========

# æ”¾åˆ°è¦ç”¨çš„æ™‚å€™æ‰ initï¼Œä¸ç„¶loading æœƒèŠ±æ™‚é–“
# å› ç‚ºè¦è®“ langgraph åœ¨æœ¬åœ°ç«¯åŸ·è¡Œï¼Œæ‰€ä»¥é€™ä¸€æ®µåˆæ¬å›åˆ°å¤–é¢äº†
from langchain_google_genai import ChatGoogleGenerativeAI

# =========
# æ”¾åˆ°è¦ç”¨çš„æ™‚å€™æ‰ importï¼Œä¸ç„¶loading æœƒèŠ±æ™‚é–“
# å› ç‚ºLangGraph åœ¨æœ¬åœ°ç«¯åŸ·è¡Œï¼Œæ‰€ä»¥é€™ä¸€æ®µåˆæ¬å›åˆ°å¤–é¢äº†
from botrun_flow_lang.langgraph_agents.agents.util.model_utils import (
    RotatingChatAnthropic,
)

# =========
# æ”¾åˆ°è¦ç”¨çš„æ™‚å€™æ‰ initï¼Œä¸ç„¶loading æœƒèŠ±æ™‚é–“
# å› ç‚ºLangGraph åœ¨æœ¬åœ°ç«¯åŸ·è¡Œï¼Œæ‰€ä»¥é€™ä¸€æ®µåˆæ¬å›åˆ°å¤–é¢äº†
from langchain_openai import ChatOpenAI

# =========
# æ”¾åˆ°è¦ç”¨çš„æ™‚å€™æ‰ initï¼Œä¸ç„¶loading æœƒèŠ±æ™‚é–“
# å› ç‚ºLangGraph åœ¨æœ¬åœ°ç«¯åŸ·è¡Œï¼Œæ‰€ä»¥é€™ä¸€æ®µåˆæ¬å›åˆ°å¤–é¢äº†
from langchain_anthropic import ChatAnthropic

# =========

# å‡è¨­ MultiServerMCPClient å’Œ StructuredTool å·²ç¶“è¢«æ­£ç¢ºå°å…¥
from langchain.tools import StructuredTool  # æˆ– langchain_core.tools
from langchain_mcp_adapters.client import MultiServerMCPClient

# ========
# for Vertex AI
from google.oauth2 import service_account
from langchain_google_vertexai import ChatVertexAI
from langchain_google_vertexai.model_garden import ChatAnthropicVertex

load_dotenv()

# logger = default_logger
logger = get_default_botrun_logger()


# Removed BotrunRateLimitException - rate limiting now handled by MCP server


# Load Anthropic API keys from environment
# anthropic_api_keys_str = os.getenv("ANTHROPIC_API_KEYS", "")
# anthropic_api_keys = [
#     key.strip() for key in anthropic_api_keys_str.split(",") if key.strip()
# ]

# Initialize the model with key rotation if multiple keys are available
# if anthropic_api_keys:
#     model = RotatingChatAnthropic(
#         model_name="claude-3-7-sonnet-latest",
#         keys=anthropic_api_keys,
#         temperature=0,
#         max_tokens=8192,
#     )
# å»ºç«‹ AWS Session
# session = boto3.Session(
#     aws_access_key_id="",
#     aws_secret_access_key="",
#     region_name="us-west-2",
# )


# # ä½¿ç”¨è©² Session åˆå§‹åŒ– Bedrock å®¢æˆ¶ç«¯
# bedrock_runtime = session.client(
#     service_name="bedrock-runtime",
# )
# model = ChatBedrockConverse(
#     model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0",
#     client=bedrock_runtime,
#     temperature=0,
#     max_tokens=8192,
# )
# else:
# Fallback to traditional initialization if no keys are specified
def get_react_agent_model_name(model_name: str = ""):
    final_model_name = model_name
    if final_model_name == "":
        final_model_name = "claude-sonnet-4-5-20250929"
    logger.info(f"final_model_name: {final_model_name}")
    return final_model_name


ANTHROPIC_MAX_TOKENS = 64000
GEMINI_MAX_TOKENS = 32000
TAIDE_MAX_TOKENS = 8192


def get_react_agent_model(model_name: str = ""):
    final_model_name = get_react_agent_model_name(model_name).strip()

    # è™•ç† taide/ å‰ç¶´çš„æ¨¡å‹
    if final_model_name.startswith("taide/"):
        taide_api_key = os.getenv("TAIDE_API_KEY", "")
        taide_base_url = os.getenv("TAIDE_BASE_URL", "")

        if not taide_api_key or not taide_base_url:
            raise ValueError(
                f"Model name starts with 'taide/' but TAIDE_API_KEY or TAIDE_BASE_URL not set. "
                f"Both environment variables are required for: {final_model_name}"
            )

        # å–å¾— taide/ å¾Œé¢çš„æ¨¡å‹åç¨±
        taide_model_name = final_model_name[len("taide/"):]

        if not taide_model_name:
            raise ValueError(
                f"Invalid taide model format: {final_model_name}. "
                "Expected format: taide/<model_name>"
            )

        model = ChatOpenAI(
            openai_api_key=taide_api_key,
            openai_api_base=taide_base_url,
            model_name=taide_model_name,
            temperature=0,
            max_tokens=TAIDE_MAX_TOKENS,
        )
        logger.info(f"model ChatOpenAI (TAIDE) {taide_model_name} @ {taide_base_url}")
        return model

    # è™•ç† vertexai/ å‰ç¶´çš„æ¨¡å‹
    if final_model_name.startswith("vertex-ai/"):
        vertex_project = os.getenv("VERTEX_AI_LANGCHAIN_PROJECT", "")

        # å¦‚æœæ²’æœ‰è¨­å®š VERTEX_AI_LANGCHAIN_PROJECTï¼Œå‰‡ä¸è™•ç† vertex-ai/ å‰ç¶´
        if not vertex_project:
            logger.warning(
                f"Model name starts with 'vertex-ai/' but VERTEX_AI_LANGCHAIN_PROJECT not set. "
                f"Skipping vertex-ai/ processing for {final_model_name}"
            )
            # ç§»é™¤ vertex-ai/ å‰ç¶´å¾Œç¹¼çºŒè™•ç†
            final_model_name = final_model_name[len("vertex-ai/"):]
            # ç§»é™¤ region éƒ¨åˆ† (å¦‚æœæœ‰çš„è©±)
            if "/" in final_model_name:
                parts = final_model_name.split("/", 1)
                if len(parts) == 2:
                    final_model_name = parts[1]
        else:
            # è§£æ vertex-ai/region/model_name æ ¼å¼
            parts = final_model_name.split("/")

            if len(parts) != 3:
                raise ValueError(
                    f"Invalid vertexai model format: {final_model_name}. "
                    "Expected format: vertex-ai/<region>/<model_name>"
                )

            vertex_region = parts[1]
            vertex_model_name = parts[2]

            if not vertex_region or not vertex_model_name:
                raise ValueError(
                    f"Missing region or model_name in: {final_model_name}. "
                    "Both region and model_name are required."
                )

            # å–å¾— credentials
            vertex_sa_path = os.getenv(
                "VERTEX_AI_LANGCHAIN_GOOGLE_APPLICATION_CREDENTIALS", ""
            )

            credentials = None
            if vertex_sa_path and os.path.exists(vertex_sa_path):
                SCOPES = ["https://www.googleapis.com/auth/cloud-platform"]
                credentials = service_account.Credentials.from_service_account_file(
                    vertex_sa_path, scopes=SCOPES
                )
                logger.info(f"Using Vertex AI service account from {vertex_sa_path}")
            else:
                logger.warning(
                    "VERTEX_AI_LANGCHAIN_GOOGLE_APPLICATION_CREDENTIALS not set. Using ADC."
                )

            # åˆ¤æ–·æ¨¡å‹é¡å‹ä¸¦å‰µå»ºç›¸æ‡‰å¯¦ä¾‹
            if vertex_model_name.startswith("gemini-"):
                # Gemini ç³»åˆ—ï¼šgemini-2.5-pro, gemini-2.5-flash, gemini-pro
                model = ChatVertexAI(
                    model=vertex_model_name,
                    location=vertex_region,
                    project=vertex_project,
                    credentials=credentials,
                    temperature=0,
                    max_tokens=GEMINI_MAX_TOKENS,
                )
                logger.info(
                    f"model ChatVertexAI {vertex_model_name} @ {vertex_region} (project: {vertex_project})"
                )

            elif "claude" in vertex_model_name.lower() or vertex_model_name.startswith("maison/"):
                # Anthropic Claude (model garden)
                model = ChatAnthropicVertex(
                    model=vertex_model_name,
                    location=vertex_region,
                    project=vertex_project,
                    credentials=credentials,
                    temperature=0,
                    max_tokens=ANTHROPIC_MAX_TOKENS,
                )
                logger.info(
                    f"model ChatAnthropicVertex {vertex_model_name} @ {vertex_region} (project: {vertex_project})"
                )

            else:
                raise ValueError(
                    f"Unsupported Vertex AI model: {vertex_model_name}. "
                    "Supported types: gemini-*, claude*, maison/*"
                )

            return model

    if final_model_name.startswith("gemini-"):
        model = ChatGoogleGenerativeAI(
            model=final_model_name, temperature=0, max_tokens=GEMINI_MAX_TOKENS
        )
        logger.info(f"model ChatGoogleGenerativeAI {final_model_name}")
    elif final_model_name.startswith("claude-"):
        # use_vertex_ai = os.getenv("USE_VERTEX_AI", "false").lower() in ("true", "1", "yes")
        vertex_project = os.getenv("VERTEX_AI_LANGCHAIN_PROJECT", "")
        vertex_location = os.getenv("VERTEX_AI_LANGCHAIN_LOCATION", "")
        vertex_model = os.getenv("VERTEX_AI_LANGCHAIN_MODEL", "")
        vertex_sa_path = os.getenv(
            "VERTEX_AI_LANGCHAIN_GOOGLE_APPLICATION_CREDENTIALS", ""
        )

        if vertex_location and vertex_model and vertex_sa_path and vertex_project:
            # å¾ç’°å¢ƒè®Šæ•¸è®€å–è¨­å®š

            # é©—è­‰ service account
            credentials = None
            if vertex_sa_path and os.path.exists(vertex_sa_path):
                # åŠ å…¥ Vertex AI éœ€è¦çš„ scopes
                SCOPES = [
                    "https://www.googleapis.com/auth/cloud-platform",
                ]
                credentials = service_account.Credentials.from_service_account_file(
                    vertex_sa_path, scopes=SCOPES
                )
                logger.info(f"Using Vertex AI service account from {vertex_sa_path}")
            else:
                logger.warning(
                    "VERTEX_AI_GOOGLE_APPLICATION_CREDENTIALS not set or file not found. Using ADC if available."
                )

            # åˆå§‹åŒ– ChatAnthropicVertex
            model = ChatAnthropicVertex(
                project=vertex_project,
                model=vertex_model,
                location=vertex_location,
                credentials=credentials,
                temperature=0,
                max_tokens=ANTHROPIC_MAX_TOKENS,
            )
            logger.info(
                f"model ChatAnthropicVertex {vertex_project} @ {vertex_model} @ {vertex_location}"
            )

        else:
            anthropic_api_keys_str = os.getenv("ANTHROPIC_API_KEYS", "")
            anthropic_api_keys = [
                key.strip() for key in anthropic_api_keys_str.split(",") if key.strip()
            ]
            if anthropic_api_keys:

                model = RotatingChatAnthropic(
                    model_name=final_model_name,
                    keys=anthropic_api_keys,
                    temperature=0,
                    max_tokens=ANTHROPIC_MAX_TOKENS,
                )
                logger.info(f"model RotatingChatAnthropic {final_model_name}")
            elif os.getenv("OPENROUTER_API_KEY") and os.getenv("OPENROUTER_BASE_URL"):

                openrouter_model_name = "anthropic/claude-sonnet-4.5"
                # openrouter_model_name = "openai/o4-mini-high"
                # openrouter_model_name = "openai/gpt-4.1"
                model = ChatOpenAI(
                    openai_api_key=os.getenv("OPENROUTER_API_KEY"),
                    openai_api_base=os.getenv("OPENROUTER_BASE_URL"),
                    model_name=openrouter_model_name,
                    temperature=0,
                    max_tokens=ANTHROPIC_MAX_TOKENS,
                    model_kwargs={
                        # "headers": {
                        #     "HTTP-Referer": getenv("YOUR_SITE_URL"),
                        #     "X-Title": getenv("YOUR_SITE_NAME"),
                        # }
                    },
                )
                logger.info(f"model OpenRouter {openrouter_model_name}")
            else:

                model = ChatAnthropic(
                    model=final_model_name,
                    temperature=0,
                    max_tokens=ANTHROPIC_MAX_TOKENS,
                    # model_kwargs={
                    # "extra_headers": {
                    # "anthropic-beta": "token-efficient-tools-2025-02-19",
                    # "anthropic-beta": "output-128k-2025-02-19",
                    # }
                    # },
                )
                logger.info(f"model ChatAnthropic {final_model_name}")

    else:
        raise ValueError(f"Unknown model name prefix: {final_model_name}")

    return model


# model = ChatOpenAI(model="gpt-4o", temperature=0)
# model = ChatGoogleGenerativeAI(model="gemini-2.0-pro-exp-02-05", temperature=0)
# model = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0)


# For this tutorial we will use custom tool that returns pre-defined values for weather in two cities (NYC & SF)


# Removed scrape and compare_date_time tools - now provided by MCP server


# Removed chat_with_pdf tool - now provided by MCP server


# Removed generate_image tool - now provided by MCP server


# Removed chat_with_imgs tool - now provided by MCP server


# Removed generate_tmp_public_url tool - now provided by MCP server


def format_dates(dt):
    """
    å°‡æ—¥æœŸæ™‚é–“æ ¼å¼åŒ–ç‚ºè¥¿å…ƒå’Œæ°‘åœ‹æ ¼å¼
    è¥¿å…ƒæ ¼å¼ï¼šyyyy-mm-dd hh:mm:ss
    æ°‘åœ‹æ ¼å¼ï¼š(yyyy-1911)-mm-dd hh:mm:ss
    """
    western_date = dt.strftime("%Y-%m-%d %H:%M:%S")
    taiwan_year = dt.year - 1911
    taiwan_date = f"{taiwan_year}-{dt.strftime('%m-%d %H:%M:%S')}"

    return {"western_date": western_date, "taiwan_date": taiwan_date}


# Removed create_html_page tool - now provided by MCP server


# DICT_VAR = {}

# Define the graph

# Removed language-specific system prompts - now using user-provided system_prompt directly


def transform_anthropic_incompatible_schema(
    schema_dict: dict,
) -> tuple[dict, bool, str]:
    """
    è½‰æ›å¯èƒ½èˆ‡ Anthropic ä¸ç›¸å®¹çš„é ‚å±¤ schema çµæ§‹ã€‚

    Args:
        schema_dict: åŸå§‹ schema å­—å…¸ã€‚

    Returns:
        tuple: (è½‰æ›å¾Œçš„ schema å­—å…¸, æ˜¯å¦é€²è¡Œäº†è½‰æ›, é™„åŠ åˆ° description çš„æç¤ºä¿¡æ¯)
    """
    if not isinstance(schema_dict, dict):
        return schema_dict, False, ""

    keys_to_check = ["anyOf", "allOf", "oneOf"]
    problematic_key = None
    for key in keys_to_check:
        if key in schema_dict:
            problematic_key = key
            break

    if problematic_key:
        print(f"  ç™¼ç¾é ‚å±¤ '{problematic_key}'ï¼Œé€²è¡Œè½‰æ›...")
        transformed = True
        new_schema = {"type": "object", "properties": {}, "required": []}
        description_notes = f"\n[é–‹ç™¼è€…è¨»è¨˜ï¼šæ­¤å·¥å…·åƒæ•¸åŸä½¿ç”¨ '{problematic_key}' çµæ§‹ï¼Œå·²è½‰æ›ã€‚è«‹ä¾è³´åƒæ•¸æè¿°åˆ¤æ–·å¿…è¦è¼¸å…¥ã€‚]"

        # 1. åˆä½µ Properties
        # å…ˆåŠ å…¥é ‚å±¤çš„ properties (å¦‚æœå­˜åœ¨)
        if "properties" in schema_dict:
            new_schema["properties"].update(copy.deepcopy(schema_dict["properties"]))
        # å†åˆä½µä¾†è‡ª problematic_key å…§éƒ¨çš„ properties
        for sub_schema in schema_dict.get(problematic_key, []):
            if isinstance(sub_schema, dict) and "properties" in sub_schema:
                # æ³¨æ„ï¼šå¦‚æœä¸åŒ sub_schema æœ‰åŒå propertyï¼Œå¾Œè€…æœƒè¦†è“‹å‰è€…
                new_schema["properties"].update(copy.deepcopy(sub_schema["properties"]))

        # 2. è™•ç† Required
        top_level_required = set(schema_dict.get("required", []))

        if problematic_key == "allOf":
            # allOf: åˆä½µæ‰€æœ‰ required
            combined_required = top_level_required
            for sub_schema in schema_dict.get(problematic_key, []):
                if isinstance(sub_schema, dict) and "required" in sub_schema:
                    combined_required.update(sub_schema["required"])
            # åªä¿ç•™å¯¦éš›å­˜åœ¨æ–¼åˆä½µå¾Œ properties ä¸­çš„ required æ¬„ä½
            new_schema["required"] = sorted(
                [req for req in combined_required if req in new_schema["properties"]]
            )
            description_notes += " æ‰€æœ‰ç›¸é—œåƒæ•¸å‡éœ€è€ƒæ…®ã€‚]"  # ç°¡å–®æç¤º
        elif problematic_key in ["anyOf", "oneOf"]:
            # anyOf/oneOf: åªä¿ç•™é ‚å±¤ requiredï¼Œä¸¦åœ¨æè¿°ä¸­èªªæ˜é¸æ“‡æ€§
            new_schema["required"] = sorted(
                [req for req in top_level_required if req in new_schema["properties"]]
            )
            # å˜—è©¦ç”Ÿæˆæ›´å…·é«”çš„æç¤º (å¦‚æœ sub_schema çµæ§‹ç°¡å–®)
            options = []
            for sub_schema in schema_dict.get(problematic_key, []):
                if isinstance(sub_schema, dict) and "required" in sub_schema:
                    options.append(f"æä¾› '{', '.join(sub_schema['required'])}'")
            if options:
                description_notes += (
                    f" é€šå¸¸éœ€è¦æ»¿è¶³ä»¥ä¸‹æ¢ä»¶ä¹‹ä¸€ï¼š{'; æˆ– '.join(options)}ã€‚]"
                )
            else:
                description_notes += " è«‹æ³¨æ„åƒæ•¸é–“çš„é¸æ“‡é—œä¿‚ã€‚]"

        print(
            f"  è½‰æ›å¾Œ schema: {json.dumps(new_schema, indent=2, ensure_ascii=False)}"
        )
        return new_schema, transformed, description_notes
    else:
        return schema_dict, False, ""


# --- Schema è½‰æ›è¼”åŠ©å‡½æ•¸ (å¾ _get_mcp_tools_async æå–) ---
def _process_mcp_tools_for_anthropic(langchain_tools: List[Any]) -> List[Any]:
    """è™•ç† MCP å·¥å…·åˆ—è¡¨ï¼Œè½‰æ›ä¸ç›¸å®¹çš„ Schema ä¸¦è¨˜éŒ„æ—¥èªŒ"""
    if not langchain_tools:
        logger.info("[_process_mcp_tools_for_anthropic] è­¦å‘Š - æœªæ‰¾åˆ°ä»»ä½•å·¥å…·ã€‚")
        return []

    logger.info(
        f"[_process_mcp_tools_for_anthropic] --- é–‹å§‹è™•ç† {len(langchain_tools)} å€‹åŸå§‹ MCP å·¥å…· ---"
    )

    processed_tools = []
    for mcp_tool in langchain_tools:
        # åªè™•ç† StructuredTool æˆ–é¡ä¼¼çš„æœ‰ args_schema çš„å·¥å…·
        if not hasattr(mcp_tool, "args_schema") or not mcp_tool.args_schema:
            logger.debug(
                f"[_process_mcp_tools_for_anthropic] å·¥å…· '{mcp_tool.name}' æ²’æœ‰ args_schemaï¼Œç›´æ¥åŠ å…¥ã€‚"
            )
            processed_tools.append(mcp_tool)
            continue

        original_schema_dict = {}
        try:
            # å˜—è©¦ç²å– schema å­—å…¸ (æ ¹æ“š Pydantic ç‰ˆæœ¬å¯èƒ½ä¸åŒ)
            if hasattr(mcp_tool.args_schema, "model_json_schema"):  # Pydantic V2
                original_schema_dict = mcp_tool.args_schema.model_json_schema()
            elif hasattr(mcp_tool.args_schema, "schema"):  # Pydantic V1
                original_schema_dict = mcp_tool.args_schema.schema()
            elif isinstance(mcp_tool.args_schema, dict):  # å·²ç¶“æ˜¯å­—å…¸ï¼Ÿ
                original_schema_dict = mcp_tool.args_schema
            else:
                logger.warning(
                    f"[_process_mcp_tools_for_anthropic] ç„¡æ³•ç²å–å·¥å…· '{mcp_tool.name}' çš„ schema å­—å…¸ ({type(mcp_tool.args_schema)})ï¼Œè·³éè½‰æ›ã€‚"
                )
                processed_tools.append(mcp_tool)
                continue

            # é€²è¡Œè½‰æ›æª¢æŸ¥
            logger.debug(
                f"[_process_mcp_tools_for_anthropic] æª¢æŸ¥å·¥å…· '{mcp_tool.name}' çš„ schema..."
            )
            new_schema_dict, transformed, desc_notes = (
                transform_anthropic_incompatible_schema(
                    copy.deepcopy(original_schema_dict)  # ä½¿ç”¨æ·±æ‹·è²æ“ä½œ
                )
            )

            if transformed:
                mcp_tool.description += desc_notes
                logger.info(
                    f"[_process_mcp_tools_for_anthropic] å·¥å…· '{mcp_tool.name}' çš„æè¿°å·²æ›´æ–°ã€‚"
                )
                if isinstance(mcp_tool.args_schema, dict):
                    logger.debug(
                        f"[_process_mcp_tools_for_anthropic] args_schema æ˜¯å­—å…¸ï¼Œç›´æ¥æ›¿æ›å·¥å…· '{mcp_tool.name}' çš„ schemaã€‚"
                    )
                    mcp_tool.args_schema = new_schema_dict
                else:
                    # å¦‚æœ args_schema æ˜¯ Pydantic æ¨¡å‹ï¼Œç›´æ¥ä¿®æ”¹å¯èƒ½ç„¡æ•ˆæˆ–å›°é›£
                    # é™„åŠ è½‰æ›å¾Œçš„å­—å…¸å¯èƒ½æ˜¯ä¸€ç¨®å‚™é¸æ–¹æ¡ˆï¼Œä½† Langchain/LangGraph å¯èƒ½ä¸ç›´æ¥ä½¿ç”¨å®ƒ
                    # æœ€å¥½çš„æ–¹æ³•æ˜¯ç¢ºä¿ get_tools è¿”å›çš„å·¥å…·çš„ args_schema å¯ä»¥è¢«ä¿®æ”¹ï¼Œ
                    # æˆ–è€…åœ¨å‰µå»ºå·¥å…·æ™‚å°±ä½¿ç”¨è½‰æ›å¾Œçš„ schemaã€‚
                    # å¦‚æœä¸èƒ½ç›´æ¥ä¿®æ”¹ï¼Œé™„åŠ å±¬æ€§æ˜¯ä¸€ç¨®æ¨™è¨˜æ–¹å¼ï¼Œä½†å¯èƒ½éœ€è¦åœ¨å·¥å…·èª¿ç”¨è™•è™•ç†ã€‚
                    logger.warning(
                        f"[_process_mcp_tools_for_anthropic] args_schema ä¸æ˜¯å­—å…¸ ({type(mcp_tool.args_schema)})ï¼Œåƒ…æ·»åŠ  _transformed_args_schema_dict å±¬æ€§åˆ°å·¥å…· '{mcp_tool.name}'ã€‚é€™å¯èƒ½ä¸è¶³ä»¥è§£æ±ºæ ¹æœ¬å•é¡Œã€‚"
                    )
                    setattr(mcp_tool, "_transformed_args_schema_dict", new_schema_dict)
            processed_tools.append(mcp_tool)

        except Exception as e_schema:
            logger.error(
                f"[_process_mcp_tools_for_anthropic] è™•ç†å·¥å…· '{mcp_tool.name}' schema æ™‚ç™¼ç”ŸéŒ¯èª¤: {e_schema}",
                exc_info=True,
            )
            processed_tools.append(mcp_tool)  # ä¿ç•™åŸå§‹å·¥å…·

    logger.info(
        f"[_process_mcp_tools_for_anthropic] --- å®Œæˆå·¥å…·è™•ç†ï¼Œè¿”å› {len(processed_tools)} å€‹å·¥å…· ---"
    )
    return processed_tools


async def create_react_agent_graph(
    system_prompt: str = "",
    botrun_flow_lang_url: str = "",
    user_id: str = "",
    model_name: str = "",
    lang: str = LANG_EN,
    mcp_config: Optional[Dict[str, Any]] = None,  # <--- æ¥æ”¶é…ç½®è€Œéå®¢æˆ¶ç«¯å¯¦ä¾‹
):
    """
    Create a react agent graph with simplified architecture.

    This function now creates a fully MCP-integrated agent with:
    - Direct system prompt usage (no language-specific prompt concatenation)
    - Zero local tools - all functionality provided by MCP server
    - Complete MCP server integration for all tools (web search, scraping, PDF/image analysis, time/date, visualizations, etc.)
    - Removed all complex conditional logic and local tool definitions

    Args:
        system_prompt: The system prompt to use for the agent (used directly, no concatenation)
        botrun_flow_lang_url: URL for botrun flow lang service (reserved for future use)
        user_id: User identifier (reserved for future use)
        model_name: AI model name to use (defaults to claude-sonnet-4-5-20250929)
        lang: Language code affecting language-specific tools (e.g., "en", "zh-TW")
        mcp_config: MCP servers configuration dict providing tools like scrape, chat_with_pdf, etc.

    Returns:
        A LangGraph react agent configured with simplified architecture

    Note:
        - Local MCP tools (scrape, chat_with_pdf, etc.) have been removed
        - compare_date_time tool has been completely removed
        - All advanced tools are now provided via MCP server configuration
        - Language-specific prompts have been removed for simplification
    """

    # Complete MCP migration - all tools are now provided by MCP server
    # No local tools remain - all functionality accessed via mcp_config
    tools = [
        # âœ… ALL MIGRATED TO MCP: scrape, chat_with_pdf, chat_with_imgs, generate_image,
        #    generate_tmp_public_url, create_html_page, create_plotly_chart,
        #    create_mermaid_diagram, current_date_time, web_search
        # âŒ REMOVED: compare_date_time (completely eliminated)
    ]

    mcp_tools = []
    if mcp_config:
        logger.info("åµæ¸¬åˆ° MCP é…ç½®ï¼Œç›´æ¥å‰µå»º MCP å·¥å…·...")
        try:
            # ç›´æ¥å‰µå»º MCP client ä¸¦ç²å–å·¥å…·ï¼Œä¸ä½¿ç”¨ context manager

            client = MultiServerMCPClient(mcp_config)
            raw_mcp_tools = await client.get_tools()
            print("raw_mcp_tools============>", raw_mcp_tools)

            if raw_mcp_tools:
                logger.info(f"å¾ MCP é…ç½®ç²å–äº† {len(raw_mcp_tools)} å€‹åŸå§‹å·¥å…·ã€‚")
                # è™•ç† Schema (ä½¿ç”¨æå–çš„è¼”åŠ©å‡½æ•¸)
                mcp_tools = _process_mcp_tools_for_anthropic(raw_mcp_tools)
                if mcp_tools:
                    tools.extend(mcp_tools)
                    logger.info(f"å·²åŠ å…¥ {len(mcp_tools)} å€‹è™•ç†å¾Œçš„ MCP å·¥å…·ã€‚")
                    logger.debug(
                        f"åŠ å…¥çš„ MCP å·¥å…·åç¨±: {[tool.name for tool in mcp_tools]}"
                    )
                else:
                    logger.warning("MCP å·¥å…·è™•ç†å¾Œåˆ—è¡¨ç‚ºç©ºã€‚")
            else:
                logger.info("MCP Client è¿”å›äº†ç©ºçš„å·¥å…·åˆ—è¡¨ã€‚")

            # æ³¨æ„ï¼šæˆ‘å€‘ä¸åœ¨é€™è£¡é—œé–‰ clientï¼Œå› ç‚º tools å¯èƒ½éœ€è¦å®ƒä¾†åŸ·è¡Œ
            # client æœƒåœ¨ graph åŸ·è¡Œå®Œç•¢å¾Œè‡ªå‹•æ¸…ç†
            logger.info("MCP client å’Œå·¥å…·å‰µå»ºå®Œæˆï¼Œclient å°‡ä¿æŒæ´»å‹•ç‹€æ…‹")

        except Exception as e_get:
            import traceback

            traceback.print_exc()
            logger.error(f"å¾ MCP é…ç½®ç²å–æˆ–è™•ç†å·¥å…·æ™‚ç™¼ç”ŸéŒ¯èª¤: {e_get}", exc_info=True)
            # å³ä½¿å‡ºéŒ¯ï¼Œä¹Ÿå¯èƒ½å¸Œæœ›ç¹¼çºŒåŸ·è¡Œï¼ˆä¸å¸¶ MCP å·¥å…·ï¼‰
    else:
        logger.info("æœªæä¾› MCP é…ç½®ï¼Œè·³é MCP å·¥å…·ã€‚")

    # Simplified: use user-provided system_prompt directly (no language-specific prompts)
    new_system_prompt = system_prompt
    if botrun_flow_lang_url and user_id:
        new_system_prompt = (
            f"""IMPORTANT: Any URL returned by tools MUST be included in your response as a markdown link [text](URL).
            Please use the standard [text](URL) format to present links, ensuring the link text remains plain and unformatted.
            Example:
            User: "Create a new page for our project documentation"
            Tool returns: {{"page_url": "https://notion.so/workspace/abc123"}}
            Assistant: "I've created the new page for your project documentation. You can access it here: [Project Documentation](https://notion.so/workspace/abc123)"
            """
            + system_prompt
            + f"""\n\n
            - If the tool needs parameter like botrun_flow_lang_url or user_id, please use the following:
            botrun_flow_lang_url: {botrun_flow_lang_url}
            user_id: {user_id}
            """
        )
    system_message = SystemMessage(
        content=[
            {
                "text": new_system_prompt,
                "type": "text",
                "cache_control": {"type": "ephemeral"},
            }
        ]
    )

    # ç›®å‰å…ˆä½¿ç”¨äº† https://docs.anthropic.com/en/docs/build-with-claude/tool-use/token-efficient-tool-use
    # é€™ä¸€æ®µæœƒé‡åˆ°
    #       File "/Users/seba/Projects/botrun_flow_lang/.venv/lib/python3.11/site-packages/langgraph/prebuilt/tool_node.py", line 218, in __init__
    #     tool_ = create_tool(tool_)
    #             ^^^^^^^^^^^^^^^^^^
    #   File "/Users/seba/Projects/botrun_flow_lang/.venv/lib/python3.11/site-packages/langchain_core/tools/convert.py", line 334, in tool
    #     raise ValueError(msg)
    # ValueError: The first argument must be a string or a callable with a __name__ for tool decorator. Got <class 'dict'>
    # æ‰€ä»¥å…ˆä¸ä½¿ç”¨é€™ä¸€æ®µï¼Œé€™ä¸€æ®µæ˜¯åƒè€ƒ https://python.langchain.com/docs/integrations/chat/anthropic/#tools
    # ä¹Ÿè¨±æœªä¾†å¯ä»¥å¼•ç”¨
    # if get_react_agent_model_name(model_name).startswith("claude-"):
    #     new_tools = []
    #     for tool in tools:
    #         new_tool = convert_to_anthropic_tool(tool)
    #         new_tool["cache_control"] = {"type": "ephemeral"}
    #         new_tools.append(new_tool)
    #     tools = new_tools

    env_name = os.getenv("ENV_NAME", "botrun-flow-lang-dev")
    result = create_react_agent(
        get_react_agent_model(model_name),
        tools=tools,
        prompt=system_message,
        checkpointer=MemorySaver(),  # å¦‚æœè¦åŸ·è¡Œåœ¨ botrun_back è£¡é¢ï¼Œå°±ä¸éœ€è¦ firestore çš„ checkpointer
        # checkpointer=AsyncFirestoreCheckpointer(env_name=env_name),
    )
    return result


# Default graph instance with empty prompt
# if True:
# react_agent_graph = create_react_agent_graph()
# LangGraph Studio æ¸¬è©¦ç”¨ï¼ŒæŠŠä»¥ä¸‹ un-comment å°±å¯ä»¥æ¸¬è©¦
# react_agent_graph = create_react_agent_graph(
#     system_prompt="",
#     botrun_flow_lang_url="https://botrun-flow-lang-fastapi-dev-36186877499.asia-east1.run.app",
#     user_id="sebastian.hsu@gmail.com",
# )
