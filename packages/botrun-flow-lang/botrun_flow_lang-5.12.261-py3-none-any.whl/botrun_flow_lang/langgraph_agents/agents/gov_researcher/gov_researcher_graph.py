import os
import time
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Annotated
from typing_extensions import TypedDict
from pydantic import BaseModel, Field

from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, ToolMessage
from langchain_core.runnables.config import RunnableConfig

from langgraph.graph import StateGraph, START, END
from langgraph.graph import MessagesState
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.base import BaseCheckpointSaver

from botrun_flow_lang.langgraph_agents.agents.util.perplexity_search import (
    respond_with_perplexity_search,
)
from botrun_flow_lang.langgraph_agents.agents.util.tavily_search import (
    respond_with_tavily_search,
)
from botrun_flow_lang.langgraph_agents.agents.util.model_utils import (
    get_model_instance,
)

from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

# ç¯€é»åç¨±å¸¸æ•¸
TOPIC_DECOMPOSITION_NODE = "topic_decomposition_node"
PARALLEL_SEARCH_NODE = "parallel_search_node"
REASONING_ANALYSIS_NODE = "reasoning_analysis_node"
COMPUTATION_VERIFICATION_NODE = "computation_verification_node"
HALLUCINATION_VERIFICATION_NODE = "hallucination_verification_node"
SUMMARY_RESPONSE_NODE = "summary_response_node"

# é è¨­ General Guide
DEFAULT_GENERAL_GUIDE = """
<General Guide>
å¦³å›æ‡‰æ™‚æœƒæ¡ç”¨è‡ºç£ç¹é«”ä¸­æ–‡ï¼Œä¸¦ä¸”é¿å…ä¸­åœ‹å¤§é™¸ç”¨èª
å¦³çµ•å°ä¸æœƒä½¿ç”¨ markdown èªæ³•å›æ‡‰
ä½†æ˜¯ä½ çµ•å°ä¸æœƒä½¿ç”¨ ** æˆ–è€… ### ï¼Œå„ç¨®é¡å‹çš„ markdown èªæ³•éƒ½ç¦æ­¢ä½¿ç”¨
å¦‚æœè¦æ¯”è¼ƒç¾è§€æ’ç‰ˆçš„è©±ï¼Œå¦³å¯ä»¥æ­é…ä½¿ç”¨ emoji or ç´”æ–‡å­— or æ–·è¡Œ or ç©ºç™½ ä¾†å±•ç¤ºä½ æƒ³è¬›çš„
æ¯ä¸€å€‹ step çš„å‰é¢å¢æ·»é©ç•¶æ–·è¡Œ
æ¯å€‹åˆ†æ®µçš„æ¨™é¡Œã€Œå‰é¢ã€è¦å¢æ·»é©ç•¶ emoji ï¼ˆé€™å€‹ emoji æŒ‘é¸å¿…é ˆè·Ÿå‹•æ…‹æƒ…å¢ƒå»åˆï¼‰
</General Guide>
"""

# é¡Œç›®æ‹†è§£æç¤ºè©æ¨¡æ¿
TOPIC_DECOMPOSITION_PROMPT = """
{general_guide}

ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ”¿åºœç ”ç©¶åˆ†æå¸«ï¼Œè² è²¬å°‡ä½¿ç”¨è€…çš„æ”¿åºœç›¸é—œå•é¡Œé€²è¡Œæ™ºèƒ½åˆ†æå’Œæ‹†è§£ã€‚

ä½ çš„ä»»å‹™ï¼š
1. åˆ†æç”¨æˆ¶æå•çš„è¤‡é›œåº¦
2. å¦‚æœæ˜¯å–®ç´”å•é¡Œï¼šè½‰åŒ–ç‚ºæ›´ç´°ç·»çš„å–®ä¸€å­é¡Œç›®
3. å¦‚æœæ˜¯è¤‡é›œå•é¡Œï¼šæ‹†è§£ç‚ºå¤šå€‹å­é¡Œç›®ä»¥ç¢ºä¿å›ç­”çš„æº–ç¢ºæ€§

é‡è¦è€ƒé‡å› ç´ ï¼š
- è€ƒé‡ä½¿ç”¨è€…çš„èº«ä»½ã€å¹´é½¡ã€æ€§åˆ¥ã€å±…ä½åœ°ç­‰å€‹äººæ¢ä»¶
- è€ƒé‡æ™‚é–“æ€§ï¼šæ”¿ç­–ã€æ³•è¦çš„ç”Ÿæ•ˆæ—¥æœŸã€ç”³è«‹æœŸé™ã€è®Šæ›´æ™‚ç¨‹
- è€ƒé‡åœ°åŸŸæ€§ï¼šä¸­å¤® vs åœ°æ–¹æ”¿åºœã€ç¸£å¸‚å·®ç•°ã€å€åŸŸç‰¹æ®Šè¦å®š
- è€ƒé‡é©ç”¨æ€§ï¼šä¸åŒèº«ä»½åˆ¥ã€ä¸åŒæ¢ä»¶ä¸‹çš„å·®ç•°åŒ–è¦å®š

è«‹å°‡ç”¨æˆ¶å•é¡Œæ‹†è§£ç‚º 1-5 å€‹å…·é«”çš„å­é¡Œç›®ï¼Œæ¯å€‹å­é¡Œç›®éƒ½æ‡‰è©²ï¼š
- æ˜ç¢ºä¸”å…·é«”ï¼Œä½†æ¶µè“‹å…¨é¢æ€§è€ƒé‡
- å¯ä»¥é€éæœå°‹æ‰¾åˆ°ç­”æ¡ˆ
- èˆ‡æ”¿åºœæ”¿ç­–ã€æ³•è¦ã€ç¨‹åºç›¸é—œ
- ç›¡é‡åŒ…å«å¤šå€‹æ€è€ƒé¢å‘ï¼šæ™‚æ•ˆæ€§ã€åœ°åŸŸæ€§ã€èº«ä»½å·®ç•°ã€é©ç”¨æ¢ä»¶ç­‰
- æ¯å€‹å­é¡Œç›®éƒ½è¦è¨­è¨ˆå¾—å»£æ³›ä¸”æ·±å…¥ï¼Œä»¥ç²å–è±å¯Œçš„æœå°‹è³‡è¨Š
- ä½¿ç”¨ç¹é«”ä¸­æ–‡è¡¨é”

**é‡è¦æŒ‡å°åŸå‰‡ï¼š**
é›–ç„¶å­é¡Œç›®æ•¸é‡é™åˆ¶åœ¨ 1-5 å€‹ï¼Œä½†æ¯å€‹å­é¡Œç›®éƒ½è¦åšå…¨é¢æ€§çš„è€ƒé‡ï¼Œ
ç›¡é‡æŠŠæ€è€ƒé¢è¨­å®šå»£æ³›ï¼Œé€™æ¨£æœå°‹æ™‚æ‰èƒ½ç²å¾—æ›´å¤šè§’åº¦çš„è³‡è¨Šï¼Œ
æœ€çµ‚ç¸½çµæ™‚å°±æœƒæœ‰æ¯”è¼ƒè±å¯Œçš„è³‡æ–™å¯ä»¥ä½¿ç”¨ã€‚

ç”¨æˆ¶å•é¡Œï¼š{user_question}

è«‹è¼¸å‡ºçµæ§‹åŒ–çš„å­é¡Œç›®åˆ—è¡¨ã€‚
"""

# æ¨ç†åˆ†ææç¤ºè©æ¨¡æ¿
REASONING_ANALYSIS_PROMPT = """
{general_guide}

ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ”¿åºœç ”ç©¶åˆ†æå¸«ï¼Œè² è²¬åŸºæ–¼æœå°‹çµæœå’Œå…§å»ºçŸ¥è­˜é€²è¡Œç¸å¯†æ¨ç†ã€‚

ä½ çš„ä»»å‹™ï¼š
1. åŸºæ–¼æœå°‹çµæœå’Œå…§å»ºçŸ¥è­˜é€²è¡Œæ¨ç†
2. åˆ†æå¸¸è¦‹éŒ¯èª¤å¾Œé€²è¡Œç¸å¯†æ¨ç†
3. é€ä¸€å›ç­”æ‰€æœ‰å­é¡Œç›®
4. ä¿æŒå®¢è§€èˆ‡æº–ç¢º

æœå°‹çµæœï¼š
{search_results}

å­é¡Œç›®ï¼š
{subtopics}

è«‹é‡å°æ¯å€‹å­é¡Œç›®æä¾›è©³ç´°çš„æ¨ç†åˆ†æï¼Œç¢ºä¿ï¼š
- åŸºæ–¼äº‹å¯¦èˆ‡è­‰æ“š
- å¼•ç”¨å…·é«”çš„æœå°‹ä¾†æº
- é¿å…æ¨æ¸¬èˆ‡è‡†æ–·
- æä¾›æ¸…æ™°çš„çµè«–
"""

# å¹»è¦ºé©—è­‰æç¤ºè©æ¨¡æ¿
HALLUCINATION_VERIFICATION_PROMPT = """
{general_guide}

ä½ æ˜¯ä¸€ä½ç¨ç«‹çš„å¯©æŸ¥å°ˆå®¶ï¼Œè² è²¬ä»¥æ‡·ç–‘çš„è§’åº¦æª¢è¦–å‰è¿°æ‰€æœ‰åˆ†æçµæœã€‚

ä½ çš„ä½¿å‘½ï¼š
1. å‡è¨­å‰é¢çµæœæœ‰é«˜æ©Ÿç‡çš„éŒ¯èª¤
2. è­˜åˆ¥å¯èƒ½çš„AIå¹»è¦ºä½ç½®
3. é€éå»¶ä¼¸æœå°‹è­‰æ˜æˆ–åé§å‰é¢çš„çµè«–
4. æä¾›å®¢è§€çš„é©—è­‰å ±å‘Š

å‰é¢çš„åˆ†æçµæœï¼š
{previous_results}

è«‹é€²è¡Œå¹»è¦ºé©—è­‰ï¼Œç‰¹åˆ¥æ³¨æ„ï¼š
- äº‹å¯¦æ€§éŒ¯èª¤
- éåº¦æ¨ç†
- ä¾†æºä¸å¯é 
- æ™‚æ•ˆæ€§å•é¡Œ
- æ³•è¦è®Šæ›´

å¦‚ç™¼ç¾å•é¡Œï¼Œè«‹æä¾›ä¿®æ­£å»ºè­°ã€‚
"""

# åŒ¯ç¸½å›ç­”æç¤ºè©æ¨¡æ¿
SUMMARY_RESPONSE_PROMPT = """
{general_guide}

ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ”¿åºœè³‡è¨Šæœå‹™å°ˆå“¡ï¼Œè² è²¬æä¾›æœ€çµ‚çš„å®Œæ•´å›ç­”ã€‚

**é‡è¦è¦æ±‚ï¼šä½ çš„å›æ‡‰å¿…é ˆå®Œå…¨åŸºæ–¼ä»¥ä¸‹æä¾›çš„è³‡è¨Šï¼Œçµ•å°ä¸èƒ½ä½¿ç”¨ä½ è‡ªå·±çš„çŸ¥è­˜æˆ–é€²è¡Œé¡å¤–æ¨æ¸¬**

ä½ çš„ä»»å‹™ï¼š
1. æä¾›ã€Œç²¾æº–å›ç­”ã€ï¼šç°¡æ½”çš„çµè«–
2. æä¾›ã€Œè©³å¯¦å›ç­”ã€ï¼šå®Œæ•´çš„æ¨ç†éç¨‹å’Œå¼•è­‰  
3. ä½¿ç”¨é©ç•¶çš„ emoji è¼”åŠ©é–±è®€
4. æ ¹æ“šç›®æ¨™å—çœ¾èª¿æ•´èªæ°£å’Œæ ¼å¼
5. **æ‰€æœ‰å›ç­”å…§å®¹å¿…é ˆåš´æ ¼åŸºæ–¼ã€Œæ¨ç†åˆ†æã€å’Œã€Œè¨ˆç®—é©—è­‰ã€çš„çµæœ**

æ‰€æœ‰è™•ç†çµæœï¼š
- åŸå§‹å•é¡Œï¼š{original_question}
- å­é¡Œç›®ï¼š{subtopics}
- æœå°‹çµæœï¼š{search_results}
- æ¨ç†åˆ†æï¼š{reasoning_results}
- è¨ˆç®—é©—è­‰ï¼š{computation_results}

**å›ç­”åŸå‰‡ï¼š**
- åªä½¿ç”¨ã€Œæ¨ç†åˆ†æã€å’Œã€Œè¨ˆç®—é©—è­‰ã€ä¸­æ˜ç¢ºæåˆ°çš„è³‡è¨Š
- å¦‚æœæŸå€‹å•é¡Œåœ¨é€™äº›è³‡è¨Šä¸­æ²’æœ‰å……åˆ†èªªæ˜ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºè³‡è¨Šä¸è¶³
- ä¸è¦æ·»åŠ ä»»ä½•æœªåœ¨ä¸Šè¿°è³‡è¨Šä¸­å‡ºç¾çš„å…§å®¹
- ç¢ºä¿æ‰€æœ‰çµè«–éƒ½æœ‰æ˜ç¢ºçš„ä¾†æºä¾æ“š

è«‹æä¾›çµæ§‹åŒ–çš„æœ€çµ‚å›ç­”ï¼ŒåŒ…å«ï¼š
ğŸ“‹ ç²¾æº–å›ç­”ï¼ˆç°¡æ½”ç‰ˆï¼‰
ğŸ“– è©³å¯¦å›ç­”ï¼ˆå®Œæ•´ç‰ˆï¼‰
ğŸ”— åƒè€ƒè³‡æ–™ä¾†æº
"""


class SubTopic(BaseModel):
    """å­é¡Œç›®çµæ§‹"""

    topic: str
    description: str


class SubTopicList(BaseModel):
    """å­é¡Œç›®åˆ—è¡¨"""

    subtopics: List[SubTopic]


class SearchResult(BaseModel):
    """æœå°‹çµæœçµæ§‹"""

    subtopic: str
    content: str
    sources: List[str]


class ReasoningResult(BaseModel):
    """æ¨ç†çµæœçµæ§‹"""

    subtopic: str
    analysis: str
    conclusion: str
    confidence: float


class VerificationResult(BaseModel):
    """é©—è­‰çµæœçµæ§‹"""

    issues_found: List[str]
    corrections: List[str]
    confidence_adjustments: Dict[str, float]


# LangGraph Assistant é…ç½® Schema
class GovResearcherConfigSchema(BaseModel):
    """æ”¿åºœç ”ç©¶å“¡åŠ©æ‰‹é…ç½® Schema - å¯åœ¨ LangGraph UI ä¸­è¨­å®š"""

    # æ¨¡å‹é¸æ“‡
    decomposition_model: str = Field(default="gemini-2.5-pro")  # é¡Œç›®æ‹†è§£æ¨¡å‹
    reasoning_model: str  # æ¨ç†åˆ†ææ¨¡å‹
    computation_model: str  # è¨ˆç®—é©—è­‰æ¨¡å‹
    verification_model: str  # å¹»è¦ºé©—è­‰æ¨¡å‹
    summary_model: str  # åŒ¯ç¸½å›ç­”æ¨¡å‹

    # æœå°‹å¼•æ“è¨­å®š
    search_vendor: str  # "perplexity" | "tavily"
    search_model: str  # æœå°‹æ¨¡å‹åç¨±
    max_parallel_searches: int  # æœ€å¤§ä¸¦è¡Œæœå°‹æ•¸é‡

    # æç¤ºè©æ¨¡æ¿ï¼ˆå¯å‹•æ…‹è¨­å®šï¼‰
    general_guide: Optional[str]  # é€šç”¨æŒ‡å°åŸå‰‡
    topic_decomposition_prompt: Optional[str]  # é¡Œç›®æ‹†è§£æç¤ºè©
    reasoning_analysis_prompt: Optional[str]  # æ¨ç†åˆ†ææç¤ºè©
    hallucination_verification_prompt: Optional[str]  # å¹»è¦ºé©—è­‰æç¤ºè©
    summary_response_prompt: Optional[str]  # åŒ¯ç¸½å›ç­”æç¤ºè©


class GovResearcherState(MessagesState):
    """æ”¿åºœç ”ç©¶å“¡ LangGraph ç‹€æ…‹"""

    original_question: str = ""
    decomposed_topics: List[SubTopic] = []
    search_tasks: List[SubTopic] = []
    search_results: Annotated[List[SearchResult], lambda x, y: x + y] = (
        []
    )  # æ”¯æ´ fan-in åˆä½µ
    reasoning_results: List[ReasoningResult] = []
    computation_results: Optional[str] = None
    needs_computation: bool = False
    hallucination_check: Optional[VerificationResult] = None
    final_answer: str = ""
    general_guide: str = DEFAULT_GENERAL_GUIDE
    search_completed: bool = False


def format_dates(dt):
    """å°‡æ—¥æœŸæ™‚é–“æ ¼å¼åŒ–ç‚ºè¥¿å…ƒå’Œæ°‘åœ‹æ ¼å¼"""
    western_date = dt.strftime("%Y-%m-%d %H:%M:%S")
    taiwan_year = dt.year - 1911
    taiwan_date = f"{taiwan_year}-{dt.strftime('%m-%d %H:%M:%S')}"
    return {"western_date": western_date, "taiwan_date": taiwan_date}


def get_config_value(config: RunnableConfig, key: str, default_value: Any) -> Any:
    """çµ±ä¸€ç²å–é…ç½®å€¼çš„è¼”åŠ©å‡½æ•¸"""
    # å¦‚æœ config.get("configurable", {}).get(key, default_value) æ˜¯ Noneï¼Œå‰‡è¿”å› default_value
    return config.get("configurable", {}).get(key, default_value) or default_value


def get_decomposition_model(config: RunnableConfig):
    """ç²å–é¡Œç›®æ‹†è§£ç”¨çš„æ¨¡å‹"""
    model_name = get_config_value(config, "decomposition_model", "gemini-2.5-pro")
    return get_model_instance(model_name, temperature=0)


def get_reasoning_model(config: RunnableConfig):
    """ç²å–æ¨ç†åˆ†æç”¨çš„æ¨¡å‹"""
    model_name = get_config_value(config, "reasoning_model", "gemini-2.5-flash")
    return get_model_instance(model_name, temperature=0)


def get_computation_model(config: RunnableConfig):
    """ç²å–è¨ˆç®—é©—è­‰ç”¨çš„æ¨¡å‹"""
    model_name = get_config_value(config, "computation_model", "gemini-2.5-flash")
    return get_model_instance(model_name, temperature=0, enable_code_execution=True)


def get_verification_model(config: RunnableConfig):
    """ç²å–å¹»è¦ºé©—è­‰ç”¨çš„æ¨¡å‹"""
    model_name = get_config_value(config, "verification_model", "gemini-2.5-flash")
    return get_model_instance(model_name, temperature=0)


def get_summary_model(config: RunnableConfig):
    """ç²å–åŒ¯ç¸½å›ç­”ç”¨çš„æ¨¡å‹"""
    model_name = get_config_value(config, "summary_model", "gemini-2.5-flash")
    return get_model_instance(model_name, temperature=0)


def get_prompt_template(
    config: RunnableConfig, prompt_key: str, default_prompt: str
) -> str:
    """ç²å–å¯é…ç½®çš„æç¤ºè©æ¨¡æ¿"""
    return get_config_value(config, prompt_key, default_prompt)


def topic_decomposition_node(
    state: GovResearcherState, config: RunnableConfig
) -> Dict[str, Any]:
    """Step-001: é¡Œç›®æ‹†è§£ç¯€é»"""
    logging.info("[GovResearcherGraph:topic_decomposition_node] é–‹å§‹é¡Œç›®æ‹†è§£")

    # ç²å–ç”¨æˆ¶æœ€æ–°å•é¡Œ
    user_question = ""
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            user_question = msg.content
            break

    if not user_question:
        logging.warning("æœªæ‰¾åˆ°ç”¨æˆ¶å•é¡Œ")
        return {"decomposed_topics": []}

    # ç²å–å¯é…ç½®çš„æç¤ºè©æ¨¡æ¿
    prompt_template = get_prompt_template(
        config, "topic_decomposition_prompt", TOPIC_DECOMPOSITION_PROMPT
    )
    general_guide = get_config_value(config, "general_guide", DEFAULT_GENERAL_GUIDE)

    # æº–å‚™æç¤ºè©
    prompt = prompt_template.format(
        general_guide=general_guide,
        user_question=user_question,
    )

    # èª¿ç”¨æ¨¡å‹
    model = get_decomposition_model(config)
    from trustcall import create_extractor

    extractor = create_extractor(
        model, tools=[SubTopicList], tool_choice="SubTopicList"
    )

    response = extractor.invoke([HumanMessage(content=prompt)])

    # è§£æçµæœ - çµ±ä¸€è™•ç† trustcall çš„å›æ‡‰æ ¼å¼
    subtopics = []

    try:
        # ç›´æ¥æ˜¯ SubTopicList å¯¦ä¾‹
        if isinstance(response, SubTopicList):
            subtopics = response.subtopics
        # æœ‰ subtopics å±¬æ€§
        elif hasattr(response, "subtopics"):
            subtopics = response.subtopics
        # trustcall å­—å…¸æ ¼å¼ï¼ˆä¸»è¦æƒ…æ³ï¼‰
        elif isinstance(response, dict):
            if "responses" in response and response["responses"]:
                first_response = response["responses"][0]
                if hasattr(first_response, "subtopics"):
                    subtopics = first_response.subtopics
            elif "subtopics" in response:
                subtopics_data = response["subtopics"]
                subtopics = [
                    SubTopic(**item) if isinstance(item, dict) else item
                    for item in subtopics_data
                ]

        logging.info(f"æˆåŠŸè§£æ {len(subtopics)} å€‹å­é¡Œç›®")

    except Exception as e:
        logging.error(f"è§£æ trustcall å›æ‡‰å¤±æ•—: {e}")
        subtopics = []

    # å‚™é¸æ–¹æ¡ˆï¼šä½¿ç”¨åŸå§‹å•é¡Œ
    if not subtopics:
        logging.warning("æœªèƒ½è§£æå‡ºå­é¡Œç›®ï¼Œä½¿ç”¨åŸå§‹å•é¡Œ")
        subtopics = [SubTopic(topic=user_question, description="åŸå§‹å•é¡Œ")]

    logging.info(f"é¡Œç›®æ‹†è§£å®Œæˆï¼Œå…± {len(subtopics)} å€‹å­é¡Œç›®")

    # é¡å¤–çš„èª¿è©¦è³‡è¨Š
    for i, subtopic in enumerate(subtopics):
        logging.info(f"å­é¡Œç›® {i+1}: {subtopic.topic[:50]}...")  # åªé¡¯ç¤ºå‰50å­—å…ƒ

    return {"original_question": user_question, "decomposed_topics": subtopics}


def search_preparation_node(
    state: GovResearcherState, config: RunnableConfig
) -> Dict[str, Any]:
    """æœå°‹æº–å‚™ç¯€é»ï¼šæº–å‚™ä¸¦åˆ†ç™¼æœå°‹ä»»å‹™"""
    logging.info("[GovResearcherGraph:search_preparation_node] æº–å‚™æœå°‹ä»»å‹™")

    subtopics = state.get("decomposed_topics", [])
    if not subtopics:
        logging.warning("ç„¡å­é¡Œç›®å¯æœå°‹")
        return {"search_tasks": []}

    # é™åˆ¶ä¸¦è¡Œæœå°‹æ•¸é‡
    max_parallel_searches = get_config_value(config, "max_parallel_searches", 5)
    limited_subtopics = subtopics[:max_parallel_searches]

    logging.info(f"æº–å‚™å¹³è¡Œæœå°‹ {len(limited_subtopics)} å€‹å­é¡Œç›®")

    return {"search_tasks": limited_subtopics, "search_completed": False}


async def search_subtopic_node(
    state: GovResearcherState, config: RunnableConfig
) -> Dict[str, Any]:
    """æœå°‹æ‰€æœ‰å­é¡Œç›®ï¼ˆæ”¯æ´å¤šæœå°‹å¼•æ“ï¼Œä¸ä½¿ç”¨LLMï¼‰"""
    logging.info("[GovResearcherGraph:search_subtopic_node] é–‹å§‹æœå°‹æ‰€æœ‰å­é¡Œç›®")

    search_tasks = state.get("search_tasks", [])
    if not search_tasks:
        logging.warning("ç„¡æœå°‹ä»»å‹™")
        return {"search_results": []}

    # ç²å–æœå°‹å¼•æ“é…ç½®
    search_vendor = get_config_value(config, "search_vendor", "tavily")
    search_model = get_config_value(config, "search_model", "sonar")
    domain_filter = get_config_value(config, "domain_filter", [])

    logging.info(f"ä½¿ç”¨æœå°‹æœå‹™å•†: {search_vendor}, æ¨¡å‹: {search_model}")

    # ä½¿ç”¨ asyncio.gather é€²è¡ŒçœŸæ­£çš„å¹³è¡Œæœå°‹ï¼ˆPRDè¦æ±‚ï¼šä¸ä½¿ç”¨LLMï¼Œåƒ…æœå°‹APIï¼‰
    async def search_single_topic(subtopic: SubTopic) -> SearchResult:
        try:
            content = ""
            sources = []
            search_query = subtopic.topic

            # æ ¹æ“šæœå°‹æœå‹™å•†é¸æ“‡ä¸åŒçš„æœå°‹æœå‹™
            if search_vendor == "tavily":
                async for event in respond_with_tavily_search(
                    search_query,
                    "",  # ç„¡å‰ç¶´
                    [{"role": "user", "content": search_query}],  # æœ€ç›´æ¥çš„æŸ¥è©¢
                    domain_filter,
                    False,  # ä¸stream
                    search_model,
                ):
                    content += event.chunk
                    if event.raw_json and "sources" in event.raw_json:
                        sources = event.raw_json["sources"]
                    else:
                        sources = ["Tavily Search"]

            else:  # é è¨­ä½¿ç”¨ perplexity
                async for event in respond_with_perplexity_search(
                    search_query,
                    "",  # ç„¡å‰ç¶´
                    [{"role": "user", "content": search_query}],  # æœ€ç›´æ¥çš„æŸ¥è©¢
                    domain_filter,
                    False,  # ä¸stream
                    search_model,
                ):
                    content += event.chunk
                    sources = ["Perplexity Search"]

            return SearchResult(
                subtopic=subtopic.topic, content=content, sources=sources
            )

        except Exception as e:
            logging.error(f"æœå°‹ '{subtopic.topic}' å¤±æ•—: {e}")
            return SearchResult(
                subtopic=subtopic.topic, content=f"æœå°‹å¤±æ•—: {str(e)}", sources=[]
            )

    # å¹³è¡ŒåŸ·è¡Œæ‰€æœ‰æœå°‹
    search_results = await asyncio.gather(
        *[search_single_topic(subtopic) for subtopic in search_tasks]
    )

    logging.info(f"æœå°‹å®Œæˆï¼Œå…± {len(search_results)} å€‹çµæœ")

    return {"search_results": search_results, "search_completed": True}


def reasoning_analysis_node(
    state: GovResearcherState, config: RunnableConfig
) -> Dict[str, Any]:
    """Step-003: æ¨ç†åˆ†æç¯€é»"""
    logging.info("[GovResearcherGraph:reasoning_analysis_node] é–‹å§‹æ¨ç†åˆ†æ")

    search_results = state.get("search_results", [])
    subtopics = state.get("decomposed_topics", [])

    if not search_results or not subtopics:
        logging.warning("ç¼ºå°‘æœå°‹çµæœæˆ–å­é¡Œç›®")
        return {"reasoning_results": []}

    # æº–å‚™æœå°‹çµæœæ–‡æœ¬
    search_text = "\n\n".join(
        [
            f"å­é¡Œç›®: {result.subtopic}\nå…§å®¹: {result.content}\nä¾†æº: {', '.join(result.sources)}"
            for result in search_results
        ]
    )

    subtopics_text = "\n".join(
        [
            f"{i+1}. {topic.topic} - {topic.description}"
            for i, topic in enumerate(subtopics)
        ]
    )

    # ç²å–å¯é…ç½®çš„æç¤ºè©æ¨¡æ¿
    prompt_template = get_prompt_template(
        config, "reasoning_analysis_prompt", REASONING_ANALYSIS_PROMPT
    )
    general_guide = get_config_value(config, "general_guide", DEFAULT_GENERAL_GUIDE)

    # æº–å‚™æç¤ºè©
    prompt = prompt_template.format(
        general_guide=general_guide,
        search_results=search_text,
        subtopics=subtopics_text,
    )

    # èª¿ç”¨æ¨¡å‹
    model = get_reasoning_model(config)
    response = model.invoke([HumanMessage(content=prompt)])

    # ç°¡åŒ–ç‰ˆçµæœè§£æ
    reasoning_results = []
    for i, subtopic in enumerate(subtopics):
        reasoning_results.append(
            ReasoningResult(
                subtopic=subtopic.topic,
                analysis=response.content,  # å¯¦éš›æ‡‰è©²åˆ†æ®µè§£æ
                conclusion=f"é‡å° '{subtopic.topic}' çš„åˆ†æçµè«–",
                confidence=0.8,
            )
        )

    # æª¢æŸ¥æ˜¯å¦éœ€è¦è¨ˆç®—é©—è­‰
    needs_computation = (
        "è¨ˆç®—" in response.content
        or "é‡‘é¡" in response.content
        or "æ•¸é‡" in response.content
    )

    logging.info(f"æ¨ç†åˆ†æå®Œæˆï¼Œéœ€è¦è¨ˆç®—é©—è­‰: {needs_computation}")

    return {
        "reasoning_results": reasoning_results,
        "needs_computation": needs_computation,
    }


def computation_verification_node(
    state: GovResearcherState, config: RunnableConfig
) -> Dict[str, Any]:
    """Step-004: è¨ˆç®—é©—è­‰ç¯€é»ï¼ˆæ¢ä»¶æ€§ï¼‰"""
    logging.info("[GovResearcherGraph:computation_verification_node] é–‹å§‹è¨ˆç®—é©—è­‰")

    if not state.get("needs_computation", False):
        logging.info("ç„¡éœ€è¨ˆç®—é©—è­‰ï¼Œè·³é")
        return {"computation_results": None}

    reasoning_results = state.get("reasoning_results", [])

    # æº–å‚™è¨ˆç®—é©—è­‰æç¤ºè©
    reasoning_text = "\n\n".join(
        [
            f"å­é¡Œç›®: {result.subtopic}\nåˆ†æ: {result.analysis}\nçµè«–: {result.conclusion}"
            for result in reasoning_results
        ]
    )

    # ç²å–å¯é…ç½®çš„é€šç”¨æŒ‡å°åŸå‰‡
    general_guide = get_config_value(config, "general_guide", DEFAULT_GENERAL_GUIDE)

    prompt = f"""
    {general_guide}
    
    ä½ æ˜¯å°ˆæ¥­çš„è¨ˆç®—é©—è­‰å°ˆå®¶ï¼Œè«‹é‡å°ä»¥ä¸‹æ¨ç†çµæœä¸­çš„è¨ˆç®—éƒ¨åˆ†é€²è¡Œç¨ç«‹é©—ç®—ï¼š
    
    {reasoning_text}
    
    è«‹ä½¿ç”¨ç¨‹å¼ç¢¼åŸ·è¡ŒåŠŸèƒ½é©—è­‰ä»»ä½•æ¶‰åŠæ•¸å­—è¨ˆç®—çš„éƒ¨åˆ†ï¼Œä¸¦æä¾›é©—è­‰çµæœã€‚
    """

    # ä½¿ç”¨æ”¯æ´ä»£ç¢¼åŸ·è¡Œçš„æ¨¡å‹
    model = get_computation_model(config)
    response = model.invoke([HumanMessage(content=prompt)])

    logging.info("è¨ˆç®—é©—è­‰å®Œæˆ")

    return {"computation_results": response.content}


async def hallucination_verification_node(
    state: GovResearcherState, config: RunnableConfig
) -> Dict[str, Any]:
    """Step-005: å¹»è¦ºé©—è­‰ç¯€é»ï¼ˆæ”¯æ´æœå°‹å¼•æ“é¸æ“‡ï¼‰"""
    logging.info("[GovResearcherGraph:hallucination_verification_node] é–‹å§‹å¹»è¦ºé©—è­‰")

    # æ”¶é›†å‰é¢æ‰€æœ‰çµæœ
    previous_results = {
        "åŸå§‹å•é¡Œ": state.get("original_question", ""),
        "å­é¡Œç›®": [topic.topic for topic in state.get("decomposed_topics", [])],
        "æœå°‹çµæœ": [result.content for result in state.get("search_results", [])],
        "æ¨ç†çµæœ": [result.analysis for result in state.get("reasoning_results", [])],
        "è¨ˆç®—çµæœ": state.get("computation_results", "ç„¡"),
    }

    results_text = "\n\n".join(
        [f"{key}: {value}" for key, value in previous_results.items()]
    )

    # ç²å–å¯é…ç½®çš„æç¤ºè©æ¨¡æ¿
    prompt_template = get_prompt_template(
        config, "hallucination_verification_prompt", HALLUCINATION_VERIFICATION_PROMPT
    )
    general_guide = get_config_value(config, "general_guide", DEFAULT_GENERAL_GUIDE)

    # æº–å‚™é©—è­‰æç¤ºè©
    prompt = prompt_template.format(
        general_guide=general_guide,
        previous_results=results_text,
    )

    # èª¿ç”¨æ¨¡å‹
    model = get_verification_model(config)
    response = model.invoke([HumanMessage(content=prompt)])

    # å¦‚æœç™¼ç¾å•é¡Œï¼Œé€²è¡Œé¡å¤–æœå°‹é©—è­‰ï¼ˆPRDè¦æ±‚ï¼šé€éå»¶ä¼¸æœå°‹è­‰æ˜æˆ–åé§å‰é¢çš„çµè«–ï¼‰
    if "å•é¡Œ" in response.content or "éŒ¯èª¤" in response.content:
        logging.info("ç™¼ç¾æ½›åœ¨å•é¡Œï¼Œé€²è¡Œé¡å¤–æœå°‹é©—è­‰")

        # ç²å–æœå°‹å¼•æ“é…ç½®ï¼ˆPRDè¦æ±‚ï¼šæœå°‹å¼•æ“é¸æ“‡ï¼‰
        search_vendor = get_config_value(
            config,
            "verification_search_vendor",
            get_config_value(config, "search_vendor", "perplexity"),
        )
        search_model = get_config_value(config, "search_model", "sonar")
        domain_filter = get_config_value(config, "domain_filter", [])

        # æå–éœ€è¦é©—è­‰çš„é—œéµå•é¡Œ
        verification_query = (
            f"é©—è­‰ä»¥ä¸‹æ”¿åºœè³‡è¨Šçš„æº–ç¢ºæ€§ï¼š{state.get('original_question', '')}"
        )

        try:
            verification_content = ""

            # æ ¹æ“šæœå°‹æœå‹™å•†é€²è¡Œé©—è­‰æœå°‹
            if search_vendor == "tavily":
                async for event in respond_with_tavily_search(
                    verification_query,
                    "",
                    [{"role": "user", "content": verification_query}],
                    domain_filter,
                    False,
                    search_model,
                ):
                    verification_content += event.chunk
            else:  # perplexity
                async for event in respond_with_perplexity_search(
                    verification_query,
                    "",
                    [{"role": "user", "content": verification_query}],
                    domain_filter,
                    False,
                    search_model,
                ):
                    verification_content += event.chunk

            logging.info(f"å®Œæˆé¡å¤–æœå°‹é©—è­‰ï¼Œä½¿ç”¨æœå‹™å•†: {search_vendor}")

        except Exception as e:
            logging.error(f"é¡å¤–æœå°‹é©—è­‰å¤±æ•—: {e}")
            verification_content = f"é©—è­‰æœå°‹å¤±æ•—: {str(e)}"
    else:
        verification_content = "æœªç™¼ç¾æ˜é¡¯å•é¡Œï¼Œç„¡éœ€é¡å¤–æœå°‹"

    # ç°¡åŒ–ç‰ˆé©—è­‰çµæœ
    verification_result = VerificationResult(
        issues_found=["å¾…å¯¦ä½œï¼šå•é¡Œè­˜åˆ¥"],
        corrections=["å¾…å¯¦ä½œï¼šä¿®æ­£å»ºè­°"],
        confidence_adjustments={},
    )

    logging.info("å¹»è¦ºé©—è­‰å®Œæˆ")

    return {
        "hallucination_check": verification_result,
        "verification_search_results": verification_content,
    }


def summary_response_node(
    state: GovResearcherState, config: RunnableConfig
) -> Dict[str, Any]:
    """Step-006: åŒ¯ç¸½å›ç­”ç¯€é»"""
    logging.info("[GovResearcherGraph:summary_response_node] é–‹å§‹åŒ¯ç¸½å›ç­”")

    # æ”¶é›†æ‰€æœ‰è™•ç†çµæœ
    summary_data = {
        "original_question": state.get("original_question", ""),
        "subtopics": [topic.topic for topic in state.get("decomposed_topics", [])],
        "search_results": "\n".join(
            [result.content for result in state.get("search_results", [])]
        ),
        "reasoning_results": "\n".join(
            [result.analysis for result in state.get("reasoning_results", [])]
        ),
        "computation_results": state.get("computation_results", "ç„¡è¨ˆç®—éœ€æ±‚"),
    }

    # ç²å–å¯é…ç½®çš„æç¤ºè©æ¨¡æ¿
    prompt_template = get_prompt_template(
        config, "summary_response_prompt", SUMMARY_RESPONSE_PROMPT
    )
    general_guide = get_config_value(config, "general_guide", DEFAULT_GENERAL_GUIDE)

    # æº–å‚™åŒ¯ç¸½æç¤ºè©
    prompt = prompt_template.format(general_guide=general_guide, **summary_data)

    # èª¿ç”¨æ¨¡å‹
    model = get_summary_model(config)
    response = model.invoke([HumanMessage(content=prompt)])

    final_answer = response.content

    logging.info("åŒ¯ç¸½å›ç­”å®Œæˆ")

    return {"final_answer": final_answer, "messages": [AIMessage(content=final_answer)]}


def should_compute(state: GovResearcherState) -> str:
    """æ¢ä»¶åˆ†æ”¯ï¼šæ±ºå®šæ˜¯å¦éœ€è¦è¨ˆç®—é©—è­‰"""
    if state.get("needs_computation", False):
        return COMPUTATION_VERIFICATION_NODE
    else:
        return SUMMARY_RESPONSE_NODE


# é è¨­é…ç½®ï¼ˆæ ¹æ“šPRDè¦æ ¼ä¿®æ­£ï¼‰
DEFAULT_GOV_RESEARCHER_CONFIG = {
    "decomposition_model": "gemini-2.5-pro",  # PRD é è¨­
    "reasoning_model": "gemini-2.5-flash",
    "computation_model": "gemini-2.5-flash",
    "verification_model": "gemini-2.5-flash",
    "summary_model": "gemini-2.5-flash",  # PRD é è¨­
    "search_vendor": "perplexity",  # PRD é è¨­
    "max_parallel_searches": 5,
    "domain_filter": [],
    "search_model": "sonar",  # PRD é è¨­ï¼Œé sonar-reasoning-pro
    "general_guide": DEFAULT_GENERAL_GUIDE,
}


def get_content_for_gov_researcher(state: Dict[str, Any]) -> str:
    """å¾ç‹€æ…‹ä¸­å–å¾—å…§å®¹"""
    return state.get("final_answer", "")


class GovResearcherGraph:
    """æ”¿åºœç ”ç©¶å“¡ LangGraph Agent"""

    def __init__(self, memory: BaseCheckpointSaver = None):
        self.memory = memory if memory is not None else MemorySaver()
        self._initialize_graph()

    def _initialize_graph(self):
        """åˆå§‹åŒ– LangGraph å·¥ä½œæµ"""
        workflow = StateGraph(
            GovResearcherState, context_schema=GovResearcherConfigSchema
        )

        # æ·»åŠ ç¯€é»
        workflow.add_node(TOPIC_DECOMPOSITION_NODE, topic_decomposition_node)
        workflow.add_node("search_preparation", search_preparation_node)
        workflow.add_node("search_subtopic", search_subtopic_node)
        workflow.add_node(REASONING_ANALYSIS_NODE, reasoning_analysis_node)
        workflow.add_node(COMPUTATION_VERIFICATION_NODE, computation_verification_node)
        workflow.add_node(
            HALLUCINATION_VERIFICATION_NODE, hallucination_verification_node
        )
        workflow.add_node(SUMMARY_RESPONSE_NODE, summary_response_node)

        # å®šç¾©é‚Šï¼ˆå·¥ä½œæµç¨‹ï¼‰
        workflow.add_edge(START, TOPIC_DECOMPOSITION_NODE)
        workflow.add_edge(TOPIC_DECOMPOSITION_NODE, "search_preparation")
        workflow.add_edge("search_preparation", "search_subtopic")
        workflow.add_edge("search_subtopic", REASONING_ANALYSIS_NODE)

        # æ¢ä»¶åˆ†æ”¯ï¼šæ˜¯å¦éœ€è¦è¨ˆç®—é©—è­‰
        workflow.add_conditional_edges(
            REASONING_ANALYSIS_NODE,
            should_compute,
            [COMPUTATION_VERIFICATION_NODE, SUMMARY_RESPONSE_NODE],
        )

        workflow.add_edge(COMPUTATION_VERIFICATION_NODE, SUMMARY_RESPONSE_NODE)
        workflow.add_edge(SUMMARY_RESPONSE_NODE, END)

        # ç·¨è­¯åœ–
        self._graph = workflow.compile(checkpointer=self.memory)
        self._graph_no_memory = workflow.compile()

    @property
    def graph(self):
        """å¸¶è¨˜æ†¶çš„åœ–"""
        return self._graph

    @property
    def graph_no_memory(self):
        """ä¸å¸¶è¨˜æ†¶çš„åœ–"""
        return self._graph_no_memory


# å°å‡ºå¯¦ä¾‹
gov_researcher_graph = GovResearcherGraph().graph_no_memory
