import os
import time
import glob
import shutil
import multiprocessing
from src.mlog_util import MultiProcessSafeSizeRotatingHandler, get_logger

# --- ä¿®å¤ï¼šå°†å·¥ä½œå‡½æ•°å®šä¹‰ä¸ºé¡¶çº§å‡½æ•° ---

def write_logs_worker(process_id, num_messages=1000):
    """åœ¨æŒ‡å®šè¿›ç¨‹ä¸­å†™å…¥æ—¥å¿—ï¼ˆç”¨äºæµ‹è¯•1ï¼‰"""
    handler = MultiProcessSafeSizeRotatingHandler(
        filename="logs/size.log",
        maxBytes=1 * 1024 * 1024,  # 1MB
        backupCount=5
    )
    logger = get_logger(f"test_process_{process_id}", custom_handlers=handler, add_console=False)
    
    for i in range(num_messages):
        logger.info(f"Process {process_id} - Message {i}: " + "x" * 100)  # æ¯æ¡æ¶ˆæ¯çº¦100å­—èŠ‚

def concurrent_rotation_worker(worker_id):
    """å¹¶å‘è½®è½¬æµ‹è¯•çš„å·¥ä½œè¿›ç¨‹ï¼ˆç”¨äºæµ‹è¯•3ï¼‰"""
    handler = MultiProcessSafeSizeRotatingHandler(
        filename="logs/size.log",
        maxBytes=500 * 1024,  # 500KB
        backupCount=3
    )
    logger = get_logger(f"worker_{worker_id}", custom_handlers=handler, add_console=False)
    
    for i in range(200):
        logger.info(f"Worker {worker_id} - Log {i}: " + "z" * 150)
        time.sleep(0.01)  # æ¨¡æ‹Ÿå®é™…å·¥ä½œè´Ÿè½½

# --- æµ‹è¯•å‡½æ•° ---

def setup_test_environment():
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒï¼Œæ¸…ç†æ—§çš„æ—¥å¿—æ–‡ä»¶"""
    if os.path.exists("logs"):
        shutil.rmtree("logs")
    os.makedirs("logs", exist_ok=True)

def test_large_backup_count():
    """æµ‹è¯•1: å¤§backupCountï¼ŒéªŒè¯è½®è½¬æ˜¯å¦ç”Ÿæ•ˆï¼Œæ—¥å¿—æ˜¯å¦ä¸¢å¤±"""
    print("\n=== æµ‹è¯•1: å¤§backupCountæµ‹è¯• ===")
    setup_test_environment()
    
    # å¯åŠ¨å¤šä¸ªè¿›ç¨‹å†™å…¥æ—¥å¿—
    processes = []
    for i in range(3):
        # ä½¿ç”¨é¡¶çº§å‡½æ•°
        p = multiprocessing.Process(target=write_logs_worker, args=(i, 500))
        processes.append(p)
        p.start()
    
    for p in processes:
        p.join()
    
    # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
    log_files = sorted(glob.glob("logs/size.log*"))
    print(f"ç”Ÿæˆçš„æ—¥å¿—æ–‡ä»¶: {log_files}")
    
    # éªŒè¯æ‰€æœ‰æ–‡ä»¶å†…å®¹å®Œæ•´æ€§
    total_messages = 0
    for log_file in log_files:
        with open(log_file, 'r') as f:
            lines = f.readlines()
            total_messages += len(lines)
    
    print(f"æ€»æ—¥å¿—æ¡æ•°: {total_messages} (é¢„æœŸ: 1500)")
    assert total_messages == 1500, f"æ—¥å¿—æ¡æ•° {total_messages} ä¸ç­‰äºé¢„æœŸ 1500"
    print("âœ“ æµ‹è¯•1é€šè¿‡: å¤šè¿›ç¨‹æ—¥å¿—å®Œæ•´æ€§")

def test_medium_backup_count():
    """æµ‹è¯•2: ä¸­ç­‰backupCountï¼Œè§¦å‘åˆ é™¤æœºåˆ¶"""
    print("\n=== æµ‹è¯•2: ä¸­ç­‰backupCountæµ‹è¯• ===")
    setup_test_environment()
    
    # ä½¿ç”¨è¾ƒå°çš„backupCount
    handler = MultiProcessSafeSizeRotatingHandler(
        filename="logs/size.log",
        maxBytes=1 * 1024 * 1024,  # 1MB
        backupCount=2
    )
    logger = get_logger("test_medium", custom_handlers=handler, add_console=False)
    
    # å†™å…¥è¶³å¤Ÿå¤šçš„æ—¥å¿—ä»¥è§¦å‘å¤šæ¬¡è½®è½¬
    for i in range(3000):
        logger.info(f"Message {i}: " + "y" * 200)  # æ¯æ¡æ¶ˆæ¯çº¦200å­—èŠ‚
    
    # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
    log_files = sorted(glob.glob("logs/size.log*"))
    print(f"ç”Ÿæˆçš„æ—¥å¿—æ–‡ä»¶: {log_files}")
    
    # éªŒè¯æ–‡ä»¶æ•°é‡ä¸è¶…è¿‡backupCount+1
    assert len(log_files) <= 3, f"æ—¥å¿—æ–‡ä»¶æ•°é‡ {len(log_files)} è¶…è¿‡é¢„æœŸ 3"
    
    # éªŒè¯æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶å­˜åœ¨
    assert os.path.exists("logs/size.log"), "ä¸»æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨"
    
    # éªŒè¯å¤‡ä»½æ–‡ä»¶ç¼–å·æ­£ç¡®
    if len(log_files) > 1:
        assert "size.log.1" in log_files[-1], "å¤‡ä»½æ–‡ä»¶ç¼–å·ä¸æ­£ç¡®"
    
    print("âœ“ æµ‹è¯•2é€šè¿‡: ä¸­ç­‰backupCountæµ‹è¯•æˆåŠŸ")

def test_concurrent_rotation():
    """æµ‹è¯•3: å¹¶å‘è½®è½¬æµ‹è¯•"""
    print("\n=== æµ‹è¯•3: å¹¶å‘è½®è½¬æµ‹è¯• ===")
    setup_test_environment()
    
    # å¯åŠ¨å¤šä¸ªå·¥ä½œè¿›ç¨‹
    processes = []
    for i in range(5):
        # ä½¿ç”¨é¡¶çº§å‡½æ•°
        p = multiprocessing.Process(target=concurrent_rotation_worker, args=(i,))
        processes.append(p)
        p.start()
    
    for p in processes:
        p.join()
    
    # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
    log_files = sorted(glob.glob("logs/size.log*"))
    print(f"å¹¶å‘æµ‹è¯•ç”Ÿæˆçš„æ—¥å¿—æ–‡ä»¶: {log_files}")
    
    # éªŒè¯æ–‡ä»¶æ•°é‡
    assert len(log_files) <= 4, f"å¹¶å‘æµ‹è¯•æ—¥å¿—æ–‡ä»¶æ•°é‡ {len(log_files)} è¶…è¿‡é¢„æœŸ 4"
    
    # éªŒè¯æ—¥å¿—å®Œæ•´æ€§
    total_lines = 0
    for log_file in log_files:
        with open(log_file, 'r') as f:
            total_lines += len(f.readlines())
    
    print(f"å¹¶å‘æµ‹è¯•æ€»æ—¥å¿—æ¡æ•°: {total_lines} (é¢„æœŸ: 1000)")
    assert total_lines == 1000, f"å¹¶å‘æµ‹è¯•æ—¥å¿—æ¡æ•° {total_lines} ä¸ç­‰äºé¢„æœŸ 1000"
    print("âœ“ æµ‹è¯•3é€šè¿‡: å¹¶å‘è½®è½¬æµ‹è¯•æˆåŠŸ")

def run_all_tests(clean=False):
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹è¿è¡Œ MultiProcessSafeSizeRotatingHandler è‡ªåŠ¨åŒ–æµ‹è¯•...")
    print(f"å½“å‰æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        test_large_backup_count()
        # test_medium_backup_count()
        # test_concurrent_rotation()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc() # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆ
    finally:
        if clean:
            # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
            if os.path.exists("logs"):
                shutil.rmtree("logs")

if __name__ == "__main__":
    # ä¸ºäº†å®‰å…¨åœ°åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸­è¿è¡Œï¼Œè®¾ç½®å¯åŠ¨æ–¹æ³•
    # 'spawn' æ˜¯è·¨å¹³å°æœ€å®‰å…¨çš„æ–¹æ³•ï¼Œä½†å¯åŠ¨å¼€é”€è¾ƒå¤§
    # 'fork' (ä»…åœ¨Unixä¸Šå¯ç”¨) å¯åŠ¨å¿«ï¼Œä½†å¯èƒ½æœ‰ä¸€äº›å‰¯ä½œç”¨
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        # åœ¨æŸäº›ç¯å¢ƒï¼ˆå¦‚Jupyter Notebookï¼‰ä¸­ï¼Œå¯èƒ½å·²ç»è®¾ç½®äº†å¯åŠ¨æ–¹æ³•
        pass
    
    run_all_tests(clean=False)
