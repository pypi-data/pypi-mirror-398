Metadata-Version: 2.4
Name: lmp-sdk
Version: 2.0.14
Summary: æ¨ç†æœåŠ¡ Python SDK
Home-page: https://github.com/lixiang/lmp-sdk-python
Author: LMP SDK Team
Project-URL: Homepage, https://github.com/lixiang/lmp-sdk-python
Project-URL: Bug Tracker, https://github.com/lixiang/lmp-sdk-python/issues
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Requires-Dist: requests>=2.25.0
Requires-Dist: lpai_asset>=2.3.38
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: pytest-cov>=2.0; extra == "dev"
Requires-Dist: black>=21.0; extra == "dev"
Requires-Dist: flake8>=3.9; extra == "dev"
Requires-Dist: mypy>=0.900; extra == "dev"
Dynamic: home-page
Dynamic: requires-python

# LMP SDK - Python SDK

[![Python Version](https://img.shields.io/badge/python-3.7%2B-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Version](https://img.shields.io/badge/version-2.0.7-orange.svg)](https://pypi.org/project/lmp-sdk/)

LMP SDK æ˜¯lpai-llmå¹³å°çš„å®˜æ–¹ Python SDKï¼Œæä¾›ç®€æ´æ˜“ç”¨çš„æ¥å£æ¥è®¿é—®å¹³å°æ‰€éƒ¨ç½²çš„å…¬å…±æ¨ç†æœåŠ¡ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡å’Œè§†é¢‘çš„å¤šæ¨¡æ€è¾“å…¥ã€‚

## âœ¨ ç‰¹æ€§

- ğŸš€ **ç®€å•æ˜“ç”¨**ï¼šæä¾›ä¸¤ç§ä½¿ç”¨æ¨¡å¼ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯éœ€æ±‚
- ğŸ”„ **å¼‚æ­¥å¤„ç†**ï¼šæ”¯æŒå¼‚æ­¥ä»»åŠ¡æäº¤å’Œè‡ªåŠ¨ç»“æœè½®è¯¢
- ğŸ“¦ **æ‰¹é‡å¤„ç†**ï¼šé«˜æ•ˆçš„æ‰¹é‡è¯·æ±‚å¤„ç†èƒ½åŠ›
- ğŸ›¡ï¸ **ç¨³å®šå¯é **ï¼šå†…ç½®é‡è¯•æœºåˆ¶ã€ä»»åŠ¡æŒä¹…åŒ–å’Œä¼˜é›…å…³é—­
- ğŸ¯ **å¤šæ¨¡æ€æ”¯æŒ**ï¼šæ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡URLã€è§†é¢‘URLå¤šç§è¾“å…¥ç±»å‹
- ğŸ”§ **çµæ´»é…ç½®**ï¼šä¸°å¯Œçš„é…ç½®é€‰é¡¹ï¼Œé€‚åº”å„ç§ä½¿ç”¨åœºæ™¯

## ğŸ“¦ å®‰è£…

### ä½¿ç”¨ pip å®‰è£…

```bash
pip install lmp-sdk==2.0.7
```


### è¦æ±‚

- Python 3.7+
- lpai_asset >= 2.3.38 (æ‰¹é‡æ¨ç†åŠŸèƒ½å¿…éœ€)
- requests >= 2.25.0

### å®‰è£… Asset SDKï¼ˆæ‰¹é‡æ¨ç†åŠŸèƒ½å¿…éœ€ï¼‰

æ‰¹é‡æ¨ç†åŠŸèƒ½ä¾èµ– lpai_asset SDKï¼Œéœ€è¦ä»æŒ‡å®šæºå®‰è£…ï¼š

```bash
pip install lpai_asset -U -i https://artifactory.ep.chehejia.com/artifactory/api/pypi/liauto-pypi-l5/simple
```

- requests >= 2.25.0

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€ç¤ºä¾‹ - ç®€å•æ¨¡å¼

é€‚åˆå¿«é€Ÿæäº¤ä»»åŠ¡å¹¶æ‰‹åŠ¨æŸ¥è¯¢ç»“æœçš„åœºæ™¯ï¼š

```python
from src import InferClient, PostAsyncInferRequest, Content, ContentType

# 1. åˆ›å»ºå®¢æˆ·ç«¯
client = InferClient(
    token="your-api-token",
    timeout=3600
)

# 2. æäº¤æ¨ç†ä»»åŠ¡
request = PostAsyncInferRequest(
    contents=[
        Content(type=ContentType.TEXT, text="è¯·æè¿°è¿™å¼ å›¾ç‰‡"),
        Content(
            type=ContentType.IMAGE_URL,
            image_url={"url": "datasets/images/example.png"}
        )
    ],
    model="qwen__qwen2_5-vl-72b-instruct"
)

response = client.post_async_infer(request)
print(f"ä»»åŠ¡å·²æäº¤ï¼Œtask_id: {response.data.task_id}")

# 3. æŸ¥è¯¢ä»»åŠ¡ç»“æœ
from src import GetAsyncInferRequest

result = client.get_async_infer_res(
    GetAsyncInferRequest(task_id=response.data.task_id)
)
print(f"ä»»åŠ¡çŠ¶æ€: {result.data.status}")
print(f"ä»»åŠ¡ç»“æœ: {result.data.task_output}")
```

### é«˜çº§ç¤ºä¾‹ - è‡ªåŠ¨æ¨¡å¼

é€‚åˆéœ€è¦è‡ªåŠ¨å¤„ç†ä»»åŠ¡é˜Ÿåˆ—å’Œè·å–ç»“æœé€šçŸ¥çš„åœºæ™¯ï¼š

```python
from src import AsyncInfer, PostAsyncInferRequest, Content, ContentType, QueueMonitor

# 1. å®šä¹‰ç»“æœå›è°ƒå‡½æ•°
def on_task_complete(response):
    print(f"ä»»åŠ¡å®Œæˆ: {response.data.task_id}")
    print(f"ç»“æœ: {response.data.task_output}")

# 2. åˆ›å»ºè‡ªåŠ¨å¤„ç†å®¢æˆ·ç«¯
infer = AsyncInfer(
    token="your-api-token",
    worker_num=100,          # å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
    callback=on_task_complete  # ä»»åŠ¡å®Œæˆå›è°ƒ
)

# 3. æ‰¹é‡æäº¤ä»»åŠ¡
batch_requests = [
    PostAsyncInferRequest(
        contents=[Content(type=ContentType.TEXT, text=f"é—®é¢˜ {i}")],
        model="qwen__qwen2_5-vl-72b-instruct"
    )
    for i in range(10)
]

responses = infer.post_async_infer_batch(batch_requests, max_workers=10)
print(f"å·²æäº¤ {len(responses)} ä¸ªä»»åŠ¡")

# 4. ç›‘æ§é˜Ÿåˆ—ç›´åˆ°å…¨éƒ¨å®Œæˆ
monitor = QueueMonitor(infer, max_duration=3600)
exit_reason = monitor.monitor()  # é˜»å¡ç›´åˆ°é˜Ÿåˆ—ä¸ºç©ºæˆ–è¶…æ—¶
print(f"é˜Ÿåˆ—å¤„ç†å®Œæˆï¼ŒåŸå› : {exit_reason}")

### æ‰¹é‡æ¨ç†ç¤ºä¾‹

é€‚åˆéœ€è¦å¤„ç†å¤§é‡æ•°æ®çš„åœºæ™¯ï¼Œæ”¯æŒJSONLæ ¼å¼çš„æ‰¹é‡æ¨ç†ä»»åŠ¡ï¼š

```python
from src import BatchClient, CreateBatchAsyncInferRequest

# 1. åˆ›å»ºæ‰¹é‡æ¨ç†å®¢æˆ·ç«¯
batch_client = BatchClient(
    token="your-api-token",
    env="prod"  # æˆ– "ontest"
)

# 2. åˆ›å»ºæ‰¹é‡ä»»åŠ¡ï¼ˆSDKä¼šè‡ªåŠ¨ä¸Šä¼ æœ¬åœ°æ–‡ä»¶ï¼‰
request = CreateBatchAsyncInferRequest(
    name="æ‰¹é‡æ¨ç†ä»»åŠ¡",
    description="å¤„ç†å®¢æˆ·åé¦ˆæ•°æ®",
    model_service="qwen__qwen2_5-vl-72b-instruct",
    dataset="/local/path/to/input.jsonl",  # æœ¬åœ°æ–‡ä»¶ï¼ŒSDKè‡ªåŠ¨ä¸Šä¼ 
    output_dataset="datasets/my-output/versions/v1",  # è¾“å‡ºæ•°æ®é›†
    max_waiting_hour=24,
    apikey="your-api-token"
)

response = batch_client.create_batch(request, auto_upload=True)
batch_id = response.data
print(f"æ‰¹é‡ä»»åŠ¡å·²åˆ›å»º: {batch_id}")

# 3. ç­‰å¾…ä»»åŠ¡å®Œæˆ
final_detail = batch_client.wait_for_completion(
    batch_id=batch_id,
    poll_interval=30,  # æ¯30ç§’æŸ¥è¯¢ä¸€æ¬¡
    max_wait_time=3600  # æœ€å¤šç­‰å¾…1å°æ—¶
)

# 4. ä¸‹è½½ç»“æœ
batch_client.download_results(
    batch_id=batch_id,
    local_dir="./batch_results",
    download_error_file=True  # åŒæ—¶ä¸‹è½½é”™è¯¯æ–‡ä»¶
)

print(f"ç»“æœå·²ä¸‹è½½åˆ°: ./batch_results/")
batch_client.close()
```

**æ‰¹é‡æ¨ç†æ•°æ®æ ¼å¼ï¼ˆJSONLï¼‰**ï¼š

è¾“å…¥æ–‡ä»¶ `input.jsonl` ç¤ºä¾‹ï¼š
```jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "qwen__qwen2_5-vl-72b-instruct", "messages": [{"role": "user", "content": "ä½ å¥½"}]}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "qwen__qwen2_5-vl-72b-instruct", "messages": [{"role": "user", "content": "ä¸–ç•Œ"}]}}
```

```

## ğŸ“– æ ¸å¿ƒæ¦‚å¿µ

### ä¸¤ç§ä½¿ç”¨æ¨¡å¼

#### 1. ç®€å•æ¨¡å¼ (InferClient)

**ç‰¹ç‚¹**ï¼š
- ç›´æ¥æäº¤ä»»åŠ¡ï¼Œç«‹å³è¿”å› task_id
- éœ€è¦æ‰‹åŠ¨æŸ¥è¯¢ä»»åŠ¡ç»“æœ
- é€‚åˆå¯¹ç»“æœè·å–æ—¶æœºæœ‰ç²¾ç¡®æ§åˆ¶éœ€æ±‚çš„åœºæ™¯

**å·¥ä½œæµç¨‹**ï¼š
```
æäº¤ä»»åŠ¡ â†’ è·å–task_id â†’ æ‰‹åŠ¨è½®è¯¢ â†’ è·å–ç»“æœ
```

#### 2. è‡ªåŠ¨æ¨¡å¼ (AsyncInfer)

**ç‰¹ç‚¹**ï¼š
- è‡ªåŠ¨ç®¡ç†ä»»åŠ¡é˜Ÿåˆ—
- åå°çº¿ç¨‹è‡ªåŠ¨è½®è¯¢ä»»åŠ¡çŠ¶æ€
- é€šè¿‡å›è°ƒå‡½æ•°é€šçŸ¥ç»“æœ
- æ”¯æŒä»»åŠ¡æŒä¹…åŒ–ï¼Œç¨‹åºé‡å¯åæ¢å¤

**å·¥ä½œæµç¨‹**ï¼š
```
æäº¤ä»»åŠ¡ â†’ è‡ªåŠ¨åŠ å…¥é˜Ÿåˆ— â†’ åå°è½®è¯¢ â†’ å›è°ƒé€šçŸ¥ç»“æœ
```

### æ•°æ®æ¨¡å‹

#### Content - å†…å®¹ç±»å‹

```python
from src import Content, ContentType

# æ–‡æœ¬å†…å®¹
text_content = Content(
    type=ContentType.TEXT,
    text="ä½ å¥½ï¼Œä¸–ç•Œï¼"
)

# å›¾ç‰‡å†…å®¹
image_content = Content(
    type=ContentType.IMAGE_URL,
    image_url={"url": "path/to/image.jpg"}
)

# è§†é¢‘å†…å®¹
video_content = Content(
    type=ContentType.VIDEO_URL,
    video_url={"url": "path/to/video.mp4"}
)
```

#### PostAsyncInferRequest - æ¨ç†è¯·æ±‚

```python
from src import PostAsyncInferRequest

request = PostAsyncInferRequest(
    contents=[text_content, image_content],  # å†…å®¹åˆ—è¡¨ï¼ˆå¿…å¡«ï¼‰
    model="qwen__qwen2_5-vl-72b-instruct",  # æ¨¡å‹åç§°ï¼ˆå¿…å¡«ï¼‰
    temperature=0.7,                         # æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤0.000001ï¼‰
    max_tokens=2048,                         # æœ€å¤§tokenæ•°ï¼ˆå¯é€‰ï¼‰
    top_p=0.9,                              # Top-pé‡‡æ ·ï¼ˆå¯é€‰ï¼‰
    frequency_penalty=1.05,                  # é¢‘ç‡æƒ©ç½šï¼ˆå¯é€‰ï¼Œé»˜è®¤1.05ï¼‰
    presence_penalty=0.0,                    # å­˜åœ¨æƒ©ç½šï¼ˆå¯é€‰ï¼‰
    lpai_callback="https://your-callback-url",  # å›è°ƒURLï¼ˆå¯é€‰ï¼‰
    lpai_max_request_retries=5,              # æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤5ï¼‰
    role="user"                              # è§’è‰²ï¼ˆå¯é€‰ï¼Œé»˜è®¤"user"ï¼‰
)
```

#### TaskStatus - ä»»åŠ¡çŠ¶æ€

```python
from src import TaskStatus

# ä»»åŠ¡çŠ¶æ€æšä¸¾
TaskStatus.PENDING    # æ’é˜Ÿä¸­
TaskStatus.RUNNING    # è¿è¡Œä¸­
TaskStatus.SUCCEEDED  # æˆåŠŸ
TaskStatus.FAILED     # å¤±è´¥
TaskStatus.UNKNOWN    # æœªçŸ¥
```

## ğŸ”§ API å‚è€ƒ

### InferClient

ç®€å•æ¨¡å¼å®¢æˆ·ç«¯ï¼Œé€‚åˆå¿«é€Ÿé›†æˆã€‚

#### åˆå§‹åŒ–

```python
client = InferClient(
    token: str,                           # API Tokenï¼ˆå¿…å¡«ï¼‰
    endpoint: str = "é»˜è®¤ç«¯ç‚¹",            # APIç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰
    worker_num: int = 100,                # è¿æ¥æ± å¤§å°ï¼ˆå¯é€‰ï¼‰
    timeout: int = 3600                   # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œå¯é€‰ï¼‰
)
```

#### æ–¹æ³•

##### post_async_infer()

æäº¤å•ä¸ªå¼‚æ­¥æ¨ç†ä»»åŠ¡ã€‚

```python
response = client.post_async_infer(
    request: PostAsyncInferRequest
) -> PostAsyncInferResponse
```

**è¿”å›å€¼**ï¼š
- `response.errno`: é”™è¯¯ç ï¼ˆ0è¡¨ç¤ºæˆåŠŸï¼‰
- `response.msg`: æ¶ˆæ¯
- `response.data.task_id`: ä»»åŠ¡ID
- `response.data.queue_length`: é˜Ÿåˆ—é•¿åº¦
- `response.data.estimated_scheduled_time`: é¢„è®¡è°ƒåº¦æ—¶é—´ï¼ˆç§’ï¼‰

##### post_async_infer_batch()

æ‰¹é‡æäº¤æ¨ç†ä»»åŠ¡ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰ã€‚

```python
responses = client.post_async_infer_batch(
    requests: List[PostAsyncInferRequest],
    max_workers: int = 10                 # å¹¶å‘çº¿ç¨‹æ•°
) -> List[PostAsyncInferResponse]
```

##### get_async_infer_res()

æŸ¥è¯¢ä»»åŠ¡ç»“æœã€‚

```python
result = client.get_async_infer_res(
    request: GetAsyncInferRequest
) -> TaskResponse
```

**è¿”å›å€¼**ï¼š
- `result.errno`: é”™è¯¯ç 
- `result.msg`: æ¶ˆæ¯
- `result.data.task_id`: ä»»åŠ¡ID
- `result.data.status`: ä»»åŠ¡çŠ¶æ€
- `result.data.task_output`: ä»»åŠ¡è¾“å‡ºç»“æœ
- `result.data.failed_reason`: å¤±è´¥åŸå› ï¼ˆå¦‚æœå¤±è´¥ï¼‰
- `result.data.e2e_latency`: ç«¯åˆ°ç«¯å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰

---

### AsyncInfer

è‡ªåŠ¨æ¨¡å¼å®¢æˆ·ç«¯ï¼Œæä¾›å®Œæ•´çš„ä»»åŠ¡ç®¡ç†èƒ½åŠ›ã€‚

#### åˆå§‹åŒ–

```python
infer = AsyncInfer(
    token: str,                           # API Tokenï¼ˆå¿…å¡«ï¼‰
    endpoint: str = "é»˜è®¤ç«¯ç‚¹",            # APIç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰
    worker_num: int = 100,                # åå°å·¥ä½œçº¿ç¨‹æ•°ï¼ˆå¯é€‰ï¼‰
    polling_interval: int = 10,           # è½®è¯¢é—´éš”ï¼ˆç§’ï¼Œå¯é€‰ï¼‰
    max_wait_time: int = 86400,          # æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼Œå¯é€‰ï¼‰
    max_queue_size: int = 100000,        # æœ€å¤§é˜Ÿåˆ—å¤§å°ï¼ˆå¯é€‰ï¼‰
    timeout: int = 3600,                  # è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼Œå¯é€‰ï¼‰
    callback: Callable = None             # ä»»åŠ¡å®Œæˆå›è°ƒï¼ˆå¯é€‰ï¼‰
)
```

#### æ–¹æ³•

##### post_async_infer()

æäº¤å•ä¸ªä»»åŠ¡ï¼ˆè‡ªåŠ¨åŠ å…¥é˜Ÿåˆ—ï¼‰ã€‚

```python
response = infer.post_async_infer(
    request: PostAsyncInferRequest
) -> PostAsyncInferResponse
```

##### post_async_infer_batch()

æ‰¹é‡æäº¤ä»»åŠ¡ï¼ˆè‡ªåŠ¨åŠ å…¥é˜Ÿåˆ—ï¼‰ã€‚

```python
responses = infer.post_async_infer_batch(
    requests: List[PostAsyncInferRequest],
    max_workers: int = 10
) -> List[PostAsyncInferResponse]
```

---

### QueueMonitor

é˜Ÿåˆ—ç›‘æ§å™¨ï¼Œç”¨äºç›‘æ§å’Œç®¡ç†ä»»åŠ¡é˜Ÿåˆ—ã€‚

#### åˆå§‹åŒ–

```python
monitor = QueueMonitor(
    infer: AsyncInfer,                    # AsyncInferå®ä¾‹ï¼ˆå¿…å¡«ï¼‰
    max_duration: int = 3600,             # æœ€å¤§è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼Œå¯é€‰ï¼‰
    check_interval: int = 30              # æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼Œå¯é€‰ï¼‰
)
```

#### æ–¹æ³•

##### monitor()

é˜»å¡ç›‘æ§é˜Ÿåˆ—ï¼Œç›´åˆ°é˜Ÿåˆ—ä¸ºç©ºæˆ–è¶…æ—¶ã€‚

```python
exit_reason = monitor.monitor() -> str
# è¿”å›å€¼: "empty"ï¼ˆé˜Ÿåˆ—ä¸ºç©ºï¼‰ æˆ– "timeout"ï¼ˆè¶…æ—¶ï¼‰
```

##### get_elapsed_time()

è·å–å·²è¿è¡Œæ—¶é—´ã€‚

```python
elapsed = monitor.get_elapsed_time() -> float  # è¿”å›ç§’æ•°
```

##### get_queue_size()

è·å–å½“å‰é˜Ÿåˆ—å¤§å°ã€‚

```python
size = monitor.get_queue_size() -> int
```

---

### BatchClient

æ‰¹é‡æ¨ç†å®¢æˆ·ç«¯ï¼Œç”¨äºå¤„ç†å¤§è§„æ¨¡æ•°æ®çš„æ‰¹é‡æ¨ç†ä»»åŠ¡ã€‚

#### åˆå§‹åŒ–

```python
batch_client = BatchClient(
    token: str,                           # JWTè®¤è¯tokenï¼ˆå¿…å¡«ï¼‰
    base_url: str = "é»˜è®¤URL",             # APIåŸºç¡€URLï¼ˆå¯é€‰ï¼‰
    env: str = "prod",                    # è¿è¡Œç¯å¢ƒï¼ˆå¯é€‰ï¼‰
    timeout: int = 60                     # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œå¯é€‰ï¼‰
)
```

#### æ–¹æ³•

##### create_batch()

åˆ›å»ºæ‰¹é‡å¼‚æ­¥æ¨ç†ä»»åŠ¡ã€‚

```python
response = batch_client.create_batch(
    request: CreateBatchAsyncInferRequest,
    auto_upload: bool = True              # æ˜¯å¦è‡ªåŠ¨ä¸Šä¼ æœ¬åœ°æ–‡ä»¶
) -> CreateBatchAsyncInferResponse
```

**è¿”å›å€¼**ï¼š
- `response.errno`: é”™è¯¯ç ï¼ˆ0è¡¨ç¤ºæˆåŠŸï¼‰
- `response.msg`: æ¶ˆæ¯
- `response.data`: æ‰¹é‡ä»»åŠ¡ID

##### list_batches()

è·å–æ‰¹é‡æ¨ç†ä»»åŠ¡åˆ—è¡¨ã€‚

```python
response = batch_client.list_batches(
    request: ListBatchAsyncInferRequest = None
) -> ListBatchAsyncInferResponse
```

##### get_batch_detail()

è·å–æ‰¹é‡æ¨ç†ä»»åŠ¡è¯¦æƒ…ã€‚

```python
response = batch_client.get_batch_detail(
    batch_id: str
) -> GetBatchAsyncInferDetailResponse
```

**è¿”å›å€¼åŒ…å«**ï¼š
- `status`: ä»»åŠ¡çŠ¶æ€ï¼ˆvalidating/failed/in_progress/finalizing/completed/expired/cancelling/cancelledï¼‰
- `progress`: ä»»åŠ¡è¿›åº¦ï¼ˆ0.0-1.0ï¼‰
- `request_num`: æ€»è¯·æ±‚æ•°
- `finished_request_num`: å·²å®Œæˆè¯·æ±‚æ•°
- `succeeded_request_num`: æˆåŠŸè¯·æ±‚æ•°
- `failed_request_num`: å¤±è´¥è¯·æ±‚æ•°

##### cancel_batch()

å–æ¶ˆæ‰¹é‡æ¨ç†ä»»åŠ¡ã€‚

```python
response = batch_client.cancel_batch(
    batch_id: str
) -> CancelBatchAsyncInferResponse
```

##### download_results()

ä¸‹è½½æ‰¹é‡æ¨ç†ä»»åŠ¡çš„ç»“æœæ–‡ä»¶ã€‚

```python
batch_client.download_results(
    batch_id: str,                        # æ‰¹é‡ä»»åŠ¡ID
    local_dir: str,                       # æœ¬åœ°ä¸‹è½½ç›®å½•
    download_error_file: bool = True,     # æ˜¯å¦ä¸‹è½½é”™è¯¯æ–‡ä»¶
    overwrite: bool = False               # æ˜¯å¦è¦†ç›–å·²å­˜åœ¨æ–‡ä»¶
) -> None
```

##### wait_for_completion()

ç­‰å¾…æ‰¹é‡ä»»åŠ¡å®Œæˆã€‚

```python
final_detail = batch_client.wait_for_completion(
    batch_id: str,
    poll_interval: int = 30,              # è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
    max_wait_time: int = 86400            # æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
) -> GetBatchAsyncInferDetailResponse
```


## ğŸ¯ ä½¿ç”¨åœºæ™¯

### åœºæ™¯1ï¼šå•æ¬¡æ¨ç†æŸ¥è¯¢

```python
from src import InferClient, PostAsyncInferRequest, Content, ContentType

client = InferClient(token="your-token")

# æäº¤ä»»åŠ¡
response = client.post_async_infer(
    PostAsyncInferRequest(
        contents=[Content(type=ContentType.TEXT, text="ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")],
        model="qwen__qwen2_5-vl-72b-instruct"
    )
)

# ç­‰å¾…ä¸€æ®µæ—¶é—´åæŸ¥è¯¢
import time
time.sleep(30)

result = client.get_async_infer_res(
    GetAsyncInferRequest(task_id=response.data.task_id)
)
```

### åœºæ™¯2ï¼šæ‰¹é‡æ•°æ®å¤„ç†

```python
from src import AsyncInfer, PostAsyncInferRequest, Content, ContentType

results = []

def collect_result(response):
    results.append(response.data.task_output)

infer = AsyncInfer(token="your-token", callback=collect_result)

# æ‰¹é‡æäº¤1000ä¸ªä»»åŠ¡
requests = [
    PostAsyncInferRequest(
        contents=[Content(type=ContentType.TEXT, text=data)],
        model="qwen__qwen2_5-vl-72b-instruct"
    )
    for data in your_data_list  # å‡è®¾æœ‰1000æ¡æ•°æ®
]

infer.post_async_infer_batch(requests, max_workers=50)

# ç­‰å¾…å…¨éƒ¨å®Œæˆ
from src import QueueMonitor
monitor = QueueMonitor(infer, max_duration=7200)
monitor.monitor()

print(f"å¤„ç†å®Œæˆï¼Œå…± {len(results)} ä¸ªç»“æœ")
```

### åœºæ™¯3ï¼šå›¾æ–‡æ··åˆæ¨ç†

```python
from src import InferClient, PostAsyncInferRequest, Content, ContentType

client = InferClient(token="your-token")

# å¤šæ¨¡æ€è¾“å…¥
response = client.post_async_infer(
    PostAsyncInferRequest(
        contents=[
            Content(type=ContentType.TEXT, text="è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹"),
            Content(
                type=ContentType.IMAGE_URL,
                image_url={"url": "datasets/images/analysis.jpg"}
            ),
            Content(type=ContentType.TEXT, text="å¹¶æä¾›è¯¦ç»†è¯´æ˜")
        ],
        model="qwen__qwen2_5-vl-72b-instruct",
        temperature=0.7,
        max_tokens=2048
    )
)
```

### åœºæ™¯4ï¼šå¸¦å›è°ƒURLçš„å¼‚æ­¥å¤„ç†

```python
from src import InferClient, PostAsyncInferRequest, Content, ContentType

client = InferClient(token="your-token")

response = client.post_async_infer(
    PostAsyncInferRequest(
        contents=[Content(type=ContentType.TEXT, text="ä»»åŠ¡å†…å®¹")],
        model="qwen__qwen2_5-vl-72b-instruct",
        lpai_callback="https://your-service.com/callback"  # ç»“æœä¼šPOSTåˆ°è¿™ä¸ªURL
    )
)

### åœºæ™¯5ï¼šå¤§è§„æ¨¡æ‰¹é‡æ¨ç†

é€‚åˆå¤„ç†æˆåƒä¸Šä¸‡æ¡æ•°æ®çš„åœºæ™¯ï¼š

```python
from src import BatchClient, CreateBatchAsyncInferRequest

# åˆ›å»ºæ‰¹é‡æ¨ç†å®¢æˆ·ç«¯
batch_client = BatchClient(token="your-token", env="prod")

# åˆ›å»ºæ‰¹é‡ä»»åŠ¡ï¼ˆæ”¯æŒæœ¬åœ°æ–‡ä»¶è‡ªåŠ¨ä¸Šä¼ ï¼‰
request = CreateBatchAsyncInferRequest(
    name="å®¢æˆ·åé¦ˆæ‰¹é‡åˆ†æ",
    description="åˆ†æ10000æ¡å®¢æˆ·åé¦ˆ",
    model_service="qwen__qwen2_5-vl-72b-instruct",
    dataset="/local/path/to/10000_feedbacks.jsonl",  # æœ¬åœ°æ–‡ä»¶
    output_dataset="datasets/feedback-analysis/versions/v1",
    max_waiting_hour=48,  # å…è®¸48å°æ—¶å®Œæˆ
    apikey="your-token"
)

# åˆ›å»ºä»»åŠ¡ï¼ˆSDKè‡ªåŠ¨ä¸Šä¼ æ–‡ä»¶åˆ°lmp-requestæ•°æ®é›†ï¼‰
response = batch_client.create_batch(request, auto_upload=True)
batch_id = response.data

# ç­‰å¾…ä»»åŠ¡å®Œæˆ
final_detail = batch_client.wait_for_completion(
    batch_id=batch_id,
    poll_interval=60,  # æ¯åˆ†é’ŸæŸ¥è¯¢ä¸€æ¬¡
    max_wait_time=172800  # æœ€å¤šç­‰å¾…48å°æ—¶
)

# ä¸‹è½½ç»“æœ
batch_client.download_results(
    batch_id=batch_id,
    local_dir="./results",
    download_error_file=True
)

print(f"æ‰¹é‡ä»»åŠ¡å®Œæˆ: {final_detail.data.succeeded_request_num}/{final_detail.data.request_num} æˆåŠŸ")
```

```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡

å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®é»˜è®¤å‚æ•°ï¼š

```bash
export LMP_API_TOKEN="your-token"
export LMP_API_ENDPOINT="https://custom-endpoint.com/api"
export LMP_DEFAULT_MODEL="your-model"
```

### ä»»åŠ¡æŒä¹…åŒ–

AsyncInfer æ¨¡å¼ä¸‹ï¼Œä»»åŠ¡ä¼šè‡ªåŠ¨æŒä¹…åŒ–åˆ°æœ¬åœ°æ–‡ä»¶ï¼š

- é»˜è®¤è·¯å¾„ï¼š`~/.lmp/task_queue.json`
- ç¨‹åºå¼‚å¸¸é€€å‡ºæ—¶è‡ªåŠ¨ä¿å­˜
- é‡å¯åè‡ªåŠ¨æ¢å¤æœªå®Œæˆçš„ä»»åŠ¡

## ğŸ” é”™è¯¯å¤„ç†

### å¼‚å¸¸ç±»å‹

```python
from src import LMPException, APIError, TaskTimeoutError, TaskFailedError, QueueFullError

try:
    response = client.post_async_infer(request)
except APIError as e:
    print(f"APIé”™è¯¯: {e.status_code} - {e.message}")
except TaskTimeoutError as e:
    print(f"ä»»åŠ¡è¶…æ—¶: {e}")
except TaskFailedError as e:
    print(f"ä»»åŠ¡å¤±è´¥: {e}")

### æ‰¹é‡æ¨ç†ä¸“ç”¨å¼‚å¸¸

```python
from src import (
    BatchCreationError,
    BatchNotFoundError,
    DatasetPathError,
    AssetDownloadError,
    AssetUploadError
)

try:
    response = batch_client.create_batch(request)
except BatchCreationError as e:
    print(f"æ‰¹é‡ä»»åŠ¡åˆ›å»ºå¤±è´¥: {e}")
except DatasetPathError as e:
    print(f"æ•°æ®é›†è·¯å¾„é”™è¯¯: {e}")
except AssetUploadError as e:
    print(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
```

except QueueFullError as e:
    print(f"é˜Ÿåˆ—å·²æ»¡: {e}")
except LMPException as e:
    print(f"SDKé”™è¯¯: {e}")
```

### é‡è¯•æœºåˆ¶

SDK å†…ç½®äº†å¤šå±‚é‡è¯•æœºåˆ¶ï¼š

1. **HTTPå±‚é‡è¯•**ï¼šç½‘ç»œè¯·æ±‚å¤±è´¥è‡ªåŠ¨é‡è¯•3æ¬¡ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
2. **ä»»åŠ¡å±‚é‡è¯•**ï¼šå¯é€šè¿‡ `lpai_max_request_retries` å‚æ•°é…ç½®ï¼ˆé»˜è®¤5æ¬¡ï¼‰

### æ—¥å¿—é…ç½®

```python
import logging

# é…ç½®SDKæ—¥å¿—çº§åˆ«
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# è°ƒè¯•æ¨¡å¼
logging.getLogger('src').setLevel(logging.DEBUG)
```

## ğŸ§ª æµ‹è¯•

```bash
# å®‰è£…æµ‹è¯•ä¾èµ–
pip install -e ".[dev]"

# è¿è¡Œæµ‹è¯•
pytest tests/

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=src tests/
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### è¿æ¥æ± é…ç½®

```python
# é«˜å¹¶å‘åœºæ™¯å»ºè®®é…ç½®
client = InferClient(
    token="your-token",
    worker_num=200,      # å¢åŠ è¿æ¥æ± å¤§å°
    timeout=7200         # å¢åŠ è¶…æ—¶æ—¶é—´
)
```

### æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
# ä½¿ç”¨æ‰¹é‡æ¥å£
responses = infer.post_async_infer_batch(
    requests,
    max_workers=50  # æ ¹æ®æœåŠ¡å™¨æ€§èƒ½è°ƒæ•´å¹¶å‘æ•°
)
```

### å†…å­˜ä¼˜åŒ–

å¯¹äºå¤§è§„æ¨¡ä»»åŠ¡å¤„ç†ï¼Œå»ºè®®åˆ†æ‰¹æäº¤ï¼š

```python
batch_size = 1000
for i in range(0, len(all_requests), batch_size):
    batch = all_requests[i:i+batch_size]
    responses = infer.post_async_infer_batch(batch)
    # ç­‰å¾…è¿™æ‰¹å®Œæˆåå†æäº¤ä¸‹ä¸€æ‰¹
    monitor.monitor()
```

## ğŸ”’ å®‰å…¨å»ºè®®

1. **Tokenç®¡ç†**ï¼šä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç tokenï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶
2. **HTTPS**ï¼šç”Ÿäº§ç¯å¢ƒå¿…é¡»ä½¿ç”¨HTTPSç«¯ç‚¹
3. **æƒé™æ§åˆ¶**ï¼šä½¿ç”¨æœ€å°æƒé™åŸåˆ™ï¼Œåªæˆäºˆå¿…è¦çš„APIæƒé™
4. **æ—¥å¿—è„±æ•**ï¼šé¿å…åœ¨æ—¥å¿—ä¸­è¾“å‡ºæ•æ„Ÿä¿¡æ¯

## ğŸ“š æ›´å¤šèµ„æº

- [å¹³å°ä½¿ç”¨æ–‡æ¡£](https://li.feishu.cn/wiki/X8yxwluDCiRci0k5e85cp3cxndg?from=from_copylink)
- [ç¤ºä¾‹ä»£ç ](./examples)


## ğŸ“ ç‰ˆæœ¬å‘å¸ƒ

### æ›´æ–°ç‰ˆæœ¬å·

ä¿®æ”¹ä»¥ä¸‹æ–‡ä»¶ä¸­çš„ç‰ˆæœ¬å·ï¼š
- `pyproject.toml`ï¼š`version = "x.x.x"`
- `setup.py`ï¼š`version = "x.x.x"`

### æ„å»ºå’Œå‘å¸ƒ

```bash
# æ¸…ç†æ—§æ„å»º
rm -rf dist/ build/ *.egg-info

# æ„å»ºåˆ†å‘åŒ…
python -m build

# ä¸Šä¼ åˆ°PyPI
twine upload dist/*
```

---

**Made with â¤ï¸ by LMP SDK Team**
