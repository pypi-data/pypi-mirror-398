Metadata-Version: 2.4
Name: beta_solver
Version: 0.0.2
Summary: A transparent, pure-math implementation of Multiple Linear Regression using the Normal Equation.
Author-email: Abhish Bondre <abhishbondre232@gmail.com>
Project-URL: Homepage, https://github.com/abhishbondre/beta_solver
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Topic :: Scientific/Engineering :: Mathematics
Classifier: Intended Audience :: Education
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy
Dynamic: license-file

# beta_solver

![Python](https://img.shields.io/badge/python-3.7+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-active%20learning-orange.svg)

**beta_solver** is a personal project where I implement Machine Learning algorithms from scratch to understand the mathematics behind the "black box."

I am currently practicing core algorithms, starting with **Linear Regression** using the Normal Equation. As I learn and practice more algorithms, I will update this library to include them.

## üéØ Project Goal
The goal of this project is not to replace libraries like `scikit-learn`, but to:
1.  **Demystify the math** behind ML algorithms.
2.  **Implement pure Python/Numpy solutions** without high-level wrappers.
3.  **Document my learning journey** in code.

---

# üìê Current Algorithm
## Linear Regression

Right now, the library solves for the coefficient vector $\beta$ using Linear Algebra:

$$\beta = (X^T X)^{-1} (X^T Y)$$

Where:
* $X$: The input feature matrix.
* $Y$: The target vector.
* $\beta$: The resulting coefficients (Intercept + Slopes).

---
## Logistic Regression
Logistic Regression is implemented using the Gradient Descent optimization technique to minimize the Log Loss function.
The model predicts probabilities using the Sigmoid function:
$$P(Y=1|X) = \sigma(X\beta) = \frac{1}{1 + e^{-X\beta}}$$
Where:
* $X$: The input feature matrix.
* $Y$: The target vector (0 or 1).
* $\beta$: The coefficient vector (Intercept + Slopes).

---

## üöÄ Installation

You can install `beta_solver` using pip:

```bash
pip install beta_solver
```

## ü§ù Contributing
This is a learning repository. If you see a way to optimize the math or make the code cleaner, feel free to open a Pull Request!

## üìÑ License
Distributed under the MIT License. See LICENSE for more information.

## üë§ Author
**Abhish Bondre**
GitHub: [abhishbondre](https://github.com/abhishbondre)

