huggingface-hub>=0.20.0

[all]
llama-cpp-python>=0.2.0
mlx>=0.4.0
mlx-lm>=0.4.0
vllm>=0.4.0

[apple]
mlx>=0.4.0
mlx-lm>=0.4.0

[cpu]
llama-cpp-python>=0.2.0

[dev]
pytest>=7.0
pytest-cov>=4.0
black>=23.0
ruff>=0.1.0
mypy>=1.0

[llama]
llama-cpp-python>=0.2.0

[mlx]
mlx>=0.4.0
mlx-lm>=0.4.0

[nvidia]
vllm>=0.4.0

[vllm]
vllm>=0.4.0
