# -*- coding: UTF-8 -*-
"""
@Project ï¼škyutil 
@File    ï¼šbuild_base.py
@IDE     ï¼šPyCharm 
@Author  ï¼šxuyong@kylinos.cn
@Date    ï¼š2025/3/27 ä¸‹åˆ11:13 
@Desc    ï¼šè¯´æ˜ï¼š
"""
import bz2
import glob
import json
import lzma
import os
import re
import shutil
import sqlite3
import subprocess
import tempfile
import time
import traceback
import uuid
from datetime import datetime
from pathlib import Path

import wget
from celery import states, exceptions
from retry import retry

from kyutil.celery_util import celery_state_update as _update_celery
from kyutil.config import BUILD_PATH, SIG_KEY, FILE_SCHEMA, REPODATA_PATH, APACHE_READ_MODEL, SENSITIVE_KEYWORDS, \
    HOST_IP, BUILD_PATH_LOGGER_FILE, HTTPS, ROOT_PATH_ISO_PATH
from kyutil.config import PYTHON_MINOR_CMD
from kyutil.data import ISOErrorEnum, KOJI_SERVER_PORT
from kyutil.date_utils import extract_time_from_line
from kyutil.download import download_file, wget_remote_dirs
from kyutil.exceptions import BuildException, BizAssert
from kyutil.file import copy_dirs, move_dirs, delete_dirs, file_write, reset_dirs, get_file_list, \
    get_comps_list, get_ks_list, get_file_sha256sum, get_file_size
from kyutil.file_compare import run_lsinitrd
from kyutil.http_util import send_request
from kyutil.inject_ks import insert_install_ks
from kyutil.iso_utils import is_isohybrid, get_base_arch, find_arch_by_name
from kyutil.log import zero_log
from kyutil.mock import into_chroot_env, exit_chroot_env, generate_mock_config
from kyutil.reg_exp import URL_REPODATA_SQLITE, BUILD_PARAMS
from kyutil.release_dependency import ReleaseDependency
from kyutil.rpms import common_split_filename, get_rpm_sign, read_rpm_header, check_rpm_name_blacklist, \
    check_rpm_name_sensitive
from kyutil.shell import run_command, rum_command_and_log, run_command_with_return
from kyutil.url import url_reachable, url_filename
from kyutil.util_rpm_info import load_repodata

EXT_SHA256SUM = ".sha256sum"

logger = zero_log(__file__, BUILD_PATH_LOGGER_FILE)


class ReleaseOSBase(ReleaseDependency):
    """é›†æˆæ„å»ºåŸºç±»"""
    user = 'pungier'  # é›†æˆæ„å»ºç”¨æˆ·
    process = 0  # å½“å‰ä»»åŠ¡è¿›åº¦ï¼Œå–å€¼ä»0-100

    def __init__(self, **kwargs):
        """initåˆå§‹åŒ–å‡½æ•°-åŸºç±»"""
        self.command_logger = None
        self.mash_sum = None
        self.mash_list = None
        self.mash_log_name = None
        self.mash_log = None
        self.mash_httpd_path = None
        self.series = "Base"
        self.nkvers = None
        self.repos = None
        self.repo_create_time = None
        # ä¿å­˜mockæ–‡ä»¶è·¯å¾„
        self.mock_cfg_file = None
        self.isos_inmock = '/root/isos'
        self.dracut = ""

        # kwargs æ˜¯APIè°ƒç”¨celeryå‡½æ•°æ˜¯ä¼ çš„å‚æ•°ï¼Œéœ€è¦å¢åŠ 8.2çš„å‚æ•°

        # æ—¶é—´å‚æ•°
        self.cur_day = kwargs.get("cur_day", time.strftime('%Y%m%d', time.localtime(time.time())))
        # celeryçŠ¶æ€æ›´æ–°å‚æ•°
        self._update_celery = kwargs.get("state")
        self.task_id = kwargs.get("id")
        self.params = kwargs.get("params")
        self.target_arch = self.params.get('target_arch')

        # æœ¬åœ°æ„å»ºç›®å½•
        self.build_path = f"{BUILD_PATH}/{self.cur_day}/{self.params.get('tag')}/{self.params.get('target_arch')}/{self.task_id[0:4]}"
        self.build_isos = f"{self.build_path}/iso"
        self.build_log = f"{self.build_isos}/logs"
        self.build_cfg_dir = f"{self.build_isos}/conf/"

        self.build_logfile = f"{self.build_log}/build-{self.task_id[0:4]}.log"

        self.build_warn_file = f"{self.build_log}/WARNING.txt"
        self.build_env = f"{self.build_log}/build_env-{self.task_id[0:4]}.txt"
        self.command_env = f"{self.build_log}/command-{self.task_id[0:4]}.txt"
        # é…ç½®æ–‡ä»¶å¤¹å®šä¹‰
        self.cfg_file = f"{self.build_cfg_dir}/build.cfg.txt"
        self.before_repo_package = f"{self.build_path}/before_repo_package/"
        self.after_repo_package = f"{self.build_path}/after_repo_package/"
        self.boot_file = f"{self.build_path}/boot_file/"
        self.not_boot_file = f"{self.build_path}/not_boot_file/"
        self.scripts_dir = f"{self.build_path}/scripts_dir/"
        # é…ç½®æ–‡ä»¶åç§°
        self.ks_file = url_filename(self.params.get('ks_path'))
        self.comps_file = url_filename(self.params.get('comps_path'))
        self.ks_file_path = os.path.join(self.build_cfg_dir, self.ks_file)
        self.comps_file_path = os.path.join(self.build_cfg_dir, self.comps_file)

        # mockç¯å¢ƒæ„å»ºç›®å½•
        self.build_mocktag = f"{self.params.get('tag')}-{self.task_id[0:4]}"
        self.build_mockroot = f"/var/lib/mock/{self.build_mocktag}/root/"
        self.build_inmock = "/root/buildiso"
        self.os_patch_inmock = f"/root/buildiso/{self.params.get('release')}/{self.target_arch}/os/"
        self.build_inmock_packages = f"{self.build_mockroot}/root/buildiso/Packages/"
        # é€šç”¨isoè¾“å‡ºåç§°ï¼Œå­å‡½æ•°è°ƒç”¨
        self.iso_name = self.params.get('iso_name') if self.params.get('iso_name') else \
            f"{self.params.get('release')}-{self.params.get('target_arch')}" \
            f"-{self.params.get('build')}-{self.cur_day}.iso"

        self.mash_httpd = kwargs.get("mash")  # éåŒç›®å½•çš„
        self.mash_result = kwargs.get("mash")
        self.pkg_list = None
        self.ks_file_path = None
        # mashæºï¼Œrepoæºè½¯ä»¶åŒ…åˆ—è¡¨è§£å‹
        self.yum_url = self.params.get("yum_url")
        self.iso_pkgs_list = f"{self.build_isos}/logs/pkg_list_iso.txt"
        self.src_pkgs_list = f"{self.isos_inmock}/%s" % '-'.join(
            [self.params.get('release'), self.params.get('target_arch'), 'src_packages.txt'])
        self.yum_pkgs_list = f"{self.build_isos}/logs/pkg_list_yum_repo.txt"
        self.mash_pkgs_list = f"{self.build_isos}/logs/pkg_list_mash_repo.txt"
        self.compress_sqlite_name = self.build_log + "/com_primary.sqlite"
        self.decompress_sqlite_name = self.build_log + "/primary.sqlite"
        self.rpm_graph_dot = self.build_log + "/dependency.dot"
        self.rpm_graph_pdf = self.build_log + "/dependency_graph.pdf"
        self.rpm_dep_csv = self.build_log + "/dependency.csv"
        self.lorax_templates_url = self.params.get("lorax_templates_url")

        self.addon_repodata = '/root/addon_repo'
        self.tags = eval(self.params.get('tags')) if self.params.get('tags') else []
        self.product_file = ""
        self.lorax_templates_sha256sum = ""
        self.build_logger = zero_log(__file__, self.build_logfile)
        self.command_logger = zero_log("command", self.command_env)

    def init_run_env_base(self):
        """è·å–é…ç½®å‚æ•°ï¼Œåˆå§‹åŒ–ç›®å½•"""
        self.process = 10
        os.chmod(self.build_logfile, APACHE_READ_MODEL)
        self._update_status("è·å–é…ç½®å‚æ•°ï¼Œä¸‹è½½é…ç½®æ–‡ä»¶", self.process, None)
        # è·å–é…ç½®å‚æ•°å¹¶å†™å…¥æ–‡ä»¶
        reset_dirs(self.build_cfg_dir)
        file_write(self.cfg_file, "æ¥æ”¶ä½¿ç”¨çš„é…ç½®å‚æ•°å¦‚ä¸‹ï¼š\n")
        with open(self.cfg_file, 'a+') as f:
            json.dump(self.params, f, indent=4)
        # é‡ç½®æœ¬åœ°é…ç½®æ–‡ä»¶ç›®å½•
        reset_dirs(self.before_repo_package)
        reset_dirs(self.after_repo_package)
        reset_dirs(self.boot_file)
        reset_dirs(self.not_boot_file)
        reset_dirs(self.scripts_dir)
        # ä¸‹è½½é…ç½®æ–‡ä»¶ks/comps/release.srpm
        self.build_logger.info("å¼€å§‹ä¸‹è½½é›†æˆæ‰€éœ€æ–‡ä»¶åˆ°é›†æˆç›®å½•")
        download_file(self.params.get('ks_path'), dir_=self.build_cfg_dir, logger=self.build_logger)
        download_file(self.params.get('comps_path'), dir_=self.build_cfg_dir, logger=self.build_logger)
        if self.lorax_templates_url and self.lorax_templates_url.startswith("http"):
            download_file(self.params.get('lorax_templates_url'), dir_=self.build_cfg_dir, logger=self.build_logger)
        download_file(self.params.get('scripts_dir'), dir_=self.scripts_dir, logger=self.build_logger)
        self.ks_file_path = os.path.join(self.build_cfg_dir, self.ks_file)
        self._update_status("å‚æ•°è·å–å®Œæˆï¼š " + str(self.params), self.process, None)

    def check_mash(self, **kwargs):
        self.mash_httpd_path = str(kwargs.get("mash_repo", kwargs.get("mash_httpd", ''))).strip()
        self.mash_httpd_path = self.mash_httpd_path.replace(",", "\n").replace("ï¼Œ", "\n")
        self.build_logger.info(f" mashä»“åº“åœ°å€æ˜¯ï¼š {self.mash_httpd_path}")
        file_write(self.cfg_file, f"\n mash-repo:{self.mash_httpd_path}")
        BizAssert.has_value(self.mash_httpd_path, "mashå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œé›†æˆã€‚")

        self.mash_log = str(kwargs.get("mash_log", ""))
        if url_reachable(self.mash_log, logger=self.build_logger) and '.log' in self.mash_log:
            self.mash_log_name = os.path.basename(self.mash_log)
        else:
            self.mash_log = None
            self.mash_log_name = None
        self.mash_list = str(kwargs.get("mash_list", ""))
        self.mash_sum = str(kwargs.get("mash_sum", ""))
        self.check_iso_log()

    @retry(delay=3, backoff=3, tries=5)
    def _update_status(self, msg, percent, status):
        """celeryå®æ—¶çŠ¶æ€æ›´æ–°ä»¥åŠä¿¡æ¯å›ä¼ """
        _update_celery(self._update_celery, self.build_logger, msg, percent, status, self.task_id)

    def init_mock_env(self, config_root_dir="/etc/mock/"):
        """åˆå§‹åŒ–mockç¯å¢ƒ"""
        self.process = 25
        self._update_status("åˆå§‹åŒ–mockæ„å»ºç¯å¢ƒ", self.process, None)
        if self.params.get("mock_cfg"):
            self.mock_cfg_file = f'/etc/mock/{self.params.get("mock_cfg")}.cfg'
        else:
            self.mock_cfg_file = generate_mock_config(self.params, self.build_mocktag, self.build_logger,
                                                      config_root_dir=config_root_dir)
        if self.params.get('pungi_url') or self.params.get('lorax_url'):
            cmd = f"sh {config_root_dir}mock_.sh  {self.lorax_templates_url} {self.build_mocktag} " \
                  f"{self.params.get('target_arch')} {self.params.get('pungi_url')} {self.params.get('lorax_url')}"
        else:
            cmd = f"sh {config_root_dir}mock_.sh  {self.lorax_templates_url} {self.build_mocktag} " \
                  f"{self.params.get('target_arch')}"
        if self.mock_cfg_file:
            res = run_command(cmd, self.build_logger, error_message="mockç¯å¢ƒåˆå§‹åŒ–å¤±è´¥")
            self.command_logger.info(f'ã€CMDã€‘mockåˆå§‹åŒ–\tå‘½ä»¤:{cmd}\tçŠ¶æ€:{res}')
            if f"{res}" != "0":
                self.build_logger.info("mockç¯å¢ƒåˆå§‹åŒ–å¤±è´¥ï¼")
                return False
            else:
                self.build_logger.info(f"mockç¯å¢ƒåˆå§‹åŒ–æˆåŠŸï¼Tag:{self.params.get('tag')}")
                return True
        self.build_logger.info("mocké…ç½®æ–‡ä»¶ç”Ÿæˆå¤±è´¥ï¼")
        return False

    def copy_iso_to_dir(self, mock_isos_dir):
        """01-æ‹·è´isoæ–‡ä»¶è‡³æœ¬åœ°æ„å»ºç›®å½•"""
        if os.path.exists(mock_isos_dir) and os.path.exists(self.build_isos):
            self._update_status('move ' + mock_isos_dir + ' to ' + self.build_isos, self.process, None)
            self.build_logger.info("å°†Mockçš„ISOç§»åŠ¨åˆ° buildç›®å½•")
            move_dirs(mock_isos_dir, self.build_isos)
        # ä¸‹è½½mashçš„æ—¥å¿—åˆ°æœ¬åœ°
        if self.mash_log:
            wget_remote_dirs(self.build_logger, self.mash_log, self.build_log)
            self.build_logger.info(f"ä¸‹è½½mashçš„æ—¥å¿—åˆ°æœ¬åœ°ï¼š{self.build_log}")
        else:
            self.build_logger.warning(f"æ²¡æœ‰mashä»»åŠ¡ï¼Œæ— Mashæ—¥å¿—æ–‡ä»¶ã€‚mashåœ°å€ï¼š{self.mash_httpd_path}")

    def find_iso_name(self):
        """02-åˆ—å‡ºisoåç§°"""
        if os.path.isdir(self.build_isos):
            for root, dirs, files in os.walk(self.build_isos):
                for f in files:
                    file_name = os.path.join(root, f)
                    if file_name.endswith('.iso') and file_name.find('netinst') == -1:
                        iso_path = file_name.replace(BUILD_PATH, '')
                        return iso_path
        return ""

    def get_iso_size(self):
        """03-æ£€æŸ¥isoå¤§å°"""
        for root, dirs, files in os.walk(self.build_isos):
            for f in files:
                file_name = os.path.join(root, f)
                if file_name.endswith('.iso') and file_name.find('netinst') == -1:
                    iso_size = os.path.getsize(file_name)
                    return iso_size
        return ""

    def check_iso_log(self):
        """04-è·å–iso_logè·¯å¾„"""
        self.build_logger.info(f"ã€Step-01/11ã€‘: {self.series} è·å–iso_logè·¯å¾„")
        try:
            mash_ = self.mash_httpd_path.strip().split("\n")
            if len(mash_) == 1:
                wget_remote_dirs(self.build_logger, mash_[0].strip() + "/repodata",
                                 self.build_log + os.sep + "mash/repodata")
            else:
                for i, repo in enumerate(mash_):
                    wget_remote_dirs(self.build_logger, repo.strip() + "/repodata",
                                     self.build_log + os.sep + f"mash/repodata-{i}")
        except Exception as e:
            self.build_logger.error(f"é”™è¯¯ä¿¡æ¯ï¼š {e}")

        if not self.mash_httpd_path and self.mash_log:
            # mashå¤±è´¥æ—¶è¦†ç›–build log
            download_file(self.mash_log, dir_=self.build_log, logger=self.build_logger)
            delete_dirs(f"{self.build_log}/build-{self.task_id[0:4]}.log", self.build_logger)
            move_dirs(f"{self.build_log}/{self.mash_log_name}",
                      f"{self.build_log}/build-{self.task_id[0:4]}.log")
        if os.path.exists(self.build_log) and len(os.listdir(self.build_log)) > 0:
            iso_log = f"{self.build_log}/build-{self.task_id[0:4]}.log"
            return iso_log.replace(BUILD_PATH, '')
        else:
            raise RuntimeError("æ—¥å¿—åˆå§‹åŒ–å¤±è´¥ã€‚")

    def check_rpm(self):
        """
        æ£€æŸ¥isoå†…åŒ…æ˜¯å¦å«æœ‰rhelå­—æ ·
        æ£€æŸ¥æ˜¯å¦åŒ…å«é»‘åå•åŒ…
        """
        packages_path = f"{self.build_mockroot}/root/buildiso/Packages"
        package_file = ""
        black_exit_rpms = []
        for f in os.listdir(self.build_isos):
            if f.lower().endswith(f"-{self.target_arch}-packages.txt"):
                package_file = f"{self.build_isos}/{f}"
                break

        self.build_logger.info(f"{self.series} Packageåˆ—è¡¨ä¸º{package_file}")
        if self.params.get('mash_blacklist_path'):
            download_file(self.params.get('mash_blacklist_path'), dir_=self.build_cfg_dir, logger=self.build_logger)
            mash_blacklist = f"{self.build_cfg_dir}/{url_filename(self.params.get('mash_blacklist_path'))}"
            black_exit_rpms = check_rpm_name_blacklist(package_file, mash_blacklist, self.build_logger)
        if self.params.get("check_blacklist", True) and len(black_exit_rpms):
            self.build_logger.info(f"ã€Step-10/11ã€‘: é»‘åå•è½¯ä»¶åŒ…ä¸º {black_exit_rpms} ")
            raise BuildException("å«æœ‰é»‘åå•è½¯ä»¶")
        sensitive_exit_rpms = check_rpm_name_sensitive(package_file, SENSITIVE_KEYWORDS, self.build_logger)
        if self.params.get("check_sensitive", True) and len(sensitive_exit_rpms):
            self.build_logger.info(f"ã€Step-10/11ã€‘: æ•æ„Ÿè½¯ä»¶åŒ…ä¸º {sensitive_exit_rpms} ")
            self.tags.append("å«æœ‰æ•æ„Ÿè½¯ä»¶")

        for root, dirs, files in os.walk(packages_path):
            for f in files:
                file_name = os.path.join(root, f)
                if not file_name.endswith(".rpm") or not read_rpm_header(file_name):
                    raise BuildException(f"è½¯ä»¶åŒ…{file_name}ä¸å¯ç”¨ï¼Œæ— æ³•è¯»å–Headerä¿¡æ¯ã€‚")
        self.build_logger.info(f"{self.series} è½¯ä»¶åŒ…åç§°é»‘åå•ã€æ•æ„Ÿè¯ã€æ˜¯å¦æ˜¯æ­£ç¡®è½¯ä»¶åŒ… æ ¡éªŒé€šè¿‡")

    def checksum_iso(self):
        """05-ç”Ÿæˆisoçš„md5å€¼å’Œsha256å€¼"""
        isos_dir = self.build_isos
        iso_name = self.iso_name
        cmd1 = f"cd {isos_dir}; isohybrid -u {iso_name}" if self.target_arch == "x86_64" else f"echo 'å½“å‰æ¶æ„ {self.target_arch} ä¸æ¶æ„æ”¯æŒisohybrid' "
        cmd2 = f"cd {isos_dir}; implantisomd5 --force {iso_name}"
        cmd3 = f"cd {isos_dir}; checkisomd5 {iso_name}"
        cmd4 = f"cd {isos_dir}; sha256sum {iso_name} > {iso_name}.sha256sum"
        index = 0
        for cmd in [cmd1, cmd2, cmd3, cmd4]:
            ok = run_command(cmd, self.build_logger, f"ISOæ ¡éªŒå¤±è´¥: {cmd}")
            self.build_logger.info(f"ã€CMDã€‘ISOé›†æˆåå¤„ç†å·¥ä½œ\tå‘½ä»¤:{cmd}\tçŠ¶æ€:{ok}")
            if index != 0 and ok != 0:
                raise BuildException(f"ISOæ ¡éªŒå¤±è´¥: {cmd}")

        iso_sha256 = f"{isos_dir}/{iso_name}.sha256sum"
        if os.path.isfile(iso_sha256):
            sha256_str = open(iso_sha256, encoding="utf-8").read()
            if sha256_str:
                self._update_status("ISOæ ¡éªŒå®Œæˆ", self.process, None)
                return sha256_str.split(" ")[0]
            else:
                raise BuildException("isoæ ¡éªŒå¤±è´¥")
        else:
            raise BuildException(f"isoæ ¡éªŒå¤±è´¥,æ— sha256æ–‡ä»¶ï¼š{iso_sha256}")

    def create_package_list(self, package_dir, pkgs_list):
        """åˆ›å»ºisoå†…packagesåˆ—è¡¨"""
        into_chroot_env(self.build_mockroot)
        self._update_status(f"{self.series}åˆ›å»ºisoå†…packagesåˆ—è¡¨", self.process, None)
        args = rf"""find {package_dir} |grep "\.rpm$"|xargs rpm -qp --qf %{{N}}-%{{V}}-%{{R}}.%{{ARCH}}.rpm\\n """ \
               + f""" | sort > {pkgs_list}"""
        ok = run_command(args, error_message="åˆ›å»ºpackages-listå¤±è´¥")
        exit_chroot_env()
        self.command_logger.info(f'ã€CMDã€‘åˆ›å»ºisoå†…packagesåˆ—è¡¨\tå‘½ä»¤:{args}\tçŠ¶æ€:{ok}')
        self.pkg_list = pkgs_list

    def create_srcpackage_list(self, package_dir):
        """åˆ›å»ºisoå†…packagesåˆ—è¡¨"""
        into_chroot_env(self.build_mockroot)
        self._update_status(f"{self.series}åˆ›å»ºisoå†…src_packagesåˆ—è¡¨", self.process, None)
        args = rf"""find {package_dir} |grep "\.rpm$"|xargs rpm -qpi |grep "Source RPM" | awk '{{print $4}}' """ \
               + f""" | sort | uniq > {self.src_pkgs_list}"""
        run_command(args, error_message="åˆ›å»ºpackages-listå¤±è´¥")
        exit_chroot_env()

    def create_package_sum(self, package_dir, pkgsum):
        """åˆ›å»ºisoå†…packages-sha256sumåˆ—è¡¨"""
        into_chroot_env(self.build_mockroot)
        md5_args = f"sha256sum `find {package_dir} |grep rpm` > {pkgsum}"
        run_command(md5_args, error_message="åˆ›å»ºsha256sum-listå¤±è´¥")
        exit_chroot_env()

    def create_env_package_list(self):
        """åˆ›å»ºisoå†…packages-sha256sumåˆ—è¡¨"""
        self.generate_build_env_info()
        env_pkg_args = f"rpm -qa >>  {self.build_env}"
        run_command(env_pkg_args, self.build_logger, "æŸ¥è¯¢å®¿ä¸»æœºç¯å¢ƒè½¯ä»¶åŒ…å¤±è´¥")

    def gen_suffix_sqlite_fp(self, repo_url):
        """
        æ ¹æ®ä»“åº“çš„è·¯å¾„ï¼Œä¸å¸¦repodataé‚£ä¸€çº§åˆ«ï¼Œè·å– primary.sqlite.xx çš„å‹ç¼©åç¼€
        Args:
            repo_url: xx/

        Returns:
            xz | bz2 | gz2
        """
        suffix = ""
        repo_url = repo_url.replace(FILE_SCHEMA, '') + REPODATA_PATH
        if repo_url.startswith("http"):
            response = send_request(repo_url, verify=False)
            url = ""
            if response.status_code == 200:
                url = re.compile(URL_REPODATA_SQLITE).findall(response.text)
            assert url
            suffix = url[0].split(".")[-1]
            repo_url += url[0]
        elif os.path.exists(repo_url):
            for fn in os.listdir(repo_url):
                if fn.find("primary.sqlite") >= 0:
                    suffix = fn.split(".")[-1]
                    repo_url = repo_url.replace(FILE_SCHEMA, '') + fn
                    break
        else:
            self.build_logger.error(
                f"è·å–primary.sqliteçš„å‹ç¼©åç¼€å¤±è´¥ï¼Œåè®®ä¸è¢«æ”¯æŒã€‚ repo_url: {repo_url} {os.path.isdir(repo_url)}")
        return suffix, repo_url

    def create_repo_package_list(self, repo_url, file_path):
        self.build_logger.info(f"é€šè¿‡repoï¼š{repo_url}, ç”Ÿæˆpkglsitï¼š{file_path}")
        try:
            suffix, _ = self.gen_suffix_sqlite_fp(repo_url)
            if not suffix:
                raise BuildException(f"repo_url ï¼š {repo_url} æ²¡æœ‰dbæ–‡ä»¶")
            self.compress_sqlite_name = self.compress_sqlite_name + "." + suffix
            self.build_logger.info(f"æ•°æ®åº“ä½ç½®æ˜¯ï¼š{self.compress_sqlite_name}")
            yum_package_sum = self.get_package_list_and_sum(repo_url, suffix)

            if yum_package_sum:
                with open(file_path, "w") as txt:
                    txt.write(f"yumæºåœ°å€ä¸ºï¼š{repo_url}\n")
                    for pak in yum_package_sum.keys():
                        txt.write(f"{pak}\t{yum_package_sum.get(pak)}\n")
            else:
                self.build_logger.warning("yumè½¯ä»¶åŒ…è®°å½•æ–‡ä»¶ç”Ÿæˆå¤±è´¥ã€‚")
        except Exception as e:
            traceback.print_exc()
            self.build_logger.error(f"åˆ›å»ºyumåŒ…åˆ—è¡¨å¤±è´¥ï¼Œå¤±è´¥åŸå› ä¸º: {e}ï¼Œ repo_url ï¼š {repo_url}")

    def create_iso_package_list(self, package_url):
        """åˆ›å»ºisoå†…packagesåˆ—è¡¨"""
        self._update_status("åˆ›å»ºiso pkg list", self.process, None)
        self.create_repo_package_list(package_url, self.iso_pkgs_list)

    def create_yum_package_list(self, yum_url):
        """åˆ›å»ºisoå†…packagesåˆ—è¡¨"""
        self._update_status("åˆ›å»ºyum pkg list", self.process, None)
        self.create_repo_package_list(yum_url, self.yum_pkgs_list)

    def create_mash_package_list(self):
        """
        åˆ›å»ºmashæºpackagesåˆ—è¡¨
        éœ€è¦ä¼ å…¥mashæºåœ°å€
        """
        self._update_status("åˆ›å»ºmashæºpackageåˆ—è¡¨", self.process, None)
        mash_database_url = self.build_log + os.sep + "mash"
        self.create_repo_package_list(mash_database_url, self.mash_pkgs_list)

    def gen_sqlite_file(self, repo_path: str, suffix="bz2"):
        """

        Args:
            repo_path:
            suffix:

        Returns:

        """
        _, sqlite_fp = self.gen_suffix_sqlite_fp(repo_path)
        self.build_logger.info(f"è·å–è½¯ä»¶åŒ…sumçš„repoæ˜¯ï¼š{repo_path}, SQLITE: {sqlite_fp}")
        delete_dirs(self.compress_sqlite_name)
        delete_dirs(self.decompress_sqlite_name)
        try:
            if sqlite_fp.startswith("http"):
                r = send_request(sqlite_fp, verify=False)
                if r.status_code == 200:
                    open(self.compress_sqlite_name, "wb").write(r.content)
                else:
                    self.build_logger.info("repoæ•°æ®åº“ä¸‹è½½å¤±è´¥")
            elif repo_path.startswith(BUILD_PATH):
                self.compress_sqlite_name = sqlite_fp
            else:
                raise BuildException(f"repo_path:  {repo_path} çš„åè®®ä¸è¢«æ”¯æŒ")
        except Exception as e:
            traceback.print_exc()
            raise ConnectionError(f"ä¸‹è½½æºæ•°æ®åº“å¤±è´¥.Msg:{e}ã€‚Url : {repo_path}")
        if os.path.isfile(self.compress_sqlite_name):
            if suffix == "bz2":
                with bz2.BZ2File(self.compress_sqlite_name) as fr, open(self.decompress_sqlite_name, "wb") as fw:
                    shutil.copyfileobj(fr, fw)
            else:
                with lzma.open(self.compress_sqlite_name, 'rb') as input_file:
                    try:
                        with open(self.decompress_sqlite_name, 'wb') as output_file:
                            shutil.copyfileobj(input_file, output_file)
                            self.build_logger.info(f"DBæ–‡ä»¶è§£å‹æˆåŠŸï¼š{self.decompress_sqlite_name}")
                    except lzma.LZMAError as e:
                        os.remove(self.decompress_sqlite_name)
                        self.build_logger.error(f"SQLiteçš„å‹ç¼©æ ¼å¼ã€Œ{_}ã€ä¸è¢«æ”¯æŒï¼šã€Œ{e}ã€")
        else:
            self.build_logger.error(
                f"primary.sqliteæ–‡ä»¶ ã€Œ{self.compress_sqlite_name}ã€ä¸å­˜åœ¨ : {os.path.isfile(self.compress_sqlite_name)}")

    def get_package_list_and_sum(self, repo_path: str, suffix="bz2") -> dict:
        """
        æ ¹æ®ä¼ å…¥çš„primary.sqlite.bz2æå–å‡ºå¯¹åº”æºçš„packageåˆ—è¡¨,md5å€¼
        return åŒ…åˆ—è¡¨ä¸å¯¹åº”md5çš„å­—å…¸
        """
        self.gen_sqlite_file(repo_path, suffix)
        pkg_sum = {}
        cs = conn = None
        try:
            if not os.path.isfile(self.decompress_sqlite_name):
                return {}
            conn = sqlite3.connect(self.decompress_sqlite_name)
            cs = conn.cursor()
            cs.execute("SELECT pkgId,name,location_href FROM packages")
            for row in cs.fetchall():
                pkg_sum[row[2]] = row[0]
            return pkg_sum

        except Exception as e:
            if cs:
                cs.close()
            if conn:
                conn.close()
            raise ConnectionError(f"æ•°æ®åº“é“¾æ¥å¤±è´¥ï¼{e} ã€‚ {self.decompress_sqlite_name}")

    def clean_mock_env(self):
        if self.params.get("clean_env", 0):
            """06-æ¸…ç†mockæ„å»ºç¯å¢ƒ"""
            self.process = 90
            delete_dirs(f"/etc/mock/{self.build_mocktag}", self.build_logger)
            self._update_status("æ¸…ç†mockæ„å»ºç¯å¢ƒ", self.process, None)
            delete_dirs(f"/var/lib/mock/{self.build_mocktag}", self.build_logger)
            delete_dirs(f"/var/lib/mock/{self.build_mocktag}-bootstrap", self.build_logger)

    def clean_build_dir(self):
        """07-æ¸…ç†ç³»ç»Ÿæ„å»ºç¯å¢ƒ"""
        self._update_status(f"clean {self.build_path}", self.process, None)
        if os.path.isdir(self.build_path):
            shutil.rmtree(self.build_path)

    def fix_mock_env(self):
        # ä¿®å¤loraxå†™pkglistsæ—¶ï¼Œé‡åˆ°è½¯ä»¶åŒ…æ–‡ä»¶åå«ä¸­æ–‡é—®é¢˜,é3.6ç‰ˆæœ¬æ²¡æ­¤é—®é¢˜å—ã€‚ã€‚ã€‚ã€‚
        process = subprocess.Popen(PYTHON_MINOR_CMD, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                                   cwd='/usr/bin/')
        out = process.stdout.readline()
        minor = out.decode(encoding='utf-8', errors='ignore') if isinstance(out, bytes) else str(out)
        if not minor:
            raise RuntimeError("æ— æ³•è·å–mockå†… pythonç‰ˆæœ¬")
        fp = f"/usr/lib/python3.{minor.strip()}/site-packages/pylorax/treebuilder.py"
        if os.path.isfile(fp):
            code = open(fp, "r", encoding="utf-8").read()
            str_old = """with open(joinpaths(pkglistdir, pkgobj.name), "w") as fobj:"""
            str_new = """with open(joinpaths(pkglistdir, pkgobj.name), "w", encoding="utf-8") as fobj: # edit by kylin """
            code = code.replace(str_old, str_new)
            open(fp, "w", encoding="utf-8").write(code)
            print("ä¿®å¤loraxå†™pkglistså¸¦ä¸­æ–‡é—®é¢˜ã€‚")

    def prep_create_iso(self):
        """æ„å»ºè¿‡ç¨‹å‡†å¤‡å·¥ä½œï¼Œå‡½æ•°å…¥å£ï¼Œæ ¹æ®ç±»å¤šæ¬¡ç»§æ‰¿"""
        self.build_logger.info(f"ã€Step-02/11ã€‘: {self.series} æ„å»ºè¿‡ç¨‹å‡†å¤‡å·¥ä½œ")
        self.build_logger.info("å‡†å¤‡ISOé›†æˆç¯å¢ƒ")
        if self.init_mock_env():
            # è¾“å‡ºloraxç‰ˆæœ¬
            try:
                self.build_logger.info(f"Mockï¼ˆ{self.mock_cfg_file}ï¼‰ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸï¼Œæ£€æŸ¥loraxã€pungiæ˜¯å¦å®‰è£…ã€‚")
                into_chroot_env(self.build_mockroot)
                _, lorax_v = run_command_with_return("rpm -qa |grep lorax")
                _, pungi_v = run_command_with_return("rpm -qa |grep pungi")
                _, oemaker_v = run_command_with_return("rpm -qa |grep oemaker")
                # mockç¯å¢ƒä¿®æ”¹
                self.fix_mock_env()
                exit_chroot_env()
                self.command_logger.info(f'ã€INFOã€‘ç‰ˆæœ¬æ£€æŸ¥:loraxç‰ˆæœ¬{lorax_v}ï¼Œpungiç‰ˆæœ¬{pungi_v}')
                self.build_logger.info(
                    f"\nlorax version:\n{lorax_v}\npungi version:\n{pungi_v}\noemaker version:{oemaker_v}\n")
                self.build_logger.info("ISOé›†æˆç¯å¢ƒæ„å»ºæˆåŠŸã€‚")
            except Exception as e:
                traceback.print_exc()
                self.build_logger.info(f"ISOé›†æˆç¯å¢ƒæ„å»ºå¤±è´¥ï¼ŒåŸå› æ˜¯{e}")
                raise AssertionError(f"ISOé›†æˆç¯å¢ƒæ„å»ºå¤±è´¥ï¼ŒåŸå› æ˜¯{e}")
        else:
            raise AssertionError("Mockç¯å¢ƒåˆå§‹åŒ–å¤±è´¥")

    def generate_build_env_info(self):
        """ è®°å½•ä¸€äº›é¢å¤–ä¿¡æ¯ ipAddr, ç£ç›˜å æœ‰ç‡ä¿¡æ¯ï¼Œ cpuä½¿ç”¨ç‡ä¿¡æ¯ï¼Œ æœºå™¨æ—¶é—´ï¼Œ ç³»ç»Ÿä¿¡æ¯ """
        file_write(self.build_env, "\nç¼–è¯‘ç¯å¢ƒå¦‚ä¸‹ï¼š\n")
        content = ""
        content += "ipAddr : " + os.getenv("IP", "localhost") + "\n"
        content += "date : " + time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time())) + "\n"
        content += "systemInformation : " + os.popen('uname -a').read() + "\n"
        content += "cpuInformation : \n" + os.popen('lscpu').read() + "\n"
        file_write(self.build_env, content)

    def check_ks_and_comps(self):
        """æ£€æŸ¥compsé‡Œé¢çš„åŒ…æ˜¯å¦éƒ½åœ¨ksæ–‡ä»¶å†…"""
        self.build_logger.info("æ ¡éªŒcompsæ‰€éœ€è½¯ä»¶åŒ…æ˜¯å¦éƒ½åœ¨ksæ–‡ä»¶å†…")
        comps_pak_set = get_comps_list(self.comps_file_path)
        comps_pak_set = {x for x in comps_pak_set if x is not None}
        ks_pak_set = get_ks_list(self.ks_file_path)
        ks_pak_set = {x for x in ks_pak_set if x is not None}
        if all([ks_pak_set, comps_pak_set]):
            more = "\n".join(comps_pak_set - ks_pak_set)
            if not more:
                self.build_logger.info("compsæ‰€éœ€è½¯ä»¶åŒ…éƒ½åœ¨ksæ–‡ä»¶å†…")
            else:
                self.build_logger.error(f"compsæ‰€éœ€è½¯ä»¶åŒ…ä¸åœ¨ksæ–‡ä»¶å†…ï¼Œè¯·æ£€æŸ¥ï¼š\n{more}")
                open(self.build_warn_file, "a", encoding="utf-8").write(f"compsè½¯ä»¶åŒ…ä¸åœ¨ksæ–‡ä»¶çš„æœ‰ï¼š{more}\n")
                open(self.build_warn_file, "a", encoding="utf-8").write(">>>" * 20)
        else:
            if not ks_pak_set:
                self.build_logger.error("ksæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ã€‚")
                open(self.build_warn_file, "a", encoding="utf-8").write("ksæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ã€‚\n")
                open(self.build_warn_file, "a", encoding="utf-8").write(">>>" * 20)
            else:
                self.build_logger.error("compsæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ã€‚")
                open(self.build_warn_file, "a", encoding="utf-8").write("compsæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ã€‚\n")
                open(self.build_warn_file, "a", encoding="utf-8").write(">>>" * 20)

    def check_ks_and_package(self):
        """æ£€æŸ¥ksæ–‡ä»¶å’Œpackagesåˆ—è¡¨æ˜¯å¦ä¸€è‡´ """
        if all([os.path.isfile(self.ks_file_path),
                os.path.isfile(self.build_isos + os.sep + os.path.basename(self.pkg_list))]):
            file_ = open(self.ks_file_path, "r")
            ks_list = file_.read().split("\n")[2:]
            ks_list = list(filter(lambda x: len(x), ks_list))[:-1]
            ks_list = set(ks_list)
            file_.close()

            file_ = open(self.build_isos + os.sep + os.path.basename(self.pkg_list), "r")
            pkgs_list = file_.read().split("\n")
            pkgs_list = list(filter(lambda x: len(x), pkgs_list))
            pkgs_list = list(map(lambda x: common_split_filename(x)[0], pkgs_list))
            pkgs_list = set(pkgs_list)
            file_.close()
            if ks_list != pkgs_list:
                self.build_logger.error("KSæ–‡ä»¶å’Œè½¯ä»¶åŒ…åˆ—è¡¨æ ¡éªŒã€ä¸ã€‘é€šè¿‡ã€‚")
                more = "\n".join(ks_list - pkgs_list)
                try:
                    open(self.build_warn_file, "a", encoding="utf-8").write(
                        f"\nKSæ–‡ä»¶æ¯”è¾ƒè½¯ä»¶åŒ…åˆ—è¡¨ä¸ä¸€è‡´ï¼Œå¤šäº†ï¼š\n{more}\n")
                    open(self.build_warn_file, "a", encoding="utf-8").write(">>>" * 20)
                except Exception as e:
                    self.build_logger.error(f"è­¦å‘Šæ–‡ä»¶å†™å…¥å¤±è´¥ï¼Œå…·ä½“ä¿¡æ¯ï¼š{e}")
                self.build_logger.error(f"KSå¤šäº†ï¼š\n{more}")
                less = "\n".join(pkgs_list - ks_list)
                try:
                    open(self.build_warn_file, "a", encoding="utf-8").write(
                        f"\nKSæ–‡ä»¶æ¯”è¾ƒè½¯ä»¶åŒ…åˆ—è¡¨ä¸ä¸€è‡´ï¼Œå°‘äº†ï¼š\n{less}\n")
                    open(self.build_warn_file, "a", encoding="utf-8").write(">>>" * 20)
                except Exception as e:
                    self.build_logger.error(f"è­¦å‘Šæ–‡ä»¶å†™å…¥å¤±è´¥ï¼Œå…·ä½“ä¿¡æ¯ï¼š{e}")
                # self.build_logger.error(f"KSå°‘äº†ï¼š\n{less}")
            else:
                self.build_logger.info("KSæ–‡ä»¶å’Œè½¯ä»¶åŒ…åˆ—è¡¨æ ¡éªŒé€šè¿‡")

        else:
            self.build_logger.warning(
                f"ks:{self.ks_file_path}ã€‚pkg:{os.path.isfile(self.build_isos + os.sep + os.path.basename(self.pkg_list))}")
            self.build_logger.warning("ksæ–‡ä»¶æˆ–è€…è½¯ä»¶åŒ…åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•å¯¹åŒ…åˆ—è¡¨æ˜¯å¦ä¸€è‡´è¿›è¡Œæ£€æµ‹ã€‚")

    def copy_repodata(self):
        """
        å°†isoå†…çš„repodataæ•°æ®å¤åˆ¶åˆ°logs/repodataç›®å½•ä¸‹
        Returns:
        """
        repodata_path = f"{self.build_mockroot}/root/buildiso/repodata"
        self.build_logger.info(
            f"{self.series} æ”¶é›†repodataæ•°æ®,æºç›®å½• {repodata_path} å­˜åœ¨ï¼š{os.path.isdir(repodata_path)}")
        if not os.path.exists(repodata_path):
            os.makedirs(repodata_path)
        if not os.path.exists(self.build_log + os.sep + 'repodata'):
            os.makedirs(self.build_log + os.sep + 'repodata')
        self.build_logger.info(
            f"{self.series} æ”¶é›†repodataæ•°æ®,ç›®çš„ç›®å½• {self.build_log + os.sep}repodata å­˜åœ¨ï¼š"
            f"{os.path.isdir(self.build_log + os.sep + 'repodata')}")
        if os.path.isdir(repodata_path):
            copy_dirs(f"{repodata_path}", self.build_log + os.sep + 'repodata', self.build_logger)

    def copy_productinfo(self):
        """
        å°†mockç¯å¢ƒå†…çš„productinfoæ–‡ä»¶å¤åˆ¶åˆ°é…ç½®æ–‡ä»¶ç›®å½•ï¼Œå¹¶é‡å‘½åä¸ºproductinfo.txt
        Returns:

        """
        copy_dirs(self.product_file, self.build_cfg_dir + "/productinfo.txt")

    def check_repo_dep(self):
        """
        æ ¡éªŒ repodata ä¾èµ–æ˜¯å¦æ»¡è¶³
        Returns:
        """
        iso_err_msg = ISOErrorEnum.ISO_REPOCLOSURE
        fp_iso_repoclosure = self.build_log + os.sep + 'iso_repoclosure.txt'

        try:
            cmd3 = f'mock -n -r {self.build_mocktag} "dnf repoclosure --arch={self.target_arch} --arch=noarch --repofrompath=MashRepoDepCheck,{self.mash_httpd_path} --repo=MashRepoDepCheck --check=MashRepoDepCheck " ' \
                   f' --chroot --enable-network '
            cmd4 = f"su pungier -c '{cmd3}' "
            self.build_logger.info(f"æ‰§è¡Œå‘½ä»¤ï¼š{cmd4}")
            res = rum_command_and_log(cmd4, self.build_log + os.sep + 'mash_repoclosure.txt', self.build_logger)
            self.command_logger.info(f'ã€CMDã€‘æ£€æµ‹Mashä»“åº“æ˜¯å¦ç¼ºå¤±ä¾èµ–\tå‘½ä»¤:{cmd4}\tçŠ¶æ€:{res}')
            if not res:
                self.tags.append(ISOErrorEnum.BASE_REPOCLOSURE)
        except Exception as e2:
            self.build_logger.error(f"Mash æ£€æŸ¥ repodata ä¾èµ–æ»¡è¶³æƒ…å†µ å¤±è´¥, {e2}")

        try:
            self.build_logger.info("ISO æ£€æŸ¥ repodata ä¾èµ–æƒ…å†µ")
            cmd = f'mock -n -r {self.build_mocktag} "dnf repoclosure --arch={self.target_arch} --arch=noarch --repofrompath=ISORepoDepCheck,file:///root/buildiso/ --repo=ISORepoDepCheck --check=ISORepoDepCheck" ' \
                  f' --chroot --enable-network '
            cmd = f"su pungier -c '{cmd}' "
            self.build_logger.info(f"æ‰§è¡Œå‘½ä»¤ï¼š{cmd}")
            res = rum_command_and_log(cmd, fp_iso_repoclosure, self.build_logger)
            self.command_logger.info(f'ã€CMDã€‘æ£€æµ‹ISOæ˜¯å¦ç¼ºå¤±ä¾èµ–\tå‘½ä»¤:{cmd}\tçŠ¶æ€:{res}')
            if not res:
                self.tags.append(iso_err_msg)
        except Exception as e1:
            self.tags.append(iso_err_msg)
            self.build_logger.error(f"{iso_err_msg}, {e1} " + "\n" + open(fp_iso_repoclosure, 'r').read())
            raise BuildException(iso_err_msg)

    def check_lorax_log(self):
        txt = open(f"{self.build_log}/lorax/pylorax.log").read()
        err_pkgs = set(list(re.findall(r"Error in .* in rpm package (.*)", txt)))
        if err_pkgs:
            self.tags.append(f"Loraxå¼‚å¸¸åŒ…: {','.join(err_pkgs)}")

    def get_nkvers_info(self):
        """
        è·å–ç³»ç»Ÿçš„nkversä¿¡æ¯
        Returns:
        """
        try:
            self.build_logger.info(f"{self.series} è·å–ç³»ç»Ÿçš„nkversä¿¡æ¯")
            cmd1 = f'mock -n -r {self.build_mocktag} --shell "nkvers" '
            cmd1 = f"su pungier -c '{cmd1}' "
            run = run_command_with_return(cmd1, self.build_logger, "è·å–nkverså¤±è´¥")
            if run[0]:
                self.nkvers = run[1].decode("utf-8")
                self.build_logger.info(f"nkversä¿¡æ¯æ˜¯ï¼š\n{self.nkvers}")
        except Exception as e1:
            self.build_logger.error(f"è·å–ç³»ç»Ÿnkversä¿¡æ¯å¤±è´¥, {e1}")

    def get_repo_create_time(self):
        """
        è·å–ä»“åº“åˆ›å»ºæ—¶é—´
        """
        mash_log_file = f'{self.build_log}/mash-*.log'

        # å®šä¹‰è¦æŸ¥æ‰¾çš„å…³é”®å­—
        keywords = ["Mashå¼€å§‹,Tag"]

        # å®šä¹‰æ—¶é—´æˆ³çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        time_pattern = r'(\d{6} \d{2}:\d{2}:\d{2})'

        try:
            # æŸ¥æ‰¾å¹¶å¤„ç†æ‰€æœ‰åŒ¹é…çš„æ—¥å¿—æ–‡ä»¶
            for log_file in glob.glob(mash_log_file):
                with open(log_file, 'r') as file:
                    for line in file:
                        for keyword in keywords:
                            if line.find(keyword) >= 0:
                                time_stamp = extract_time_from_line(line, time_pattern)
                                if time_stamp:
                                    time_array = time.strptime("20" + time_stamp, "%Y%m%d %H:%M:%S")
                                    self.repo_create_time = time.strftime("%Y-%m-%d %H:%M:%S", time_array)
                                    self.build_logger.info(f"è·å–ä»“åº“åˆ›å»ºæ—¶é—´æ˜¯,{self.repo_create_time}")
                                    break
        except Exception as e1:
            traceback.print_exc()
            self.build_logger.error(f"è·å–ä»“åº“åˆ›å»ºæ—¶é—´å¤±è´¥,{e1}")

    def copy_lorax_log(self):
        """
        å°†loraxç›®å½•ä¸‹çš„ logã€confã€txt å¤åˆ¶åˆ°logs/loraxç›®å½•ä¸‹
        Returns:

        """
        lorax_path = f"{self.build_mockroot}/lorax"
        self.build_logger.info(f"{self.series} æ”¶é›†loraxæ—¥å¿—,æºç›®å½• {lorax_path} å­˜åœ¨ï¼š{os.path.isdir(lorax_path)}")
        if not os.path.isdir(self.build_log + os.sep + "lorax"):
            os.makedirs(self.build_log + os.sep + "lorax")
        self.build_logger.info(
            f"{self.series} æ”¶é›†loraxæ—¥å¿—,ç›®çš„ç›®å½• {self.build_log + os.sep}lorax å­˜åœ¨ï¼š"
            f"{os.path.isdir(self.build_log + os.sep + 'lorax')}")
        if os.path.isdir(lorax_path):
            copy_dirs(f"{lorax_path}", self.build_log + os.sep + 'lorax', self.build_logger)
            delete_dirs(self.build_log + os.sep + 'lorax/outfiles/', self.build_logger)

    def iso_file_check(self):
        """
        æ ¡éªŒISOæ–‡ä»¶æ˜¯å¦ç¬¦åˆè¦æ±‚
            ä¸èƒ½åŒ…å«debuginfoã€debugsourceã€sourceè½¯ä»¶åŒ…
            æ˜¯å¦éƒ½ç­¾å
        Returns:

        """
        for fp, fn in get_file_list(self.build_isos):
            if fn.endswith(".rpm") and (
                    fn.find("-debuginfo-") >= 0 or fn.find("-debugsource-") >= 0 or fn.endswith(".src.rpm")):
                self.build_logger.warning(f"ISOæ–‡ä»¶ä¸­åŒ…å«debuginfoã€debugsourceã€sourceè½¯ä»¶åŒ…ï¼Œæ–‡ä»¶åï¼š{fn}")
                open(self.build_warn_file, "a", encoding="utf-8").write(f"æ–‡ä»¶ä¸åº”è¯¥å­˜åœ¨ï¼š{fn}\n")
                open(self.build_warn_file, "a", encoding="utf-8").write(">>>" * 20)
            sig = get_rpm_sign(fn + os.sep + fp)
            if fn.endswith(".rpm") and sig.find(SIG_KEY) < 0:
                open(self.build_warn_file, "a", encoding="utf-8").write(f"è½¯ä»¶åŒ…ç­¾åä¸å¯¹ï¼š{fn}\n")
                open(self.build_warn_file, "a", encoding="utf-8").write(">>>" * 20)

    def get_lorax_templates_sha256sum(self) -> str:
        if self.lorax_templates_url.startswith("http"):
            fp = self.build_cfg_dir + self.lorax_templates_url.split("/")[-1]
            v = get_file_sha256sum(fp)
            self.lorax_templates_sha256sum = v
            self.build_logger.info(f"è·å–lorax_templates_sha256sumçš„å€¼ï¼š{v}")
            cmd = f"sha256sum {fp} > {fp}.sha256sum"
            run_command(cmd, self.build_logger, "lorax_templates_sha256sumæ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼")
            return v
        else:
            self.build_logger.info(f"lorax_templates_sha256sumçš„å€¼ä¸ºç©ºï¼Œå› ä¸ºä¸æ˜¯urlï¼š{self.lorax_templates_url}")
            return ""

    def post_create_iso(self):
        """#11 æ„å»ºè¿‡ç¨‹å®Œæˆåæ•´ç†å·¥ä½œï¼Œå‡½æ•°å…¥å£ï¼Œæ ¹æ®ç±»å¤šæ¬¡ç»§æ‰¿"""
        iso_name = self.find_iso_name()
        iso_size = self.get_iso_size()
        sha256_value = self.checksum_iso()
        self.build_logger.info(f"ã€Step-11/11ã€‘: {self.series} æ„å»ºè¿‡ç¨‹å®Œæˆåçš„æ•´ç†å·¥ä½œ")
        self.check_ks_and_package()
        self.check_ks_and_comps()
        self.check_rpm()  # å¯¹packageè¿›è¡Œæ£€æµ‹
        if all([self.mock_cfg_file, self.build_cfg_dir]):
            self.build_logger.info(f"mockå†…çš„é…ç½®æ–‡ä»¶({self.mock_cfg_file})å¤åˆ¶åˆ°buildç›®å½•:{self.build_cfg_dir}")
            shutil.copy(self.mock_cfg_file, self.build_cfg_dir)
        else:
            self.build_logger.warning(f"Mocké…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥ã€‚ {self.mock_cfg_file} && {self.build_cfg_dir}")
        self.copy_lorax_log()
        self.check_lorax_log()
        self.copy_repodata()
        self.get_nkvers_info()
        self.copy_productinfo()
        self.get_repo_create_time()
        self.release_resource()
        self.clean_mock_env()
        self.get_lorax_templates_sha256sum()
        self.process = 100
        self.dracut = self.get_dracut_params()
        self._update_status("ğŸ‰ğŸ§¨æœ¬æ¬¡ISOé›†æˆæ„å»ºå·¥ä½œå®ŒæˆğŸ§¨ğŸ‰", self.process, None)
        self.repos = get_repos_info_rpm(base_path=self.build_mockroot, pungi=False)
        return iso_name, iso_size, sha256_value, self.nkvers, self.tags, self.repos, self.lorax_templates_sha256sum, \
            self.repo_create_time, f'''{self.dracut}'''

    def create_manifest(self):
        """
        åˆ›å»ºISOçš„manifestæ–‡ä»¶
        Returns:

        """
        self.build_logger.info(f"åˆ›å»ºISOçš„manifestæ–‡ä»¶ {self.build_isos}/{self.iso_name}.manifest")
        cmd = f"isoinfo -R -f -l -i {self.build_isos}/{self.iso_name} | grep -v '/TRANS.TBL$' | sort >> {self.build_isos}/{self.iso_name}.manifest"
        run_command(cmd, self.build_logger, "manifestæ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼")

    def download_post_action(self, u):
        # ä¸‹è½½ kylin-post-actions æ–‡ä»¶
        self.build_logger.info("å°è¯•ä¸‹è½½ post-action æ–‡ä»¶")

        post_files = ["/.kylin-post-actions-nochroot", "/.kylin-post-actions", "/.discinfo", "/.kyinfo"]
        for file in post_files:
            url = u.rstrip("/") + file
            if send_request(url, verify=False, method="HEAD").status_code == 200:
                ok = download_file(url, self.not_boot_file, logger=self.build_logger)
                if not ok:
                    raise RuntimeError(f"ä¸‹è½½{file}æ–‡ä»¶å¤±è´¥")
                self.build_logger.info(f"ä¸‹è½½{file}æ–‡ä»¶æˆåŠŸ: {url}->{self.not_boot_file}")
            else:
                self.build_logger.debug(f"{file} æ–‡ä»¶ä¸å­˜åœ¨")

        # è€V10 x86æ¶æ„æ”¯æŒ
        post_dir_files = ["/.post/fonts-gb18030.sh", "/.post/runatinstall", "/.post/runatroot"]
        for one in post_dir_files:
            url = u.rstrip("/") + one
            if send_request(url, verify=False, method="HEAD").status_code == 200:
                ok = download_file(u + one, self.not_boot_file + ".post/", logger=self.build_logger)
                if not ok:
                    raise RuntimeError(f"ä¸‹è½½{one}æ–‡ä»¶å¤±è´¥")
                self.build_logger.info(f"ä¸‹è½½{one}æ–‡ä»¶æˆåŠŸ: {url} -> {self.not_boot_file}")
            else:
                self.build_logger.debug(f"{one} æ–‡ä»¶ä¸å­˜åœ¨ {url}")

    def check_iso_file(self):
        """
        ISOé›†æˆåçš„è‡ªæ£€
        Returns:

        """
        iso_path = os.path.join(self.build_isos + os.sep + self.iso_name)
        if os.path.isfile(iso_path):
            build_warn_info = []

            # æ ¡éªŒISOæ–‡ä»¶æ˜¯å¦æ˜¯ISOæ ¼å¼æ–‡ä»¶
            cmd = f"isoinfo -d -i {iso_path} | grep ISO"
            if run_command(cmd, self.build_logger, error_message="") != 0:
                build_warn_info.append("ISOæ–‡ä»¶éISOæ ¼å¼æ–‡ä»¶")
                self.tags.append("éISO")

            if self.target_arch == "x86_64" and not is_isohybrid(iso_path):
                build_warn_info.append("x86 ISOæœªå¯ç”¨isohybridï¼")
                self.tags.append("æœªå¯ç”¨isohybrid")
            if os.path.getsize(iso_path) < 500 * 1024 * 1024:  # æ ¡éªŒISOå¤§å°æ˜¯å¦è¶…è¿‡500M
                build_warn_info.append("ISOæ–‡ä»¶å¤§å°å°äº500M")
                self.tags.append("å¤§å°å¼‚å¸¸")

            # å¯¹äºä¸ç¬¦åˆçš„æƒ…å†µï¼Œå†™åˆ°WARNINGæ–‡ä»¶ä¸­
            reason = "\n".join(build_warn_info)
            open(self.build_warn_file, "a", encoding="utf-8").write(f"{reason}\n")
            self.build_logger.info(f"ISOæ–‡ä»¶ {self.iso_name} æ ¡éªŒé€šè¿‡")
        else:
            self.build_logger.error(f"ISOæ–‡ä»¶ï¼š{self.iso_name} ä¸å­˜åœ¨")

    def download_not_boot_file(self):
        """Step-5.5"""
        self.build_logger.info("å¼€å§‹ä¸‹è½½ éå¯åŠ¨æ–‡ä»¶")
        for u in self.params.get('not_boot_dir').split("\n"):
            if u and u.strip():
                wget_remote_dirs(self.build_logger, u, self.not_boot_file)
                self.download_post_action(u)
        copy_dirs(self.not_boot_file, f"{self.build_mockroot}{self.build_inmock}", logger_=self.build_logger)
        self.command_logger.info(
            f'ã€MOVEã€‘å°† {self.not_boot_file} ä¸‹çš„æ–‡ä»¶å¤åˆ¶åˆ° {f"{self.build_mockroot}{self.build_inmock}"}')
        self.check_addon_dep()

    def check_addon_dep(self):
        self.gen_addon_repodata()
        iso_err_msg = ISOErrorEnum.BASE_REPOCLOSURE
        fp_addon_repoclosure = self.build_log + os.sep + 'addon_repoclosure.txt'
        try:
            self.build_logger.info("ISO æ£€æŸ¥ å†…è½¯ä»¶åŒ… ä¾èµ–æƒ…å†µ")
            cmd1 = f'mock -n -r {self.build_mocktag} "dnf repoclosure --arch={self.target_arch} --arch=noarch --repofrompath=AddonRepoDepCheck,file://{self.addon_repodata}  --repo=AddonRepoDepCheck --check=AddonRepoDepCheck" ' \
                   f' --chroot --enable-network '
            cmd2 = f"su pungier -c '{cmd1}' "
            self.build_logger.info(f"æ‰§è¡Œå‘½ä»¤ï¼š{cmd2}")
            if not rum_command_and_log(cmd2, fp_addon_repoclosure, self.build_logger):
                self.tags.append(iso_err_msg)
        except Exception as e1:
            self.tags.append(iso_err_msg)
            self.build_logger.error(f"{iso_err_msg}, {e1} " + "\n" + open(fp_addon_repoclosure, 'r').read())

    def gen_addon_repodata(self):
        try:
            self.build_logger.info(f"ã€Step-5.5/11ã€‘: {self.series} createrepoåˆ›å»º addon repoæº")
            self.process = 60
            self._update_status(f"{self.series} createrepoåˆ›å»ºaddon repoæº", self.process, None)
            into_chroot_env(self.build_mockroot)
            cmd = f'mkdir {self.addon_repodata} && cd {self.build_inmock} && createrepo -d -g "/root/{self.comps_file}" --outputdir {self.addon_repodata} {self.build_inmock}'
            ok = run_command(cmd, error_message=f"{self.series} createrepoå¤±è´¥")
            if ok != 0:
                raise RuntimeError(f"{self.series} [{self.task_id[:4]}] createrepoå¤±è´¥")
            exit_chroot_env()
            self._update_status("addon createrepo æ£€æµ‹å®Œæˆï¼", self.process, None)
        except Exception as e:
            self.build_logger.error(f"ã€Step-5.5/11ã€‘: {self.series} createrepoåˆ›å»ºaddon repoæºå¤±è´¥.{e}")

    def write_productinfo2file(self, product_file):
        """ #7 7/8/SP ç³»-ç”Ÿæˆ.productinfoæ–‡ä»¶"""
        product_file = product_file or self.product_file
        self.process = 70
        self.build_logger.info(f"ã€Step-07/11ã€‘: {self.series} mockå†…ç”Ÿæˆäº§å“ä¿¡æ¯æ–‡ä»¶")
        self._update_status(f"{self.series} mockå†…ç”Ÿæˆäº§å“ä¿¡æ¯æ–‡ä»¶", self.process, None)
        info = self.params.get('product_info')

        # å®¢æˆ·ç«¯è‡ªå®šä¹‰
        if info:
            with open(product_file, 'w') as f:
                f.write(self.params.get('product_name'))
                f.write('\n')
                f.write(info)
                f.write(os.linesep)
        elif os.path.isfile(f"{self.build_mockroot}/etc/.productinfo"):
            # è¯»å–kylin-releaseå†…productinfo
            shutil.copy(f"{self.build_mockroot}/etc/.productinfo", product_file)
            self.build_logger.info("==ç§»åŠ¨kylin-releaseçš„productinfoæ–‡ä»¶")
        else:
            # å®¢æˆ·ç«¯æœªå®šä¹‰ä¸”kylin-releaseä¹Ÿæ²¡æœ‰ï¼Œè‡ªåŠ¨ç”Ÿæˆ
            info = '/'.join([self.params.get('release'), f"{self.params.get('target_arch')}-{self.params.get('build')}",
                             self.cur_day])
            with open(product_file, 'w') as f:
                f.write(self.params.get('product_name'))
                f.write('\n')
                f.write(info)
                f.write(os.linesep)

    def release_resource(self):
        """é›†æˆåé‡Šæ”¾èµ„æº"""
        for i in open("/proc/mounts", "r"):
            if i.find(self.build_mocktag) >= 0:
                mount_path = i.split(" ")[1]
                cmd = f"umount {mount_path}"
                ok, err = run_command_with_return(cmd)
                self.build_logger.info(f"é›†æˆåumount loraxèµ„æºï¼š{ok} {err} ON {mount_path}")

    def insert_grub_ks_cmd(self, root_path):
        """æ’å…¥ksæ–‡ä»¶è‡ªåŠ¨å®‰è£…æŒ‡ä»¤"""
        install_ks_url = self.params.get("ks_install_url")
        if install_ks_url:
            self.build_logger.info(f"ã€æ— äººå€¼å®ˆã€‘æ’å…¥ksæ–‡ä»¶è‡ªåŠ¨å®‰è£…æŒ‡ä»¤ï¼š{install_ks_url}")
            ks_name = "." + install_ks_url.split("/")[-1]
            wget.download(install_ks_url, out=root_path + os.sep + ks_name, bar=None)
            insert_res = insert_install_ks(root_path, ks_name, self.build_logger, self.params.get("target_arch"))
            if not insert_res:
                raise BuildException(f"æ ¹æ®{install_ks_url} æ–°å¢å…¥grubæ–‡ä»¶å¤±è´¥")


def match_build_id(n):
    # build**
    match = re.findall(r"-(build\d+)-", n.lower())
    if not match:
        # alpha-**
        match = re.findall(r"-(alpha-?\d+)-", n.lower())
    return match[0] if match else ""


def extract_arch_from_path(path) -> str:
    """
     ä»è·¯å¾„ä¸­æå–æ¶æ„
    Args:
        ipaddress:è·¯å¾„ ä¾‹å¦‚ï¼š/opt/integration_iso_files/pungi-kylin/Kylin-Server-11-1215_1455-83.97/compose/Server/sw_64/iso/xxxxxx.iso
    Returns: æ¶æ„ ä¾‹å¦‚ï¼šsw_64

    """
    path_obj = Path(path)
    parts = path_obj.parts
    result = ""
    arch_map = {
        "ARM64": "aarch64",
        "X86": "x86_64",
        "Loongarch64": "loongarch64",
    }
    arch_list = ["aarch64", "x86_64", "loongarch64", "sw_64", "ARM64", "X86", "Loongarch64"]
    for i, part in enumerate(parts):
        for arch in arch_list:
            if arch in part:
                result = arch
                break
        if result:
            break
    if not result:
        result = find_arch_by_name(path)
    if result in arch_map:
        return arch_map[result]
    return result


def iso_upload(params, work_dir, pattern, task_id, pungi_koji,
               upload_result_url="/iso-manager/server-api/iso-manager/isos"):
    iso_filepaths = []
    files = get_file_list(work_dir, exclude_keywords=['net', 'boot', 'source'])
    server_api = upload_result_url.split('api')[0]
    for fp, file in files:
        if file.endswith(".iso"):
            iso_filepaths.append(fp + os.sep + file)
    if not iso_filepaths:
        data = {
            **params,
            "task_id": task_id,
            "task_status": states.FAILURE,
            "ipaddress": HOST_IP,
            "isoname": "",
            "isosize": 0,
            "isolog": pungi_koji.path.log.topdir().replace(BUILD_PATH, '') if pungi_koji.path.log.topdir().startswith(
                BUILD_PATH) else pungi_koji.path.log.topdir(),
            'repo_path': pungi_koji.path.work.pkgset_repo(),
            "sha256": None,
            "err_msg": "",
            "md5": "",
            "work_dir": work_dir,
            "upload_result_url": upload_result_url,
            'is_pungi': True,
            "tag": pungi_koji.conf.get("pkgset_koji_tag"),
            "build_id": pungi_koji.build_id or match_build_id(pungi_koji.conf.get("image_name_format").get("^Server")),
            "repos": pungi_koji.repos,
            "dracut": pungi_koji.dracut
        }
        logger.info(f"å›ä¼ æ•°æ®ï¼š{data}")

        r = send_request(upload_result_url, data=data, method="POST")
        logger.info(f"{work_dir} å›ä¼ ç»“æœï¼š{r.text}")
        return {'current': 100, 'total': 100, 'status': 'Task Failed!',
                'result': {"work_dir": work_dir, "msg": "æ²¡æœ‰ISOæ–‡ä»¶æˆ–è€…sha256æ–‡ä»¶"}}

    for iso_filepath in iso_filepaths:
        isoname = iso_filepath.replace(BUILD_PATH, '')
        isosize = get_file_size(iso_filepath)
        # ç”Ÿæˆæ–‡ä»¶sha256ä¿¡æ¯
        generate_sha256sum(iso_filepath)
        # ç”Ÿæˆreposä¿¡æ¯
        arch = extract_arch_from_path(iso_filepath)
        pungi_koji.repos = get_repos_info_rpm(base_path=work_dir, pungi=True, arch=arch)
        # ç”Ÿæˆdracutå‚æ•°
        pungi_koji.dracut = get_drauct_augment_pungi(iso_filepath)
        # ç”Ÿæˆé•œåƒå†…è½¯ä»¶åŒ…åˆ—è¡¨ä¿¡æ¯
        generate_packages_list(iso_filepath, arch)
        # ç”Ÿæˆä»“åº“ç”Ÿæˆæ—¶é—´
        repo_create_time = get_repo_create_time(iso_filepath)
        if os.path.isfile(iso_filepath) and os.path.isfile(iso_filepath + EXT_SHA256SUM):
            sha256 = open(iso_filepath + EXT_SHA256SUM).read().split(" ")[0]
            arch = get_base_arch(isoname)
            # ISOå›ä¼ 
            data = {
                **params,
                "task_id": task_id,
                "task_status": states.SUCCESS,
                "ipaddress": HOST_IP,
                "isoname": isoname,
                "mash_httpd": server_api + 'integration_iso_files' + isoname.split("Server/")[0] + f"Everything/{arch}/",
                "isosize": isosize,
                "isolog": isoname.split("compose")[0] + "logs/",
                "sha256": sha256,
                "err_msg": "",
                "md5": sha256,
                "work_dir": work_dir,
                "upload_result_url": upload_result_url,
                'is_pungi': True,
                'target_arch': arch,
                'tag': pungi_koji.conf.get('pkgset_koji_tag'),
                "csrf": uuid.uuid4().hex,
                "compose_type": pungi_koji.conf.get("compose_type"),
                "koji_ip": KOJI_SERVER_PORT[pungi_koji.conf.get("koji_profile").split("_")[-1]],
                "repos": pungi_koji.repos,
                "dracut": pungi_koji.dracut,
                "repo_create_time": repo_create_time
            }
            logger.info(f"å›ä¼ æ•°æ®ï¼š{data}")
            r = send_request(upload_result_url, method="POST", data=data)
            logger.info(f"ISO: {iso_filepath} å›ä¼ ç»“æœï¼š{r.text}")
        else:
            print(f"æ²¡æœ‰ISOæ–‡ä»¶æˆ–è€…sha256æ–‡ä»¶ï¼Œ{iso_filepath} . {iso_filepath + EXT_SHA256SUM}")
            raise exceptions.Ignore(f"æ²¡æœ‰ISOæ–‡ä»¶æˆ–è€…sha256æ–‡ä»¶ï¼Œ{iso_filepath} . {iso_filepath + EXT_SHA256SUM}")


def generate_sha256sum(iso_filepath):
    cmd = f"sha256sum {iso_filepath} > {iso_filepath}.sha256sum"
    run_command(cmd)


def generate_packages_list(iso_filepath, arch):
    rpm_packages_file = iso_filepath.replace('.iso', f'-{arch}-packages.txt')
    srpm_packages_file = iso_filepath.replace('.iso', '-src_packages.txt')
    repodata_path = os.path.join(Path(iso_filepath).parent.parent, 'os')  # å¯»æ‰¾osæ–‡ä»¶è·¯å¾„

    if not os.path.exists(repodata_path):
        return False, 'repodataè·¯å¾„ä¸å­˜åœ¨'
    try:
        rim = load_repodata(repodata_path)
        rpms = rim.get_rpm_fullname_list_all()
        srpms = rim.get_srpm_fullname_list_all()
        if rpms:
            with open(rpm_packages_file, 'w') as f: f.write('\n'.join(rpms))
        if srpms:
            with open(srpm_packages_file, 'w') as f: f.write('\n'.join(srpms))
        return True, 'è½¯ä»¶åŒ…æ•°æ®è·å–æˆåŠŸ'
    except Exception as e:
        return False, e


def get_repo_create_time(iso_filepath):
    pungi_log = os.path.join(iso_filepath[:iso_filepath.find('compose')], 'logs/global/pungi.global.log')  # å¯»æ‰¾logæ–‡ä»¶è·¯å¾„
    repo_time = None
    with open(pungi_log, 'r') as f:
        lines = f.readlines()
        for line in lines:
            if line.find('Getting koji event') > 0:  # ä»logæ—¥å¿—è·å–koji eventæ—¶é—´
                repo_time = line.split(' ')[:2]
                repo_time = datetime.strptime(' '.join(repo_time), "%Y-%m-%d %H:%M:%S")
                break
        return repo_time


def get_drauct_augment_pungi(iso_filepath) -> str:
    """
    è·å–ç³»ç»Ÿçš„drauctä¿¡æ¯
    Returns:
    """
    initrd_file = "/images/pxeboot/initrd.img"
    temp_dir = tempfile.mkdtemp()
    cmd = ['isoinfo', '-i', iso_filepath, '-x', initrd_file]
    try:
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if result.returncode == 0:
            with open(f'{temp_dir}/initrd.img', 'wb') as f:
                f.write(result.stdout)
        initrd_fp = f'{temp_dir}/initrd.img'
        drauct = run_lsinitrd(initrd_fp)
        return drauct
    except Exception as e:
        return f"è·å–drauctæ¶ˆæ¯å¤±è´¥,{str(e)}"
    finally:
        shutil.rmtree(temp_dir)


def get_data_from_build_cfg(_iso, WEB_URL, _logger=logger):
    """
    é€šè¿‡ISOï¼Œè·å–ISOçš„æ„å»ºä¿¡æ¯
    Args:
        _logger:
        WEB_URL:
        _iso:

    Returns:
        {
            "mash-repo"ï¼š"",
            "release":"",
            "build":"",
            "yum_url":"",
        }
    """
    _build_info = {}
    try:
        work_dir = ROOT_PATH_ISO_PATH + _iso.isoname[:_iso.isoname.rfind("/") + 1]
        path_build_cfg = os.path.join(work_dir, "conf", "build.cfg.txt")
        if os.path.exists(path_build_cfg):
            _logger.info(f"é…ç½®æ–‡ä»¶: {str(path_build_cfg)}")
            with open(path_build_cfg, encoding="utf-8") as f:
                for line in f.readlines():
                    re_findall = re.findall(BUILD_PARAMS, line)
                    for re_find in re_findall:
                        if len(re_find) == 2:
                            _build_info[re_find[0].strip('\'",')] = re_find[1].strip('\'",')
        else:
            conf_file = ROOT_PATH_ISO_PATH + _iso.isoname.split("compose")[0] + "logs/global/config-copy/kylin.conf"
            if os.path.exists(conf_file):
                _logger.info(f"é…ç½®æ–‡ä»¶: {str(path_build_cfg)}")
                with open(path_build_cfg, encoding="utf-8") as f:
                    for line in f.readlines():
                        if line.startswith("#"):
                            continue
                        line = line.split("#")[0].strip()
                        re_findall = re.findall(BUILD_PARAMS, line)
                        for re_find in re_findall:
                            if len(re_find) == 2:
                                _build_info[re_find[0].strip()] = re_find[1].strip('\'", ')
                _build_info = {
                    "mash-repo": WEB_URL + "/compose" + _iso.isoname.split("Server")[0] + "Everything/",
                    "release": _build_info.get('release_short') + "-" + _build_info.get('release_version'),
                    "build": _build_info.get('release_type'),
                }
    except Exception as _e:
        _logger.error(f"ä»build_cfgæ–‡ä»¶è·å–æ•°æ®å¤±è´¥, åŸå› : {str(_e)}")

    return _build_info


def get_kylin_repos_path(repos_path):
    for root, dirs, files in os.walk(repos_path):
        for file in files:
            if file.startswith('kylin-repos') and file.endswith('.rpm'):
                return os.path.join(root, file)


def get_repos_info_rpm(base_path, pungi, arch: str = "") -> str:
    """
    è·å–ç³»ç»Ÿçš„reposä¿¡æ¯
    Returns:
    """
    if pungi:
        repo_path = f"{base_path}/compose/Everything/{arch}/os/Packages"
    else:
        repo_path = f"{base_path}root/buildiso/Packages/"
    temp_dir = tempfile.mkdtemp()
    repos_contents = []
    kylin_repos_path = get_kylin_repos_path(repo_path)
    try:
        proc1 = subprocess.Popen(['rpm2cpio', kylin_repos_path], stdout=subprocess.PIPE)
        proc2 = subprocess.Popen(['cpio', '-idmv'], stdin=proc1.stdout, stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE, cwd=temp_dir)
        proc1.stdout.close()
        proc2.communicate()

        for root, dirs, files in os.walk(temp_dir):
            for file in files:
                file_path = os.path.join(root, file)
                with open(file_path, 'r') as f:
                    repos_contents.append(f.read())
        repos = "\n".join(repos_contents)
        return repos
    except Exception as e:
        return f"è·å–reposæ¶ˆæ¯å¤±è´¥,{str(e)}"
    finally:
        shutil.rmtree(temp_dir)
