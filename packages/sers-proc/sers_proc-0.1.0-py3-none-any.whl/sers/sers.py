import numpy as np
import pandas as pd
from typing import Tuple, Union, Literal, Sequence, Optional, Dict, Any
from scipy import sparse
from scipy.signal import find_peaks, peak_widths, savgol_filter, peak_prominences
from scipy.sparse.linalg import spsolve

ArrayLike = Union[Sequence[float], np.ndarray, pd.Series]


def baseline_als(y, lam, p, niter=10):
  L = len(y)
  D = sparse.diags([1,-2,1],[0,-1,-2], shape=(L,L-2))
  w = np.ones(L)
  for i in range(niter):
    W = sparse.spdiags(w, 0, L, L)
    Z = W + lam * D.dot(D.transpose())
    z = spsolve(Z, w*y)
    w = p * (y > z) + (1-p) * (y < z)
  return z

    
def load_excel(file, index_col = 0):

    """
    ------------------------------------------------------------
    ğŸ“Œ Load Excel (multiple sheets) into dict of DataFrames
    ------------------------------------------------------------

    Parameters
    ----------
    file : str
        Excel filename (.xlsx)

    index_col : int (default=0)
        Column to use as DataFrame index for each sheet.

    ------------------------------------------------------------
    Returns
    -------
    df_res_all : dict
        { sheet_name : DataFrame }

    ------------------------------------------------------------
    Usage
    ------
    >>> df_res = load_excel("DEG_results.xlsx")
    >>> df_res.keys()
    dict_keys(['Group_A','Group_B','Group_C'])

    >>> df_res['Group_A'].head()
    # â†’ first sheet as pandas DataFrame
    """
    
    xls = pd.ExcelFile(file)
    lst = xls.sheet_names
    df_res_all = {}
    for s in lst:
        df_res_all[s] = pd.read_excel(xls, s, index_col = index_col) 
        
    return df_res_all


def resample_spectra_to_matrix(
    spectra: Dict[str, Union[Tuple[ArrayLike, ArrayLike], pd.DataFrame]],
    x_min: float = None,
    x_max: float = None,
    step: float = None,
    x_new: ArrayLike = None,
    x_col: str = "shift",
    y_col: str = "intensity",
) -> pd.DataFrame:
    """
    Resample multiple (x, y) spectra onto a common Raman-shift axis
    and return a matrix (DataFrame) whose index is the Raman shift
    and whose columns are spectrum names.

    PARAMETERS
    ----------
    spectra : dict
        Dictionary of spectra. Keys are spectrum names (column names).
        Values can be:
          - (x, y) tuple: array-like objects of same length
          - DataFrame with columns [x_col, y_col]
    x_min : float, optional
        Minimum Raman shift of the common grid.
        If None, uses the minimum over all spectra.
    x_max : float, optional
        Maximum Raman shift of the common grid.
        If None, uses the maximum over all spectra.
    step : float, optional
        Step size of the common Raman-shift grid.
        If None and x_new is also None, the union of all x values is used
        (i.e., all unique xâ€™s of all spectra).
    x_new : array-like, optional
        Custom x grid. If given, this is used as the common Raman-shift axis
        and x_min/x_max/step are ignored.
    x_col, y_col : str
        Column names used when each spectrum is given as a DataFrame.

    RETURNS
    -------
    df : pandas.DataFrame
        DataFrame with:
          - index : common Raman-shift axis (x_grid)
          - columns : spectrum names (keys of `spectra`)
          - values : interpolated intensities (float), NaN where out of range

    EXAMPLE
    -------
    >>> spec_dict = {
    ...     "sample1": (x1, y1),
    ...     "sample2": (x2, y2),
    ... }
    >>> df = resample_spectra_to_matrix(spec_dict, step=1.0)
    >>> print(df.head())
    """

    # ---- 1. ê° ìŠ¤í™íŠ¸ëŸ¼ì—ì„œ x, y ë½‘ì•„ì„œ ëª¨ìœ¼ê¸° ----
    all_x = []

    parsed = {}  # name -> (x_array, y_array)

    for name, xy in spectra.items():
        if isinstance(xy, pd.DataFrame):
            x = np.asarray(xy[x_col], dtype=float)
            y = np.asarray(xy[y_col], dtype=float)
        else:
            # assume tuple/list: (x, y)
            if not isinstance(xy, (tuple, list)) or len(xy) != 2:
                raise ValueError(f"Spectrum '{name}' must be (x, y) tuple or DataFrame.")
            x = np.asarray(xy[0], dtype=float)
            y = np.asarray(xy[1], dtype=float)

        if x.shape != y.shape:
            raise ValueError(f"Spectrum '{name}' has mismatched x and y lengths.")

        # ì •ë ¬ (xê°€ ì¦ê°€í•˜ëŠ” ìˆœì„œë¡œ)
        order = np.argsort(x)
        x = x[order]
        y = y[order]

        parsed[name] = (x, y)
        all_x.append(x)

    all_x_concat = np.concatenate(all_x)

    # ---- 2. ê³µí†µ x-grid (Raman shift ì¶•) ë§Œë“¤ê¸° ----
    if x_new is not None:
        x_grid = np.asarray(x_new, dtype=float)
    else:
        if x_min is None:
            x_min = float(np.min(all_x_concat))
        if x_max is None:
            x_max = float(np.max(all_x_concat))

        if step is not None:
            # step ê°„ê²©ìœ¼ë¡œ ë“±ê°„ê²© grid
            # + step/2 ëŠ” floating point ë¬¸ì œë¡œ ë§ˆì§€ë§‰ ì  í¬í•¨í•˜ê¸° ìœ„í•œ íŠ¸ë¦­
            x_grid = np.arange(x_min, x_max + step / 2.0, step, dtype=float)
        else:
            # stepë„ x_newë„ ì—†ìœ¼ë©´: ëª¨ë“  ìŠ¤í™íŠ¸ëŸ¼ì˜ xë¥¼ union í•´ì„œ ì‚¬ìš©
            x_grid = np.unique(all_x_concat)

    # indexê°€ ì˜ˆì˜ê²Œ ë˜ë„ë¡ ì •ë ¬
    x_grid = np.asarray(x_grid, dtype=float)
    x_grid.sort()

    # ---- 3. ê° ìŠ¤í™íŠ¸ëŸ¼ì„ x_gridì— ëŒ€í•´ ë³´ê°„ ----
    df = pd.DataFrame(index=x_grid)

    for name, (x, y) in parsed.items():
        # numpy.interp ëŠ” ê²½ê³„ ë°–ì—ì„œ edge ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ
        # ë²”ìœ„ ë°–ì€ NaNìœ¼ë¡œ ë°”ê¾¸ì–´ ì£¼ëŠ” í›„ì²˜ë¦¬ ì ìš©
        y_interp = np.interp(x_grid, x, y)
        mask_in_range = (x_grid >= x.min()) & (x_grid <= x.max())
        y_interp[~mask_in_range] = np.nan

        df[name] = y_interp

    df.index.name = "Raman_shift"

    return df


def detect_sers_peaks(
    x: ArrayLike,
    y: ArrayLike,
    min_prominence: float = 0.0,
    min_height: Optional[float] = None,
    min_distance_cm: Optional[float] = None,
    min_width_cm: Optional[float] = None,
    smooth: bool = True,
    smooth_window: int = 7,
    smooth_poly: int = 2,
    return_smoothed: bool = False,
    # â†“ ìƒˆë¡œ ì¶”ê°€ëœ ì˜µì…˜ë“¤
    score_col: str = "prominence",
    max_peaks: Optional[int] = None,
    min_peaks: int = 0,
    sample_name: str = ''
) -> Dict[str, Any]:
    """
    Detect peaks from a baseline-corrected SERS spectrum,
    and optionally keep only top-N peaks by a given score.

    PARAMETERS
    ----------
    x, y : array-like
        Raman shift (cm^-1) and baseline-corrected intensity.
    min_prominence, min_height, min_distance_cm, min_width_cm :
        Thresholds passed to scipy.signal.find_peaks (in cm^-1 space
        for distance/width).
    smooth : bool
        Whether to apply Savitzky-Golay smoothing before detection.
    smooth_window, smooth_poly :
        Parameters for Savitzky-Golay filter.
    return_smoothed : bool
        If True, include 'y_smooth' in the returned dict.

    score_col : {"prominence", "height", "intensity", ...}
        Column in the peak table to use as score for ranking.
        - "prominence" : peak strength (ê¸°ë³¸ê°’, ì¶”ì²œ)
        - "height"     : peak height (peak_heights)
        - "intensity"  : smoothed intensity at the peak
        ë“±ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.
    max_peaks : int, optional
        Keep at most this many peaks (top by score_col).
        If None, keep all.
    min_peaks : int, default 0
        Desired minimum number of peaks. If the number of detected
        peaks is smaller than min_peaks, all detected peaks are kept
        (ë¶€ì¡±í•œ ê°œìˆ˜ë¥¼ ì±„ì›Œ ë„£ì§€ëŠ” ì•Šê³ , ìˆëŠ” ê²ƒë§Œ ë°˜í™˜).

    RETURNS
    -------
    result : dict
        {
          "peaks_df": DataFrame of detected peaks (possibly truncated),
          "properties": dict from scipy.signal.find_peaks,
          "x": original x (sorted),
          "y": original y (sorted),
          "y_smooth": smoothed y (if return_smoothed=True)
        }
    """

    # ---- 0. ì…ë ¥ ì²˜ë¦¬ ----
    x = np.asarray(x, dtype=float)
    y = np.asarray(y, dtype=float)

    if x.shape != y.shape:
        raise ValueError("x and y must have the same shape.")

    # x ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
    if not np.all(np.diff(x) >= 0):
        order = np.argsort(x)
        x = x[order]
        y = y[order]

    # ---- 1. smoothing (ì„ íƒ) ----
    if smooth:
        n = len(y)
        win = min(smooth_window, n if n % 2 == 1 else n - 1)
        if win < 3:
            y_smooth = y.copy()
        else:
            if win % 2 == 0:
                win -= 1
            y_smooth = savgol_filter(
                y, window_length=win, polyorder=min(smooth_poly, win - 1)
            )
    else:
        y_smooth = y.copy()

    # ---- 2. x spacing ê¸°ë°˜ distance/width ë³€í™˜ ----
    dx = np.median(np.diff(x))

    distance_pts = None
    if min_distance_cm is not None and dx > 0:
        distance_pts = max(int(round(min_distance_cm / dx)), 1)

    width_pts = None
    if min_width_cm is not None and dx > 0:
        width_pts = max(int(round(min_width_cm / dx)), 1)

    # ---- 3. find_peaks í˜¸ì¶œ ----
    peaks, properties = find_peaks(
        y_smooth,
        height=min_height,
        prominence=min_prominence,
        distance=distance_pts,
        width=width_pts,
    )

    # í”¼í¬ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë¹ˆ DF ë¦¬í„´
    if peaks.size == 0:
        peaks_df = pd.DataFrame(
            columns=[
                "idx",
                "Raman_shift",
                "intensity",
                "prominence",
                "height",
                "left_base_x",
                "right_base_x",
                "fwhm_cm",
            ]
        )
        result = {
            "peaks_df": peaks_df,
            "properties": properties,
            "x": x,
            "y": y,
        }
        if return_smoothed:
            result["y_smooth"] = y_smooth
        return result

    # ---- 3-1. prominence ì •ë³´ ë³´ì¥ (í‚¤ ì´ë¦„: 'prominences') ----
    if "prominences" not in properties:
        prominences, left_bases, right_bases = peak_prominences(y_smooth, peaks)
        properties["prominences"] = prominences
        properties["left_bases"] = left_bases
        properties["right_bases"] = right_bases

    # ---- 4. FWHM ì¶”ì • ----
    widths_res = peak_widths(y_smooth, peaks, rel_height=0.5)
    widths_idx = widths_res[0]
    # left_ips, right_ips = widths_res[2], widths_res[3]  # í•„ìš”í•˜ë©´ ì‚¬ìš©

    fwhm_cm = widths_idx * dx

    # base (prominence ê¸°ì¤€) ì¢Œìš° index â†’ x ì¢Œí‘œ
    left_bases_idx = properties.get(
        "left_bases", np.full(peaks.shape, np.nan)
    )
    right_bases_idx = properties.get(
        "right_bases", np.full(peaks.shape, np.nan)
    )

    left_base_x = np.where(
        np.isfinite(left_bases_idx),
        x[left_bases_idx.astype(int)],
        np.nan,
    )
    right_base_x = np.where(
        np.isfinite(right_bases_idx),
        x[right_bases_idx.astype(int)],
        np.nan,
    )

    # ---- 5. ì „ì²´ peak table êµ¬ì„± ----
    peak_x = x[peaks]
    peak_y = y_smooth[peaks]

    prominences = properties.get(
        "prominences",
        np.full(peaks.shape, np.nan, dtype=float),
    )
    height = properties.get("peak_heights", peak_y)

    peaks_df = pd.DataFrame(
        {
            "idx": peaks,
            "Raman_shift": peak_x,
            "intensity": peak_y,
            "prominence": prominences,
            "height": height,
            "left_base_x": left_base_x,
            "right_base_x": right_base_x,
            "fwhm_cm": fwhm_cm,
        }
    )

    # ---- 6. score ê¸°ì¤€ìœ¼ë¡œ top-N ì„ íƒ ----
    if score_col not in peaks_df.columns:
        raise ValueError(
            f"score_col='{score_col}' not found in peaks_df columns: "
            f"{list(peaks_df.columns)}"
        )

    # score ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (scoreê°€ NaNì¸ ê²ƒì€ ë§¨ ë’¤ë¡œ)
    peaks_df = peaks_df.sort_values(
        by=score_col, ascending=False, na_position="last"
    ).reset_index(drop=True)

    n_detected = len(peaks_df)

    # max_peaksê°€ ì„¤ì •ëœ ê²½ìš°, ê·¸ ìˆ˜ë§Œí¼ ìë¥´ë˜,
    # ì‹¤ì œ í”¼í¬ ìˆ˜ê°€ min_peaksë³´ë‹¤ ì ìœ¼ë©´ ê·¸ëƒ¥ ìˆëŠ” ê²ƒë§Œ ë°˜í™˜
    if max_peaks is not None and max_peaks > 0:
        # ì‹¤ì œë¡œ ìë¥´ëŠ” ê°œìˆ˜
        n_keep = min(max_peaks, n_detected)
        peaks_df = peaks_df.iloc[:n_keep, :].copy()

    # min_peaksëŠ” â€œìµœì†Œ ì´ ì •ë„ëŠ” ê¸°ëŒ€í•œë‹¤â€ëŠ” ì˜ë¯¸ë¡œ,
    # ì‹¤ì œ ê²€ì¶œ ê°œìˆ˜ê°€ min_peaksë³´ë‹¤ ì ìœ¼ë©´ ê·¸ëƒ¥ ìˆëŠ” ê°œìˆ˜ë§Œ ë°˜í™˜
    # (ì¶”ê°€ë¡œ ë§Œë“¤ì–´ì„œ ì±„ìš°ì§„ ì•ŠìŒ). í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ warning/log ì¶œë ¥ ê°€ëŠ¥.
    if n_detected < min_peaks:
        print(f"Warning: only {n_detected} peaks detected in {sample_name} (< min_peaks={min_peaks})")

    # Raman shift ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬í•˜ê³  ì‹¶ìœ¼ë©´ ì´ ì¤„ì„ ì“°ê³ ,
    # score ìˆœì„œ ìœ ì§€ê°€ ì¢‹ìœ¼ë©´ ì´ ì¤„ì€ ì£¼ì„ ì²˜ë¦¬
    peaks_df = peaks_df.sort_values("Raman_shift").reset_index(drop=True)

    result = {
        "peaks_df": peaks_df,
        "properties": properties,
        "x": x,
        "y": y,
    }
    if return_smoothed:
        result["y_smooth"] = y_smooth

    return result


def detect_peaks_for_sers_matrix(
    df_sers: pd.DataFrame,
    min_prominence: float = 0.0,
    min_height: Optional[float] = None,
    min_distance_cm: Optional[float] = None,
    min_width_cm: Optional[float] = None,
    smooth: bool = True,
    smooth_window: int = 7,
    smooth_poly: int = 2,
    score_col: str = "prominence",
    max_peaks: Optional[int] = None,
    min_peaks: int = 0
) -> pd.DataFrame:
    """
    Apply detect_sers_peaks to each column of a SERS matrix.

    df_sers : DataFrame
        index: Raman_shift, columns: samples (each column is a spectrum)
    ë°˜í™˜ê°’ : long-format DataFrame
        columns: ['sample', 'idx', 'Raman_shift', 'intensity',
                  'prominence', 'height', 'left_base_x',
                  'right_base_x', 'fwhm_cm']
    """
    x = df_sers.index.values.astype(float)
    all_peaks = []

    for sample in df_sers.columns:
        y = df_sers[sample].values.astype(float)
        res = detect_sers_peaks(
            x, y,
            min_prominence=min_prominence,
            min_height=min_height,
            min_distance_cm=min_distance_cm,
            min_width_cm=min_width_cm,
            smooth=smooth,
            smooth_window=smooth_window,
            smooth_poly=smooth_poly,
            return_smoothed=False,
            score_col = score_col,
            max_peaks = max_peaks,
            min_peaks = min_peaks,
            sample_name = sample
        )
        df_peaks = res["peaks_df"].copy()
        df_peaks.insert(0, "sample", sample)
        all_peaks.append(df_peaks)

    if len(all_peaks) == 0:
        return pd.DataFrame(
            columns=[
                "sample", "idx", "Raman_shift", "intensity",
                "prominence", "height", "left_base_x",
                "right_base_x", "fwhm_cm",
            ]
        )

    return pd.concat(all_peaks, axis=0, ignore_index=True)


def make_peak_boolean_matrix(df_sers_matrix: pd.DataFrame,
                             df_peaks_long: pd.DataFrame,
                             tol: float = 1e-6) -> pd.DataFrame:
    """
    Create a boolean peak matrix with the same shape as df_sers_matrix.
    True at Raman shifts where a peak was detected for each sample.

    PARAMETERS
    ----------
    df_sers_matrix : DataFrame
        index: Raman_shift, columns: samples
    df_peaks_long : DataFrame
        Output from detect_peaks_for_sers_matrix()
        Required columns: ["sample", "Raman_shift"]
    tol : float
        Tolerance used when matching Raman shift positions (floating point issue)

    RETURNS
    -------
    df_peak_bool : DataFrame (boolean)
        Same shape as df_sers_matrix
        True = peak detected, False = no peak
    """

    # ì´ˆê¸° false matrix
    df_peak_bool = pd.DataFrame(
        False,
        index=df_sers_matrix.index,
        columns=df_sers_matrix.columns
    )

    # ê° sampleì— ëŒ€í•´ peak ìœ„ì¹˜ë¥¼ True ë¡œ ì„¤ì •
    for sample in df_sers_matrix.columns:
        # í•´ë‹¹ sampleì˜ peak shift ê°’ë“¤
        df_peaks_sample = df_peaks_long[df_peaks_long["sample"] == sample]

        if len(df_peaks_sample) == 0:
            continue

        peak_positions = df_peaks_sample["Raman_shift"].values

        # float ë¬¸ì œ í•´ê²° ìœ„í•´ tolerance ê¸°ë°˜ ë§¤ì¹­
        for shift in peak_positions:
            # df_sers_matrix.index ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê·¼ì ‘ê°’ ì°¾ê¸°
            idx_match = np.isclose(df_sers_matrix.index.values, shift, atol=tol)
            df_peak_bool.loc[idx_match, sample] = True

    return df_peak_bool


def make_peak_score_matrix(
    df_sers_matrix: pd.DataFrame,
    df_peaks_long: pd.DataFrame,
    score_col: str = "prominence",
    tol: float = 1e-6
) -> pd.DataFrame:
    """
    Create a peak score matrix with same shape as df_sers_matrix.
    Peak positions receive a score (e.g., prominence), non-peak = 0.

    PARAMETERS
    ----------
    df_sers_matrix : DataFrame
        index: Raman_shift, columns: samples
    df_peaks_long : DataFrame
        Output from detect_peaks_for_sers_matrix()
        Must contain columns: ["sample", "Raman_shift", score_col]
    score_col : str
        Which peak property to use as score ("prominence", "height", "intensity", ...)
    tol : float
        Floating-point tolerance when matching Raman_shift positions

    RETURNS
    -------
    df_peak_score : DataFrame (float)
        Same shape as df_sers_matrix
        Peak positions = score
        Non-peak = 0
    """

    # ì´ˆê¸° score matrix = ëª¨ë‘ 0
    df_peak_score = pd.DataFrame(
        0.0,
        index=df_sers_matrix.index,
        columns=df_sers_matrix.columns
    )

    # sampleë³„ peak score ì±„ìš°ê¸°
    for sample in df_sers_matrix.columns:
        df_peaks_sample = df_peaks_long[df_peaks_long["sample"] == sample]

        if df_peaks_sample.empty:
            continue

        for _, row in df_peaks_sample.iterrows():
            shift = row["Raman_shift"]
            score = row[score_col]

            # Raman shift ë§¤ì¹­ (float tolerance ì‚¬ìš©)
            idx_match = np.isclose(df_sers_matrix.index.values, shift, atol=tol)
            df_peak_score.loc[idx_match, sample] = score

    return df_peak_score


def broaden_peak_score_matrix(
    df_peak_score: pd.DataFrame,
    half_width_cm: float = 5.0,
    kernel_type: Literal["triangular", "gaussian", "rectangular"] = "triangular",
    sigma_cm: Optional[float] = None,
    normalize_kernel: bool = True,
) -> pd.DataFrame:
    """
    Convolve peak score matrix along Raman-shift axis to 'broaden' peaks.
    (í›„ ë³´ì •ìš©: í”¼í¬ ì£¼ë³€ íŒŒìˆ˜ì—ë„ scoreë¥¼ í¼ëœ¨ë¦¬ëŠ” í•¨ìˆ˜)

    PARAMETERS
    ----------
    df_peak_score : DataFrame
        index   : Raman_shift (cm^-1, ascending)
        columns : samples
        values  : peak scores (e.g., prominence), non-peak = 0
    half_width_cm : float, default 5.0
        Convolution ë°˜í­ (Â± cm^-1).
        ì˜ˆ: 5.0ì´ë©´ ì¤‘ì‹¬ Â±5 cm^-1 ë²”ìœ„ê¹Œì§€ kernelì´ í¼ì§.
    kernel_type : {"triangular", "gaussian", "rectangular"}, default "triangular"
        ì‚¬ìš©í•  ì»¤ë„ ëª¨ì–‘.
        - "triangular"   : ê°€ìš´ë°ê°€ ê°€ì¥ í¬ê³  ì–‘ìª½ìœ¼ë¡œ ì„ í˜• ê°ì†Œ
        - "gaussian"     : Gaussian kernel ì‚¬ìš©
        - "rectangular"  : ìœˆë„ìš° ë‚´ì—ì„œ ë™ì¼í•œ ê°€ì¤‘ì¹˜ (boxcar / moving-average í˜•íƒœ)
    sigma_cm : float, optional
        Gaussian kernel ì‚¬ìš© ì‹œ í‘œì¤€í¸ì°¨ (cm^-1).
        Noneì´ë©´ half_width_cmì˜ ì•½ 1/2ë¡œ ìë™ ì„¤ì •.
    normalize_kernel : bool, default True
        Trueì´ë©´ kernel í•©ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”.
        Falseì´ë©´ ì›ë˜ peak scoreê°€ ì£¼ë³€ìœ¼ë¡œ 'í¼ì§€ë©´ì„œ' ì´í•©ì´ ì»¤ì§ˆ ìˆ˜ ìˆìŒ.

    RETURNS
    -------
    df_broadened : DataFrame
        index, columnsëŠ” df_peak_scoreì™€ ë™ì¼.
        ê° columnì— ëŒ€í•´ Raman_shift ì¶• ë°©í–¥ìœ¼ë¡œ 1D convolution í•œ ê²°ê³¼.
    """

    if df_peak_score.shape[0] < 2:
        # í•œ ì ë°–ì— ì—†ìœ¼ë©´ í•  ê²Œ ì—†ìŒ
        return df_peak_score.copy()

    x = df_peak_score.index.values.astype(float)

    # Raman shift ì¶• spacing ì¶”ì • (ë¹„ê· ì¼í•´ë„ median spacing ì‚¬ìš©)
    dx = np.median(np.diff(x))
    if dx <= 0:
        raise ValueError("Raman_shift index must be strictly increasing.")

    # half_width_cm ë¥¼ point ë‹¨ìœ„ë¡œ ë³€í™˜
    half_width_pts = max(int(round(half_width_cm / dx)), 1)
    kernel_size = 2 * half_width_pts + 1
    center = half_width_pts

    # ---- 1. kernel ìƒì„± ----
    if kernel_type == "triangular":
        # ì¤‘ì•™ì—ì„œ ì–‘ìª½ìœ¼ë¡œ ì„ í˜• ê°ì†Œí•˜ëŠ” ì‚¼ê°í˜• kernel
        # ex) half_width_pts=2 -> weights ~ [1,2,3,2,1]
        distances = np.abs(np.arange(kernel_size) - center)
        kernel = (half_width_pts + 1) - distances
        kernel[kernel < 0] = 0.0  # ì´ë¡ ìƒ í•„ìš” ì—†ì§€ë§Œ ì•ˆì „ìš©

    elif kernel_type == "gaussian":
        # Gaussian kernel: exp(-0.5 * (x/sigma)^2)
        if sigma_cm is None:
            sigma_cm = half_width_cm / 2.0  # ëŒ€ì¶© half_widthì˜ ì ˆë°˜ ì •ë„
        sigma_pts = sigma_cm / dx
        xs = np.arange(kernel_size) - center
        kernel = np.exp(-0.5 * (xs / sigma_pts) ** 2)

    elif kernel_type == "rectangular":
        # ìœˆë„ìš° ë‚´ ë™ì¼ ê°€ì¤‘ì¹˜ (boxcar kernel)
        kernel = np.ones(kernel_size, dtype=float)

    else:
        raise ValueError(f"Unknown kernel_type: {kernel_type}")

    kernel = kernel.astype(float)

    if normalize_kernel:
        kernel_sum = kernel.sum()
        if kernel_sum > 0:
            kernel /= kernel_sum

    # ---- 2. ê° sample columnì— ëŒ€í•´ convolution ----
    arr = df_peak_score.values.astype(float)  # shape: (n_shift, n_samples)
    n_shift, n_samples = arr.shape

    broadened = np.zeros_like(arr)

    for j in range(n_samples):
        col = arr[:, j]
        broadened[:, j] = np.convolve(col, kernel, mode="same")

    df_broadened = pd.DataFrame(
        broadened,
        index=df_peak_score.index,
        columns=df_peak_score.columns,
    )

    return df_broadened

    
def cross_correlate_peak_bool(
    df_peak_bool: pd.DataFrame,
    sample1: str,
    sample2: str,
    max_lag: int = 10
):
    """
    Compute cross-correlation between two boolean peak vectors
    for wave-number lag = -max_lag ... +max_lag.

    PARAMETERS
    ----------
    df_peak_bool : DataFrame
        index : Raman shift (monotonic)
        columns : samples
        values : True/False (peak map)
    sample1, sample2 : str
        column names of df_peak_bool to compare
    max_lag : int, default 10
        compute correlation for shift differences from -max_lag to +max_lag

    RETURNS
    -------
    best_corr : float
        maximum correlation value
    best_lag : int
        lag (difference in index positions) giving that maximum correlation
    corr_dict : dict
        {lag: correlation_value}
    """

    # Boolean â†’ integer (True=1, False=0)
    a = df_peak_bool[sample1].astype(float).values
    b = df_peak_bool[sample2].astype(float).values
    n = len(a)

    corr_dict = {}

    for lag in range(-max_lag, max_lag + 1):
        if lag < 0:
            # b shifted left
            a_seg = a[-lag:]
            b_seg = b[:n + lag]
        elif lag > 0:
            # b shifted right
            a_seg = a[:n - lag]
            b_seg = b[lag:]
        else:  # lag == 0
            a_seg = a
            b_seg = b

        if len(a_seg) == 0:
            corr = 0.0
        else:
            # simple correlation = dot product of 0/1 vectors
            corr = np.sum(a_seg * b_seg)

        corr_dict[lag] = float(corr)

    # find max correlation
    best_lag = max(corr_dict, key=lambda k: corr_dict[k])
    best_corr = corr_dict[best_lag]

    return best_corr, best_lag, corr_dict


def group_wise_peak_aggregation( df_peak_score, groups, r_th = 0.5 ):

    df_tmp = df_peak_score.copy(deep = True)
    cols = df_tmp.columns.values
    
    group_ary = np.array(groups)
    group_lst = list(set( groups ))
    group_lst.sort()

    df_rwn = pd.DataFrame( 0, index = df_tmp.index, columns = group_lst )
    
    score_rev = []
    for j, g in enumerate(group_lst):
        b = group_ary == g
    
        mnp = df_tmp.loc[:,cols[b]].median(axis = 1)
        bx = (df_tmp.loc[:,cols[b]] > 0).sum(axis = 1) >= b.sum()*r_th

        df_rwn.loc[bx, g] = mnp[bx]
        score_rev = score_rev + list(mnp[bx])
        for c in cols[b]:
            df_tmp.loc[bx, c] = mnp[bx]
            df_tmp.loc[~bx, c] = 0
    
    return df_rwn, score_rev


def get_peak_rm_shifts( df_peak_score_rev ):
    group_lst = df_peak_score_rev.columns.values.tolist()
    rm_shifts = {}
    for g in group_lst:
        b = df_peak_score_rev[g] > 0
        rm_shifts[g] = df_peak_score_rev.index.values[b].tolist()

    return rm_shifts


def peak_rm_shifts_hit_rate( df_peak_score_broadened, rm_shifts_dct ):

    group_lst = list( rm_shifts_dct.keys() )
    df_hits = pd.DataFrame( 0, index = group_lst, columns = df_peak_score_broadened.columns.values.tolist() )
    for g in group_lst:
        hits = (df_peak_score_broadened.loc[ rm_shifts_dct[g] ] > 0).mean()
        df_hits.loc[g] = hits

    return df_hits

    