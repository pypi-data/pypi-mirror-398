#!/usr/bin/env python
"""
Generate pools as in https://doi.org/10.1101/2025.07.01.662654 except that interactions are weighted by the product of the protein sizes

For Mgen, this leads to 1,791 pools generated in 29 seconds (vs 2,027 pools)

"""

import argparse, itertools, sys, numpy as np, pandas as pd, tqdm, numba
from pprint import pprint

def eprint(*args, **kwargs): # https://stackoverflow.com/questions/5574702/how-do-i-print-to-stderr-in-python
    print(*args, file=sys.stderr, **kwargs)

@numba.jit(nopython=True, nogil=True) # https://github.com/numba/numba/issues/5894#issuecomment-974701551
def numba_ix(arr, rows, cols):
    """
    Numba compatible implementation of arr[np.ix_(rows, cols)] for 2D arrays.
    :param arr: 2D array to be indexed
    :param rows: Row indices
    :param cols: Column indices
    :return: 2D array with the given rows and columns of the input array
    """
    one_d_index = np.zeros(len(rows) * len(cols), dtype=np.int32)
    for i, r in enumerate(rows):
        start = i * len(cols)
        one_d_index[start: start + len(cols)] = cols + arr.shape[1] * r

    arr_1d = arr.reshape((arr.shape[0] * arr.shape[1], 1))
    slice_1d = np.take(arr_1d, one_d_index)
    return slice_1d.reshape((len(rows), len(cols)))

@numba.jit(nopython=True, nogil=True)
def calculate_replication_fraction(pool, i, sizes, cov, all, max_size):
    """
    Calculate fraction of effort replicated in the current pool if i were to be added
    """
    # pool_try is pool with ith protein included
    pool_try = pool.astype(sizes.dtype)
    pool_try[i] = 1
    pool_size = np.multiply(pool_try, sizes).sum()
    if pool_size > max_size:
        return 2

    # pool_ix has indices of proteins in the pool - more efficient than the boolean mask as pool has small number of proteins
    pool_ix = np.where(pool_try)[0]

    #pool_cov = numba_ix(cov, pool_ix, pool_ix)
    #pool_all = numba_ix(all, pool_ix, pool_ix)
    #repl_factor = pool_cov.sum() / pool_all.sum()
    return numba_ix(cov, pool_ix, pool_ix).sum() / numba_ix(all, pool_ix, pool_ix).sum()

@numba.jit(nopython=True, parallel=True, nogil=True)
def pool_expand(pool, sizes, cov, all, max_size):
    """
    Find protein to expand pool while optimising for replication factor
    Returns -1 if nothing can be added
    """
    repl = np.zeros(sizes.shape[0])
    for i in numba.prange(sizes.shape[0]):
        if pool[i]:
            repl[i] = 2
        elif cov[i,:].sum() == all[i,:].sum():
            repl[i] = 2
        else:
            repl[i] = calculate_replication_fraction(pool, i, sizes, cov, all, max_size)

    best_i = np.argmin(repl)
    if repl[best_i] < 2:
        return best_i
    else:
        return -1

def generate_pools(sizes, max_size=5120, skip_pairs=[], rng = np.random.default_rng(seed=4)):
    all = np.outer(sizes, sizes) # interactions "weighted" by the product of their sizes
    np.fill_diagonal(all, 0)
    cov = np.zeros(all.shape) # interactions covered by finished pools

    # Return all interactions above max_size as individual two-protein pools
    for i, j in np.ndindex(all.shape):
        if (i < j) and (sizes[i] + sizes[j] > max_size):
            yield(set([i, j]), sizes[i] + sizes[j])
            cov[i, j] = all[i, j]
            cov[j, i] = all[j, i]

    for i, j in skip_pairs:
        cov[i, j] = all[i, j]
        cov[j, i] = all[j, i]

    pool = np.full(sizes.shape, False) # current pool
    pbar = tqdm.tqdm(total=np.triu(all == all, 1).sum()) # https://github.com/tqdm/tqdm#usage
    while not np.array_equal(all, cov):
        if pool.sum() == 0: # current pool is empty
            # randomly selected protein with incomplete coverage
            avail = np.where(all.sum(axis=1) != cov.sum(axis=1))[0] # proteins with incomplete coverage
            avail_choice = rng.choice(avail, 1).squeeze()
            # add to current pool
            pool[avail_choice] = True

        while True:
            best_i = pool_expand(pool, sizes, cov, all, max_size)  # Search for a protein to add to the pool minimising the replication factor
            if (best_i >= 0):
                pool[best_i] = True # add optimal protein to the current pool
            else:
                # Cannot increase current pool anymore; yield as-is and reset search with an empty pool
                pool_ix_ = np.where(pool)[0]
                pool_set_ = set(map(lambda x: x.item(), pool_ix_))
                pool_size_ = np.dot(pool, sizes).item()
                yield(pool_set_, pool_size_)
                cov[np.ix_(pool_ix_, pool_ix_)] = all[np.ix_(pool_ix_, pool_ix_)] # Update global coverage map
                pool = np.full(sizes.shape, False) # Reset pool
                # Update progress bar
                interactions_covered = np.triu(all == cov, 1).sum()
                pbar.update(interactions_covered - pbar.n) # https://github.com/tqdm/tqdm/issues/1264
                break
    pbar.close()

def main():
    parser = argparse.ArgumentParser(
        description="Sample random pools minimising overlap"
    )
    parser.add_argument(
        "--init_pools", 
        "-p", 
        help="Pools to skip"
    )
    parser.add_argument(
        "--max_pool_size", 
        "-s", 
        help="Maximum size for pool",
        default=5120,
        type=int,
    )
    parser.add_argument(
        "--max_pools", 
        "-n", 
        help="Maximum number of pools to sample",
        type=int,
    )
    args = parser.parse_args()
    eprint('--init_pools', args.init_pools)
    eprint('--max_pool_size', args.max_pool_size)
    eprint('--max_pools', args.max_pools)

    proteins = pd.read_csv(sys.stdin, sep='\s+', names=['seq_id', 'seq_len'])#.head(1000)
    def get_protein_id(ix):
        proteins_id_col = proteins.columns[0]
        return proteins.loc[ix, proteins_id_col]

    #eprint(proteins)
    
    id_to_ix = proteins.reset_index().set_index(proteins.columns[0])['index'].to_dict()
    #eprint(id_to_ix)

    def to_ix_(s):
        return [* map(lambda id_: id_to_ix[id_], s.split('_')) ]

    skip_pairs = []
    if args.init_pools is not None:
        initial_pools = pd.read_csv(args.init_pools, sep='\s+')
        initial_pools['pool_ix'] = [ *map(to_ix_, initial_pools['pool_id']) ]
        initial_pools['pool_ix_pairs'] = [ *map(lambda pool_ix: list(itertools.combinations(pool_ix, 2)), initial_pools['pool_ix'] )]
        #eprint(initial_pools)
        def flatten(xss):
            return [x for xs in xss for x in xs]
        skip_pairs = flatten(initial_pools['pool_ix_pairs'].tolist())
        #pprint(skip_pairs)

    #numba.set_num_threads(64) # as things are, multiple threads slow down the code instead of speeding up..
    eprint(numba.get_num_threads(), 'threads available for numba')
    eprint(len(proteins), 'proteins in input')

    sizes = proteins['seq_len'].values
    pools = pd.DataFrame.from_records(itertools.islice(generate_pools(sizes, max_size=args.max_pool_size, skip_pairs=skip_pairs), args.max_pools), columns=['pool_ixs', 'pool_size'])
    pools['pool_ids'] = pools['pool_ixs'].map(lambda ixs: set(map(get_protein_id, ixs)))
    pools['pool_id'] = pools['pool_ids'].map(lambda ids: '_'.join(sorted(ids)))

    eprint(len(pools), 'pools generated')

    def generate_interactions(ids):
        # Generate all possible interactions between ids
        return set(itertools.combinations(sorted(ids), 2))

    # Sanity check - generate a list of all interactions & count size
    all = set(generate_interactions(range(len(sizes))))
    all_sum = 0
    for i, j in all:
        all_sum += sizes[i] * sizes[j]

    # Sanity check - compare to analytic adhoc formula
    ref_sum = (sum(sizes)**2 - sum(sizes*sizes)) / 2
    assert all_sum == ref_sum

    # Sanity check - generate list of interactions from the pools, compare to reference list
    gen = set()
    gen_sum = 0
    for i, r in pools.iterrows():
        pool_interactions = generate_interactions(r.pool_ixs)
        gen |= pool_interactions
        for i, j in pool_interactions:
            gen_sum += sizes[i] * sizes[j]

    eprint(len(all), 'interactions expected')
    eprint(len(gen), 'interactions across all pools generated')
    eprint(gen == all, 'pools include all possible interactions')
    eprint(gen_sum / all_sum, 'length-weighted redundancy factor across all pools') # Should be proportional to the added runtime from the redundancy in the pools

    pools[['pool_id', 'pool_size']].to_csv(sys.stdout, sep='\t', index=False)
