"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import Any, List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class PartitioningScheme(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class S3CollectorConfExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3CollectorConfExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3CollectorConfAuthenticationMethod(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class S3CollectorConfSignatureVersion(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing S3 requests"""

    V2 = "v2"
    V4 = "v4"


class S3CollectorConfTypedDict(TypedDict):
    bucket: str
    r"""S3 Bucket from which to collect data"""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    partitioning_scheme: NotRequired[PartitioningScheme]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    extractors: NotRequired[List[S3CollectorConfExtractorTypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    aws_authentication_method: NotRequired[S3CollectorConfAuthenticationMethod]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[S3CollectorConfSignatureVersion]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    recurse: NotRequired[Any]
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""


class S3CollectorConf(BaseModel):
    bucket: str
    r"""S3 Bucket from which to collect data"""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    partitioning_scheme: Annotated[
        Annotated[
            Optional[PartitioningScheme], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="partitioningScheme"),
    ] = PartitioningScheme.NONE
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    extractors: Optional[List[S3CollectorConfExtractor]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[S3CollectorConfAuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = S3CollectorConfAuthenticationMethod.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[S3CollectorConfSignatureVersion],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = S3CollectorConfSignatureVersion.V4
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    recurse: Optional[Any] = None

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = True
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Disable Collector event time filtering when a date range is specified"""

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.PartitioningScheme(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.S3CollectorConfAuthenticationMethod(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.S3CollectorConfSignatureVersion(value)
            except ValueError:
                return value
        return value
