"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class AzureBlobCollectorConfAuthenticationMethod(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Enter authentication data directly, or select a secret referencing your auth data"""

    MANUAL = "manual"
    SECRET = "secret"
    CLIENT_SECRET = "clientSecret"
    CLIENT_CERT = "clientCert"


class AzureBlobCollectorConfExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""A JavaScript expression that accesses a corresponding <token> through the value variable and evaluates the token to populate event fields. Example: {date: new Date(+value*1000)}"""


class AzureBlobCollectorConfExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""A JavaScript expression that accesses a corresponding <token> through the value variable and evaluates the token to populate event fields. Example: {date: new Date(+value*1000)}"""


class AzureBlobCollectorConfTypedDict(TypedDict):
    container_name: str
    r"""Container to collect from. This value can be a constant, or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: myBucket-${C.vars.myVar}"""
    output_name: NotRequired[str]
    r"""An optional predefined Destination that will be used to auto-populate Collector settings"""
    auth_type: NotRequired[AzureBlobCollectorConfAuthenticationMethod]
    r"""Enter authentication data directly, or select a secret referencing your auth data"""
    path: NotRequired[str]
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""
    extractors: NotRequired[List[AzureBlobCollectorConfExtractorTypedDict]]
    r"""Extractors allow use of template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)} will enrich discovery results with a human-readable \"date\" field."""
    recurse: NotRequired[bool]
    r"""Recurse through subdirectories"""
    include_metadata: NotRequired[bool]
    r"""Include Azure Blob metadata in collected events. In each event, metadata will be located at: __collectible.metadata."""
    include_tags: NotRequired[bool]
    r"""Include Azure Blob tags in collected events. In each event, tags will be located at: __collectible.tags. Disable this feature when using a Shared Access Signature Connection String, to prevent errors."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""


class AzureBlobCollectorConf(BaseModel):
    container_name: Annotated[str, pydantic.Field(alias="containerName")]
    r"""Container to collect from. This value can be a constant, or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: myBucket-${C.vars.myVar}"""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""An optional predefined Destination that will be used to auto-populate Collector settings"""

    auth_type: Annotated[
        Annotated[
            Optional[AzureBlobCollectorConfAuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = AzureBlobCollectorConfAuthenticationMethod.MANUAL
    r"""Enter authentication data directly, or select a secret referencing your auth data"""

    path: Optional[str] = None
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""

    extractors: Optional[List[AzureBlobCollectorConfExtractor]] = None
    r"""Extractors allow use of template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)} will enrich discovery results with a human-readable \"date\" field."""

    recurse: Optional[bool] = True
    r"""Recurse through subdirectories"""

    include_metadata: Annotated[
        Optional[bool], pydantic.Field(alias="includeMetadata")
    ] = True
    r"""Include Azure Blob metadata in collected events. In each event, metadata will be located at: __collectible.metadata."""

    include_tags: Annotated[Optional[bool], pydantic.Field(alias="includeTags")] = True
    r"""Include Azure Blob tags in collected events. In each event, tags will be located at: __collectible.tags. Disable this feature when using a Shared Access Signature Connection String, to prevent errors."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AzureBlobCollectorConfAuthenticationMethod(value)
            except ValueError:
                return value
        return value
