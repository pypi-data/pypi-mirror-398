"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .authenticationmethodoptions import AuthenticationMethodOptions
from .signatureversionoptions import SignatureVersionOptions
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class CollectorS3Type5(str, Enum):
    r"""Collector type: s3"""

    S3 = "s3"


class PartitioningScheme5(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class CollectorS3Extractor5TypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class CollectorS3Extractor5(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class CollectorS3S35TypedDict(TypedDict):
    type: CollectorS3Type5
    r"""Collector type: s3"""
    bucket: str
    r"""S3 Bucket from which to collect data"""
    aws_authentication_method: NotRequired[AuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references AWS access key and secret key."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    partitioning_scheme: NotRequired[PartitioningScheme5]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    extractors: NotRequired[List[CollectorS3Extractor5TypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    recurse: NotRequired[bool]
    r"""Traverse and include files from subdirectories. Leave this option enabled to ensure that all nested directories are searched and their contents collected."""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""
    aws_api_key: NotRequired[str]
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    aws_secret_key: NotRequired[str]
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""


class CollectorS3S35(BaseModel):
    type: CollectorS3Type5
    r"""Collector type: s3"""

    bucket: str
    r"""S3 Bucket from which to collect data"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references AWS access key and secret key."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    partitioning_scheme: Annotated[
        Annotated[
            Optional[PartitioningScheme5], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="partitioningScheme"),
    ] = PartitioningScheme5.NONE
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    extractors: Optional[List[CollectorS3Extractor5]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    recurse: Optional[bool] = True
    r"""Traverse and include files from subdirectories. Leave this option enabled to ensure that all nested directories are searched and their contents collected."""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = True
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Disable Collector event time filtering when a date range is specified"""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.PartitioningScheme5(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class CollectorS3Type4(str, Enum):
    r"""Collector type: s3"""

    S3 = "s3"


class PartitioningScheme4(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class CollectorS3Extractor4TypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class CollectorS3Extractor4(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class CollectorS3S34TypedDict(TypedDict):
    type: CollectorS3Type4
    r"""Collector type: s3"""
    bucket: str
    r"""S3 Bucket from which to collect data"""
    aws_authentication_method: NotRequired[AuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    aws_secret_key: NotRequired[str]
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    partitioning_scheme: NotRequired[PartitioningScheme4]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    extractors: NotRequired[List[CollectorS3Extractor4TypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    recurse: NotRequired[bool]
    r"""Traverse and include files from subdirectories. Leave this option enabled to ensure that all nested directories are searched and their contents collected."""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references AWS access key and secret key."""


class CollectorS3S34(BaseModel):
    type: CollectorS3Type4
    r"""Collector type: s3"""

    bucket: str
    r"""S3 Bucket from which to collect data"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    partitioning_scheme: Annotated[
        Annotated[
            Optional[PartitioningScheme4], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="partitioningScheme"),
    ] = PartitioningScheme4.NONE
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    extractors: Optional[List[CollectorS3Extractor4]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    recurse: Optional[bool] = True
    r"""Traverse and include files from subdirectories. Leave this option enabled to ensure that all nested directories are searched and their contents collected."""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = True
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Disable Collector event time filtering when a date range is specified"""

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references AWS access key and secret key."""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.PartitioningScheme4(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class CollectorS3Type3(str, Enum):
    r"""Collector type: s3"""

    S3 = "s3"


class PartitioningScheme3(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class CollectorS3Extractor3TypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class CollectorS3Extractor3(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class CollectorS3S33TypedDict(TypedDict):
    type: CollectorS3Type3
    r"""Collector type: s3"""
    bucket: str
    r"""S3 Bucket from which to collect data"""
    aws_authentication_method: NotRequired[AuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    partitioning_scheme: NotRequired[PartitioningScheme3]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    extractors: NotRequired[List[CollectorS3Extractor3TypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    recurse: NotRequired[bool]
    r"""Traverse and include files from subdirectories. Leave this option enabled to ensure that all nested directories are searched and their contents collected."""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""
    aws_api_key: NotRequired[str]
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    aws_secret_key: NotRequired[str]
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references AWS access key and secret key."""


class CollectorS3S33(BaseModel):
    type: CollectorS3Type3
    r"""Collector type: s3"""

    bucket: str
    r"""S3 Bucket from which to collect data"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    partitioning_scheme: Annotated[
        Annotated[
            Optional[PartitioningScheme3], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="partitioningScheme"),
    ] = PartitioningScheme3.NONE
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    extractors: Optional[List[CollectorS3Extractor3]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    recurse: Optional[bool] = True
    r"""Traverse and include files from subdirectories. Leave this option enabled to ensure that all nested directories are searched and their contents collected."""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = True
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Disable Collector event time filtering when a date range is specified"""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references AWS access key and secret key."""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.PartitioningScheme3(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class PartitioningScheme2(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class CollectorS3Type2(str, Enum):
    r"""Collector type: s3"""

    S3 = "s3"


class CollectorS3Extractor2TypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class CollectorS3Extractor2(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class CollectorS3S32TypedDict(TypedDict):
    type: CollectorS3Type2
    r"""Collector type: s3"""
    bucket: str
    r"""S3 Bucket from which to collect data"""
    partitioning_scheme: NotRequired[PartitioningScheme2]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    recurse: NotRequired[bool]
    r"""Traverse and include files from subdirectories. Leave this option enabled to ensure that all nested directories are searched and their contents collected."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    extractors: NotRequired[List[CollectorS3Extractor2TypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    aws_authentication_method: NotRequired[AuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""
    aws_api_key: NotRequired[str]
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    aws_secret_key: NotRequired[str]
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references AWS access key and secret key."""


class CollectorS3S32(BaseModel):
    type: CollectorS3Type2
    r"""Collector type: s3"""

    bucket: str
    r"""S3 Bucket from which to collect data"""

    partitioning_scheme: Annotated[
        Annotated[
            Optional[PartitioningScheme2], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="partitioningScheme"),
    ] = PartitioningScheme2.NONE
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    recurse: Optional[bool] = True
    r"""Traverse and include files from subdirectories. Leave this option enabled to ensure that all nested directories are searched and their contents collected."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    extractors: Optional[List[CollectorS3Extractor2]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = True
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Disable Collector event time filtering when a date range is specified"""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references AWS access key and secret key."""

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.PartitioningScheme2(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


class PartitioningScheme1(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class CollectorS3Type1(str, Enum):
    r"""Collector type: s3"""

    S3 = "s3"


class CollectorS3Extractor1TypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class CollectorS3Extractor1(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class CollectorS3S31TypedDict(TypedDict):
    type: CollectorS3Type1
    r"""Collector type: s3"""
    bucket: str
    r"""S3 Bucket from which to collect data"""
    partitioning_scheme: NotRequired[PartitioningScheme1]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    extractors: NotRequired[List[CollectorS3Extractor1TypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    aws_authentication_method: NotRequired[AuthenticationMethodOptions]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    recurse: NotRequired[bool]
    r"""Traverse and include files from subdirectories. Leave this option enabled to ensure that all nested directories are searched and their contents collected."""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""
    aws_api_key: NotRequired[str]
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    aws_secret_key: NotRequired[str]
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references AWS access key and secret key."""


class CollectorS3S31(BaseModel):
    type: CollectorS3Type1
    r"""Collector type: s3"""

    bucket: str
    r"""S3 Bucket from which to collect data"""

    partitioning_scheme: Annotated[
        Annotated[
            Optional[PartitioningScheme1], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="partitioningScheme"),
    ] = PartitioningScheme1.NONE
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    extractors: Optional[List[CollectorS3Extractor1]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[AuthenticationMethodOptions],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = AuthenticationMethodOptions.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[SignatureVersionOptions], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = SignatureVersionOptions.V4
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    recurse: Optional[bool] = True
    r"""Traverse and include files from subdirectories. Leave this option enabled to ensure that all nested directories are searched and their contents collected."""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = True
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Disable Collector event time filtering when a date range is specified"""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references AWS access key and secret key."""

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.PartitioningScheme1(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value


CollectorS3TypedDict = TypeAliasType(
    "CollectorS3TypedDict",
    Union[
        CollectorS3S31TypedDict,
        CollectorS3S32TypedDict,
        CollectorS3S33TypedDict,
        CollectorS3S34TypedDict,
        CollectorS3S35TypedDict,
    ],
)


CollectorS3 = TypeAliasType(
    "CollectorS3",
    Union[
        CollectorS3S31, CollectorS3S32, CollectorS3S33, CollectorS3S34, CollectorS3S35
    ],
)
