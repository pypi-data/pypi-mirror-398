"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class CollectorGoogleCloudStorageAuthenticationMethod3(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Enter account credentials manually, select a secret that references your credentials, or use Google Application Default Credentials"""

    AUTO = "auto"
    MANUAL = "manual"
    SECRET = "secret"


class CollectorGoogleCloudStorageType3(str, Enum):
    r"""Collector type: google_cloud_storage"""

    GOOGLE_CLOUD_STORAGE = "google_cloud_storage"


class CollectorGoogleCloudStorageExtractor3TypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields, such as {date: new Date(+value*1000)}"""


class CollectorGoogleCloudStorageExtractor3(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields, such as {date: new Date(+value*1000)}"""


class CollectorGoogleCloudStorageGoogleCloudStorage3TypedDict(TypedDict):
    text_secret: str
    r"""Select or create a stored text secret that references your credentials"""
    type: CollectorGoogleCloudStorageType3
    r"""Collector type: google_cloud_storage"""
    bucket: str
    r"""Name of the bucket to collect from. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`."""
    auth_type: NotRequired[CollectorGoogleCloudStorageAuthenticationMethod3]
    r"""Enter account credentials manually, select a secret that references your credentials, or use Google Application Default Credentials"""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    path: NotRequired[str]
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are also supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""
    extractors: NotRequired[List[CollectorGoogleCloudStorageExtractor3TypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    endpoint: NotRequired[str]
    r"""Google Cloud Storage service endpoint. If empty, the endpoint will default to https://storage.googleapis.com."""
    disable_time_filter: NotRequired[bool]
    r"""Used to disable Collector event time filtering when a date range is specified"""
    recurse: NotRequired[bool]
    r"""Recurse through subdirectories"""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""
    service_account_credentials: NotRequired[str]
    r"""Contents of Google Cloud service account credentials (JSON keys) file. To upload a file, click the upload button at this field's upper right."""


class CollectorGoogleCloudStorageGoogleCloudStorage3(BaseModel):
    text_secret: Annotated[str, pydantic.Field(alias="textSecret")]
    r"""Select or create a stored text secret that references your credentials"""

    type: CollectorGoogleCloudStorageType3
    r"""Collector type: google_cloud_storage"""

    bucket: str
    r"""Name of the bucket to collect from. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`."""

    auth_type: Annotated[
        Annotated[
            Optional[CollectorGoogleCloudStorageAuthenticationMethod3],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = CollectorGoogleCloudStorageAuthenticationMethod3.MANUAL
    r"""Enter account credentials manually, select a secret that references your credentials, or use Google Application Default Credentials"""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    path: Optional[str] = None
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are also supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""

    extractors: Optional[List[CollectorGoogleCloudStorageExtractor3]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    endpoint: Optional[str] = None
    r"""Google Cloud Storage service endpoint. If empty, the endpoint will default to https://storage.googleapis.com."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Used to disable Collector event time filtering when a date range is specified"""

    recurse: Optional[bool] = True
    r"""Recurse through subdirectories"""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of Google Cloud service account credentials (JSON keys) file. To upload a file, click the upload button at this field's upper right."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.CollectorGoogleCloudStorageAuthenticationMethod3(value)
            except ValueError:
                return value
        return value


class CollectorGoogleCloudStorageAuthenticationMethod2(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Enter account credentials manually, select a secret that references your credentials, or use Google Application Default Credentials"""

    AUTO = "auto"
    MANUAL = "manual"
    SECRET = "secret"


class CollectorGoogleCloudStorageType2(str, Enum):
    r"""Collector type: google_cloud_storage"""

    GOOGLE_CLOUD_STORAGE = "google_cloud_storage"


class CollectorGoogleCloudStorageExtractor2TypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields, such as {date: new Date(+value*1000)}"""


class CollectorGoogleCloudStorageExtractor2(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields, such as {date: new Date(+value*1000)}"""


class CollectorGoogleCloudStorageGoogleCloudStorage2TypedDict(TypedDict):
    service_account_credentials: str
    r"""Contents of Google Cloud service account credentials (JSON keys) file. To upload a file, click the upload button at this field's upper right."""
    type: CollectorGoogleCloudStorageType2
    r"""Collector type: google_cloud_storage"""
    bucket: str
    r"""Name of the bucket to collect from. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`."""
    auth_type: NotRequired[CollectorGoogleCloudStorageAuthenticationMethod2]
    r"""Enter account credentials manually, select a secret that references your credentials, or use Google Application Default Credentials"""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    path: NotRequired[str]
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are also supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""
    extractors: NotRequired[List[CollectorGoogleCloudStorageExtractor2TypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    endpoint: NotRequired[str]
    r"""Google Cloud Storage service endpoint. If empty, the endpoint will default to https://storage.googleapis.com."""
    disable_time_filter: NotRequired[bool]
    r"""Used to disable Collector event time filtering when a date range is specified"""
    recurse: NotRequired[bool]
    r"""Recurse through subdirectories"""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret that references your credentials"""


class CollectorGoogleCloudStorageGoogleCloudStorage2(BaseModel):
    service_account_credentials: Annotated[
        str, pydantic.Field(alias="serviceAccountCredentials")
    ]
    r"""Contents of Google Cloud service account credentials (JSON keys) file. To upload a file, click the upload button at this field's upper right."""

    type: CollectorGoogleCloudStorageType2
    r"""Collector type: google_cloud_storage"""

    bucket: str
    r"""Name of the bucket to collect from. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`."""

    auth_type: Annotated[
        Annotated[
            Optional[CollectorGoogleCloudStorageAuthenticationMethod2],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = CollectorGoogleCloudStorageAuthenticationMethod2.MANUAL
    r"""Enter account credentials manually, select a secret that references your credentials, or use Google Application Default Credentials"""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    path: Optional[str] = None
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are also supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""

    extractors: Optional[List[CollectorGoogleCloudStorageExtractor2]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    endpoint: Optional[str] = None
    r"""Google Cloud Storage service endpoint. If empty, the endpoint will default to https://storage.googleapis.com."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Used to disable Collector event time filtering when a date range is specified"""

    recurse: Optional[bool] = True
    r"""Recurse through subdirectories"""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret that references your credentials"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.CollectorGoogleCloudStorageAuthenticationMethod2(value)
            except ValueError:
                return value
        return value


class CollectorGoogleCloudStorageAuthenticationMethod1(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Enter account credentials manually, select a secret that references your credentials, or use Google Application Default Credentials"""

    AUTO = "auto"
    MANUAL = "manual"
    SECRET = "secret"


class CollectorGoogleCloudStorageType1(str, Enum):
    r"""Collector type: google_cloud_storage"""

    GOOGLE_CLOUD_STORAGE = "google_cloud_storage"


class CollectorGoogleCloudStorageExtractor1TypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields, such as {date: new Date(+value*1000)}"""


class CollectorGoogleCloudStorageExtractor1(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields, such as {date: new Date(+value*1000)}"""


class CollectorGoogleCloudStorageGoogleCloudStorage1TypedDict(TypedDict):
    type: CollectorGoogleCloudStorageType1
    r"""Collector type: google_cloud_storage"""
    bucket: str
    r"""Name of the bucket to collect from. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`."""
    auth_type: NotRequired[CollectorGoogleCloudStorageAuthenticationMethod1]
    r"""Enter account credentials manually, select a secret that references your credentials, or use Google Application Default Credentials"""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    path: NotRequired[str]
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are also supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""
    extractors: NotRequired[List[CollectorGoogleCloudStorageExtractor1TypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    endpoint: NotRequired[str]
    r"""Google Cloud Storage service endpoint. If empty, the endpoint will default to https://storage.googleapis.com."""
    disable_time_filter: NotRequired[bool]
    r"""Used to disable Collector event time filtering when a date range is specified"""
    recurse: NotRequired[bool]
    r"""Recurse through subdirectories"""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""
    service_account_credentials: NotRequired[str]
    r"""Contents of Google Cloud service account credentials (JSON keys) file. To upload a file, click the upload button at this field's upper right."""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret that references your credentials"""


class CollectorGoogleCloudStorageGoogleCloudStorage1(BaseModel):
    type: CollectorGoogleCloudStorageType1
    r"""Collector type: google_cloud_storage"""

    bucket: str
    r"""Name of the bucket to collect from. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`."""

    auth_type: Annotated[
        Annotated[
            Optional[CollectorGoogleCloudStorageAuthenticationMethod1],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="authType"),
    ] = CollectorGoogleCloudStorageAuthenticationMethod1.MANUAL
    r"""Enter account credentials manually, select a secret that references your credentials, or use Google Application Default Credentials"""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    path: Optional[str] = None
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are also supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""

    extractors: Optional[List[CollectorGoogleCloudStorageExtractor1]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    endpoint: Optional[str] = None
    r"""Google Cloud Storage service endpoint. If empty, the endpoint will default to https://storage.googleapis.com."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Used to disable Collector event time filtering when a date range is specified"""

    recurse: Optional[bool] = True
    r"""Recurse through subdirectories"""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""

    service_account_credentials: Annotated[
        Optional[str], pydantic.Field(alias="serviceAccountCredentials")
    ] = None
    r"""Contents of Google Cloud service account credentials (JSON keys) file. To upload a file, click the upload button at this field's upper right."""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret that references your credentials"""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.CollectorGoogleCloudStorageAuthenticationMethod1(value)
            except ValueError:
                return value
        return value


CollectorGoogleCloudStorageTypedDict = TypeAliasType(
    "CollectorGoogleCloudStorageTypedDict",
    Union[
        CollectorGoogleCloudStorageGoogleCloudStorage1TypedDict,
        CollectorGoogleCloudStorageGoogleCloudStorage2TypedDict,
        CollectorGoogleCloudStorageGoogleCloudStorage3TypedDict,
    ],
)


CollectorGoogleCloudStorage = TypeAliasType(
    "CollectorGoogleCloudStorage",
    Union[
        CollectorGoogleCloudStorageGoogleCloudStorage1,
        CollectorGoogleCloudStorageGoogleCloudStorage2,
        CollectorGoogleCloudStorageGoogleCloudStorage3,
    ],
)
