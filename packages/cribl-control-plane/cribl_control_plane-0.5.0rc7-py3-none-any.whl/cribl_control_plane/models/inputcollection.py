"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .itemstypeconnections import ItemsTypeConnections, ItemsTypeConnectionsTypedDict
from .itemstypenotificationmetadata import (
    ItemsTypeNotificationMetadata,
    ItemsTypeNotificationMetadataTypedDict,
)
from .pqtype import PqType, PqTypeTypedDict
from .preprocesstypesavedjobcollectioninput import (
    PreprocessTypeSavedJobCollectionInput,
    PreprocessTypeSavedJobCollectionInputTypedDict,
)
from cribl_control_plane.types import BaseModel
from enum import Enum
import pydantic
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class InputCollectionType(str, Enum):
    COLLECTION = "collection"


class InputCollectionTypedDict(TypedDict):
    id: NotRequired[str]
    r"""Unique ID for this input"""
    type: NotRequired[InputCollectionType]
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process results"""
    send_to_routes: NotRequired[bool]
    r"""Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    preprocess: NotRequired[PreprocessTypeSavedJobCollectionInputTypedDict]
    throttle_rate_per_sec: NotRequired[str]
    r"""Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling."""
    metadata: NotRequired[List[ItemsTypeNotificationMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    output: NotRequired[str]
    r"""Destination to send results to"""


class InputCollection(BaseModel):
    id: Optional[str] = None
    r"""Unique ID for this input"""

    type: Optional[InputCollectionType] = InputCollectionType.COLLECTION

    disabled: Optional[bool] = False

    pipeline: Optional[str] = None
    r"""Pipeline to process results"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        True
    )
    r"""Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = False
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnections]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = 10000
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    preprocess: Optional[PreprocessTypeSavedJobCollectionInput] = None

    throttle_rate_per_sec: Annotated[
        Optional[str], pydantic.Field(alias="throttleRatePerSec")
    ] = "0"
    r"""Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling."""

    metadata: Optional[List[ItemsTypeNotificationMetadata]] = None
    r"""Fields to add to events from this input"""

    output: Optional[str] = None
    r"""Destination to send results to"""
