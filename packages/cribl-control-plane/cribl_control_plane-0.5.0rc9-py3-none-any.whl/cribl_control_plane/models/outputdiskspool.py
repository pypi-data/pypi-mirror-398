"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class OutputDiskSpoolType(str, Enum):
    DISK_SPOOL = "disk_spool"


class OutputDiskSpoolCompression(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Data compression format. Default is gzip."""

    NONE = "none"
    GZIP = "gzip"


class OutputDiskSpoolTypedDict(TypedDict):
    type: OutputDiskSpoolType
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    time_window: NotRequired[str]
    r"""Time period for grouping spooled events. Default is 10m."""
    max_data_size: NotRequired[str]
    r"""Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB."""
    max_data_time: NotRequired[str]
    r"""Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h."""
    compress: NotRequired[OutputDiskSpoolCompression]
    r"""Data compression format. Default is gzip."""
    partition_expr: NotRequired[str]
    r"""JavaScript expression defining how files are partitioned and organized within the time-buckets. If blank, the event's __partition property is used and otherwise, events go directly into the time-bucket directory."""
    description: NotRequired[str]


class OutputDiskSpool(BaseModel):
    type: OutputDiskSpoolType

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    time_window: Annotated[Optional[str], pydantic.Field(alias="timeWindow")] = "10m"
    r"""Time period for grouping spooled events. Default is 10m."""

    max_data_size: Annotated[Optional[str], pydantic.Field(alias="maxDataSize")] = "1GB"
    r"""Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB."""

    max_data_time: Annotated[Optional[str], pydantic.Field(alias="maxDataTime")] = "24h"
    r"""Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h."""

    compress: Annotated[
        Optional[OutputDiskSpoolCompression], PlainValidator(validate_open_enum(False))
    ] = OutputDiskSpoolCompression.GZIP
    r"""Data compression format. Default is gzip."""

    partition_expr: Annotated[Optional[str], pydantic.Field(alias="partitionExpr")] = (
        None
    )
    r"""JavaScript expression defining how files are partitioned and organized within the time-buckets. If blank, the event's __partition property is used and otherwise, events go directly into the time-bucket directory."""

    description: Optional[str] = None

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDiskSpoolCompression(value)
            except ValueError:
                return value
        return value
