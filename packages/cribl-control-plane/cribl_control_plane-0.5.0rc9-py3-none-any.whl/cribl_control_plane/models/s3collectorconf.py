"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import get_discriminator, validate_open_enum
from enum import Enum
import pydantic
from pydantic import Discriminator, Tag, field_serializer
from pydantic.functional_validators import PlainValidator
from typing import Any, List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class S3AwsAuthenticationMethodSecretAuthenticationMethod(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class S3AwsAuthenticationMethodSecretPartitioningScheme(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class S3AwsAuthenticationMethodSecretExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3AwsAuthenticationMethodSecretExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3AwsAuthenticationMethodSecretSignatureVersion(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Signature version to use for signing S3 requests"""

    V2 = "v2"
    V4 = "v4"


class S3AwsAuthenticationMethodSecretTypedDict(TypedDict):
    bucket: str
    r"""S3 Bucket from which to collect data"""
    aws_authentication_method: NotRequired[
        S3AwsAuthenticationMethodSecretAuthenticationMethod
    ]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references AWS access key and secret key."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    partitioning_scheme: NotRequired[S3AwsAuthenticationMethodSecretPartitioningScheme]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    extractors: NotRequired[List[S3AwsAuthenticationMethodSecretExtractorTypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[S3AwsAuthenticationMethodSecretSignatureVersion]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    recurse: NotRequired[Any]
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""


class S3AwsAuthenticationMethodSecret(BaseModel):
    bucket: str
    r"""S3 Bucket from which to collect data"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[S3AwsAuthenticationMethodSecretAuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = S3AwsAuthenticationMethodSecretAuthenticationMethod.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references AWS access key and secret key."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    partitioning_scheme: Annotated[
        Annotated[
            Optional[S3AwsAuthenticationMethodSecretPartitioningScheme],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="partitioningScheme"),
    ] = S3AwsAuthenticationMethodSecretPartitioningScheme.NONE
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    extractors: Optional[List[S3AwsAuthenticationMethodSecretExtractor]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[S3AwsAuthenticationMethodSecretSignatureVersion],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = S3AwsAuthenticationMethodSecretSignatureVersion.V4
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    recurse: Optional[Any] = None

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = True
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Disable Collector event time filtering when a date range is specified"""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.S3AwsAuthenticationMethodSecretAuthenticationMethod(value)
            except ValueError:
                return value
        return value

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.S3AwsAuthenticationMethodSecretPartitioningScheme(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.S3AwsAuthenticationMethodSecretSignatureVersion(value)
            except ValueError:
                return value
        return value


class S3AwsAuthenticationMethodManualAuthenticationMethod(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class S3AwsAuthenticationMethodManualPartitioningScheme(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class S3AwsAuthenticationMethodManualExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3AwsAuthenticationMethodManualExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3AwsAuthenticationMethodManualSignatureVersion(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Signature version to use for signing S3 requests"""

    V2 = "v2"
    V4 = "v4"


class S3AwsAuthenticationMethodManualTypedDict(TypedDict):
    bucket: str
    r"""S3 Bucket from which to collect data"""
    aws_authentication_method: NotRequired[
        S3AwsAuthenticationMethodManualAuthenticationMethod
    ]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_api_key: NotRequired[str]
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    aws_secret_key: NotRequired[str]
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    partitioning_scheme: NotRequired[S3AwsAuthenticationMethodManualPartitioningScheme]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    extractors: NotRequired[List[S3AwsAuthenticationMethodManualExtractorTypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[S3AwsAuthenticationMethodManualSignatureVersion]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    recurse: NotRequired[Any]
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""


class S3AwsAuthenticationMethodManual(BaseModel):
    bucket: str
    r"""S3 Bucket from which to collect data"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[S3AwsAuthenticationMethodManualAuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = S3AwsAuthenticationMethodManualAuthenticationMethod.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None
    r"""Access key. If not present, will fall back to env.AWS_ACCESS_KEY_ID, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )
    r"""Secret key. If not present, will fall back to env.AWS_SECRET_ACCESS_KEY, or to the metadata endpoint for IAM creds. Optional when running on AWS. This value can be a constant or a JavaScript expression."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    partitioning_scheme: Annotated[
        Annotated[
            Optional[S3AwsAuthenticationMethodManualPartitioningScheme],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="partitioningScheme"),
    ] = S3AwsAuthenticationMethodManualPartitioningScheme.NONE
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    extractors: Optional[List[S3AwsAuthenticationMethodManualExtractor]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[S3AwsAuthenticationMethodManualSignatureVersion],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = S3AwsAuthenticationMethodManualSignatureVersion.V4
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    recurse: Optional[Any] = None

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = True
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Disable Collector event time filtering when a date range is specified"""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.S3AwsAuthenticationMethodManualAuthenticationMethod(value)
            except ValueError:
                return value
        return value

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.S3AwsAuthenticationMethodManualPartitioningScheme(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.S3AwsAuthenticationMethodManualSignatureVersion(value)
            except ValueError:
                return value
        return value


class S3AwsAuthenticationMethodAutoAuthenticationMethod(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class S3AwsAuthenticationMethodAutoPartitioningScheme(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class S3AwsAuthenticationMethodAutoExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3AwsAuthenticationMethodAutoExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3AwsAuthenticationMethodAutoSignatureVersion(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Signature version to use for signing S3 requests"""

    V2 = "v2"
    V4 = "v4"


class S3AwsAuthenticationMethodAutoTypedDict(TypedDict):
    bucket: str
    r"""S3 Bucket from which to collect data"""
    aws_authentication_method: NotRequired[
        S3AwsAuthenticationMethodAutoAuthenticationMethod
    ]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    partitioning_scheme: NotRequired[S3AwsAuthenticationMethodAutoPartitioningScheme]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    extractors: NotRequired[List[S3AwsAuthenticationMethodAutoExtractorTypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[S3AwsAuthenticationMethodAutoSignatureVersion]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    recurse: NotRequired[Any]
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""


class S3AwsAuthenticationMethodAuto(BaseModel):
    bucket: str
    r"""S3 Bucket from which to collect data"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[S3AwsAuthenticationMethodAutoAuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = S3AwsAuthenticationMethodAutoAuthenticationMethod.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    partitioning_scheme: Annotated[
        Annotated[
            Optional[S3AwsAuthenticationMethodAutoPartitioningScheme],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="partitioningScheme"),
    ] = S3AwsAuthenticationMethodAutoPartitioningScheme.NONE
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    extractors: Optional[List[S3AwsAuthenticationMethodAutoExtractor]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[S3AwsAuthenticationMethodAutoSignatureVersion],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = S3AwsAuthenticationMethodAutoSignatureVersion.V4
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    recurse: Optional[Any] = None

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = True
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Disable Collector event time filtering when a date range is specified"""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.S3AwsAuthenticationMethodAutoAuthenticationMethod(value)
            except ValueError:
                return value
        return value

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.S3AwsAuthenticationMethodAutoPartitioningScheme(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.S3AwsAuthenticationMethodAutoSignatureVersion(value)
            except ValueError:
                return value
        return value


class S3PartitioningSchemeNonePartitioningScheme(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class S3PartitioningSchemeNoneExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3PartitioningSchemeNoneExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3PartitioningSchemeNoneAuthenticationMethod(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class S3PartitioningSchemeNoneSignatureVersion(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing S3 requests"""

    V2 = "v2"
    V4 = "v4"


class S3PartitioningSchemeNoneTypedDict(TypedDict):
    bucket: str
    r"""S3 Bucket from which to collect data"""
    partitioning_scheme: NotRequired[S3PartitioningSchemeNonePartitioningScheme]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    recurse: NotRequired[Any]
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    extractors: NotRequired[List[S3PartitioningSchemeNoneExtractorTypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    aws_authentication_method: NotRequired[S3PartitioningSchemeNoneAuthenticationMethod]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[S3PartitioningSchemeNoneSignatureVersion]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""


class S3PartitioningSchemeNone(BaseModel):
    bucket: str
    r"""S3 Bucket from which to collect data"""

    partitioning_scheme: Annotated[
        Annotated[
            Optional[S3PartitioningSchemeNonePartitioningScheme],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="partitioningScheme"),
    ] = S3PartitioningSchemeNonePartitioningScheme.NONE
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    recurse: Optional[Any] = None

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    extractors: Optional[List[S3PartitioningSchemeNoneExtractor]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[S3PartitioningSchemeNoneAuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = S3PartitioningSchemeNoneAuthenticationMethod.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[S3PartitioningSchemeNoneSignatureVersion],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = S3PartitioningSchemeNoneSignatureVersion.V4
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = True
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Disable Collector event time filtering when a date range is specified"""

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.S3PartitioningSchemeNonePartitioningScheme(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.S3PartitioningSchemeNoneAuthenticationMethod(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.S3PartitioningSchemeNoneSignatureVersion(value)
            except ValueError:
                return value
        return value


class S3PartitioningSchemeDdssPartitioningScheme(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    # Defined in Path
    NONE = "none"
    # DDSS
    DDSS = "ddss"


class S3PartitioningSchemeDdssExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3PartitioningSchemeDdssExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""JavaScript expression that receives token under \"value\" variable, and evaluates to populate event fields. Example: {date: new Date(+value*1000)}"""


class S3PartitioningSchemeDdssAuthenticationMethod(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class S3PartitioningSchemeDdssSignatureVersion(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing S3 requests"""

    V2 = "v2"
    V4 = "v4"


class S3PartitioningSchemeDdssTypedDict(TypedDict):
    bucket: str
    r"""S3 Bucket from which to collect data"""
    partitioning_scheme: NotRequired[S3PartitioningSchemeDdssPartitioningScheme]
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""
    output_name: NotRequired[str]
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    region: NotRequired[str]
    r"""Region from which to retrieve data"""
    path: NotRequired[str]
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""
    extractors: NotRequired[List[S3PartitioningSchemeDdssExtractorTypedDict]]
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""
    aws_authentication_method: NotRequired[S3PartitioningSchemeDdssAuthenticationMethod]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    endpoint: NotRequired[str]
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""
    signature_version: NotRequired[S3PartitioningSchemeDdssSignatureVersion]
    r"""Signature version to use for signing S3 requests"""
    enable_assume_role: NotRequired[bool]
    r"""Use AssumeRole credentials"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    recurse: NotRequired[Any]
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests to improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""
    disable_time_filter: NotRequired[bool]
    r"""Disable Collector event time filtering when a date range is specified"""


class S3PartitioningSchemeDdss(BaseModel):
    bucket: str
    r"""S3 Bucket from which to collect data"""

    partitioning_scheme: Annotated[
        Annotated[
            Optional[S3PartitioningSchemeDdssPartitioningScheme],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="partitioningScheme"),
    ] = S3PartitioningSchemeDdssPartitioningScheme.NONE
    r"""Partitioning scheme used for this dataset. Using a known scheme like DDSS enables more efficient data reading and retrieval."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""Name of the predefined Destination that will be used to auto-populate Collector settings"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = 5
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = 600
    r"""Maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    region: Optional[str] = None
    r"""Region from which to retrieve data"""

    path: Optional[str] = None
    r"""Directory where data will be collected. Templating (such as 'myDir/${datacenter}/${host}/${app}/') and time-based tokens (such as 'myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/') are supported. Can be a constant (enclosed in quotes) or a JavaScript expression."""

    extractors: Optional[List[S3PartitioningSchemeDdssExtractor]] = None
    r"""Allows using template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)}, will enrich discovery results with a human readable \"date\" field."""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[S3PartitioningSchemeDdssAuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = S3PartitioningSchemeDdssAuthenticationMethod.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    endpoint: Optional[str] = None
    r"""Must point to an S3-compatible endpoint. If empty, defaults to an AWS region-specific endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[S3PartitioningSchemeDdssSignatureVersion],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = S3PartitioningSchemeDdssSignatureVersion.V4
    r"""Signature version to use for signing S3 requests"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use AssumeRole credentials"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the Assumed Role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        10
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    recurse: Optional[Any] = None

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests to improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA (such as a self-signed certificate)"""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = True
    r"""Disable if you can access files within the bucket but not the bucket itself. Resolves errors of the form \"discover task initialization failed...error: Forbidden\"."""

    disable_time_filter: Annotated[
        Optional[bool], pydantic.Field(alias="disableTimeFilter")
    ] = False
    r"""Disable Collector event time filtering when a date range is specified"""

    @field_serializer("partitioning_scheme")
    def serialize_partitioning_scheme(self, value):
        if isinstance(value, str):
            try:
                return models.S3PartitioningSchemeDdssPartitioningScheme(value)
            except ValueError:
                return value
        return value

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.S3PartitioningSchemeDdssAuthenticationMethod(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.S3PartitioningSchemeDdssSignatureVersion(value)
            except ValueError:
                return value
        return value


S3CollectorConfTypedDict = TypeAliasType(
    "S3CollectorConfTypedDict",
    Union[
        S3PartitioningSchemeDdssTypedDict,
        S3PartitioningSchemeNoneTypedDict,
        S3AwsAuthenticationMethodAutoTypedDict,
        S3AwsAuthenticationMethodSecretTypedDict,
        S3AwsAuthenticationMethodManualTypedDict,
    ],
)


S3CollectorConf = Annotated[
    Union[
        Annotated[S3AwsAuthenticationMethodAuto, Tag("auto")],
        Annotated[S3AwsAuthenticationMethodManual, Tag("manual")],
        Annotated[S3AwsAuthenticationMethodSecret, Tag("secret")],
    ],
    Discriminator(
        lambda m: get_discriminator(
            m, "aws_authentication_method", "awsAuthenticationMethod"
        )
    ),
]
