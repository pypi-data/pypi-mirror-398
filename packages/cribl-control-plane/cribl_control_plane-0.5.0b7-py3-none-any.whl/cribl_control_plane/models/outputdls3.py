"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic import field_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class OutputDlS3Type(str, Enum):
    DL_S3 = "dl_s3"


class OutputDlS3AuthenticationMethod(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    # Auto
    AUTO = "auto"
    # Manual
    MANUAL = "manual"
    # Secret Key pair
    SECRET = "secret"


class OutputDlS3SignatureVersion(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing S3 requests"""

    V2 = "v2"
    V4 = "v4"


class OutputDlS3ObjectACL(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Object ACL to assign to uploaded objects"""

    # Private
    PRIVATE = "private"
    # Public Read Only
    PUBLIC_READ = "public-read"
    # Public Read/Write
    PUBLIC_READ_WRITE = "public-read-write"
    # Authenticated Read Only
    AUTHENTICATED_READ = "authenticated-read"
    # AWS EC2 AMI Read Only
    AWS_EXEC_READ = "aws-exec-read"
    # Bucket Owner Read Only
    BUCKET_OWNER_READ = "bucket-owner-read"
    # Bucket Owner Full Control
    BUCKET_OWNER_FULL_CONTROL = "bucket-owner-full-control"


class OutputDlS3StorageClass(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Storage class to select for uploaded objects"""

    # Standard
    STANDARD = "STANDARD"
    # Reduced Redundancy Storage
    REDUCED_REDUNDANCY = "REDUCED_REDUNDANCY"
    # Standard, Infrequent Access
    STANDARD_IA = "STANDARD_IA"
    # One Zone, Infrequent Access
    ONEZONE_IA = "ONEZONE_IA"
    # Intelligent Tiering
    INTELLIGENT_TIERING = "INTELLIGENT_TIERING"
    # Glacier Flexible Retrieval
    GLACIER = "GLACIER"
    # Glacier Instant Retrieval
    GLACIER_IR = "GLACIER_IR"
    # Glacier Deep Archive
    DEEP_ARCHIVE = "DEEP_ARCHIVE"


class OutputDlS3ServerSideEncryptionForUploadedObjects(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    # Amazon S3 Managed Key
    AES256 = "AES256"
    # AWS KMS Managed Key
    AWS_KMS = "aws:kms"


class OutputDlS3DataFormat(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Format of the output data"""

    # JSON
    JSON = "json"
    # Raw
    RAW = "raw"
    # Parquet
    PARQUET = "parquet"


class OutputDlS3BackpressureBehavior(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""How to handle events when all receivers are exerting backpressure"""

    # Block
    BLOCK = "block"
    # Drop
    DROP = "drop"


class OutputDlS3DiskSpaceProtection(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""How to handle events when disk space is below the global 'Min free disk space' limit"""

    # Block
    BLOCK = "block"
    # Drop
    DROP = "drop"


class OutputDlS3Compression(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Data compression format to apply to HTTP content before it is delivered"""

    NONE = "none"
    GZIP = "gzip"


class OutputDlS3CompressionLevel(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Compression level to apply before moving files to final destination"""

    # Best Speed
    BEST_SPEED = "best_speed"
    # Normal
    NORMAL = "normal"
    # Best Compression
    BEST_COMPRESSION = "best_compression"


class OutputDlS3ParquetVersion(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Determines which data types are supported and how they are represented"""

    # 1.0
    PARQUET_1_0 = "PARQUET_1_0"
    # 2.4
    PARQUET_2_4 = "PARQUET_2_4"
    # 2.6
    PARQUET_2_6 = "PARQUET_2_6"


class OutputDlS3DataPageVersion(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it."""

    # V1
    DATA_PAGE_V1 = "DATA_PAGE_V1"
    # V2
    DATA_PAGE_V2 = "DATA_PAGE_V2"


class OutputDlS3KeyValueMetadatumTypedDict(TypedDict):
    value: str
    key: NotRequired[str]


class OutputDlS3KeyValueMetadatum(BaseModel):
    value: str

    key: Optional[str] = ""


class OutputDlS3TypedDict(TypedDict):
    type: OutputDlS3Type
    bucket: str
    r"""Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`"""
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    region: NotRequired[str]
    r"""Region where the S3 bucket is located"""
    aws_secret_key: NotRequired[str]
    r"""Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)"""
    aws_authentication_method: NotRequired[OutputDlS3AuthenticationMethod]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    endpoint: NotRequired[str]
    r"""S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint."""
    signature_version: NotRequired[OutputDlS3SignatureVersion]
    r"""Signature version to use for signing S3 requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access S3"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    stage_path: NotRequired[str]
    r"""Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage."""
    add_id_to_stage_path: NotRequired[bool]
    r"""Add the Output ID value to staging location"""
    dest_path: NotRequired[str]
    r"""Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`"""
    object_acl: NotRequired[OutputDlS3ObjectACL]
    r"""Object ACL to assign to uploaded objects"""
    storage_class: NotRequired[OutputDlS3StorageClass]
    r"""Storage class to select for uploaded objects"""
    server_side_encryption: NotRequired[
        OutputDlS3ServerSideEncryptionForUploadedObjects
    ]
    kms_key_id: NotRequired[str]
    r"""ID or ARN of the KMS customer-managed key to use for encryption"""
    remove_empty_dirs: NotRequired[bool]
    r"""Remove empty staging directories after moving files"""
    format_: NotRequired[OutputDlS3DataFormat]
    r"""Format of the output data"""
    base_file_name: NotRequired[str]
    r"""JavaScript expression to define the output filename prefix (can be constant)"""
    file_name_suffix: NotRequired[str]
    r"""JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`)."""
    max_file_size_mb: NotRequired[float]
    r"""Maximum uncompressed output file size. Files of this size will be closed and moved to final output location."""
    max_open_files: NotRequired[float]
    r"""Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location."""
    header_line: NotRequired[str]
    r"""If set, this line will be written to the beginning of each output file"""
    write_high_water_mark: NotRequired[float]
    r"""Buffer size used to write to a file"""
    on_backpressure: NotRequired[OutputDlS3BackpressureBehavior]
    r"""How to handle events when all receivers are exerting backpressure"""
    deadletter_enabled: NotRequired[bool]
    r"""If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors"""
    on_disk_full_backpressure: NotRequired[OutputDlS3DiskSpaceProtection]
    r"""How to handle events when disk space is below the global 'Min free disk space' limit"""
    force_close_on_shutdown: NotRequired[bool]
    r"""Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss."""
    max_file_open_time_sec: NotRequired[float]
    r"""Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location."""
    max_file_idle_time_sec: NotRequired[float]
    r"""Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location."""
    max_concurrent_file_parts: NotRequired[float]
    r"""Maximum number of parts to upload in parallel per file. Minimum part size is 5MB."""
    verify_permissions: NotRequired[bool]
    r"""Disable if you can access files within the bucket but not the bucket itself"""
    max_closing_files_to_backpressure: NotRequired[float]
    r"""Maximum number of files that can be waiting for upload before backpressure is applied"""
    partitioning_fields: NotRequired[List[str]]
    r"""List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>."""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    r"""This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)"""
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    compress: NotRequired[OutputDlS3Compression]
    r"""Data compression format to apply to HTTP content before it is delivered"""
    compression_level: NotRequired[OutputDlS3CompressionLevel]
    r"""Compression level to apply before moving files to final destination"""
    automatic_schema: NotRequired[bool]
    r"""Automatically calculate the schema based on the events of each Parquet file generated"""
    parquet_schema: NotRequired[str]
    r"""To add a new schema, navigate to Processing > Knowledge > Parquet Schemas"""
    parquet_version: NotRequired[OutputDlS3ParquetVersion]
    r"""Determines which data types are supported and how they are represented"""
    parquet_data_page_version: NotRequired[OutputDlS3DataPageVersion]
    r"""Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it."""
    parquet_row_group_length: NotRequired[float]
    r"""The number of rows that every group will contain. The final group can contain a smaller number of rows."""
    parquet_page_size: NotRequired[str]
    r"""Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression."""
    should_log_invalid_rows: NotRequired[bool]
    r"""Log up to 3 rows that @{product} skips due to data mismatch"""
    key_value_metadata: NotRequired[List[OutputDlS3KeyValueMetadatumTypedDict]]
    r"""The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: \"key\":\"OCSF Event Class\", \"value\":\"9001\" """
    enable_statistics: NotRequired[bool]
    r"""Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics."""
    enable_write_page_index: NotRequired[bool]
    r"""One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping."""
    enable_page_checksum: NotRequired[bool]
    r"""Parquet tools can use the checksum of a Parquet page to verify data integrity"""
    empty_dir_cleanup_sec: NotRequired[float]
    r"""How frequently, in seconds, to clean up empty directories"""
    directory_batch_size: NotRequired[float]
    r"""Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory."""
    deadletter_path: NotRequired[str]
    r"""Storage location for files that fail to reach their final destination after maximum retries are exceeded"""
    max_retry_num: NotRequired[float]
    r"""The maximum number of times a file will attempt to move to its final destination before being dead-lettered"""


class OutputDlS3(BaseModel):
    type: OutputDlS3Type

    bucket: str
    r"""Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`"""

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    region: Optional[str] = None
    r"""Region where the S3 bucket is located"""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )
    r"""Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[OutputDlS3AuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = OutputDlS3AuthenticationMethod.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    endpoint: Optional[str] = None
    r"""S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[OutputDlS3SignatureVersion],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = OutputDlS3SignatureVersion.V4
    r"""Signature version to use for signing S3 requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access S3"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    stage_path: Annotated[Optional[str], pydantic.Field(alias="stagePath")] = (
        "$CRIBL_HOME/state/outputs/staging"
    )
    r"""Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage."""

    add_id_to_stage_path: Annotated[
        Optional[bool], pydantic.Field(alias="addIdToStagePath")
    ] = True
    r"""Add the Output ID value to staging location"""

    dest_path: Annotated[Optional[str], pydantic.Field(alias="destPath")] = ""
    r"""Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`"""

    object_acl: Annotated[
        Annotated[
            Optional[OutputDlS3ObjectACL], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="objectACL"),
    ] = OutputDlS3ObjectACL.PRIVATE
    r"""Object ACL to assign to uploaded objects"""

    storage_class: Annotated[
        Annotated[
            Optional[OutputDlS3StorageClass], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="storageClass"),
    ] = None
    r"""Storage class to select for uploaded objects"""

    server_side_encryption: Annotated[
        Annotated[
            Optional[OutputDlS3ServerSideEncryptionForUploadedObjects],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="serverSideEncryption"),
    ] = None

    kms_key_id: Annotated[Optional[str], pydantic.Field(alias="kmsKeyId")] = None
    r"""ID or ARN of the KMS customer-managed key to use for encryption"""

    remove_empty_dirs: Annotated[
        Optional[bool], pydantic.Field(alias="removeEmptyDirs")
    ] = True
    r"""Remove empty staging directories after moving files"""

    format_: Annotated[
        Annotated[
            Optional[OutputDlS3DataFormat], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="format"),
    ] = OutputDlS3DataFormat.JSON
    r"""Format of the output data"""

    base_file_name: Annotated[Optional[str], pydantic.Field(alias="baseFileName")] = (
        "`CriblOut`"
    )
    r"""JavaScript expression to define the output filename prefix (can be constant)"""

    file_name_suffix: Annotated[
        Optional[str], pydantic.Field(alias="fileNameSuffix")
    ] = '`.${C.env["CRIBL_WORKER_ID"]}.${__format}${__compression === "gzip" ? ".gz" : ""}`'
    r"""JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`)."""

    max_file_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="maxFileSizeMB")
    ] = 32
    r"""Maximum uncompressed output file size. Files of this size will be closed and moved to final output location."""

    max_open_files: Annotated[Optional[float], pydantic.Field(alias="maxOpenFiles")] = (
        100
    )
    r"""Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location."""

    header_line: Annotated[Optional[str], pydantic.Field(alias="headerLine")] = ""
    r"""If set, this line will be written to the beginning of each output file"""

    write_high_water_mark: Annotated[
        Optional[float], pydantic.Field(alias="writeHighWaterMark")
    ] = 64
    r"""Buffer size used to write to a file"""

    on_backpressure: Annotated[
        Annotated[
            Optional[OutputDlS3BackpressureBehavior],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OutputDlS3BackpressureBehavior.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    deadletter_enabled: Annotated[
        Optional[bool], pydantic.Field(alias="deadletterEnabled")
    ] = False
    r"""If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors"""

    on_disk_full_backpressure: Annotated[
        Annotated[
            Optional[OutputDlS3DiskSpaceProtection],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="onDiskFullBackpressure"),
    ] = OutputDlS3DiskSpaceProtection.BLOCK
    r"""How to handle events when disk space is below the global 'Min free disk space' limit"""

    force_close_on_shutdown: Annotated[
        Optional[bool], pydantic.Field(alias="forceCloseOnShutdown")
    ] = False
    r"""Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss."""

    max_file_open_time_sec: Annotated[
        Optional[float], pydantic.Field(alias="maxFileOpenTimeSec")
    ] = 300
    r"""Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location."""

    max_file_idle_time_sec: Annotated[
        Optional[float], pydantic.Field(alias="maxFileIdleTimeSec")
    ] = 30
    r"""Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location."""

    max_concurrent_file_parts: Annotated[
        Optional[float], pydantic.Field(alias="maxConcurrentFileParts")
    ] = 4
    r"""Maximum number of parts to upload in parallel per file. Minimum part size is 5MB."""

    verify_permissions: Annotated[
        Optional[bool], pydantic.Field(alias="verifyPermissions")
    ] = True
    r"""Disable if you can access files within the bucket but not the bucket itself"""

    max_closing_files_to_backpressure: Annotated[
        Optional[float], pydantic.Field(alias="maxClosingFilesToBackpressure")
    ] = 100
    r"""Maximum number of files that can be waiting for upload before backpressure is applied"""

    partitioning_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="partitioningFields")
    ] = None
    r"""List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>."""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None
    r"""This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)"""

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    compress: Annotated[
        Optional[OutputDlS3Compression], PlainValidator(validate_open_enum(False))
    ] = OutputDlS3Compression.GZIP
    r"""Data compression format to apply to HTTP content before it is delivered"""

    compression_level: Annotated[
        Annotated[
            Optional[OutputDlS3CompressionLevel],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="compressionLevel"),
    ] = OutputDlS3CompressionLevel.BEST_SPEED
    r"""Compression level to apply before moving files to final destination"""

    automatic_schema: Annotated[
        Optional[bool], pydantic.Field(alias="automaticSchema")
    ] = False
    r"""Automatically calculate the schema based on the events of each Parquet file generated"""

    parquet_schema: Annotated[Optional[str], pydantic.Field(alias="parquetSchema")] = (
        None
    )
    r"""To add a new schema, navigate to Processing > Knowledge > Parquet Schemas"""

    parquet_version: Annotated[
        Annotated[
            Optional[OutputDlS3ParquetVersion],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="parquetVersion"),
    ] = OutputDlS3ParquetVersion.PARQUET_2_6
    r"""Determines which data types are supported and how they are represented"""

    parquet_data_page_version: Annotated[
        Annotated[
            Optional[OutputDlS3DataPageVersion],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="parquetDataPageVersion"),
    ] = OutputDlS3DataPageVersion.DATA_PAGE_V2
    r"""Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it."""

    parquet_row_group_length: Annotated[
        Optional[float], pydantic.Field(alias="parquetRowGroupLength")
    ] = 10000
    r"""The number of rows that every group will contain. The final group can contain a smaller number of rows."""

    parquet_page_size: Annotated[
        Optional[str], pydantic.Field(alias="parquetPageSize")
    ] = "1MB"
    r"""Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression."""

    should_log_invalid_rows: Annotated[
        Optional[bool], pydantic.Field(alias="shouldLogInvalidRows")
    ] = None
    r"""Log up to 3 rows that @{product} skips due to data mismatch"""

    key_value_metadata: Annotated[
        Optional[List[OutputDlS3KeyValueMetadatum]],
        pydantic.Field(alias="keyValueMetadata"),
    ] = None
    r"""The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: \"key\":\"OCSF Event Class\", \"value\":\"9001\" """

    enable_statistics: Annotated[
        Optional[bool], pydantic.Field(alias="enableStatistics")
    ] = True
    r"""Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics."""

    enable_write_page_index: Annotated[
        Optional[bool], pydantic.Field(alias="enableWritePageIndex")
    ] = True
    r"""One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping."""

    enable_page_checksum: Annotated[
        Optional[bool], pydantic.Field(alias="enablePageChecksum")
    ] = False
    r"""Parquet tools can use the checksum of a Parquet page to verify data integrity"""

    empty_dir_cleanup_sec: Annotated[
        Optional[float], pydantic.Field(alias="emptyDirCleanupSec")
    ] = 300
    r"""How frequently, in seconds, to clean up empty directories"""

    directory_batch_size: Annotated[
        Optional[float], pydantic.Field(alias="directoryBatchSize")
    ] = 1000
    r"""Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory."""

    deadletter_path: Annotated[
        Optional[str], pydantic.Field(alias="deadletterPath")
    ] = "$CRIBL_HOME/state/outputs/dead-letter"
    r"""Storage location for files that fail to reach their final destination after maximum retries are exceeded"""

    max_retry_num: Annotated[Optional[float], pydantic.Field(alias="maxRetryNum")] = 20
    r"""The maximum number of times a file will attempt to move to its final destination before being dead-lettered"""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDlS3AuthenticationMethod(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDlS3SignatureVersion(value)
            except ValueError:
                return value
        return value

    @field_serializer("object_acl")
    def serialize_object_acl(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDlS3ObjectACL(value)
            except ValueError:
                return value
        return value

    @field_serializer("storage_class")
    def serialize_storage_class(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDlS3StorageClass(value)
            except ValueError:
                return value
        return value

    @field_serializer("server_side_encryption")
    def serialize_server_side_encryption(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDlS3ServerSideEncryptionForUploadedObjects(value)
            except ValueError:
                return value
        return value

    @field_serializer("format_")
    def serialize_format_(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDlS3DataFormat(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDlS3BackpressureBehavior(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_disk_full_backpressure")
    def serialize_on_disk_full_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDlS3DiskSpaceProtection(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDlS3Compression(value)
            except ValueError:
                return value
        return value

    @field_serializer("compression_level")
    def serialize_compression_level(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDlS3CompressionLevel(value)
            except ValueError:
                return value
        return value

    @field_serializer("parquet_version")
    def serialize_parquet_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDlS3ParquetVersion(value)
            except ValueError:
                return value
        return value

    @field_serializer("parquet_data_page_version")
    def serialize_parquet_data_page_version(self, value):
        if isinstance(value, str):
            try:
                return models.OutputDlS3DataPageVersion(value)
            except ValueError:
                return value
        return value
