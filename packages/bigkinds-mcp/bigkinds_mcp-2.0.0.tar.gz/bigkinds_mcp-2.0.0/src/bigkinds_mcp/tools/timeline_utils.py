"""íƒ€ì„ë¼ì¸ ë¶„ì„ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜.

ì´ë²¤íŠ¸ íƒì§€, í‚¤ì›Œë“œ ì¶”ì¶œ, ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • ë“±ì˜ NLP ê¸°ëŠ¥ ì œê³µ.
v2.0: kiwipiepy í˜•íƒœì†Œ ë¶„ì„, ë¶„ê¸°ë³„ í•„ìˆ˜ ì¶”ì¶œ, next_steps ìƒì„±
"""

from __future__ import annotations

import re
from collections import Counter
from typing import Any

# kiwipiepy lazy loading (ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™” ì§€ì—°)
_kiwi = None


def _get_kiwi():
    """Kiwi ì¸ìŠ¤í„´ìŠ¤ë¥¼ lazy loadingìœ¼ë¡œ ë°˜í™˜."""
    global _kiwi
    if _kiwi is None:
        from kiwipiepy import Kiwi
        _kiwi = Kiwi()
    return _kiwi


# í’ˆì‚¬ íƒœê·¸ ë¼ë²¨
POS_LABELS = {
    "NNP": "ê³ ìœ ëª…ì‚¬",
    "NNG": "ì¼ë°˜ëª…ì‚¬",
    "NNB": "ì˜ì¡´ëª…ì‚¬",
    "NR": "ìˆ˜ì‚¬",
    "NP": "ëŒ€ëª…ì‚¬",
    "VV": "ë™ì‚¬",
    "VA": "í˜•ìš©ì‚¬",
    "VX": "ë³´ì¡°ìš©ì–¸",
    "VCP": "ê¸ì •ì§€ì •ì‚¬",
    "VCN": "ë¶€ì •ì§€ì •ì‚¬",
    "MM": "ê´€í˜•ì‚¬",
    "MAG": "ì¼ë°˜ë¶€ì‚¬",
    "MAJ": "ì ‘ì†ë¶€ì‚¬",
}

# ê¸°ë³¸ ë¶ˆìš©ì–´
STOP_WORDS = {
    # ì¼ë°˜ ë¶ˆìš©ì–´
    "ê²ƒ", "ë“±", "ë°", "ë”", "ë˜", "ê·¸", "ì €", "ì´ëŸ°", "ì €ëŸ°",
    "ìœ„í•´", "ëŒ€í•´", "í†µí•´", "ê´€ë ¨", "ëŒ€í•œ", "ë”°ë¥¸", "ìœ„í•œ",
    "ì˜¤ëŠ˜", "ë‚´ì¼", "ì–´ì œ", "ì˜¬í•´", "ì§€ë‚œí•´", "ì‘ë…„", "ì§€ë‚œ",
    "ê¸°ì", "ë‰´ìŠ¤", "ë³´ë„", "ì·¨ì¬", "ì†ë³´", "ë‹¨ë…", "ì¢…í•©",
    "ì‚¬ì§„", "ì˜ìƒ", "ë™ì˜ìƒ", "ì œê³µ", "ì—°í•©ë‰´ìŠ¤",
    # ì¼ë°˜ ëª…ì‚¬
    "ê²½ìš°", "ë•Œë¬¸", "ì´í›„", "ì´ì „", "í˜„ì¬", "ë‹¹ì‹œ", "ìµœê·¼",
    "ê°€ëŠ¥", "í•„ìš”", "ì˜ˆì •", "ê³„íš", "ë°©ì¹¨", "ë°©ì•ˆ", "ì „ë§",
    "ì£¼ì¥", "ë°œí‘œ", "ì„¤ëª…", "ì§€ì ", "ê°•ì¡°", "ì–¸ê¸‰", "ìš”ì²­",
}


def detect_spikes(
    monthly_counts: dict[str, int],
    threshold: float = 1.5,
) -> dict[str, dict]:
    """ì›”ë³„ ê¸°ì‚¬ ìˆ˜ì—ì„œ ê¸‰ì¦ ì‹œì (ìŠ¤íŒŒì´í¬)ì„ íƒì§€í•©ë‹ˆë‹¤.

    Args:
        monthly_counts: ì›”ë³„ ê¸°ì‚¬ ìˆ˜ {"2024-01": 100, "2024-02": 500, ...}
        threshold: í‰ê·  ëŒ€ë¹„ ë°°ìˆ˜ ê¸°ì¤€ (ê¸°ë³¸ê°’: 1.5 = í‰ê· ì˜ 1.5ë°° ì´ìƒ)

    Returns:
        ìŠ¤íŒŒì´í¬ ì›”ê³¼ ì •ë³´ {"2024-02": {"count": 500, "ratio": 3.2, "type": "spike"}, ...}
    """
    if not monthly_counts:
        return {}

    counts = list(monthly_counts.values())
    avg = sum(counts) / len(counts)

    if avg == 0:
        return {}

    spikes = {}
    for period, count in monthly_counts.items():
        ratio = count / avg
        if ratio >= threshold:
            spikes[period] = {
                "count": count,
                "ratio": round(ratio, 2),
                "average": round(avg, 1),
                "type": "spike",
            }

    return spikes


def ensure_quarterly_events(
    monthly_counts: dict[str, int],
    spikes: dict[str, dict],
    max_events: int = 10,
) -> list[dict]:
    """ë¶„ê¸°ë³„ ìµœì†Œ 1ê°œ ì´ë²¤íŠ¸ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.

    ìŠ¤íŒŒì´í¬ê°€ ì—†ëŠ” ë¶„ê¸°ì—ì„œë„ í•´ë‹¹ ë¶„ê¸°ì˜ ìµœëŒ€ ê¸°ì‚¬ ì›”ì„ ì„ íƒí•©ë‹ˆë‹¤.

    Args:
        monthly_counts: ì›”ë³„ ê¸°ì‚¬ ìˆ˜
        spikes: detect_spikes ê²°ê³¼
        max_events: ìµœëŒ€ ì´ë²¤íŠ¸ ìˆ˜

    Returns:
        ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ì‹œê°„ìˆœ ì •ë ¬)
    """
    if not monthly_counts:
        return []

    # ë¶„ê¸°ë³„ ê·¸ë£¹í™”
    quarters: dict[str, list[tuple[str, int]]] = {}
    for period, count in monthly_counts.items():
        try:
            year, month = period.split("-")
            quarter = f"{year}-Q{(int(month) - 1) // 3 + 1}"
            if quarter not in quarters:
                quarters[quarter] = []
            quarters[quarter].append((period, count))
        except (ValueError, IndexError):
            continue

    events = []
    avg = sum(monthly_counts.values()) / len(monthly_counts)

    for quarter in sorted(quarters.keys()):
        months = quarters[quarter]

        # 1. í•´ë‹¹ ë¶„ê¸°ì— ìŠ¤íŒŒì´í¬ê°€ ìˆìœ¼ë©´ ê°€ì¥ í° ê²ƒ ì‚¬ìš©
        quarter_spikes = [
            (period, spikes[period])
            for period, _ in months
            if period in spikes
        ]

        if quarter_spikes:
            # ê°€ì¥ í° ìŠ¤íŒŒì´í¬ ì„ íƒ
            best = max(quarter_spikes, key=lambda x: x[1]["count"])
            events.append({
                "period": best[0],
                "count": best[1]["count"],
                "ratio": best[1]["ratio"],
                "average": best[1]["average"],
                "type": "spike",
                "quarter": quarter,
            })
        else:
            # 2. ìŠ¤íŒŒì´í¬ ì—†ìœ¼ë©´ í•´ë‹¹ ë¶„ê¸°ì—ì„œ ê°€ì¥ ê¸°ì‚¬ ë§ì€ ì›” ì„ íƒ
            best_month = max(months, key=lambda x: x[1])
            events.append({
                "period": best_month[0],
                "count": best_month[1],
                "ratio": round(best_month[1] / avg, 2) if avg > 0 else 1.0,
                "average": round(avg, 1),
                "type": "quarterly_peak",
                "quarter": quarter,
            })

    # max_events ì´ˆê³¼ ì‹œ ìš°ì„ ìˆœìœ„ ì •ë ¬
    if len(events) > max_events:
        # ìŠ¤íŒŒì´í¬ ìš°ì„ , ê·¸ ë‹¤ìŒ ê¸°ì‚¬ ìˆ˜ ìˆœ
        events.sort(key=lambda x: (x["type"] != "spike", -x["count"]))
        events = events[:max_events]

    # ì‹œê°„ìˆœ ì •ë ¬
    events.sort(key=lambda x: x["period"])

    return events


def extract_keywords(
    titles: list[str],
    top_n: int = 5,
    exclude_words: set[str] | None = None,
) -> list[str]:
    """ì œëª© ëª©ë¡ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (í•˜ìœ„ í˜¸í™˜ìš©).

    Args:
        titles: ê¸°ì‚¬ ì œëª© ë¦¬ìŠ¤íŠ¸
        top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜
        exclude_words: ì œì™¸í•  ë‹¨ì–´ ì„¸íŠ¸

    Returns:
        í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ë¹ˆë„ìˆœ)
    """
    result = extract_keywords_nlp(titles, top_n, exclude_words)
    return [item["word"] for item in result]


def extract_keywords_nlp(
    titles: list[str],
    top_n: int = 5,
    exclude_words: set[str] | None = None,
    pos_filter: set[str] | None = None,
) -> list[dict]:
    """í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ.

    kiwipiepyë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“±ì„ ì •í™•í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        titles: ê¸°ì‚¬ ì œëª© ë¦¬ìŠ¤íŠ¸
        top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜
        exclude_words: ì œì™¸í•  ë‹¨ì–´ ì„¸íŠ¸
        pos_filter: ì¶”ì¶œí•  í’ˆì‚¬ íƒœê·¸ (ê¸°ë³¸: ê³ ìœ ëª…ì‚¬, ì¼ë°˜ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬)

    Returns:
        [{"word": "í•œë™í›ˆ", "pos": "NNP", "pos_label": "ê³ ìœ ëª…ì‚¬", "count": 45}, ...]
    """
    if not titles:
        return []

    # ê¸°ë³¸ í’ˆì‚¬ í•„í„°: ê³ ìœ ëª…ì‚¬, ì¼ë°˜ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬
    if pos_filter is None:
        pos_filter = {"NNP", "NNG", "VV", "VA"}

    # ë¶ˆìš©ì–´ ì„¸íŠ¸ êµ¬ì„±
    stop_words = STOP_WORDS.copy()
    if exclude_words:
        stop_words.update(exclude_words)

    kiwi = _get_kiwi()
    word_counts: Counter = Counter()
    word_pos: dict[str, str] = {}

    for title in titles:
        try:
            tokens = kiwi.tokenize(title)
            for token in tokens:
                word = token.form
                pos = token.tag

                # í’ˆì‚¬ í•„í„°ë§
                if pos not in pos_filter:
                    continue

                # ê¸¸ì´ í•„í„° (2ê¸€ì ì´ìƒ)
                if len(word) < 2:
                    continue

                # ë¶ˆìš©ì–´ ì œì™¸
                if word in stop_words:
                    continue

                # ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ë‹¨ì–´ ì œì™¸
                if word.isdigit():
                    continue

                word_counts[word] += 1
                word_pos[word] = pos

        except Exception:
            # í† í°í™” ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ fallback
            matches = re.findall(r"[ê°€-í£]{2,}", title)
            for word in matches:
                if word not in stop_words and len(word) >= 2:
                    word_counts[word] += 1
                    if word not in word_pos:
                        word_pos[word] = "NNG"  # ê¸°ë³¸ê°’

    # ìƒìœ„ Nê°œ ë°˜í™˜
    result = []
    for word, count in word_counts.most_common(top_n):
        pos = word_pos.get(word, "NNG")
        result.append({
            "word": word,
            "pos": pos,
            "pos_label": POS_LABELS.get(pos, "ê¸°íƒ€"),
            "count": count,
        })

    return result


def select_representative_articles(
    articles: list[dict],
    max_count: int = 3,
) -> list[dict]:
    """ëŒ€í‘œ ê¸°ì‚¬ë¥¼ ì„ ì •í•©ë‹ˆë‹¤.

    ì„ ì • ê¸°ì¤€:
    1. ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì—ì„œ ì„ íƒ (ë‹¤ì–‘ì„±)
    2. ì‹œê°„ìˆœ ë¶„ì‚° (ì´ˆë°˜, ì¤‘ë°˜, í›„ë°˜)

    Args:
        articles: ê¸°ì‚¬ ëª©ë¡ [{"title", "date", "publisher", "url"}, ...]
        max_count: ì„ ì •í•  ê¸°ì‚¬ ìˆ˜

    Returns:
        ëŒ€í‘œ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    if not articles:
        return []

    if len(articles) <= max_count:
        return articles

    # ì–¸ë¡ ì‚¬ë³„ ê·¸ë£¹í™”
    by_publisher: dict[str, list] = {}
    for article in articles:
        publisher = article.get("publisher", "unknown")
        if publisher not in by_publisher:
            by_publisher[publisher] = []
        by_publisher[publisher].append(article)

    selected = []
    publishers_used = set()

    # 1. ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì—ì„œ ì„ íƒ
    for publisher, pub_articles in sorted(by_publisher.items(), key=lambda x: -len(x[1])):
        if len(selected) >= max_count:
            break
        if publisher not in publishers_used and pub_articles:
            selected.append(pub_articles[0])
            publishers_used.add(publisher)

    # 2. ë¶€ì¡±í•˜ë©´ ë‚ ì§œìˆœìœ¼ë¡œ ì±„ìš°ê¸°
    if len(selected) < max_count:
        remaining = [a for a in articles if a not in selected]
        remaining.sort(key=lambda x: x.get("date", ""))
        step = max(1, len(remaining) // (max_count - len(selected)))
        for i in range(0, len(remaining), step):
            if len(selected) >= max_count:
                break
            if remaining[i] not in selected:
                selected.append(remaining[i])

    # ë‚ ì§œìˆœ ì •ë ¬ í›„ ë°˜í™˜
    selected.sort(key=lambda x: x.get("date", ""))
    return selected[:max_count]


def generate_timeline_summary(
    keyword: str,
    events: list[dict],
) -> str:
    """íƒ€ì„ë¼ì¸ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        events: ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸

    Returns:
        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìš”ì•½ ë¬¸ìì—´
    """
    if not events:
        return f"'{keyword}' ê´€ë ¨ ì£¼ìš” ì´ë²¤íŠ¸ê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    lines = [f"## '{keyword}' ì£¼ìš” íƒ€ì„ë¼ì¸\n"]

    for event in events:
        period = event.get("period", "")
        count = event.get("article_count", event.get("count", 0))
        ratio = event.get("spike_ratio", event.get("ratio", 1.0))
        event_type = event.get("type", "spike")

        # í‚¤ì›Œë“œ ì²˜ë¦¬ (êµ¬ë²„ì „ í˜¸í™˜)
        keywords = event.get("top_keywords", [])
        if keywords and isinstance(keywords[0], dict):
            keyword_str = ", ".join(k["word"] for k in keywords[:3])
        elif keywords:
            keyword_str = ", ".join(keywords[:3])
        else:
            keyword_str = ""

        # ì›” í˜•ì‹ ë³€í™˜ (2024-03 -> 2024ë…„ 3ì›”)
        if "-" in period:
            year, month = period.split("-")
            period_display = f"{year}ë…„ {int(month)}ì›”"
        else:
            period_display = period

        # ì´ë²¤íŠ¸ íƒ€ì… í‘œì‹œ
        type_marker = "ğŸ”¥" if event_type == "spike" else "ğŸ“Š"

        lines.append(f"### {type_marker} {period_display}")
        lines.append(f"- ê¸°ì‚¬ ìˆ˜: {count:,}ê±´ (í‰ê·  ëŒ€ë¹„ {ratio:.1f}ë°°)")
        if keyword_str:
            lines.append(f"- í•µì‹¬ í‚¤ì›Œë“œ: {keyword_str}")
        lines.append("")

    return "\n".join(lines)


def parse_period_to_dates(period: str) -> tuple[str, str]:
    """ì›” ê¸°ê°„ì„ ì‹œì‘ì¼/ì¢…ë£Œì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        period: "2024-03" í˜•ì‹

    Returns:
        (ì‹œì‘ì¼, ì¢…ë£Œì¼) íŠœí”Œ ("2024-03-01", "2024-03-31")
    """
    import calendar

    year, month = period.split("-")
    year, month = int(year), int(month)

    _, last_day = calendar.monthrange(year, month)

    start_date = f"{year:04d}-{month:02d}-01"
    end_date = f"{year:04d}-{month:02d}-{last_day:02d}"

    return start_date, end_date


def generate_next_steps(
    tool_name: str,
    result: dict,
    context: dict | None = None,
) -> list[dict]:
    """ë„êµ¬ ê²°ê³¼ì— ê¸°ë°˜í•œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        tool_name: ë„êµ¬ ì´ë¦„
        result: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (keyword, start_date, end_date ë“±)

    Returns:
        next_steps ë¦¬ìŠ¤íŠ¸
    """
    if context is None:
        context = {}

    generators = {
        "search_news": _generate_next_steps_search,
        "analyze_timeline": _generate_next_steps_timeline,
        "compare_keywords": _generate_next_steps_compare,
        "get_today_issues": _generate_next_steps_issues,
        "export_all_articles": _generate_next_steps_export,
    }

    generator = generators.get(tool_name)
    if generator:
        return generator(result, context)

    return []


def _generate_next_steps_search(result: dict, context: dict) -> list[dict]:
    """search_news ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []
    total = result.get("total_count", 0)
    keyword = context.get("keyword", result.get("keyword", ""))
    start_date = context.get("start_date", "")
    end_date = context.get("end_date", "")

    if total >= 100:
        steps.append({
            "priority": "high",
            "action": "export_all_articles",
            "reason": f"{total:,}ê±´ì€ ì»¨í…ìŠ¤íŠ¸ ì œí•œ ì´ˆê³¼. ë¡œì»¬ ì €ì¥ í›„ ë¶„ì„ í•„ìˆ˜",
            "params": {
                "keyword": keyword,
                "start_date": start_date,
                "end_date": end_date,
                "output_format": "json",
            },
        })

    if total >= 500:
        steps.append({
            "priority": "high",
            "action": "analyze_timeline",
            "reason": "ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ ì‹œê°„ë³„ ì£¼ìš” ì´ë²¤íŠ¸ íŒŒì•…",
            "params": {
                "keyword": keyword,
                "start_date": start_date,
                "end_date": end_date,
                "max_events": 10,
            },
        })

    if 0 < total < 100:
        articles = result.get("articles", [])[:3]
        for article in articles:
            if article.get("news_id"):
                steps.append({
                    "priority": "medium",
                    "action": "get_article",
                    "reason": f"'{article.get('title', '')[:30]}...' ìƒì„¸ ì¡°íšŒ",
                    "params": {"news_id": article["news_id"]},
                })

    return steps


def _generate_next_steps_timeline(result: dict, context: dict) -> list[dict]:
    """analyze_timeline ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []
    events = result.get("events", [])
    keyword = result.get("keyword", context.get("keyword", ""))
    period = result.get("period", {})
    start_date = period.get("start_date", context.get("start_date", ""))
    end_date = period.get("end_date", context.get("end_date", ""))

    if events:
        # ê°€ì¥ í° ì´ë²¤íŠ¸ ì‹¬ì¸µ ë¶„ì„
        top_event = max(events, key=lambda e: e.get("article_count", e.get("count", 0)))
        period_start, period_end = parse_period_to_dates(top_event["period"])

        steps.append({
            "priority": "high",
            "action": "search_news",
            "reason": f"{top_event['period']} ì´ë²¤íŠ¸ ì‹¬ì¸µ ë¶„ì„",
            "params": {
                "keyword": keyword,
                "start_date": period_start,
                "end_date": period_end,
                "page_size": 50,
            },
        })

        # ë°œê²¬ëœ í‚¤ì›Œë“œë¡œ ë¹„êµ ë¶„ì„
        all_keywords = set()
        for event in events[:3]:
            top_kw = event.get("top_keywords", [])
            if top_kw:
                if isinstance(top_kw[0], dict):
                    all_keywords.update(k["word"] for k in top_kw[:2])
                else:
                    all_keywords.update(top_kw[:2])

        if len(all_keywords) >= 2:
            steps.append({
                "priority": "medium",
                "action": "compare_keywords",
                "reason": "ë°œê²¬ëœ í‚¤ì›Œë“œ ê°„ ê´€ê³„ ë¶„ì„",
                "params": {
                    "keywords": list(all_keywords)[:5],
                    "start_date": start_date,
                    "end_date": end_date,
                    "group_by": "month",
                },
            })

    # ì—°ê´€ì–´ ë¶„ì„ ê¶Œì¥ (ë¡œê·¸ì¸ í•„ìš”)
    steps.append({
        "priority": "low",
        "action": "get_related_keywords",
        "reason": "ì—°ê´€ì–´ ë„¤íŠ¸ì›Œí¬ë¡œ ìˆ¨ê²¨ì§„ ì—°ê²°ê³ ë¦¬ ë°œê²¬",
        "params": {
            "keyword": keyword,
            "start_date": start_date,
            "end_date": end_date,
        },
        "requires_auth": True,
    })

    return steps


def _generate_next_steps_compare(result: dict, context: dict) -> list[dict]:
    """compare_keywords ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []
    comparisons = result.get("comparisons", [])
    date_range = result.get("date_range", "")

    if " to " in date_range:
        start_date, end_date = date_range.split(" to ")
    else:
        start_date = context.get("start_date", "")
        end_date = context.get("end_date", "")

    # ê° í‚¤ì›Œë“œë³„ íƒ€ì„ë¼ì¸ ë¶„ì„
    for comp in comparisons[:3]:
        if comp.get("total_count", 0) >= 500:
            steps.append({
                "priority": "medium",
                "action": "analyze_timeline",
                "reason": f"'{comp['keyword']}' ì£¼ìš” ì´ë²¤íŠ¸ íŒŒì•…",
                "params": {
                    "keyword": comp["keyword"],
                    "start_date": start_date,
                    "end_date": end_date,
                    "max_events": 5,
                },
            })

    return steps


def _generate_next_steps_issues(result: dict, context: dict) -> list[dict]:
    """get_today_issues ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []
    results = result.get("results", {})

    for date_key, date_data in results.items():
        top_issues = date_data.get("issues", [])[:2]
        for issue in top_issues:
            steps.append({
                "priority": "medium",
                "action": "search_news",
                "reason": f"'{issue['title']}' ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰",
                "params": {
                    "keyword": issue["title"],
                    "start_date": date_key,
                    "end_date": date_key,
                    "page_size": 20,
                },
            })
            steps.append({
                "priority": "low",
                "action": "analyze_timeline",
                "reason": f"'{issue['title']}' ì´ìŠˆ ë°°ê²½ ë¶„ì„",
                "params": {
                    "keyword": issue["title"],
                    "start_date": (
                        f"{int(date_key[:4]) - 1}-{date_key[5:7]}-{date_key[8:10]}"
                        if len(date_key) >= 10 else date_key
                    ),
                    "end_date": date_key,
                },
            })

    return steps[:6]  # ìµœëŒ€ 6ê°œ


def _generate_next_steps_export(result: dict, context: dict) -> list[dict]:
    """export_all_articles ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []

    if result.get("success"):
        output_path = result.get("output_path", "")
        keyword = result.get("keyword", "")
        safe_keyword = keyword.replace(" ", "_").replace("/", "_")[:20]

        steps.append({
            "priority": "high",
            "action": "execute_code",
            "reason": "ì €ì¥ëœ ë°ì´í„°ë¡œ Python ë¶„ì„ ì‹¤í–‰",
            "params": {
                "script_path": f"scripts/analyze_{safe_keyword}.py",
                "data_path": output_path,
            },
            "instruction": (
                "1. result['analysis_code']ë¥¼ íŒŒì¼ë¡œ ì €ì¥\n"
                f"2. python scripts/analyze_{safe_keyword}.py ì‹¤í–‰"
            ),
        })

        # ìƒ˜í”Œ ê¸°ì‚¬ ìƒì„¸ ì¡°íšŒ
        articles = result.get("articles", [])[:2]
        for article in articles:
            if article.get("news_id"):
                steps.append({
                    "priority": "low",
                    "action": "get_article",
                    "reason": "ìƒ˜í”Œ ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© í™•ì¸",
                    "params": {"news_id": article["news_id"]},
                })

    return steps
