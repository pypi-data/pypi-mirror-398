"""ë¶„ì„ ê´€ë ¨ MCP Tools."""

from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from ..core.async_client import AsyncBigKindsClient
    from ..core.cache import MCPCache

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_client: AsyncBigKindsClient | None = None
_cache: MCPCache | None = None


def init_analysis_tools(client: AsyncBigKindsClient, cache: MCPCache) -> None:
    """ë¶„ì„ ë„êµ¬ ì´ˆê¸°í™”."""
    global _client, _cache
    _client = client
    _cache = cache


async def compare_keywords(
    keywords: list[str],
    start_date: str,
    end_date: str,
    group_by: str = "day",
) -> dict:
    """
    ì—¬ëŸ¬ í‚¤ì›Œë“œì˜ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.

    Args:
        keywords: ë¹„êµí•  í‚¤ì›Œë“œ ëª©ë¡ (2-5ê°œ ê¶Œì¥)
        start_date: ê²€ìƒ‰ ì‹œì‘ì¼ (YYYY-MM-DD)
        end_date: ê²€ìƒ‰ ì¢…ë£Œì¼ (YYYY-MM-DD)
        group_by: ì§‘ê³„ ë‹¨ìœ„
            - "total": ì „ì²´ ê¸°ê°„ ì´í•©
            - "day": ì¼ë³„ ì§‘ê³„ (ìµœëŒ€ 31ì¼ ê¶Œì¥)
            - "week": ì£¼ë³„ ì§‘ê³„
            - "month": ì›”ë³„ ì§‘ê³„

    Returns:
        í‚¤ì›Œë“œ ë¹„êµ ê²°ê³¼:
            - keywords: ë¹„êµ í‚¤ì›Œë“œ ëª©ë¡
            - date_range: ë¶„ì„ ê¸°ê°„
            - comparisons: í‚¤ì›Œë“œë³„ ê²°ê³¼
                - keyword: í‚¤ì›Œë“œ
                - total_count: ì´ ê¸°ì‚¬ ìˆ˜
                - counts: ê¸°ê°„ë³„ ê¸°ì‚¬ ìˆ˜ (group_by != "total"ì¸ ê²½ìš°)
                - rank: ìˆœìœ„ (ê¸°ì‚¬ ìˆ˜ ê¸°ì¤€)
            - summary: ë¶„ì„ ìš”ì•½
                - most_popular: ê°€ì¥ ë§ì€ í‚¤ì›Œë“œ
                - least_popular: ê°€ì¥ ì ì€ í‚¤ì›Œë“œ
                - total_articles: ì „ì²´ ê¸°ì‚¬ ìˆ˜

    Example:
        >>> result = await compare_keywords(
        ...     keywords=["AI", "ë°˜ë„ì²´", "ì „ê¸°ì°¨"],
        ...     start_date="2025-12-01",
        ...     end_date="2025-12-15",
        ...     group_by="day"
        ... )
        >>> print(result["summary"]["most_popular"])
        {"keyword": "AI", "count": 15432}
    """
    if _client is None or _cache is None:
        raise RuntimeError("Analysis tools not initialized")

    # ì…ë ¥ ê²€ì¦
    if not keywords or len(keywords) < 2:
        return {
            "success": False,
            "error": "INVALID_PARAMS",
            "message": "ìµœì†Œ 2ê°œ ì´ìƒì˜ í‚¤ì›Œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.",
        }

    if len(keywords) > 10:
        return {
            "success": False,
            "error": "INVALID_PARAMS",
            "message": "í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤.",
        }

    # ê° í‚¤ì›Œë“œë³„ë¡œ get_article_count í˜¸ì¶œ
    from .search import get_article_count

    results = []
    for keyword in keywords:
        result = await get_article_count(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            group_by=group_by,
        )

        if result.get("success", False):
            results.append({
                "keyword": keyword,
                "total_count": result["total_count"],
                "counts": result.get("counts", []),
            })
        else:
            # ì—ëŸ¬ê°€ ë°œìƒí•œ í‚¤ì›Œë“œëŠ” 0ê±´ìœ¼ë¡œ ì²˜ë¦¬
            results.append({
                "keyword": keyword,
                "total_count": 0,
                "counts": [],
                "error": result.get("message", "ì¡°íšŒ ì‹¤íŒ¨"),
            })

    # ê¸°ì‚¬ ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ìˆœìœ„ ë¶€ì—¬
    results.sort(key=lambda x: x["total_count"], reverse=True)
    for i, result in enumerate(results, 1):
        result["rank"] = i

    # ìš”ì•½ ì •ë³´
    total_articles = sum(r["total_count"] for r in results)
    most_popular = results[0] if results else None
    least_popular = results[-1] if results else None

    return {
        "success": True,
        "keywords": keywords,
        "date_range": f"{start_date} to {end_date}",
        "group_by": group_by,
        "comparisons": results,
        "summary": {
            "most_popular": {
                "keyword": most_popular["keyword"],
                "count": most_popular["total_count"],
            } if most_popular else None,
            "least_popular": {
                "keyword": least_popular["keyword"],
                "count": least_popular["total_count"],
            } if least_popular else None,
            "total_articles": total_articles,
            "average_count": total_articles // len(results) if results else 0,
        },
    }


async def smart_sample(
    keyword: str,
    start_date: str,
    end_date: str,
    sample_size: int = 100,
    strategy: str = "stratified",
) -> dict:
    """
    ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ëŒ€í‘œ ìƒ˜í”Œì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        start_date: ê²€ìƒ‰ ì‹œì‘ì¼ (YYYY-MM-DD)
        end_date: ê²€ìƒ‰ ì¢…ë£Œì¼ (YYYY-MM-DD)
        sample_size: ì¶”ì¶œí•  ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 100, ìµœëŒ€: 500)
        strategy: ìƒ˜í”Œë§ ì „ëµ
            - "stratified": ê¸°ê°„ë³„ ê· ë“± ë¶„í¬ (ê¸°ë³¸ê°’)
            - "latest": ìµœì‹  ê¸°ì‚¬ ìš°ì„ 
            - "random": ë¬´ì‘ìœ„ ìƒ˜í”Œë§

    Returns:
        ìƒ˜í”Œë§ ê²°ê³¼:
            - success: ì„±ê³µ ì—¬ë¶€
            - keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            - total_count: ì „ì²´ ê¸°ì‚¬ ìˆ˜
            - sample_size: ì¶”ì¶œëœ ìƒ˜í”Œ ìˆ˜
            - strategy: ì‚¬ìš©ëœ ì „ëµ
            - articles: ìƒ˜í”Œ ê¸°ì‚¬ ëª©ë¡
            - coverage: ìƒ˜í”Œë§ ì»¤ë²„ë¦¬ì§€ ì •ë³´

    Example:
        ëŒ€ìš©ëŸ‰ ë°ì´í„°(112ë§Œ ê±´)ì—ì„œ ëŒ€í‘œ 100ê±´ ì¶”ì¶œ:
        >>> result = await smart_sample(
        ...     keyword="ì´ì¬ëª…",
        ...     start_date="2005-01-01",
        ...     end_date="2025-12-15",
        ...     sample_size=100,
        ...     strategy="stratified"
        ... )
        >>> print(f"{result['total_count']}ê±´ â†’ {result['sample_size']}ê±´")
    """
    if _client is None or _cache is None:
        raise RuntimeError("Analysis tools not initialized")

    # ì…ë ¥ ê²€ì¦
    if sample_size > 500:
        return {
            "success": False,
            "error": "INVALID_PARAMS",
            "message": "sample_sizeëŠ” ìµœëŒ€ 500ê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤.",
        }

    from .search import search_news, get_article_count

    # 1ë‹¨ê³„: ì „ì²´ ê¸°ì‚¬ ìˆ˜ í™•ì¸
    count_result = await get_article_count(
        keyword=keyword,
        start_date=start_date,
        end_date=end_date,
        group_by="total",
    )

    if not count_result.get("success", False):
        return count_result  # ì—ëŸ¬ ë°˜í™˜

    total_count = count_result["total_count"]

    # 2ë‹¨ê³„: ì „ëµë³„ ìƒ˜í”Œë§
    if strategy == "stratified":
        # ê¸°ê°„ì„ ê· ë“± ë¶„í• í•˜ì—¬ ìƒ˜í”Œë§
        from datetime import datetime, timedelta

        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
        total_days = (end - start).days + 1

        # ìƒ˜í”Œ êµ¬ê°„ ìˆ˜ (ìµœëŒ€ 20ê°œ êµ¬ê°„)
        num_intervals = min(20, sample_size // 5)
        samples_per_interval = sample_size // num_intervals

        articles = []
        for i in range(num_intervals):
            # êµ¬ê°„ ê³„ì‚°
            interval_days = total_days // num_intervals
            interval_start = start + timedelta(days=i * interval_days)
            interval_end = start + timedelta(days=(i + 1) * interval_days - 1)
            if i == num_intervals - 1:
                interval_end = end  # ë§ˆì§€ë§‰ êµ¬ê°„ì€ ëê¹Œì§€

            # êµ¬ê°„ë³„ ê²€ìƒ‰
            result = await search_news(
                keyword=keyword,
                start_date=interval_start.strftime("%Y-%m-%d"),
                end_date=interval_end.strftime("%Y-%m-%d"),
                page_size=samples_per_interval,
                sort_by="date",
            )

            if result.get("success", False):
                articles.extend(result["articles"])

    elif strategy == "latest":
        # ìµœì‹  ê¸°ì‚¬ ìš°ì„ 
        result = await search_news(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            page_size=sample_size,
            sort_by="date",
        )

        if not result.get("success", False):
            return result

        articles = result["articles"]

    elif strategy == "random":
        # ë¬´ì‘ìœ„ í˜ì´ì§€ì—ì„œ ìƒ˜í”Œë§
        import random

        page_size = 20
        # BigKinds APIëŠ” ìµœëŒ€ ì•½ 15-17í˜ì´ì§€ê¹Œì§€ë§Œ í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›
        # ë³´ìˆ˜ì ìœ¼ë¡œ 15í˜ì´ì§€ë¡œ ì œí•œ (ì•½ 300ê±´)
        api_max_pages = 15
        max_pages = min(total_count // page_size, api_max_pages)

        # max_pagesê°€ 0ì´ë©´ ìƒ˜í”Œë§ ë¶ˆê°€
        if max_pages < 1:
            return {
                "success": False,
                "error": "INSUFFICIENT_DATA",
                "message": f"ì „ì²´ ê¸°ì‚¬ ìˆ˜({total_count})ê°€ ë„ˆë¬´ ì ì–´ random ìƒ˜í”Œë§ì´ ë¶ˆê°€í•©ë‹ˆë‹¤.",
            }

        articles = []
        # ìµœì†Œ 1í˜ì´ì§€ëŠ” ìƒ˜í”Œë§í•˜ë„ë¡ ë³´ì¥
        num_pages_to_sample = max(1, min(sample_size // page_size, max_pages))
        pages_to_sample = random.sample(range(1, max_pages + 1), num_pages_to_sample)

        for page in pages_to_sample:
            result = await search_news(
                keyword=keyword,
                start_date=start_date,
                end_date=end_date,
                page=page,
                page_size=page_size,
                sort_by="date",
            )

            if result.get("success", False):
                articles.extend(result["articles"])

    else:
        return {
            "success": False,
            "error": "INVALID_PARAMS",
            "message": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì „ëµ: {strategy}",
        }

    # ì¤‘ë³µ ì œê±° (news_id ê¸°ì¤€)
    seen_ids = set()
    unique_articles = []
    for article in articles:
        news_id = article.get("news_id")
        if news_id and news_id not in seen_ids:
            seen_ids.add(news_id)
            unique_articles.append(article)

    return {
        "success": True,
        "keyword": keyword,
        "date_range": f"{start_date} to {end_date}",
        "total_count": total_count,
        "sample_size": len(unique_articles),
        "strategy": strategy,
        "articles": unique_articles[:sample_size],  # ìš”ì²­ í¬ê¸°ë§Œí¼ë§Œ ë°˜í™˜
        "coverage": {
            "ratio": len(unique_articles) / total_count if total_count > 0 else 0,
            "description": f"{total_count:,}ê±´ ì¤‘ {len(unique_articles):,}ê±´ ìƒ˜í”Œë§",
        },
    }


def cache_stats() -> dict:
    """
    ìºì‹œ í†µê³„ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Returns:
        ìºì‹œ í†µê³„:
            - search: ê²€ìƒ‰ ìºì‹œ í†µê³„
                - size: í˜„ì¬ í¬ê¸°
                - maxsize: ìµœëŒ€ í¬ê¸°
                - usage_percent: ì‚¬ìš©ë¥ 
            - article: ê¸°ì‚¬ ìºì‹œ í†µê³„
            - count: ì¹´ìš´íŠ¸ ìºì‹œ í†µê³„
            - generic: ì¼ë°˜ ìºì‹œ í†µê³„
            - url: URL ë§¤í•‘ ìºì‹œ í†µê³„

    Example:
        >>> stats = cache_stats()
        >>> print(f"ê²€ìƒ‰ ìºì‹œ ì‚¬ìš©ë¥ : {stats['search']['usage_percent']:.1f}%")
    """
    if _cache is None:
        raise RuntimeError("Analysis tools not initialized")

    raw_stats = _cache.stats()

    # ì‚¬ìš©ë¥  ê³„ì‚°
    def add_usage(stat):
        size = stat["size"]
        maxsize = stat["maxsize"]
        stat["usage_percent"] = (size / maxsize * 100) if maxsize > 0 else 0
        return stat

    return {
        "search": add_usage(raw_stats["search"]),
        "article": add_usage(raw_stats["article"]),
        "count": add_usage(raw_stats["count"]),
        "generic": add_usage(raw_stats["generic"]),
    }


async def export_all_articles(
    keyword: str,
    start_date: str,
    end_date: str,
    output_format: str = "json",
    output_path: str | None = None,
    max_articles: int = 10000,
    providers: list[str] | None = None,
    categories: list[str] | None = None,
    include_content: bool = False,
) -> dict:
    """
    ê²€ìƒ‰ ê²°ê³¼ ì „ì²´ë¥¼ ì¼ê´„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

    âš ï¸ ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¶„ì„ì˜ í•µì‹¬ ë„êµ¬ì…ë‹ˆë‹¤.
    search_news ê²°ê³¼ê°€ 100ê±´ ì´ìƒì¼ ë•Œ ë°˜ë“œì‹œ ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        start_date: ê²€ìƒ‰ ì‹œì‘ì¼ (YYYY-MM-DD)
        end_date: ê²€ìƒ‰ ì¢…ë£Œì¼ (YYYY-MM-DD)
        output_format: ì¶œë ¥ í˜•ì‹
            - "json": JSON íŒŒì¼ (ê¸°ë³¸ê°’, ë¶„ì„ì— ì í•©)
            - "csv": CSV íŒŒì¼ (ìŠ¤í”„ë ˆë“œì‹œíŠ¸ í˜¸í™˜)
            - "jsonl": JSON Lines íŒŒì¼ (ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì— ì í•©)
        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
        max_articles: ìµœëŒ€ ë‹¤ìš´ë¡œë“œ ìˆ˜ (ê¸°ë³¸ê°’: 10000, ìµœëŒ€: 50000)
        providers: ì–¸ë¡ ì‚¬ í•„í„° (ì˜ˆ: ["ê²½í–¥ì‹ ë¬¸", "í•œê²¨ë ˆ"])
        categories: ì¹´í…Œê³ ë¦¬ í•„í„° (ì˜ˆ: ["ê²½ì œ", "IT_ê³¼í•™"])
        include_content: ê¸°ì‚¬ ë³¸ë¬¸ í¬í•¨ ì—¬ë¶€
            - False: ì œëª©, ìš”ì•½, ë©”íƒ€ë°ì´í„°ë§Œ (ë¹ ë¦„)
            - True: ì „ë¬¸ í¬í•¨ (ëŠë¦¼, ì–¸ë¡ ì‚¬ë‹¹ 1ê±´ì”© ìˆ˜ì§‘ ê¶Œì¥)

    Returns:
        ë‚´ë³´ë‚´ê¸° ê²°ê³¼:
            - success: ì„±ê³µ ì—¬ë¶€
            - output_path: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œ)
            - exported_count: ë‚´ë³´ë‚¸ ê¸°ì‚¬ ìˆ˜
            - total_count: ì „ì²´ ê¸°ì‚¬ ìˆ˜
            - file_size_human: íŒŒì¼ í¬ê¸° (ì½ê¸° ì‰¬ìš´ í˜•ì‹)
            - analysis_code: ë¶„ì„ ì½”ë“œ í…œí”Œë¦¿ (Python)

    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ğŸ“Š ëŒ€ìš©ëŸ‰ ë¶„ì„ ì›Œí¬í”Œë¡œìš° (100ê±´ ì´ìƒì¼ ë•Œ í•„ìˆ˜)
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    Step 1: ë°ì´í„° ì €ì¥
        result = export_all_articles(
            keyword="ë¶„ì„ ì£¼ì œ",
            start_date="2025-01-01",
            end_date="2025-12-15",
            output_path="data/articles.json"
        )

    Step 2: ë°˜í™˜ëœ analysis_codeë¥¼ íŒŒì¼ë¡œ ì €ì¥
        - result["analysis_code"]ì— Python ë¶„ì„ í…œí”Œë¦¿ í¬í•¨
        - ì´ ì½”ë“œë¥¼ scripts/analyze.pyë¡œ ì €ì¥

    Step 3: ë¶„ì„ ì‹¤í–‰ ì•ˆë‚´
        - "uv run python scripts/analyze.py" ë˜ëŠ”
        - "python scripts/analyze.py" ì‹¤í–‰

    âš ï¸ ì£¼ì˜: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì œí•œìœ¼ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì§ì ‘ ë¶„ì„í•˜ë©´
    ì •ë³´ ì†ì‹¤ì´ ë°œìƒí•©ë‹ˆë‹¤. ë°˜ë“œì‹œ ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥ í›„ ì½”ë“œë¡œ ë¶„ì„í•˜ì„¸ìš”.
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """
    import json
    import csv
    import os
    from datetime import datetime

    if _client is None or _cache is None:
        raise RuntimeError("Analysis tools not initialized")

    # ì…ë ¥ ê²€ì¦
    if output_format not in ("json", "csv", "jsonl"):
        return {
            "success": False,
            "error": "INVALID_PARAMS",
            "message": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {output_format}. json, csv, jsonl ì¤‘ ì„ íƒí•˜ì„¸ìš”.",
        }

    if max_articles > 50000:
        return {
            "success": False,
            "error": "INVALID_PARAMS",
            "message": "max_articlesëŠ” ìµœëŒ€ 50000ê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤.",
        }

    from .search import search_news, get_article_count
    from ..core.progress import ProgressTracker

    # 1ë‹¨ê³„: ì „ì²´ ê¸°ì‚¬ ìˆ˜ í™•ì¸
    count_result = await get_article_count(
        keyword=keyword,
        start_date=start_date,
        end_date=end_date,
        group_by="total",
    )

    if not count_result.get("success", False):
        return count_result

    total_count = count_result["total_count"]
    articles_to_fetch = min(total_count, max_articles)
    truncated = total_count > max_articles

    # 2ë‹¨ê³„: ProgressTracker ìƒì„± (5000ê±´ ì´ìƒì¼ ë•Œë§Œ í™œì„±í™”)
    progress = ProgressTracker(
        total=articles_to_fetch,
        description=f"'{keyword}' ê¸°ì‚¬ ë‚´ë³´ë‚´ê¸°",
        threshold=5000,
        interval=10,
    )

    # 3ë‹¨ê³„: í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ì „ì²´ ìˆ˜ì§‘
    all_articles = []
    page = 1
    page_size = 100  # ìµœëŒ€ í˜ì´ì§€ í¬ê¸°

    while len(all_articles) < articles_to_fetch:
        result = await search_news(
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            page=page,
            page_size=page_size,
            providers=providers,
            categories=categories,
            sort_by="date",
        )

        if not result.get("success", False):
            break

        articles = result.get("articles", [])
        if not articles:
            break

        all_articles.extend(articles)
        progress.update(len(articles))  # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        page += 1

    # max_articlesë§Œí¼ë§Œ ìœ ì§€
    all_articles = all_articles[:articles_to_fetch]

    # ì¤‘ë³µ ì œê±°
    seen_ids = set()
    unique_articles = []
    for article in all_articles:
        news_id = article.get("news_id")
        if news_id and news_id not in seen_ids:
            seen_ids.add(news_id)
            unique_articles.append(article)

    # 3ë‹¨ê³„ (ì„ íƒ): ê¸°ì‚¬ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘
    if include_content:
        from .article import scrape_article_url

        for i, article in enumerate(unique_articles):
            url = article.get("url")
            if url:
                try:
                    scraped = await scrape_article_url(url=url, extract_images=False)
                    if scraped.get("success", False):
                        article["full_content"] = scraped.get("content", "")
                except Exception:
                    article["full_content"] = ""

    # 4ë‹¨ê³„: íŒŒì¼ ì €ì¥
    # safe_keywordëŠ” íŒŒì¼ëª… ë° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ëª…ì— ì‚¬ìš©
    safe_keyword = keyword.replace(" ", "_").replace("/", "_")[:20]
    if output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"bigkinds_export_{safe_keyword}_{timestamp}.{output_format}"

    try:
        if output_format == "json":
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump({
                    "metadata": {
                        "keyword": keyword,
                        "date_range": f"{start_date} to {end_date}",
                        "total_count": total_count,
                        "exported_count": len(unique_articles),
                        "exported_at": datetime.now().isoformat(),
                        "truncated": truncated,
                    },
                    "articles": unique_articles,
                }, f, ensure_ascii=False, indent=2)

        elif output_format == "jsonl":
            with open(output_path, "w", encoding="utf-8") as f:
                for article in unique_articles:
                    f.write(json.dumps(article, ensure_ascii=False) + "\n")

        elif output_format == "csv":
            if unique_articles:
                # CSV í•„ë“œ ì •ì˜
                fieldnames = [
                    "news_id", "title", "summary", "publisher",
                    "published_date", "category", "url"
                ]
                if include_content:
                    fieldnames.append("full_content")

                with open(output_path, "w", encoding="utf-8-sig", newline="") as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
                    writer.writeheader()
                    writer.writerows(unique_articles)

        file_size = os.path.getsize(output_path)

    except Exception as e:
        return {
            "success": False,
            "error": "FILE_ERROR",
            "message": f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}",
        }

    # ì§„í–‰ë¥  ì™„ë£Œ ë¡œê¹…
    progress.complete()

    # ë¶„ì„ ì½”ë“œ í…œí”Œë¦¿ ìƒì„±
    analysis_code = _generate_analysis_code(
        output_path=os.path.abspath(output_path),
        keyword=keyword,
        output_format=output_format,
    )

    return {
        "success": True,
        "keyword": keyword,
        "date_range": f"{start_date} to {end_date}",
        "total_count": total_count,
        "exported_count": len(unique_articles),
        "output_path": os.path.abspath(output_path),
        "format": output_format,
        "file_size_bytes": file_size,
        "file_size_human": _format_file_size(file_size),
        "truncated": truncated,
        "truncated_message": f"max_articles({max_articles})ë¡œ ì œí•œë¨. ì „ì²´: {total_count:,}ê±´" if truncated else None,
        "analysis_code": analysis_code,
        "next_steps": [
            f"1. ë¶„ì„ ì½”ë“œë¥¼ íŒŒì¼ë¡œ ì €ì¥: scripts/analyze_{safe_keyword}.py",
            "2. ì½”ë“œ ì‹¤í–‰: python scripts/analyze_*.py",
            "3. ê²°ê³¼ í™•ì¸ ë° ì¶”ê°€ ë¶„ì„ ìˆ˜í–‰",
        ],
    }


def _format_file_size(size_bytes: int) -> str:
    """íŒŒì¼ í¬ê¸°ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜."""
    for unit in ["B", "KB", "MB", "GB"]:
        if size_bytes < 1024:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024
    return f"{size_bytes:.1f} TB"


def _generate_analysis_code(output_path: str, keyword: str, output_format: str) -> str:
    """ë¶„ì„ ì½”ë“œ í…œí”Œë¦¿ ìƒì„±."""
    if output_format == "csv":
        load_code = (
            "import pandas as pd\n"
            "    data = pd.read_csv(DATA_FILE)\n"
            "    articles = data.to_dict('records')"
        )
    else:  # json or jsonl
        load_code = (
            'with open(DATA_FILE, "r", encoding="utf-8") as f:\n'
            "        data = json.load(f)\n"
            '    articles = data.get("articles", data) if isinstance(data, dict) else data'
        )

    return f'''"""BigKinds ë°ì´í„° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸.

ìë™ ìƒì„±ë¨ - í•„ìš”ì— ë”°ë¼ ìˆ˜ì •í•˜ì„¸ìš”.

Usage:
    python scripts/analyze.py
"""

import json
from collections import Counter
from pathlib import Path

DATA_FILE = "{output_path}"


def load_data():
    """ë°ì´í„° ë¡œë“œ."""
    {load_code}
    return articles


def analyze_publishers(articles):
    """ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ìˆ˜ ë¶„ì„."""
    publishers = Counter(a.get("publisher", "Unknown") for a in articles)
    print("\\nğŸ“° ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ìˆ˜:")
    for pub, count in publishers.most_common(10):
        print(f"  {{pub}}: {{count}}ê±´")
    return publishers


def analyze_timeline(articles):
    """ì‹œê°„ëŒ€ë³„ ê¸°ì‚¬ ë¶„í¬."""
    dates = Counter(a.get("published_date", "")[:10] for a in articles if a.get("published_date"))
    print("\\nğŸ“… ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜:")
    for date, count in sorted(dates.items())[-10:]:
        print(f"  {{date}}: {{count}}ê±´")
    return dates


def analyze_keywords(articles, top_n=20):
    """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„."""
    # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
    import re
    words = []
    for a in articles:
        title = a.get("title", "")
        # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ
        words.extend(re.findall(r"[ê°€-í£]{{2,}}", title))

    word_counts = Counter(words)
    print("\\nğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ (ì œëª© ê¸°ì¤€):")
    for word, count in word_counts.most_common(top_n):
        print(f"  {{word}}: {{count}}íšŒ")
    return word_counts


def generate_summary(articles):
    """ë¶„ì„ ìš”ì•½."""
    print("\\n" + "=" * 50)
    print(f"ğŸ“Š ë¶„ì„ ìš”ì•½: {keyword}")
    print("=" * 50)
    print(f"ì´ ê¸°ì‚¬ ìˆ˜: {{len(articles):,}}ê±´")

    publishers = set(a.get("publisher") for a in articles if a.get("publisher"))
    print(f"ì–¸ë¡ ì‚¬ ìˆ˜: {{len(publishers)}}ê°œ")

    dates = [a.get("published_date", "")[:10] for a in articles if a.get("published_date")]
    if dates:
        print(f"ê¸°ê°„: {{min(dates)}} ~ {{max(dates)}}")


def main():
    """ë©”ì¸ ë¶„ì„."""
    print(f"ë°ì´í„° ë¡œë“œ ì¤‘: {{DATA_FILE}}")
    articles = load_data()

    generate_summary(articles)
    analyze_publishers(articles)
    analyze_timeline(articles)
    analyze_keywords(articles)

    print("\\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print("\\nğŸ’¡ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•˜ë©´ ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()
'''


async def analyze_timeline(
    keyword: str,
    start_date: str,
    end_date: str,
    max_events: int = 10,
    articles_per_event: int = 3,
) -> dict:
    """
    í‚¤ì›Œë“œì˜ íƒ€ì„ë¼ì¸ì„ ë¶„ì„í•˜ì—¬ ì£¼ìš” ì´ë²¤íŠ¸ë¥¼ ìë™ íƒì§€í•©ë‹ˆë‹¤.

    25ë§Œê±´ ì´ìƒì˜ ëŒ€ìš©ëŸ‰ ê¸°ì‚¬ì—ì„œ ì‹œê°„ë³„ ì£¼ìš” ì‚¬ê±´ì„ ìë™ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    NLP ê¸°ë°˜ìœ¼ë¡œ ê¸‰ì¦ ì‹œì  íƒì§€, í‚¤ì›Œë“œ ì¶”ì¶œ, ëŒ€í‘œ ê¸°ì‚¬ ì„ ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        keyword: ë¶„ì„í•  í‚¤ì›Œë“œ
        start_date: ë¶„ì„ ì‹œì‘ì¼ (YYYY-MM-DD)
        end_date: ë¶„ì„ ì¢…ë£Œì¼ (YYYY-MM-DD)
        max_events: ì¶”ì¶œí•  ìµœëŒ€ ì´ë²¤íŠ¸ ìˆ˜ (ê¸°ë³¸ê°’: 10)
        articles_per_event: ì´ë²¤íŠ¸ë‹¹ ëŒ€í‘œ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 3)

    Returns:
        íƒ€ì„ë¼ì¸ ë¶„ì„ ê²°ê³¼:
            - keyword: ë¶„ì„ í‚¤ì›Œë“œ
            - period: ë¶„ì„ ê¸°ê°„ ì •ë³´
            - total_articles: ì „ì²´ ê¸°ì‚¬ ìˆ˜
            - events: ì£¼ìš” ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸
                - period: ì›” (YYYY-MM)
                - article_count: ê¸°ì‚¬ ìˆ˜
                - spike_ratio: í‰ê·  ëŒ€ë¹„ ë¹„ìœ¨
                - top_keywords: í•µì‹¬ í‚¤ì›Œë“œ
                - representative_articles: ëŒ€í‘œ ê¸°ì‚¬
            - timeline_summary: íƒ€ì„ë¼ì¸ ìš”ì•½ (ë§ˆí¬ë‹¤ìš´)

    Example:
        >>> result = await analyze_timeline(
        ...     keyword="í•œë™í›ˆ",
        ...     start_date="2015-01-01",
        ...     end_date="2025-12-20",
        ...     max_events=20
        ... )
        >>> print(result["timeline_summary"])
    """
    from datetime import datetime
    from .search import search_news, get_article_count
    from .timeline_utils import (
        detect_spikes,
        extract_keywords,
        select_representative_articles,
        generate_timeline_summary,
        parse_period_to_dates,
    )

    if _client is None or _cache is None:
        raise RuntimeError("Analysis tools not initialized")

    # ì…ë ¥ ê²€ì¦
    try:
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    except ValueError:
        raise ValueError("ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")

    days_diff = (end_dt - start_dt).days
    if days_diff < 30:
        raise ValueError("ë¶„ì„ ê¸°ê°„ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ 1ê°œì›” ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")

    # 1ë‹¨ê³„: ì›”ë³„ ê¸°ì‚¬ ìˆ˜ ì§‘ê³„
    count_result = await get_article_count(
        keyword=keyword,
        start_date=start_date,
        end_date=end_date,
        group_by="month",
    )

    if not count_result.get("success", False):
        return {
            "success": False,
            "error": count_result.get("error", "UNKNOWN"),
            "message": count_result.get("message", "ê¸°ì‚¬ ìˆ˜ ì§‘ê³„ ì‹¤íŒ¨"),
        }

    total_count = count_result["total_count"]
    monthly_counts = {
        item["date"]: item["count"]
        for item in count_result.get("counts", [])
    }

    if total_count == 0:
        return {
            "success": True,
            "keyword": keyword,
            "period": {
                "start_date": start_date,
                "end_date": end_date,
                "months": len(monthly_counts),
            },
            "total_articles": 0,
            "events": [],
            "timeline_summary": f"'{keyword}' ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
        }

    # 2ë‹¨ê³„: ìŠ¤íŒŒì´í¬(ê¸‰ì¦) íƒì§€
    spikes = detect_spikes(monthly_counts, threshold=1.5)

    # ìŠ¤íŒŒì´í¬ë¥¼ ê¸°ì‚¬ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    sorted_spikes = sorted(
        spikes.items(),
        key=lambda x: x[1]["count"],
        reverse=True
    )[:max_events]

    # 3ë‹¨ê³„: ê° ìŠ¤íŒŒì´í¬ ê¸°ê°„ì˜ ìƒì„¸ ë¶„ì„
    events = []
    for period, spike_info in sorted_spikes:
        period_start, period_end = parse_period_to_dates(period)

        # í•´ë‹¹ ê¸°ê°„ì˜ ê¸°ì‚¬ ê²€ìƒ‰
        search_result = await search_news(
            keyword=keyword,
            start_date=period_start,
            end_date=period_end,
            page_size=50,  # í‚¤ì›Œë“œ ì¶”ì¶œìš©
            sort_by="date",
        )

        if not search_result.get("success", False):
            continue

        articles = search_result.get("articles", [])

        # í‚¤ì›Œë“œ ì¶”ì¶œ (ê²€ìƒ‰ í‚¤ì›Œë“œëŠ” ì œì™¸)
        titles = [a.get("title", "") for a in articles]
        top_keywords = extract_keywords(
            titles,
            top_n=5,
            exclude_words={keyword} | set(keyword.split())
        )

        # ëŒ€í‘œ ê¸°ì‚¬ ì„ ì •
        representative = select_representative_articles(
            [
                {
                    "title": a.get("title", ""),
                    "date": a.get("published_date", ""),
                    "url": a.get("url", ""),
                    "publisher": a.get("publisher", ""),
                    "news_id": a.get("news_id", ""),
                }
                for a in articles
            ],
            max_count=articles_per_event,
        )

        events.append({
            "period": period,
            "article_count": spike_info["count"],
            "spike_ratio": spike_info["ratio"],
            "average_count": spike_info["average"],
            "top_keywords": top_keywords,
            "representative_articles": representative,
        })

    # ì‹œê°„ìˆœ ì •ë ¬
    events.sort(key=lambda x: x["period"])

    # 4ë‹¨ê³„: ìš”ì•½ ìƒì„±
    timeline_summary = generate_timeline_summary(keyword, events)

    return {
        "success": True,
        "keyword": keyword,
        "period": {
            "start_date": start_date,
            "end_date": end_date,
            "months": len(monthly_counts),
        },
        "total_articles": total_count,
        "events": events,
        "event_count": len(events),
        "timeline_summary": timeline_summary,
    }
