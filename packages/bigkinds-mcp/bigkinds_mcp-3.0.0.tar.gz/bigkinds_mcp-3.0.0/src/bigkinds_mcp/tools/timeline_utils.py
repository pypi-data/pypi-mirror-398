"""íƒ€ì„ë¼ì¸ ë¶„ì„ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜.

ì´ë²¤íŠ¸ íƒì§€, í‚¤ì›Œë“œ ì¶”ì¶œ, ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • ë“±ì˜ NLP ê¸°ëŠ¥ ì œê³µ.
v2.0: kiwipiepy í˜•íƒœì†Œ ë¶„ì„, ë¶„ê¸°ë³„ í•„ìˆ˜ ì¶”ì¶œ, next_steps ìƒì„±
v2.1: ìœ ì¦ˆì¼€ì´ìŠ¤ë³„ next_steps í™•ì¥, ì „ì²´ ë„êµ¬ ì»¤ë²„ë¦¬ì§€
"""

from __future__ import annotations

import re
from collections import Counter
from typing import Any, Literal


# =============================================================================
# ìœ ì¦ˆì¼€ì´ìŠ¤ íƒ€ì… ì •ì˜
# =============================================================================

UsecaseType = Literal[
    "news_monitoring",    # ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì¶”ì 
    "deep_research",      # ì¸ë¬¼/ì‚¬ê±´ ì‹¬ì¸µ ë¶„ì„
    "trend_analysis",     # íŠ¸ë Œë“œ/ë¹„êµ ë¶„ì„
    "data_collection",    # ëŒ€ìš©ëŸ‰ ìˆ˜ì§‘
    "article_detail",     # ê°œë³„ ê¸°ì‚¬ ë¶„ì„
]


def detect_usecase(
    tool_name: str,
    result: dict,
    context: dict | None = None,
) -> UsecaseType:
    """ê²°ê³¼ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ì¦ˆì¼€ì´ìŠ¤ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.

    Args:
        tool_name: ë„êµ¬ ì´ë¦„
        result: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸

    Returns:
        ê°ì§€ëœ ìœ ì¦ˆì¼€ì´ìŠ¤ íƒ€ì…
    """
    context = context or {}
    total_count = result.get("total_count", 0)

    # 1. ë°ì´í„° ìˆ˜ì§‘ íŒ¨í„´: 100ê±´ ì´ìƒ + export ê´€ë ¨
    if tool_name == "export_all_articles":
        return "data_collection"

    if total_count >= 100:
        return "data_collection"

    # 2. ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ íŒ¨í„´: ì˜¤ëŠ˜ ì´ìŠˆ, ë‹¹ì¼ ê²€ìƒ‰
    if tool_name == "get_today_issues":
        return "news_monitoring"

    start_date = context.get("start_date", "")
    end_date = context.get("end_date", "")
    if start_date and end_date and start_date == end_date:
        return "news_monitoring"

    # 3. íŠ¸ë Œë“œ ë¶„ì„ íŒ¨í„´: ë¹„êµ, íŠ¸ë Œë“œ ë„êµ¬
    if tool_name in ("compare_keywords", "get_keyword_trends"):
        return "trend_analysis"

    # 4. ê°œë³„ ê¸°ì‚¬ ë¶„ì„ íŒ¨í„´
    if tool_name in ("get_article", "scrape_article_url", "get_article_thumbnail"):
        return "article_detail"

    # 5. ì‹¬ì¸µ ë¦¬ì„œì¹˜ íŒ¨í„´: ì¥ê¸°ê°„, íƒ€ì„ë¼ì¸ ë¶„ì„
    if tool_name == "analyze_timeline":
        return "deep_research"

    # 6. ê¸°ë³¸ê°’: ê¸°ê°„ì´ ê¸¸ë©´ ì‹¬ì¸µ ë¦¬ì„œì¹˜
    if start_date and end_date:
        from datetime import datetime
        try:
            start = datetime.strptime(start_date, "%Y-%m-%d")
            end = datetime.strptime(end_date, "%Y-%m-%d")
            if (end - start).days >= 90:
                return "deep_research"
        except ValueError:
            pass

    # ê¸°ë³¸ê°’
    return "news_monitoring"

# kiwipiepy lazy loading (ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™” ì§€ì—°)
_kiwi = None


def _get_kiwi():
    """Kiwi ì¸ìŠ¤í„´ìŠ¤ë¥¼ lazy loadingìœ¼ë¡œ ë°˜í™˜."""
    global _kiwi
    if _kiwi is None:
        from kiwipiepy import Kiwi
        _kiwi = Kiwi()
    return _kiwi


# í’ˆì‚¬ íƒœê·¸ ë¼ë²¨
POS_LABELS = {
    "NNP": "ê³ ìœ ëª…ì‚¬",
    "NNG": "ì¼ë°˜ëª…ì‚¬",
    "NNB": "ì˜ì¡´ëª…ì‚¬",
    "NR": "ìˆ˜ì‚¬",
    "NP": "ëŒ€ëª…ì‚¬",
    "VV": "ë™ì‚¬",
    "VA": "í˜•ìš©ì‚¬",
    "VX": "ë³´ì¡°ìš©ì–¸",
    "VCP": "ê¸ì •ì§€ì •ì‚¬",
    "VCN": "ë¶€ì •ì§€ì •ì‚¬",
    "MM": "ê´€í˜•ì‚¬",
    "MAG": "ì¼ë°˜ë¶€ì‚¬",
    "MAJ": "ì ‘ì†ë¶€ì‚¬",
}

# ê¸°ë³¸ ë¶ˆìš©ì–´
STOP_WORDS = {
    # ì¼ë°˜ ë¶ˆìš©ì–´
    "ê²ƒ", "ë“±", "ë°", "ë”", "ë˜", "ê·¸", "ì €", "ì´ëŸ°", "ì €ëŸ°",
    "ìœ„í•´", "ëŒ€í•´", "í†µí•´", "ê´€ë ¨", "ëŒ€í•œ", "ë”°ë¥¸", "ìœ„í•œ",
    "ì˜¤ëŠ˜", "ë‚´ì¼", "ì–´ì œ", "ì˜¬í•´", "ì§€ë‚œí•´", "ì‘ë…„", "ì§€ë‚œ",
    "ê¸°ì", "ë‰´ìŠ¤", "ë³´ë„", "ì·¨ì¬", "ì†ë³´", "ë‹¨ë…", "ì¢…í•©",
    "ì‚¬ì§„", "ì˜ìƒ", "ë™ì˜ìƒ", "ì œê³µ", "ì—°í•©ë‰´ìŠ¤",
    # ì¼ë°˜ ëª…ì‚¬
    "ê²½ìš°", "ë•Œë¬¸", "ì´í›„", "ì´ì „", "í˜„ì¬", "ë‹¹ì‹œ", "ìµœê·¼",
    "ê°€ëŠ¥", "í•„ìš”", "ì˜ˆì •", "ê³„íš", "ë°©ì¹¨", "ë°©ì•ˆ", "ì „ë§",
    "ì£¼ì¥", "ë°œí‘œ", "ì„¤ëª…", "ì§€ì ", "ê°•ì¡°", "ì–¸ê¸‰", "ìš”ì²­",
}


def detect_spikes(
    monthly_counts: dict[str, int],
    threshold: float = 1.5,
) -> dict[str, dict]:
    """ì›”ë³„ ê¸°ì‚¬ ìˆ˜ì—ì„œ ê¸‰ì¦ ì‹œì (ìŠ¤íŒŒì´í¬)ì„ íƒì§€í•©ë‹ˆë‹¤.

    Args:
        monthly_counts: ì›”ë³„ ê¸°ì‚¬ ìˆ˜ {"2024-01": 100, "2024-02": 500, ...}
        threshold: í‰ê·  ëŒ€ë¹„ ë°°ìˆ˜ ê¸°ì¤€ (ê¸°ë³¸ê°’: 1.5 = í‰ê· ì˜ 1.5ë°° ì´ìƒ)

    Returns:
        ìŠ¤íŒŒì´í¬ ì›”ê³¼ ì •ë³´ {"2024-02": {"count": 500, "ratio": 3.2, "type": "spike"}, ...}
    """
    if not monthly_counts:
        return {}

    counts = list(monthly_counts.values())
    avg = sum(counts) / len(counts)

    if avg == 0:
        return {}

    spikes = {}
    for period, count in monthly_counts.items():
        ratio = count / avg
        if ratio >= threshold:
            spikes[period] = {
                "count": count,
                "ratio": round(ratio, 2),
                "average": round(avg, 1),
                "type": "spike",
            }

    return spikes


def ensure_quarterly_events(
    monthly_counts: dict[str, int],
    spikes: dict[str, dict],
    max_events: int = 10,
) -> list[dict]:
    """ë¶„ê¸°ë³„ ìµœì†Œ 1ê°œ ì´ë²¤íŠ¸ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.

    ìŠ¤íŒŒì´í¬ê°€ ì—†ëŠ” ë¶„ê¸°ì—ì„œë„ í•´ë‹¹ ë¶„ê¸°ì˜ ìµœëŒ€ ê¸°ì‚¬ ì›”ì„ ì„ íƒí•©ë‹ˆë‹¤.

    Args:
        monthly_counts: ì›”ë³„ ê¸°ì‚¬ ìˆ˜
        spikes: detect_spikes ê²°ê³¼
        max_events: ìµœëŒ€ ì´ë²¤íŠ¸ ìˆ˜

    Returns:
        ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ì‹œê°„ìˆœ ì •ë ¬)
    """
    if not monthly_counts:
        return []

    # ë¶„ê¸°ë³„ ê·¸ë£¹í™”
    quarters: dict[str, list[tuple[str, int]]] = {}
    for period, count in monthly_counts.items():
        try:
            year, month = period.split("-")
            quarter = f"{year}-Q{(int(month) - 1) // 3 + 1}"
            if quarter not in quarters:
                quarters[quarter] = []
            quarters[quarter].append((period, count))
        except (ValueError, IndexError):
            continue

    events = []
    avg = sum(monthly_counts.values()) / len(monthly_counts)

    for quarter in sorted(quarters.keys()):
        months = quarters[quarter]

        # 1. í•´ë‹¹ ë¶„ê¸°ì— ìŠ¤íŒŒì´í¬ê°€ ìˆìœ¼ë©´ ê°€ì¥ í° ê²ƒ ì‚¬ìš©
        quarter_spikes = [
            (period, spikes[period])
            for period, _ in months
            if period in spikes
        ]

        if quarter_spikes:
            # ê°€ì¥ í° ìŠ¤íŒŒì´í¬ ì„ íƒ
            best = max(quarter_spikes, key=lambda x: x[1]["count"])
            events.append({
                "period": best[0],
                "count": best[1]["count"],
                "ratio": best[1]["ratio"],
                "average": best[1]["average"],
                "type": "spike",
                "quarter": quarter,
            })
        else:
            # 2. ìŠ¤íŒŒì´í¬ ì—†ìœ¼ë©´ í•´ë‹¹ ë¶„ê¸°ì—ì„œ ê°€ì¥ ê¸°ì‚¬ ë§ì€ ì›” ì„ íƒ
            best_month = max(months, key=lambda x: x[1])
            events.append({
                "period": best_month[0],
                "count": best_month[1],
                "ratio": round(best_month[1] / avg, 2) if avg > 0 else 1.0,
                "average": round(avg, 1),
                "type": "quarterly_peak",
                "quarter": quarter,
            })

    # max_events ì´ˆê³¼ ì‹œ ìš°ì„ ìˆœìœ„ ì •ë ¬
    if len(events) > max_events:
        # ìŠ¤íŒŒì´í¬ ìš°ì„ , ê·¸ ë‹¤ìŒ ê¸°ì‚¬ ìˆ˜ ìˆœ
        events.sort(key=lambda x: (x["type"] != "spike", -x["count"]))
        events = events[:max_events]

    # ì‹œê°„ìˆœ ì •ë ¬
    events.sort(key=lambda x: x["period"])

    return events


def extract_keywords(
    titles: list[str],
    top_n: int = 5,
    exclude_words: set[str] | None = None,
) -> list[str]:
    """ì œëª© ëª©ë¡ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (í•˜ìœ„ í˜¸í™˜ìš©).

    Args:
        titles: ê¸°ì‚¬ ì œëª© ë¦¬ìŠ¤íŠ¸
        top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜
        exclude_words: ì œì™¸í•  ë‹¨ì–´ ì„¸íŠ¸

    Returns:
        í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ë¹ˆë„ìˆœ)
    """
    result = extract_keywords_nlp(titles, top_n, exclude_words)
    return [item["word"] for item in result]


def extract_keywords_nlp(
    titles: list[str],
    top_n: int = 5,
    exclude_words: set[str] | None = None,
    pos_filter: set[str] | None = None,
) -> list[dict]:
    """í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ.

    kiwipiepyë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“±ì„ ì •í™•í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        titles: ê¸°ì‚¬ ì œëª© ë¦¬ìŠ¤íŠ¸
        top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜
        exclude_words: ì œì™¸í•  ë‹¨ì–´ ì„¸íŠ¸
        pos_filter: ì¶”ì¶œí•  í’ˆì‚¬ íƒœê·¸ (ê¸°ë³¸: ê³ ìœ ëª…ì‚¬, ì¼ë°˜ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬)

    Returns:
        [{"word": "í•œë™í›ˆ", "pos": "NNP", "pos_label": "ê³ ìœ ëª…ì‚¬", "count": 45}, ...]
    """
    if not titles:
        return []

    # ê¸°ë³¸ í’ˆì‚¬ í•„í„°: ê³ ìœ ëª…ì‚¬, ì¼ë°˜ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬
    if pos_filter is None:
        pos_filter = {"NNP", "NNG", "VV", "VA"}

    # ë¶ˆìš©ì–´ ì„¸íŠ¸ êµ¬ì„±
    stop_words = STOP_WORDS.copy()
    if exclude_words:
        stop_words.update(exclude_words)

    kiwi = _get_kiwi()
    word_counts: Counter = Counter()
    word_pos: dict[str, str] = {}

    for title in titles:
        try:
            tokens = kiwi.tokenize(title)
            for token in tokens:
                word = token.form
                pos = token.tag

                # í’ˆì‚¬ í•„í„°ë§
                if pos not in pos_filter:
                    continue

                # ê¸¸ì´ í•„í„° (2ê¸€ì ì´ìƒ)
                if len(word) < 2:
                    continue

                # ë¶ˆìš©ì–´ ì œì™¸
                if word in stop_words:
                    continue

                # ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ë‹¨ì–´ ì œì™¸
                if word.isdigit():
                    continue

                word_counts[word] += 1
                word_pos[word] = pos

        except Exception:
            # í† í°í™” ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ fallback
            matches = re.findall(r"[ê°€-í£]{2,}", title)
            for word in matches:
                if word not in stop_words and len(word) >= 2:
                    word_counts[word] += 1
                    if word not in word_pos:
                        word_pos[word] = "NNG"  # ê¸°ë³¸ê°’

    # ìƒìœ„ Nê°œ ë°˜í™˜
    result = []
    for word, count in word_counts.most_common(top_n):
        pos = word_pos.get(word, "NNG")
        result.append({
            "word": word,
            "pos": pos,
            "pos_label": POS_LABELS.get(pos, "ê¸°íƒ€"),
            "count": count,
        })

    return result


def select_representative_articles(
    articles: list[dict],
    max_count: int = 3,
) -> list[dict]:
    """ëŒ€í‘œ ê¸°ì‚¬ë¥¼ ì„ ì •í•©ë‹ˆë‹¤.

    ì„ ì • ê¸°ì¤€:
    1. ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì—ì„œ ì„ íƒ (ë‹¤ì–‘ì„±)
    2. ì‹œê°„ìˆœ ë¶„ì‚° (ì´ˆë°˜, ì¤‘ë°˜, í›„ë°˜)

    Args:
        articles: ê¸°ì‚¬ ëª©ë¡ [{"title", "date", "publisher", "url"}, ...]
        max_count: ì„ ì •í•  ê¸°ì‚¬ ìˆ˜

    Returns:
        ëŒ€í‘œ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    if not articles:
        return []

    if len(articles) <= max_count:
        return articles

    # ì–¸ë¡ ì‚¬ë³„ ê·¸ë£¹í™”
    by_publisher: dict[str, list] = {}
    for article in articles:
        publisher = article.get("publisher", "unknown")
        if publisher not in by_publisher:
            by_publisher[publisher] = []
        by_publisher[publisher].append(article)

    selected = []
    publishers_used = set()

    # 1. ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì—ì„œ ì„ íƒ
    for publisher, pub_articles in sorted(by_publisher.items(), key=lambda x: -len(x[1])):
        if len(selected) >= max_count:
            break
        if publisher not in publishers_used and pub_articles:
            selected.append(pub_articles[0])
            publishers_used.add(publisher)

    # 2. ë¶€ì¡±í•˜ë©´ ë‚ ì§œìˆœìœ¼ë¡œ ì±„ìš°ê¸°
    if len(selected) < max_count:
        remaining = [a for a in articles if a not in selected]
        remaining.sort(key=lambda x: x.get("date", ""))
        step = max(1, len(remaining) // (max_count - len(selected)))
        for i in range(0, len(remaining), step):
            if len(selected) >= max_count:
                break
            if remaining[i] not in selected:
                selected.append(remaining[i])

    # ë‚ ì§œìˆœ ì •ë ¬ í›„ ë°˜í™˜
    selected.sort(key=lambda x: x.get("date", ""))
    return selected[:max_count]


def generate_timeline_summary(
    keyword: str,
    events: list[dict],
) -> str:
    """íƒ€ì„ë¼ì¸ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        events: ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸

    Returns:
        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìš”ì•½ ë¬¸ìì—´
    """
    if not events:
        return f"'{keyword}' ê´€ë ¨ ì£¼ìš” ì´ë²¤íŠ¸ê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    lines = [f"## '{keyword}' ì£¼ìš” íƒ€ì„ë¼ì¸\n"]

    for event in events:
        period = event.get("period", "")
        count = event.get("article_count", event.get("count", 0))
        ratio = event.get("spike_ratio", event.get("ratio", 1.0))
        event_type = event.get("type", "spike")

        # í‚¤ì›Œë“œ ì²˜ë¦¬ (êµ¬ë²„ì „ í˜¸í™˜)
        keywords = event.get("top_keywords", [])
        if keywords and isinstance(keywords[0], dict):
            keyword_str = ", ".join(k["word"] for k in keywords[:3])
        elif keywords:
            keyword_str = ", ".join(keywords[:3])
        else:
            keyword_str = ""

        # ì›” í˜•ì‹ ë³€í™˜ (2024-03 -> 2024ë…„ 3ì›”)
        if "-" in period:
            year, month = period.split("-")
            period_display = f"{year}ë…„ {int(month)}ì›”"
        else:
            period_display = period

        # ì´ë²¤íŠ¸ íƒ€ì… í‘œì‹œ
        type_marker = "ğŸ”¥" if event_type == "spike" else "ğŸ“Š"

        lines.append(f"### {type_marker} {period_display}")
        lines.append(f"- ê¸°ì‚¬ ìˆ˜: {count:,}ê±´ (í‰ê·  ëŒ€ë¹„ {ratio:.1f}ë°°)")
        if keyword_str:
            lines.append(f"- í•µì‹¬ í‚¤ì›Œë“œ: {keyword_str}")
        lines.append("")

    return "\n".join(lines)


def parse_period_to_dates(period: str) -> tuple[str, str]:
    """ì›” ê¸°ê°„ì„ ì‹œì‘ì¼/ì¢…ë£Œì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        period: "2024-03" í˜•ì‹

    Returns:
        (ì‹œì‘ì¼, ì¢…ë£Œì¼) íŠœí”Œ ("2024-03-01", "2024-03-31")
    """
    import calendar

    year, month = period.split("-")
    year, month = int(year), int(month)

    _, last_day = calendar.monthrange(year, month)

    start_date = f"{year:04d}-{month:02d}-01"
    end_date = f"{year:04d}-{month:02d}-{last_day:02d}"

    return start_date, end_date


def generate_next_steps(
    tool_name: str,
    result: dict,
    context: dict | None = None,
) -> list[dict]:
    """ë„êµ¬ ê²°ê³¼ì— ê¸°ë°˜í•œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    v2.1: ìœ ì¦ˆì¼€ì´ìŠ¤ ê°ì§€ ë° ì „ì²´ ë„êµ¬ ì»¤ë²„ë¦¬ì§€

    Args:
        tool_name: ë„êµ¬ ì´ë¦„
        result: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (keyword, start_date, end_date ë“±)

    Returns:
        next_steps ë¦¬ìŠ¤íŠ¸ (í™•ì¥ëœ ìŠ¤í‚¤ë§ˆ):
            - usecase: í˜„ì¬ ìœ ì¦ˆì¼€ì´ìŠ¤ ì»¨í…ìŠ¤íŠ¸
            - priority: high/medium/low
            - action: í˜¸ì¶œí•  ë„êµ¬ëª…
            - reason: ê¶Œì¥ ì´ìœ 
            - params: ë„êµ¬ í˜¸ì¶œ íŒŒë¼ë¯¸í„°
            - depends_on: ì„ í–‰ ì¡°ê±´ (ì´ë¯¸ ì‹¤í–‰ëœ ë„êµ¬)
            - requires_auth: ë¡œê·¸ì¸ í•„ìš” ì—¬ë¶€
            - context_hint: LLM íŒíŠ¸
    """
    if context is None:
        context = {}

    # ìœ ì¦ˆì¼€ì´ìŠ¤ ê°ì§€
    usecase = detect_usecase(tool_name, result, context)

    generators = {
        "search_news": _generate_next_steps_search,
        "analyze_timeline": _generate_next_steps_timeline,
        "compare_keywords": _generate_next_steps_compare,
        "get_today_issues": _generate_next_steps_issues,
        "export_all_articles": _generate_next_steps_export,
        # v2.1 ì¶”ê°€
        "get_article": _generate_next_steps_article,
        "get_article_count": _generate_next_steps_article_count,
        "get_keyword_trends": _generate_next_steps_trends,
        "get_related_keywords": _generate_next_steps_related,
        "smart_sample": _generate_next_steps_sample,
        "scrape_article_url": _generate_next_steps_scrape,
    }

    generator = generators.get(tool_name)
    if generator:
        steps = generator(result, context)
        # ëª¨ë“  stepì— usecase ì¶”ê°€
        for step in steps:
            step["usecase"] = usecase
        return steps

    return []


def _generate_next_steps_search(result: dict, context: dict) -> list[dict]:
    """search_news ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []
    total = result.get("total_count", 0)
    keyword = context.get("keyword", result.get("keyword", ""))
    start_date = context.get("start_date", "")
    end_date = context.get("end_date", "")

    # ëŒ€ìš©ëŸ‰ ë°ì´í„° (100ê±´ ì´ìƒ): ë¡œì»¬ ì €ì¥ í•„ìˆ˜
    if total >= 100:
        steps.append({
            "priority": "high",
            "action": "export_all_articles",
            "reason": f"{total:,}ê±´ì€ ì»¨í…ìŠ¤íŠ¸ ì œí•œ ì´ˆê³¼. ë¡œì»¬ ì €ì¥ í›„ ë¶„ì„ í•„ìˆ˜",
            "params": {
                "keyword": keyword,
                "start_date": start_date,
                "end_date": end_date,
                "output_format": "json",
            },
            "depends_on": [],
            "requires_auth": False,
            "context_hint": "ëŒ€ìš©ëŸ‰ ë¶„ì„ ì›Œí¬í”Œë¡œìš°: ë¡œì»¬ ì €ì¥ â†’ Python ìŠ¤í¬ë¦½íŠ¸ ë¶„ì„",
        })

    # 500ê±´ ì´ìƒ: íƒ€ì„ë¼ì¸ ë¶„ì„ ê¶Œì¥
    if total >= 500:
        steps.append({
            "priority": "high",
            "action": "analyze_timeline",
            "reason": "ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ ì‹œê°„ë³„ ì£¼ìš” ì´ë²¤íŠ¸ íŒŒì•…",
            "params": {
                "keyword": keyword,
                "start_date": start_date,
                "end_date": end_date,
                "max_events": 10,
            },
            "depends_on": [],
            "requires_auth": False,
            "context_hint": "ì‹¬ì¸µ ë¦¬ì„œì¹˜: ìŠ¤íŒŒì´í¬ íƒì§€ â†’ ì´ë²¤íŠ¸ë³„ ë¶„ì„",
        })

    # ì¤‘ê°„ ê·œëª¨ (20-100ê±´): ìƒ˜í”Œë§ ê¶Œì¥
    if 20 <= total < 100:
        steps.append({
            "priority": "medium",
            "action": "smart_sample",
            "reason": f"{total:,}ê±´ì—ì„œ ëŒ€í‘œ ìƒ˜í”Œ ì¶”ì¶œ",
            "params": {
                "keyword": keyword,
                "start_date": start_date,
                "end_date": end_date,
                "sample_size": min(50, total),
                "strategy": "stratified",
            },
            "depends_on": [],
            "requires_auth": False,
            "context_hint": "íš¨ìœ¨ì  ë¶„ì„ì„ ìœ„í•œ ìƒ˜í”Œë§",
        })

    # ì†Œê·œëª¨ (100ê±´ ë¯¸ë§Œ): ê°œë³„ ê¸°ì‚¬ ìƒì„¸ ì¡°íšŒ
    if 0 < total < 100:
        articles = result.get("articles", [])[:3]
        for i, article in enumerate(articles):
            if article.get("news_id"):
                steps.append({
                    "priority": "medium" if i == 0 else "low",
                    "action": "get_article",
                    "reason": f"'{article.get('title', '')[:30]}...' ìƒì„¸ ì¡°íšŒ",
                    "params": {"news_id": article["news_id"]},
                    "depends_on": ["search_news"],
                    "requires_auth": False,
                    "context_hint": "ê°œë³„ ê¸°ì‚¬ ì‹¬ì¸µ ë¶„ì„",
                })

    # í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ê¶Œì¥ (ë¡œê·¸ì¸ í•„ìš”)
    if total >= 50 and start_date and end_date:
        steps.append({
            "priority": "low",
            "action": "get_keyword_trends",
            "reason": "ì‹œê°„ì¶• íŠ¸ë Œë“œ ì‹œê°í™”",
            "params": {
                "keyword": keyword,
                "start_date": start_date,
                "end_date": end_date,
            },
            "depends_on": [],
            "requires_auth": True,
            "context_hint": "íŠ¸ë Œë“œ ë¶„ì„: ì‹œê³„ì—´ ê·¸ë˜í”„",
        })

    return steps


def _generate_next_steps_timeline(result: dict, context: dict) -> list[dict]:
    """analyze_timeline ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []
    events = result.get("events", [])
    keyword = result.get("keyword", context.get("keyword", ""))
    period = result.get("period", {})
    start_date = period.get("start_date", context.get("start_date", ""))
    end_date = period.get("end_date", context.get("end_date", ""))
    total_articles = result.get("total_articles", 0)

    if events:
        # ê°€ì¥ í° ì´ë²¤íŠ¸ ì‹¬ì¸µ ë¶„ì„
        top_event = max(events, key=lambda e: e.get("article_count", e.get("count", 0)))
        period_start, period_end = parse_period_to_dates(top_event["period"])

        steps.append({
            "priority": "high",
            "action": "search_news",
            "reason": f"{top_event['period']} ì´ë²¤íŠ¸ ì‹¬ì¸µ ë¶„ì„ ({top_event.get('article_count', 0):,}ê±´)",
            "params": {
                "keyword": keyword,
                "start_date": period_start,
                "end_date": period_end,
                "page_size": 50,
            },
            "depends_on": ["analyze_timeline"],
            "requires_auth": False,
            "context_hint": "ì‹¬ì¸µ ë¦¬ì„œì¹˜: í”¼í¬ ê¸°ê°„ ì§‘ì¤‘ ë¶„ì„",
        })

        # ë°œê²¬ëœ í‚¤ì›Œë“œë¡œ ë¹„êµ ë¶„ì„
        all_keywords = set()
        for event in events[:3]:
            top_kw = event.get("top_keywords", [])
            if top_kw:
                if isinstance(top_kw[0], dict):
                    all_keywords.update(k["word"] for k in top_kw[:2])
                else:
                    all_keywords.update(top_kw[:2])

        if len(all_keywords) >= 2:
            steps.append({
                "priority": "medium",
                "action": "compare_keywords",
                "reason": f"ë°œê²¬ëœ í‚¤ì›Œë“œ ê°„ ê´€ê³„ ë¶„ì„: {', '.join(list(all_keywords)[:3])}",
                "params": {
                    "keywords": list(all_keywords)[:5],
                    "start_date": start_date,
                    "end_date": end_date,
                    "group_by": "month",
                },
                "depends_on": ["analyze_timeline"],
                "requires_auth": False,
                "context_hint": "íŠ¸ë Œë“œ ë¶„ì„: ì—°ê´€ í‚¤ì›Œë“œ ë¹„êµ",
            })

    # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ê¶Œì¥
    if total_articles >= 1000:
        steps.append({
            "priority": "medium",
            "action": "export_all_articles",
            "reason": f"{total_articles:,}ê±´ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘",
            "params": {
                "keyword": keyword,
                "start_date": start_date,
                "end_date": end_date,
                "output_format": "json",
            },
            "depends_on": [],
            "requires_auth": False,
            "context_hint": "ë°ì´í„° ìˆ˜ì§‘: ì „ì²´ ê¸°ì‚¬ ë¡œì»¬ ì €ì¥",
        })

    # ì—°ê´€ì–´ ë¶„ì„ ê¶Œì¥ (ë¡œê·¸ì¸ í•„ìš”)
    steps.append({
        "priority": "low",
        "action": "get_related_keywords",
        "reason": "ì—°ê´€ì–´ ë„¤íŠ¸ì›Œí¬ë¡œ ìˆ¨ê²¨ì§„ ì—°ê²°ê³ ë¦¬ ë°œê²¬",
        "params": {
            "keyword": keyword,
            "start_date": start_date,
            "end_date": end_date,
        },
        "depends_on": [],
        "requires_auth": True,
        "context_hint": "ì‹¬ì¸µ ë¦¬ì„œì¹˜: TF-IDF ê¸°ë°˜ ì—°ê´€ì–´",
    })

    return steps


def _generate_next_steps_compare(result: dict, context: dict) -> list[dict]:
    """compare_keywords ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []
    comparisons = result.get("comparisons", [])
    date_range = result.get("date_range", "")

    if " to " in date_range:
        start_date, end_date = date_range.split(" to ")
    else:
        start_date = context.get("start_date", "")
        end_date = context.get("end_date", "")

    # ê°€ì¥ ì¸ê¸° ìˆëŠ” í‚¤ì›Œë“œ ë¶„ì„
    if comparisons:
        top_keyword = comparisons[0]
        if top_keyword.get("total_count", 0) >= 100:
            steps.append({
                "priority": "high",
                "action": "analyze_timeline",
                "reason": f"'{top_keyword['keyword']}' (1ìœ„) ì£¼ìš” ì´ë²¤íŠ¸ íŒŒì•…",
                "params": {
                    "keyword": top_keyword["keyword"],
                    "start_date": start_date,
                    "end_date": end_date,
                    "max_events": 10,
                },
                "depends_on": ["compare_keywords"],
                "requires_auth": False,
                "context_hint": "íŠ¸ë Œë“œ ë¶„ì„: ìƒìœ„ í‚¤ì›Œë“œ ì‹¬ì¸µ ë¶„ì„",
            })

    # ê° í‚¤ì›Œë“œë³„ íƒ€ì„ë¼ì¸ ë¶„ì„ (500ê±´ ì´ìƒ)
    for comp in comparisons[1:3]:
        if comp.get("total_count", 0) >= 500:
            steps.append({
                "priority": "medium",
                "action": "analyze_timeline",
                "reason": f"'{comp['keyword']}' ({comp['rank']}ìœ„) ì£¼ìš” ì´ë²¤íŠ¸ íŒŒì•…",
                "params": {
                    "keyword": comp["keyword"],
                    "start_date": start_date,
                    "end_date": end_date,
                    "max_events": 5,
                },
                "depends_on": ["compare_keywords"],
                "requires_auth": False,
                "context_hint": "íŠ¸ë Œë“œ ë¶„ì„: ê°œë³„ í‚¤ì›Œë“œ ë¶„ì„",
            })

    # ì‹œê³„ì—´ íŠ¸ë Œë“œ ì‹œê°í™” ê¶Œì¥
    keywords = [c["keyword"] for c in comparisons[:3] if c.get("total_count", 0) > 0]
    if len(keywords) >= 2:
        steps.append({
            "priority": "medium",
            "action": "get_keyword_trends",
            "reason": f"ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¹„êµ: {', '.join(keywords)}",
            "params": {
                "keyword": ",".join(keywords),
                "start_date": start_date,
                "end_date": end_date,
            },
            "depends_on": [],
            "requires_auth": True,
            "context_hint": "íŠ¸ë Œë“œ ë¶„ì„: ì‹œê³„ì—´ ê·¸ë˜í”„ ë¹„êµ",
        })

    return steps


def _generate_next_steps_issues(result: dict, context: dict) -> list[dict]:
    """get_today_issues ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []
    results = result.get("results", {})

    for date_key, date_data in results.items():
        top_issues = date_data.get("issues", [])[:2]
        for i, issue in enumerate(top_issues):
            # ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ (ë†’ì€ ìš°ì„ ìˆœìœ„)
            steps.append({
                "priority": "high" if i == 0 else "medium",
                "action": "search_news",
                "reason": f"'{issue['title']}' ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ({issue.get('news_count', 0)}ê±´)",
                "params": {
                    "keyword": issue["title"],
                    "start_date": date_key,
                    "end_date": date_key,
                    "page_size": 20,
                },
                "depends_on": ["get_today_issues"],
                "requires_auth": False,
                "context_hint": "ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§: ì˜¤ëŠ˜ ì£¼ìš” ì´ìŠˆ ìƒì„¸",
            })
            # ì´ìŠˆ ë°°ê²½ ë¶„ì„ (1ë…„ ì „ë¶€í„°)
            if i == 0:  # ì²« ë²ˆì§¸ ì´ìŠˆë§Œ ë°°ê²½ ë¶„ì„
                steps.append({
                    "priority": "low",
                    "action": "analyze_timeline",
                    "reason": f"'{issue['title']}' ì´ìŠˆ ë°°ê²½ ë¶„ì„ (1ë…„)",
                    "params": {
                        "keyword": issue["title"],
                        "start_date": (
                            f"{int(date_key[:4]) - 1}-{date_key[5:7]}-{date_key[8:10]}"
                            if len(date_key) >= 10 else date_key
                        ),
                        "end_date": date_key,
                        "max_events": 5,
                    },
                    "depends_on": ["get_today_issues"],
                    "requires_auth": False,
                    "context_hint": "ì‹¬ì¸µ ë¦¬ì„œì¹˜: ì´ìŠˆ ë°œìƒ ë°°ê²½",
                })

    return steps[:6]  # ìµœëŒ€ 6ê°œ


def _generate_next_steps_export(result: dict, context: dict) -> list[dict]:
    """export_all_articles ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []

    if result.get("success"):
        output_path = result.get("output_path", "")
        keyword = result.get("keyword", "")
        safe_keyword = keyword.replace(" ", "_").replace("/", "_")[:20]
        exported_count = result.get("exported_count", 0)

        # Python ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì•ˆë‚´
        steps.append({
            "priority": "high",
            "action": "execute_code",
            "reason": f"ì €ì¥ëœ {exported_count:,}ê±´ ë°ì´í„°ë¡œ Python ë¶„ì„ ì‹¤í–‰",
            "params": {
                "script_path": f"scripts/analyze_{safe_keyword}.py",
                "data_path": output_path,
            },
            "depends_on": ["export_all_articles"],
            "requires_auth": False,
            "context_hint": "ë°ì´í„° ìˆ˜ì§‘: ë¶„ì„ ì½”ë“œ ì‹¤í–‰ ì•ˆë‚´",
            "instruction": (
                "1. result['analysis_code']ë¥¼ íŒŒì¼ë¡œ ì €ì¥\n"
                f"2. python scripts/analyze_{safe_keyword}.py ì‹¤í–‰"
            ),
        })

        # íƒ€ì„ë¼ì¸ ë¶„ì„ ê¶Œì¥ (ì•„ì§ ì•ˆ í–ˆë‹¤ë©´)
        date_range = result.get("date_range", "")
        if " to " in date_range and exported_count >= 500:
            start_date, end_date = date_range.split(" to ")
            steps.append({
                "priority": "medium",
                "action": "analyze_timeline",
                "reason": f"'{keyword}' ì£¼ìš” ì´ë²¤íŠ¸ íŒŒì•…",
                "params": {
                    "keyword": keyword,
                    "start_date": start_date,
                    "end_date": end_date,
                    "max_events": 10,
                },
                "depends_on": [],
                "requires_auth": False,
                "context_hint": "ì‹¬ì¸µ ë¦¬ì„œì¹˜: ì´ë²¤íŠ¸ íƒì§€",
            })

        # ìƒ˜í”Œ ê¸°ì‚¬ ìƒì„¸ ì¡°íšŒ
        articles = result.get("articles", [])[:2]
        for article in articles:
            if article.get("news_id"):
                steps.append({
                    "priority": "low",
                    "action": "get_article",
                    "reason": f"ìƒ˜í”Œ ê¸°ì‚¬ '{article.get('title', '')[:20]}...' ìƒì„¸ í™•ì¸",
                    "params": {"news_id": article["news_id"]},
                    "depends_on": ["export_all_articles"],
                    "requires_auth": False,
                    "context_hint": "ë°ì´í„° ê²€ì¦: ìƒ˜í”Œ í™•ì¸",
                })

    return steps


# =============================================================================
# v2.1 ì‹ ê·œ ë„êµ¬ next_steps ìƒì„± í•¨ìˆ˜
# =============================================================================


def _generate_next_steps_article(result: dict, context: dict) -> list[dict]:
    """get_article ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []

    if result.get("scrape_status") != "success":
        return steps

    title = result.get("title", "")
    publisher = result.get("publisher", "")
    keywords = result.get("keywords", [])
    published_date = result.get("published_date", "")

    # ì œëª©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
    if title:
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ í•œê¸€)
        import re
        title_keywords = re.findall(r"[ê°€-í£]{2,}", title)[:3]

        if title_keywords:
            main_keyword = title_keywords[0]
            steps.append({
                "priority": "medium",
                "action": "search_news",
                "reason": f"'{main_keyword}' ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰",
                "params": {
                    "keyword": main_keyword,
                    "start_date": published_date[:10] if published_date else "",
                    "end_date": published_date[:10] if published_date else "",
                    "page_size": 20,
                },
                "depends_on": ["get_article"],
                "requires_auth": False,
                "context_hint": "ê¸°ì‚¬ ë¶„ì„: ê´€ë ¨ ê¸°ì‚¬ íƒìƒ‰",
            })

    # ë™ì¼ ì–¸ë¡ ì‚¬ì˜ ë‹¤ë¥¸ ê¸°ì‚¬ ê²€ìƒ‰
    if publisher and title:
        steps.append({
            "priority": "low",
            "action": "search_news",
            "reason": f"{publisher}ì˜ ê´€ë ¨ ë³´ë„ ê²€ìƒ‰",
            "params": {
                "keyword": title[:20],
                "providers": [publisher],
                "start_date": published_date[:10] if published_date else "",
                "end_date": published_date[:10] if published_date else "",
                "page_size": 10,
            },
            "depends_on": ["get_article"],
            "requires_auth": False,
            "context_hint": "ê¸°ì‚¬ ë¶„ì„: ë™ì¼ ì–¸ë¡ ì‚¬ ë³´ë„",
        })

    # í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì—°ê´€ì–´ ë¶„ì„ ê¶Œì¥
    if keywords and len(keywords) >= 2:
        steps.append({
            "priority": "low",
            "action": "compare_keywords",
            "reason": f"ê¸°ì‚¬ í‚¤ì›Œë“œ ë¹„êµ: {', '.join(keywords[:3])}",
            "params": {
                "keywords": keywords[:5],
                "start_date": published_date[:10] if published_date else "",
                "end_date": published_date[:10] if published_date else "",
            },
            "depends_on": ["get_article"],
            "requires_auth": False,
            "context_hint": "íŠ¸ë Œë“œ ë¶„ì„: í‚¤ì›Œë“œ ë¹„êµ",
        })

    return steps


def _generate_next_steps_article_count(result: dict, context: dict) -> list[dict]:
    """get_article_count ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []

    if not result.get("success"):
        return steps

    keyword = result.get("keyword", "")
    total_count = result.get("total_count", 0)
    date_range = result.get("date_range", "")
    counts = result.get("counts", [])

    if " to " in date_range:
        start_date, end_date = date_range.split(" to ")
    else:
        start_date = context.get("start_date", "")
        end_date = context.get("end_date", "")

    # ëŒ€ìš©ëŸ‰ ë°ì´í„°: íƒ€ì„ë¼ì¸ ë¶„ì„ ê¶Œì¥
    if total_count >= 500:
        steps.append({
            "priority": "high",
            "action": "analyze_timeline",
            "reason": f"{total_count:,}ê±´ ë°ì´í„°ì˜ ì‹œê°„ë³„ ì£¼ìš” ì´ë²¤íŠ¸ íŒŒì•…",
            "params": {
                "keyword": keyword,
                "start_date": start_date,
                "end_date": end_date,
                "max_events": 10,
            },
            "depends_on": ["get_article_count"],
            "requires_auth": False,
            "context_hint": "ì‹¬ì¸µ ë¦¬ì„œì¹˜: ìŠ¤íŒŒì´í¬ íƒì§€",
        })

    # ê¸°ì‚¬ ìˆ˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í”¼í¬ ê¸°ê°„ ê²€ìƒ‰ ê¶Œì¥
    if counts:
        # ê°€ì¥ ë§ì€ ê¸°ê°„ ì°¾ê¸°
        peak = max(counts, key=lambda x: x.get("count", 0))
        if peak.get("count", 0) > 0:
            peak_date = peak.get("date", "")
            # ì›”ë³„/ì£¼ë³„ ê¸°ê°„ì¸ ê²½ìš° ì²˜ë¦¬
            if "month_start" in peak:
                peak_start = peak["month_start"]
                peak_end = peak["month_end"]
            elif "week_start" in peak:
                peak_start = peak["week_start"]
                peak_end = peak["week_end"]
            else:
                peak_start = peak_date
                peak_end = peak_date

            steps.append({
                "priority": "medium",
                "action": "search_news",
                "reason": f"í”¼í¬ ê¸°ê°„({peak_date}) ê¸°ì‚¬ ê²€ìƒ‰ ({peak.get('count', 0)}ê±´)",
                "params": {
                    "keyword": keyword,
                    "start_date": peak_start,
                    "end_date": peak_end,
                    "page_size": 50,
                },
                "depends_on": ["get_article_count"],
                "requires_auth": False,
                "context_hint": "íŠ¸ë Œë“œ ë¶„ì„: í”¼í¬ ê¸°ê°„ ì§‘ì¤‘",
            })

    # ì‹œê³„ì—´ íŠ¸ë Œë“œ ì‹œê°í™” ê¶Œì¥
    if total_count >= 50:
        steps.append({
            "priority": "low",
            "action": "get_keyword_trends",
            "reason": "ì‹œê³„ì—´ íŠ¸ë Œë“œ ì‹œê°í™”",
            "params": {
                "keyword": keyword,
                "start_date": start_date,
                "end_date": end_date,
            },
            "depends_on": [],
            "requires_auth": True,
            "context_hint": "íŠ¸ë Œë“œ ë¶„ì„: ì‹œê³„ì—´ ê·¸ë˜í”„",
        })

    return steps


def _generate_next_steps_trends(result: dict, context: dict) -> list[dict]:
    """get_keyword_trends ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []

    if not result.get("success"):
        return steps

    keyword = result.get("keyword", "")
    trends = result.get("trends", [])
    date_range = result.get("date_range", "")

    if " to " in date_range:
        start_date, end_date = date_range.split(" to ")
    else:
        start_date = context.get("start_date", "")
        end_date = context.get("end_date", "")

    # íŠ¸ë Œë“œì—ì„œ í”¼í¬ ì°¾ê¸°
    for trend in trends[:2]:
        data = trend.get("data", [])
        if data:
            # ê°€ì¥ ë†’ì€ ë°ì´í„° í¬ì¸íŠ¸ ì°¾ê¸°
            peak = max(data, key=lambda x: x.get("count", 0))
            if peak.get("count", 0) > 0:
                peak_date = peak.get("date", "")
                steps.append({
                    "priority": "high",
                    "action": "search_news",
                    "reason": f"'{trend.get('keyword', keyword)}' í”¼í¬ ì‹œì ({peak_date}) ê¸°ì‚¬ ê²€ìƒ‰",
                    "params": {
                        "keyword": trend.get("keyword", keyword),
                        "start_date": peak_date,
                        "end_date": peak_date,
                        "page_size": 50,
                    },
                    "depends_on": ["get_keyword_trends"],
                    "requires_auth": False,
                    "context_hint": "íŠ¸ë Œë“œ ë¶„ì„: í”¼í¬ ì‹œì  ì‹¬ì¸µ ë¶„ì„",
                })

    # ì—¬ëŸ¬ í‚¤ì›Œë“œì¸ ê²½ìš° ë¹„êµ ë¶„ì„ ê¶Œì¥
    if len(trends) >= 2:
        keywords = [t.get("keyword") for t in trends[:5] if t.get("keyword")]
        steps.append({
            "priority": "medium",
            "action": "compare_keywords",
            "reason": f"í‚¤ì›Œë“œ ë¹„êµ ë¶„ì„: {', '.join(keywords[:3])}",
            "params": {
                "keywords": keywords,
                "start_date": start_date,
                "end_date": end_date,
                "group_by": "month",
            },
            "depends_on": ["get_keyword_trends"],
            "requires_auth": False,
            "context_hint": "íŠ¸ë Œë“œ ë¶„ì„: í‚¤ì›Œë“œ ë¹„êµ",
        })

    return steps


def _generate_next_steps_related(result: dict, context: dict) -> list[dict]:
    """get_related_keywords ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []

    if not result.get("success"):
        return steps

    keyword = result.get("keyword", "")
    top_words = result.get("top_words", [])
    date_range = result.get("date_range", "")

    if " to " in date_range:
        start_date, end_date = date_range.split(" to ")
    else:
        start_date = context.get("start_date", "")
        end_date = context.get("end_date", "")

    # ìƒìœ„ ì—°ê´€ì–´ë¡œ ë¹„êµ ë¶„ì„
    if len(top_words) >= 3:
        related_keywords = [w.get("name") for w in top_words[:5] if w.get("name")]
        steps.append({
            "priority": "high",
            "action": "compare_keywords",
            "reason": f"ì—°ê´€ì–´ ë¹„êµ ë¶„ì„: {', '.join(related_keywords[:3])}",
            "params": {
                "keywords": related_keywords,
                "start_date": start_date,
                "end_date": end_date,
                "group_by": "month",
            },
            "depends_on": ["get_related_keywords"],
            "requires_auth": False,
            "context_hint": "ì‹¬ì¸µ ë¦¬ì„œì¹˜: ì—°ê´€ì–´ ë„¤íŠ¸ì›Œí¬ í™•ì¥",
        })

    # ê°€ì¥ ê´€ë ¨ ë†’ì€ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
    if top_words:
        top_related = top_words[0].get("name", "")
        if top_related and top_related != keyword:
            steps.append({
                "priority": "medium",
                "action": "search_news",
                "reason": f"ìµœìƒìœ„ ì—°ê´€ì–´ '{top_related}' ê¸°ì‚¬ ê²€ìƒ‰",
                "params": {
                    "keyword": f"{keyword} {top_related}",
                    "start_date": start_date,
                    "end_date": end_date,
                    "page_size": 30,
                },
                "depends_on": ["get_related_keywords"],
                "requires_auth": False,
                "context_hint": "ì‹¬ì¸µ ë¦¬ì„œì¹˜: ì—°ê´€ í‚¤ì›Œë“œ ì¡°í•© ê²€ìƒ‰",
            })

    # íƒ€ì„ë¼ì¸ ë¶„ì„ ê¶Œì¥
    steps.append({
        "priority": "low",
        "action": "analyze_timeline",
        "reason": f"'{keyword}' ì£¼ìš” ì´ë²¤íŠ¸ íŒŒì•…",
        "params": {
            "keyword": keyword,
            "start_date": start_date,
            "end_date": end_date,
            "max_events": 10,
        },
        "depends_on": [],
        "requires_auth": False,
        "context_hint": "ì‹¬ì¸µ ë¦¬ì„œì¹˜: ì´ë²¤íŠ¸ íƒì§€",
    })

    return steps


def _generate_next_steps_sample(result: dict, context: dict) -> list[dict]:
    """smart_sample ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []

    if not result.get("success"):
        return steps

    keyword = result.get("keyword", "")
    total_count = result.get("total_count", 0)
    sample_size = result.get("sample_size", 0)
    articles = result.get("articles", [])
    date_range = result.get("date_range", "")

    if " to " in date_range:
        start_date, end_date = date_range.split(" to ")
    else:
        start_date = context.get("start_date", "")
        end_date = context.get("end_date", "")

    # ì „ì²´ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ê¶Œì¥
    if total_count >= 100:
        steps.append({
            "priority": "high",
            "action": "export_all_articles",
            "reason": f"ì „ì²´ {total_count:,}ê±´ ë¡œì»¬ ì €ì¥ (ìƒ˜í”Œ: {sample_size}ê±´)",
            "params": {
                "keyword": keyword,
                "start_date": start_date,
                "end_date": end_date,
                "output_format": "json",
            },
            "depends_on": [],
            "requires_auth": False,
            "context_hint": "ë°ì´í„° ìˆ˜ì§‘: ì „ì²´ ê¸°ì‚¬ ì €ì¥",
        })

    # ìƒ˜í”Œ ê¸°ì‚¬ ìƒì„¸ ì¡°íšŒ
    for i, article in enumerate(articles[:3]):
        if article.get("news_id"):
            steps.append({
                "priority": "medium" if i == 0 else "low",
                "action": "get_article",
                "reason": f"ìƒ˜í”Œ ê¸°ì‚¬ '{article.get('title', '')[:25]}...' ìƒì„¸ ì¡°íšŒ",
                "params": {"news_id": article["news_id"]},
                "depends_on": ["smart_sample"],
                "requires_auth": False,
                "context_hint": "ë°ì´í„° ê²€ì¦: ìƒ˜í”Œ í™•ì¸",
            })

    # íƒ€ì„ë¼ì¸ ë¶„ì„ ê¶Œì¥
    if total_count >= 500:
        steps.append({
            "priority": "medium",
            "action": "analyze_timeline",
            "reason": f"{total_count:,}ê±´ ë°ì´í„°ì˜ ì£¼ìš” ì´ë²¤íŠ¸ íŒŒì•…",
            "params": {
                "keyword": keyword,
                "start_date": start_date,
                "end_date": end_date,
                "max_events": 10,
            },
            "depends_on": [],
            "requires_auth": False,
            "context_hint": "ì‹¬ì¸µ ë¦¬ì„œì¹˜: ì´ë²¤íŠ¸ íƒì§€",
        })

    return steps


def _generate_next_steps_scrape(result: dict, context: dict) -> list[dict]:
    """scrape_article_url ê²°ê³¼ì— ëŒ€í•œ next_steps ìƒì„±."""
    steps = []

    if not result.get("success"):
        return steps

    title = result.get("title", "")
    publisher = result.get("publisher", "")
    keywords = result.get("keywords", [])
    published_date = result.get("published_date", "")

    # ì œëª©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
    if title:
        import re
        title_keywords = re.findall(r"[ê°€-í£]{2,}", title)[:3]

        if title_keywords:
            main_keyword = title_keywords[0]
            steps.append({
                "priority": "medium",
                "action": "search_news",
                "reason": f"'{main_keyword}' ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰",
                "params": {
                    "keyword": main_keyword,
                    "start_date": published_date[:10] if published_date else "",
                    "end_date": published_date[:10] if published_date else "",
                    "page_size": 20,
                },
                "depends_on": ["scrape_article_url"],
                "requires_auth": False,
                "context_hint": "ê¸°ì‚¬ ë¶„ì„: ê´€ë ¨ ê¸°ì‚¬ íƒìƒ‰",
            })

    # ë™ì¼ ì–¸ë¡ ì‚¬ ê²€ìƒ‰
    if publisher and title:
        steps.append({
            "priority": "low",
            "action": "search_news",
            "reason": f"{publisher}ì˜ ê´€ë ¨ ë³´ë„ ê²€ìƒ‰",
            "params": {
                "keyword": title[:20],
                "providers": [publisher],
                "start_date": published_date[:10] if published_date else "",
                "end_date": published_date[:10] if published_date else "",
                "page_size": 10,
            },
            "depends_on": ["scrape_article_url"],
            "requires_auth": False,
            "context_hint": "ê¸°ì‚¬ ë¶„ì„: ë™ì¼ ì–¸ë¡ ì‚¬ ë³´ë„",
        })

    return steps
