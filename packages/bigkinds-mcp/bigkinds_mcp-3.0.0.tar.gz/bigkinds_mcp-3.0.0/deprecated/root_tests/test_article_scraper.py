"""BigKinds â†’ ì›ë³¸ ê¸°ì‚¬ hopping í…ŒìŠ¤íŠ¸."""

from data_scrapers.bigkinds import (
    ArticleScraper,
    BigKindsClient,
    SearchRequest,
    scrape_article,
)


def test_full_pipeline():
    """BigKinds ê²€ìƒ‰ â†’ ì›ë³¸ ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸."""
    print("\n" + "=" * 70)
    print("ğŸ”„ BigKinds â†’ ì›ë³¸ ê¸°ì‚¬ Hopping í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    # 1. BigKindsì—ì„œ ê¸°ì‚¬ ê²€ìƒ‰
    print("\nğŸ“¡ Step 1: BigKinds API ê²€ìƒ‰...")
    with BigKindsClient() as client:
        request = SearchRequest(
            keyword="AI",
            start_date="2024-12-01",
            end_date="2024-12-10",
            result_number=5,
        )
        response = client.search(request)

    if not response.success:
        print(f"âŒ BigKinds ê²€ìƒ‰ ì‹¤íŒ¨: {response.error_message}")
        return

    print(f"âœ… {len(response.articles)}ê±´ ê²€ìƒ‰ë¨\n")

    # 2. ê° ê¸°ì‚¬ ì›ë³¸ URLë¡œ hopping
    print("ğŸŒ Step 2: ì›ë³¸ ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘...")
    print("=" * 70)

    with ArticleScraper() as scraper:
        for i, article in enumerate(response.articles, 1):
            raw = article.raw_data or {}
            original_url = raw.get("PROVIDER_LINK_PAGE")
            bigkinds_title = raw.get("TITLE", "")[:50]
            bigkinds_content = raw.get("CONTENT", "")

            print(f"\n{'â”€' * 60}")
            print(f"ğŸ“° ê¸°ì‚¬ #{i}: {bigkinds_title}...")
            print(f"   BigKinds ë³¸ë¬¸ ê¸¸ì´: {len(bigkinds_content)}ì")
            print(f"   ì›ë³¸ URL: {original_url}")

            if not original_url:
                print("   âš ï¸ ì›ë³¸ URL ì—†ìŒ - ìŠ¤í‚µ")
                continue

            # ìŠ¤í¬ë˜í•‘
            scraped = scraper.scrape(original_url)

            if scraped.success:
                print(f"\n   âœ… ìŠ¤í¬ë˜í•‘ ì„±ê³µ!")
                print(f"   â”œâ”€ HTTP: {scraped.http_status}")
                print(f"   â”œâ”€ ì œëª©: {scraped.title[:60] if scraped.title else '(ì—†ìŒ)'}...")
                print(f"   â”œâ”€ ì–¸ë¡ ì‚¬: {scraped.publisher}")
                print(f"   â”œâ”€ ì‘ì„±ì: {scraped.author}")
                print(f"   â”œâ”€ ë°œí–‰ì¼: {scraped.published_date}")
                print(f"   â”œâ”€ í‚¤ì›Œë“œ: {scraped.keywords[:5] if scraped.keywords else []}")

                # ë³¸ë¬¸ ë¹„êµ
                scraped_len = len(scraped.content) if scraped.content else 0
                print(f"   â”œâ”€ ìŠ¤í¬ë˜í•‘ ë³¸ë¬¸: {scraped_len}ì")

                if scraped.content:
                    print(f"   â”‚  ë¯¸ë¦¬ë³´ê¸°: {scraped.content[:150]}...")

                # ì´ë¯¸ì§€
                print(f"   â”œâ”€ ì´ë¯¸ì§€ ìˆ˜: {len(scraped.images)}")
                if scraped.main_image:
                    print(f"   â”‚  ë©”ì¸ ì´ë¯¸ì§€: {scraped.main_image[:70]}...")
                for img in scraped.images[:3]:
                    if not img.get("is_main"):
                        print(f"   â”‚  - {img['url'][:60]}...")

                # ë³¸ë¬¸ ì¦ê°€ìœ¨
                if bigkinds_content and scraped.content:
                    increase = (scraped_len / len(bigkinds_content) - 1) * 100
                    print(f"   â””â”€ ë³¸ë¬¸ ì¦ê°€: +{increase:.0f}%")

            else:
                print(f"   âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {scraped.error}")


def test_various_publishers():
    """ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸."""
    print("\n" + "=" * 70)
    print("ğŸ¢ ì–¸ë¡ ì‚¬ë³„ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    # BigKindsì—ì„œ ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ ê¸°ì‚¬ ìˆ˜ì§‘
    with BigKindsClient() as client:
        request = SearchRequest(
            keyword="ê²½ì œ",
            start_date="2024-12-01",
            end_date="2024-12-10",
            result_number=20,
        )
        response = client.search(request)

    if not response.success:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨")
        return

    # ì–¸ë¡ ì‚¬ë³„ë¡œ ê·¸ë£¹í•‘
    by_publisher = {}
    for article in response.articles:
        raw = article.raw_data or {}
        publisher = raw.get("PROVIDER", "Unknown")
        if publisher not in by_publisher:
            by_publisher[publisher] = []
        by_publisher[publisher].append(article)

    print(f"\në°œê²¬ëœ ì–¸ë¡ ì‚¬: {list(by_publisher.keys())}\n")

    # ì–¸ë¡ ì‚¬ë³„ 1ê°œì”© í…ŒìŠ¤íŠ¸
    with ArticleScraper() as scraper:
        for publisher, articles in list(by_publisher.items())[:8]:
            article = articles[0]
            raw = article.raw_data or {}
            url = raw.get("PROVIDER_LINK_PAGE")

            print(f"\n{'â”€' * 50}")
            print(f"ğŸ¢ {publisher}")

            if not url:
                print("   âš ï¸ URL ì—†ìŒ")
                continue

            scraped = scraper.scrape(url)

            if scraped.success:
                content_len = len(scraped.content) if scraped.content else 0
                img_count = len(scraped.images)
                print(f"   âœ… ì„±ê³µ | ë³¸ë¬¸: {content_len}ì | ì´ë¯¸ì§€: {img_count}ê°œ")
                print(f"   ì œëª©: {scraped.title[:50] if scraped.title else '-'}...")
            else:
                print(f"   âŒ ì‹¤íŒ¨: {scraped.error}")


def test_simple():
    """ë‹¨ìˆœ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸."""
    print("\n" + "=" * 70)
    print("ğŸ§ª ë‹¨ìˆœ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    # í…ŒìŠ¤íŠ¸ URLë“¤
    test_urls = [
        "https://www.mk.co.kr/news/economy/11190021",
        "https://www.ilyo.co.kr/?ac=article_view&entry_id=483644",
        "http://www.breaknews.com/1078756",
    ]

    for url in test_urls:
        print(f"\nğŸ“° {url[:50]}...")
        result = scrape_article(url)

        if result.success:
            print(f"   âœ… ì œëª©: {result.title[:50] if result.title else '-'}...")
            print(f"   ë³¸ë¬¸: {len(result.content) if result.content else 0}ì")
            print(f"   ì´ë¯¸ì§€: {len(result.images)}ê°œ")
        else:
            print(f"   âŒ {result.error}")


if __name__ == "__main__":
    # 1. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
    test_full_pipeline()

    # 2. ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ í…ŒìŠ¤íŠ¸
    test_various_publishers()

    # 3. ë‹¨ìˆœ í…ŒìŠ¤íŠ¸
    # test_simple()
