# VTK Python Docs - LLM Configuration
# Copy this file to .env and configure your LLM provider

# LLM Model (uses LiteLLM format)
# Examples:
#   gpt-4o-mini          - OpenAI (requires OPENAI_API_KEY)
#   gpt-4o               - OpenAI (requires OPENAI_API_KEY)
#   claude-3-haiku-20240307 - Anthropic (requires ANTHROPIC_API_KEY)
#   claude-3-5-sonnet-20241022 - Anthropic (requires ANTHROPIC_API_KEY)
#   ollama/llama3.2      - Ollama local (free, requires Ollama running)
#   ollama/mistral       - Ollama local (free, requires Ollama running)
#   gemini/gemini-1.5-flash - Google (requires GEMINI_API_KEY)
#
# See https://docs.litellm.ai/docs/providers for all supported models
LLM_MODEL=gpt-4o-mini

# API Keys (set the one for your chosen provider)
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=

# For Ollama (local models)
# OLLAMA_API_BASE=http://localhost:11434

# Rate limiting (requests per minute) - adjust based on your API tier
LLM_RATE_LIMIT=60

# Max concurrent requests for async processing
LLM_MAX_CONCURRENT=10

# Fallback to TextRank summarizer if LLM fails or is not configured
LLM_FALLBACK=true
