# CodeRAG Model Configuration

llm:
  # Primary model: Qwen2.5-Coder-7B-Instruct
  name: Qwen/Qwen2.5-Coder-7B-Instruct
  max_new_tokens: 1024
  temperature: 0.1
  top_p: 0.95
  repetition_penalty: 1.1

  # Quantization settings for 8GB VRAM
  quantization:
    enabled: true
    load_in_4bit: true
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: float16
    bnb_4bit_use_double_quant: true

  # Device settings
  device_map: auto
  trust_remote_code: true

  # Fallback model for constrained environments
  fallback:
    name: meta-llama/Llama-3.2-3B-Instruct
    max_new_tokens: 512

embeddings:
  # Primary embedding model: nomic-embed-text v1.5
  name: nomic-ai/nomic-embed-text-v1.5
  dimension: 768
  max_length: 8192

  # Processing settings
  batch_size: 32
  device: cuda
  normalize: true

  # Task prefix for search queries
  query_prefix: "search_query: "
  document_prefix: "search_document: "

# VRAM Budget Estimation (RTX 4060 8GB)
# - LLM (4-bit): ~4.5GB
# - Embeddings: ~0.5GB
# - Overhead: ~1GB
# - Total: ~6GB (within 8GB budget)
