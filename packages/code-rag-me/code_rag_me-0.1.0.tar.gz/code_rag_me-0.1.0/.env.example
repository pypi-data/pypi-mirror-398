# CodeRAG Environment Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# QUICK START (no GPU required):
#   1. Copy this file: cp .env.example .env
#   2. Get free API key from https://console.groq.com/keys
#   3. Set MODEL_LLM_API_KEY below
#   4. Run: python -m coderag.main
# =============================================================================

# =============================================================================
# Application Settings
# =============================================================================
DEBUG=false
APP_NAME=CodeRAG
APP_VERSION=0.1.0

# =============================================================================
# Server Settings
# =============================================================================
SERVER_HOST=0.0.0.0
SERVER_PORT=8000
SERVER_LOG_LEVEL=info
SERVER_WORKERS=1
SERVER_RELOAD=false

# =============================================================================
# Model Settings
# =============================================================================
# LLM Provider: "local", "openai", "groq", "anthropic", "openrouter", "together"
# Default: "groq" (free tier, no GPU required). Use "local" for on-device inference (requires GPU)
MODEL_LLM_PROVIDER=groq

# API Configuration (required for non-local providers)
# Get your API key from:
#   - Groq (FREE, fast): https://console.groq.com/keys
#   - OpenAI: https://platform.openai.com/api-keys
#   - Anthropic: https://console.anthropic.com/settings/keys
#   - OpenRouter: https://openrouter.ai/keys
#   - Together: https://api.together.xyz/settings/api-keys
MODEL_LLM_API_KEY=
MODEL_LLM_API_BASE=

# LLM Model Configuration
# For groq: llama-3.3-70b-versatile (default), mixtral-8x7b-32768
# For openai: gpt-4o-mini (default), gpt-4o, gpt-4-turbo
# For anthropic: claude-3-5-sonnet-20241022 (default)
# For openrouter: anthropic/claude-3.5-sonnet (default)
# For together: meta-llama/Llama-3.3-70B-Instruct-Turbo (default)
# For local: Qwen/Qwen2.5-Coder-3B-Instruct (requires GPU with 8GB VRAM)
MODEL_LLM_NAME=llama-3.3-70b-versatile
MODEL_LLM_MAX_NEW_TOKENS=1024
MODEL_LLM_TEMPERATURE=0.1
MODEL_LLM_TOP_P=0.95
MODEL_LLM_USE_4BIT=true
MODEL_LLM_DEVICE_MAP=auto

# Embedding Configuration
MODEL_EMBEDDING_NAME=nomic-ai/nomic-embed-text-v1.5
MODEL_EMBEDDING_DIMENSION=768
MODEL_EMBEDDING_BATCH_SIZE=32  # 32 for CPU, reduce to 8 if using GPU with limited VRAM
# Device: "auto" (recommended), "cuda", or "cpu"
MODEL_EMBEDDING_DEVICE=auto

# =============================================================================
# Vector Store Settings
# =============================================================================
VECTORSTORE_PERSIST_DIRECTORY=./data/chroma_db
VECTORSTORE_COLLECTION_NAME=coderag_chunks
VECTORSTORE_DISTANCE_METRIC=cosine
VECTORSTORE_ANONYMIZED_TELEMETRY=false

# =============================================================================
# Ingestion Settings
# =============================================================================
INGESTION_REPOS_CACHE_DIR=./data/repos
INGESTION_MAX_FILE_SIZE_KB=500
INGESTION_DEFAULT_BRANCH=main
INGESTION_CHUNK_SIZE=1500
INGESTION_CHUNK_OVERLAP=200

# Large Repository Handling
INGESTION_MAX_FILES_PER_REPO=5000
INGESTION_MAX_TOTAL_CHUNKS=50000
INGESTION_BATCH_SIZE=100
INGESTION_STREAM_PROCESSING=true
INGESTION_WARN_FILES_THRESHOLD=1000
INGESTION_WARN_CHUNKS_THRESHOLD=10000

# =============================================================================
# Retrieval Settings
# =============================================================================
RETRIEVAL_DEFAULT_TOP_K=5
RETRIEVAL_MAX_TOP_K=20
RETRIEVAL_SIMILARITY_THRESHOLD=0.3

# =============================================================================
# HuggingFace Settings (Optional)
# =============================================================================
# HF_HOME=./data/hf_cache
# TRANSFORMERS_CACHE=./data/hf_cache
# HF_TOKEN=your_token_here  # Only needed for gated models
