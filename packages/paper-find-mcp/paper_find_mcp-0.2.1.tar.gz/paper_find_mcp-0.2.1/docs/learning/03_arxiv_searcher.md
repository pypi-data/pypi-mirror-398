# ArxivSearcher è¯¦è§£

> **æ–‡ä»¶ä½ç½®**: `paper_search_mcp/academic_platforms/arxiv.py`  
> **éš¾åº¦**: â­â­ (é€‚åˆå…¥é—¨)  
> **æ›´æ–°**: 2025å¹´12æœˆ - ä½¿ç”¨ PyMuPDF4LLM æ›¿ä»£ PyPDF2

---

## æ¦‚è¿°

`ArxivSearcher` æ˜¯ä¸€ä¸ªå®Œæ•´çš„å­¦æœ¯å¹³å°æœç´¢å™¨å®ç°ï¼Œå±•ç¤ºäº†ï¼š

- å¦‚ä½•è°ƒç”¨ arXiv API
- å¦‚ä½•è§£æ Atom/RSS æ ¼å¼çš„å“åº”
- **å¦‚ä½•ä½¿ç”¨ PyMuPDF4LLM è¿›è¡Œé«˜è´¨é‡ PDF æ–‡æœ¬æå–**ï¼ˆ2025 æœ€ä½³å®è·µï¼‰
- å¦‚ä½•ç”Ÿæˆ LLM å‹å¥½çš„ Markdown è¾“å‡º

---

## 2025 æœ€ä½³å®è·µæ›´æ–°

### ä¸ºä»€ä¹ˆä» PyPDF2 è¿ç§»åˆ° PyMuPDF4LLMï¼Ÿ

| ç‰¹æ€§ | PyPDF2 (æ—§) | PyMuPDF4LLM (æ–°) |
|------|:-----------:|:----------------:|
| è¡¨æ ¼æå– | â­ å·® | â­â­â­â­â­ ä¼˜ç§€ |
| æ•°å­¦å…¬å¼ | â­ ä¹±ç  | â­â­â­ å¯è¯» |
| Markdown è¾“å‡º | âŒ ä¸æ”¯æŒ | âœ… åŸç”Ÿæ”¯æŒ |
| LLM å‹å¥½åº¦ | â­ ä½ | â­â­â­â­â­ é«˜ |
| é€Ÿåº¦ | â­â­â­ ä¸­ç­‰ | â­â­â­â­ å¿« |

### PyMuPDF4LLM ç‰¹ç‚¹

```python
import pymupdf4llm

# ä¸€è¡Œä»£ç æå– Markdown æ ¼å¼å†…å®¹
md_text = pymupdf4llm.to_markdown("paper.pdf")

# æ”¯æŒè¡¨æ ¼æ£€æµ‹ç­–ç•¥
md_text = pymupdf4llm.to_markdown(
    "paper.pdf",
    table_strategy="lines_strict"  # ä¸¥æ ¼è¡¨æ ¼æ£€æµ‹
)
```

---

## å®Œæ•´ä»£ç åˆ†æ

### 1. å¯¼å…¥å’Œé…ç½®

```python
# paper_search_mcp/academic_platforms/arxiv.py
"""
ArxivSearcher - arXiv è®ºæ–‡æœç´¢ã€ä¸‹è½½ä¸é˜…è¯»

2025 å¹´æœ€ä½³å®è·µç‰ˆæœ¬ï¼š
- ä½¿ç”¨ PyMuPDF4LLM æ›¿ä»£ PyPDF2ï¼Œæä¾›æ›´å¥½çš„è¡¨æ ¼å’Œå…¬å¼æå–
- è¾“å‡º Markdown æ ¼å¼ï¼Œå¯¹ LLM æ›´å‹å¥½
- æ”¯æŒå¤šç§è¡¨æ ¼æ£€æµ‹ç­–ç•¥
"""
from typing import List, Literal, Optional
from datetime import datetime
import requests
import feedparser
import pymupdf4llm    # 2025 æœ€ä½³å®è·µï¼šä¸“ä¸º LLM ä¼˜åŒ–çš„ PDF æå–
import pymupdf        # åº•å±‚ PDF åº“ï¼Œç”¨äºå¤‡ç”¨æ–‡æœ¬æå–
import os
import logging

from ..paper import Paper

logger = logging.getLogger(__name__)
```

**ğŸ’¡ å­¦ä¹ è¦ç‚¹**ï¼š

1. **æ¨¡å—æ–‡æ¡£å­—ç¬¦ä¸²**: ç®€è¦è¯´æ˜æ¨¡å—åŠŸèƒ½å’Œç‰ˆæœ¬ç‰¹ç‚¹
2. **ç±»å‹æç¤º**: ä½¿ç”¨ `Literal` é™å®šå‚æ•°å–å€¼èŒƒå›´
3. **æ—¥å¿—**: ä½¿ç”¨ `logging` è€Œä¸æ˜¯ `print`

---

### 2. è¡¨æ ¼æ£€æµ‹ç­–ç•¥

```python
class ArxivSearcher(PaperSource):
    """arXiv è®ºæ–‡æœç´¢å™¨"""
    
    BASE_URL = "http://export.arxiv.org/api/query"
    
    # è¡¨æ ¼æ£€æµ‹ç­–ç•¥è¯´æ˜
    TABLE_STRATEGIES = {
        "lines_strict": "ä¸¥æ ¼æ¨¡å¼ï¼šåªæ£€æµ‹æœ‰å®Œæ•´è¾¹æ¡†çº¿çš„è¡¨æ ¼",
        "lines": "çº¿æ¡æ¨¡å¼ï¼šæ£€æµ‹æœ‰éƒ¨åˆ†è¾¹æ¡†çº¿çš„è¡¨æ ¼",
        "text": "æ–‡æœ¬æ¨¡å¼ï¼šåŸºäºæ–‡æœ¬å¯¹é½æ£€æµ‹è¡¨æ ¼ï¼ˆé€‚åˆæ— è¾¹æ¡†è¡¨æ ¼ï¼‰",
        "explicit": "æ˜¾å¼æ¨¡å¼ï¼šåªæ£€æµ‹ PDF ä¸­æ˜ç¡®æ ‡è®°çš„è¡¨æ ¼",
    }
```

**ğŸ’¡ å¦‚ä½•é€‰æ‹©è¡¨æ ¼ç­–ç•¥**ï¼š

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | å‡†ç¡®åº¦ | é€Ÿåº¦ |
|------|---------|:------:|:----:|
| `lines_strict` | æœ‰å®Œæ•´è¾¹æ¡†çš„è¡¨æ ¼ | â­â­â­â­â­ | â­â­â­â­â­ |
| `lines` | æœ‰éƒ¨åˆ†è¾¹æ¡†çš„è¡¨æ ¼ | â­â­â­â­ | â­â­â­â­ |
| `text` | æ— è¾¹æ¡†çš„å¯¹é½æ–‡æœ¬ | â­â­â­ | â­â­â­ |
| `explicit` | PDF æ˜ç¡®æ ‡è®°çš„è¡¨æ ¼ | â­â­â­â­â­ | â­â­â­â­â­ |

---

### 3. æœç´¢åŠŸèƒ½

```python
def search(self, query: str, max_results: int = 10) -> List[Paper]:
    """æœç´¢ arXiv è®ºæ–‡
    
    Args:
        query: æœç´¢å…³é”®è¯ï¼Œæ”¯æŒ arXiv æŸ¥è¯¢è¯­æ³•
               ä¾‹å¦‚: "ti:attention" (æ ‡é¢˜), "au:hinton" (ä½œè€…)
        max_results: æœ€å¤§è¿”å›æ•°é‡
        
    Returns:
        List[Paper]: è®ºæ–‡åˆ—è¡¨
    """
    params = {
        'search_query': query,
        'max_results': max_results,
        'sortBy': 'submittedDate',
        'sortOrder': 'descending'
    }
    
    try:
        # æ·»åŠ è¶…æ—¶é˜²æ­¢é˜»å¡
        response = requests.get(self.BASE_URL, params=params, timeout=30)
        response.raise_for_status()  # æ£€æŸ¥ HTTP çŠ¶æ€ç 
    except requests.RequestException as e:
        logger.error(f"arXiv API request failed: {e}")
        return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
    
    feed = feedparser.parse(response.content)
    papers = []
    
    for entry in feed.entries:
        try:
            # æ¸…ç†æ ‡é¢˜å’Œæ‘˜è¦ä¸­çš„æ¢è¡Œç¬¦
            papers.append(Paper(
                paper_id=entry.id.split('/')[-1],
                title=entry.title.replace('\n', ' ').strip(),
                abstract=entry.summary.replace('\n', ' ').strip(),
                # ... å…¶ä»–å­—æ®µ
            ))
        except Exception as e:
            logger.warning(f"Error parsing arXiv entry: {e}")
            
    return papers
```

**ğŸ’¡ æ”¹è¿›ç‚¹**ï¼š
- æ·»åŠ äº† `timeout=30` é˜²æ­¢ç½‘ç»œé˜»å¡
- ä½¿ç”¨ `raise_for_status()` æ£€æŸ¥ HTTP çŠ¶æ€ç 
- æ¸…ç†æ–‡æœ¬ä¸­çš„æ¢è¡Œç¬¦

---

### 4. ä¸‹è½½åŠŸèƒ½

```python
def download_pdf(self, paper_id: str, save_path: str = "./downloads") -> str:
    """ä¸‹è½½ arXiv è®ºæ–‡ PDF
    
    æ”¹è¿›ï¼š
    - è‡ªåŠ¨åˆ›å»ºç›®å½•
    - æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤ä¸‹è½½ï¼‰
    - å¤„ç†ç‰¹æ®Šæ–‡ä»¶åï¼ˆæ–œæ ã€å†’å·ç­‰ï¼‰
    """
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(save_path, exist_ok=True)
    
    # å¤„ç†å¸¦ç‰ˆæœ¬å·çš„ ID (ä¾‹å¦‚ 2106.12345v2)
    safe_id = paper_id.replace('/', '_').replace(':', '_')
    output_file = os.path.join(save_path, f"{safe_id}.pdf")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(output_file):
        logger.info(f"PDF already exists: {output_file}")
        return output_file
    
    # ä¸‹è½½ PDF
    pdf_url = f"https://arxiv.org/pdf/{paper_id}.pdf"
    
    try:
        response = requests.get(pdf_url, timeout=60)
        response.raise_for_status()
        
        with open(output_file, 'wb') as f:
            f.write(response.content)
        
        logger.info(f"PDF downloaded: {output_file}")
        return output_file
        
    except requests.RequestException as e:
        raise RuntimeError(f"Failed to download PDF: {e}")
```

---

### 5. æ ¸å¿ƒï¼šread_paper() æ–¹æ³•

```python
def read_paper(
    self, 
    paper_id: str, 
    save_path: str = "./downloads",
    output_format: Literal["markdown", "text"] = "markdown",
    table_strategy: Literal["lines_strict", "lines", "text", "explicit"] = "lines_strict",
    pages: Optional[List[int]] = None
) -> str:
    """è¯»å–è®ºæ–‡å¹¶æå–å†…å®¹
    
    ä½¿ç”¨ PyMuPDF4LLM è¿›è¡Œé«˜è´¨é‡æ–‡æœ¬æå–ï¼Œæ”¯æŒï¼š
    - Markdown æ ¼å¼è¾“å‡ºï¼ˆæ¨èï¼Œå¯¹ LLM å‹å¥½ï¼‰
    - è¡¨æ ¼è‡ªåŠ¨è½¬æ¢ä¸º Markdown è¡¨æ ¼
    - å¤šç§è¡¨æ ¼æ£€æµ‹ç­–ç•¥
    
    Args:
        paper_id: arXiv è®ºæ–‡ ID
        save_path: PDF å­˜å‚¨ç›®å½•
        output_format: è¾“å‡ºæ ¼å¼
            - "markdown": Markdown æ ¼å¼ï¼ˆæ¨èï¼ŒåŒ…å«è¡¨æ ¼ï¼‰
            - "text": çº¯æ–‡æœ¬æ ¼å¼
        table_strategy: è¡¨æ ¼æ£€æµ‹ç­–ç•¥
        pages: è¦æå–çš„é¡µé¢åˆ—è¡¨ï¼ˆ0-indexedï¼‰ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨é¡µé¢
        
    Returns:
        str: æå–çš„è®ºæ–‡å†…å®¹
    """
    # ç¡®ä¿ PDF å·²ä¸‹è½½
    pdf_path = self._ensure_pdf_downloaded(paper_id, save_path)
    
    if output_format == "markdown":
        return self._extract_markdown(pdf_path, table_strategy, pages)
    else:
        return self._extract_text(pdf_path, pages)
```

**ğŸ’¡ è®¾è®¡æ¨¡å¼**ï¼š
- ä½¿ç”¨ `Literal` ç±»å‹æç¤ºé™å®šå‚æ•°å–å€¼
- åˆ†ç¦»ä¸‹è½½å’Œæå–é€»è¾‘
- æ”¯æŒä¸¤ç§è¾“å‡ºæ ¼å¼

---

### 6. Markdown æå–å®ç°

```python
def _extract_markdown(
    self, 
    pdf_path: str, 
    table_strategy: str,
    pages: Optional[List[int]] = None
) -> str:
    """ä½¿ç”¨ PyMuPDF4LLM æå– Markdown æ ¼å¼å†…å®¹
    
    PyMuPDF4LLM ç‰¹ç‚¹ï¼š
    - ä¸“ä¸º LLM ä¼˜åŒ–çš„è¾“å‡ºæ ¼å¼
    - è‡ªåŠ¨æ£€æµ‹å¹¶æ ¼å¼åŒ–è¡¨æ ¼
    - ä¿ç•™æ–‡æ¡£ç»“æ„ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ç­‰ï¼‰
    """
    try:
        md_text = pymupdf4llm.to_markdown(
            pdf_path,
            pages=pages,
            table_strategy=table_strategy,
            show_progress=False  # é™é»˜æ¨¡å¼
        )
        return md_text
    except Exception as e:
        logger.error(f"Markdown extraction failed: {e}")
        # å›é€€åˆ°çº¯æ–‡æœ¬æå–
        logger.info("Falling back to plain text extraction")
        return self._extract_text(pdf_path, pages)
```

**ğŸ’¡ å­¦ä¹ è¦ç‚¹**ï¼š
- ä¼˜é›…é™çº§ï¼šMarkdown æå–å¤±è´¥æ—¶å›é€€åˆ°çº¯æ–‡æœ¬
- é™é»˜æ¨¡å¼ï¼š`show_progress=False` é¿å…è¾“å‡ºè¿›åº¦æ¡

---

### 7. çº¯æ–‡æœ¬æå–ï¼ˆå¤‡ç”¨ï¼‰

```python
def _extract_text(
    self, 
    pdf_path: str, 
    pages: Optional[List[int]] = None
) -> str:
    """ä½¿ç”¨ PyMuPDF æå–çº¯æ–‡æœ¬å†…å®¹"""
    try:
        doc = pymupdf.open(pdf_path)
        text_parts = []
        
        page_range = pages if pages else range(len(doc))
        
        for page_num in page_range:
            if 0 <= page_num < len(doc):
                page = doc[page_num]
                text_parts.append(page.get_text())
        
        doc.close()
        return "\n".join(text_parts)
        
    except Exception as e:
        logger.error(f"Text extraction failed: {e}")
        return f"Error extracting text: {e}"
```

---

## è¾“å‡ºç¤ºä¾‹å¯¹æ¯”

### PyPDF2 (æ—§) è¾“å‡º

```
Table 1: Results
Model Accuracy F1
BERT 0.89 0.87
GPT 0.92 0.90
```

è¡¨æ ¼ç»“æ„å®Œå…¨ä¸¢å¤±ï¼

### PyMuPDF4LLM (æ–°) è¾“å‡º

```markdown
**Table 1: Results**

| Model | Accuracy | F1 |
|-------|----------|-----|
| BERT  | 0.89     | 0.87 |
| GPT   | 0.92     | 0.90 |
```

è¡¨æ ¼ç»“æ„å®Œç¾ä¿ç•™ï¼

---

## ä½¿ç”¨ç¤ºä¾‹

```python
from paper_search_mcp.academic_platforms.arxiv import ArxivSearcher

searcher = ArxivSearcher()

# æœç´¢è®ºæ–‡
papers = searcher.search("attention mechanism", max_results=5)

# ä¸‹è½½å¹¶è¯»å–ï¼ˆMarkdown æ ¼å¼ï¼‰
content = searcher.read_paper(
    papers[0].paper_id,
    output_format="markdown",
    table_strategy="lines_strict"
)

# åªæå–å‰ä¸¤é¡µ
content = searcher.read_paper(
    papers[0].paper_id,
    pages=[0, 1]
)
```

---

## å¯ä»¥ç»§ç»­æ”¹è¿›çš„åœ°æ–¹

### 1. æ·»åŠ æ•°å­¦å…¬å¼è½¬ LaTeX

å¦‚æœéœ€è¦å®Œç¾çš„æ•°å­¦å…¬å¼è½¬æ¢ï¼Œå¯ä»¥è€ƒè™‘æ·»åŠ  `marker-pdf`ï¼š

```python
# å®‰è£…: pip install marker-pdf

# ä½¿ç”¨ marker è½¬æ¢ï¼ˆé€Ÿåº¦è¾ƒæ…¢ä½†å…¬å¼æ›´å‡†ç¡®ï¼‰
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict

converter = PdfConverter(artifact_dict=create_model_dict())
result = converter("paper.pdf")
```

### 2. æ·»åŠ ç¼“å­˜

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def read_paper_cached(self, paper_id: str, output_format: str) -> str:
    return self.read_paper(paper_id, output_format=output_format)
```

### 3. å¼‚æ­¥ä¸‹è½½

```python
import aiohttp
import asyncio

async def download_pdf_async(self, paper_id: str, save_path: str) -> str:
    async with aiohttp.ClientSession() as session:
        async with session.get(f"https://arxiv.org/pdf/{paper_id}.pdf") as response:
            content = await response.read()
            # ...
```

---

## ä¾èµ–è¦æ±‚

```toml
# pyproject.toml
dependencies = [
    "pymupdf4llm>=0.2.0",  # æ›¿ä»£ PyPDF2
    "requests",
    "feedparser",
    # ...
]
```

---

## æ€»ç»“

æ›´æ–°åçš„ `ArxivSearcher` ä½“ç°äº† 2025 å¹´ PDF æå–æœ€ä½³å®è·µï¼š

| æ”¹è¿› | æ•ˆæœ |
|------|------|
| PyMuPDF4LLM | é«˜è´¨é‡ Markdown è¾“å‡º |
| è¡¨æ ¼ç­–ç•¥ | çµæ´»çš„è¡¨æ ¼æ£€æµ‹ |
| ä¼˜é›…é™çº§ | å¤±è´¥æ—¶è‡ªåŠ¨å›é€€ |
| ç±»å‹æç¤º | æ›´å¥½çš„ä»£ç æç¤º |
| æ—¥å¿—è®°å½• | ä¾¿äºè°ƒè¯• |

è¿™æ˜¯ä¸€ä¸ªç”Ÿäº§çº§åˆ«çš„ PDF æå–å®ç°ï¼
