{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7539dc5-46e6-4740-890e-0b233f8a3534",
   "metadata": {},
   "source": [
    "## Load the extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103e7975-8774-457c-8ef1-162573ca6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport -sparkmagic # it loses the references to the sessions if it reloads\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddde0b3a-a5b3-4e9c-8a97-0fe590e79dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext livy_uploads.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8270623-2d54-4bdb-9c8d-80e76bdecf5f",
   "metadata": {},
   "source": [
    "## Fetching remote variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46dfcbed-0a59-4533-bf3c-a1922ae663ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HiveContext', 'StreamingContext', '__builtins__', 'cloudpickle', 'sc', 'spark', 'sqlContext']"
     ]
    }
   ],
   "source": [
    "print(list(sorted(globals())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1a1a1a-6777-4e4b-a5c0-dee6c15657cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__session__', '__spec__', '_dh', '_i', '_i1', '_i2', '_i3', '_i4', '_ih', '_ii', '_iii', '_oh', 'display_dataframe', 'exit', 'get_ipython', 'ip', 'open', 'quit']\n"
     ]
    }
   ],
   "source": [
    "%local\n",
    "\n",
    "print(list(sorted(globals())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46582d08-4d7d-45a5-9dab-a59ae4a037f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.datetime(2025, 12, 20, 21, 2, 22, 103000, tzinfo=datetime.timezone(datetime.timedelta(0), 'UTC'))"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().astimezone()\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89bdda1f-5e64-4dd4-ace7-aa7ca1349910",
   "metadata": {},
   "outputs": [],
   "source": [
    "%get_obj_from_spark -n now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc84d9e-d1c2-4cc2-8f5b-a999e53d9a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2025, 12, 20, 21, 2, 22, 103000, tzinfo=datetime.timezone(datetime.timedelta(0), 'UTC'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%local\n",
    "\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492ba5f0-1df9-472e-8bd9-e06d7af2b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "delta = (datetime.now().astimezone() - now)\n",
    "assert delta.total_seconds() < 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e7b51-1a37-45ea-acc0-7be7ac355d3f",
   "metadata": {},
   "source": [
    "## Sending local variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "343468f8-6c26-4439-abf4-b02525785cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "foo = {2, 3, 4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43925e6-ed31-4421-ad1a-d431d7d8fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%send_obj_to_spark -n foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea7a1719-7cef-4e3c-be92-f0c25211054c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "foo_total = sum(foo)\n",
    "\n",
    "assert foo == {2, 3, 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49925f03-75bc-4fdc-8040-5e6f7bf0a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%get_obj_from_spark -n foo_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ab5942b-943d-4610-90f9-ed88f0bb07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "assert foo_total == 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eaa787-5d90-4a70-86bd-fd0224f27718",
   "metadata": {},
   "source": [
    "## Running commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89dabe55-534a-4f6d-a6a4-641df6c322df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 96K\n",
      "drwxrwxrwt  1 root root 4.0K Dec 20 21:02 ./\n",
      "drwxr-xr-x  1 root root 4.0K Dec 20 21:01 ../\n",
      "-rw-------  1 app  app   22K Dec 20 21:02 4818623367592642843\n",
      "drwxr-xr-x  2 app  app  4.0K Dec 20 21:02 blockmgr-d98206a4-d920-4f49-b294-9378ee6c5d5b/\n",
      "drwxr-xr-x  2 app  app  4.0K Dec 20 21:02 hsperfdata_app/\n",
      "drwxr-xr-x  2 root root 4.0K Oct 19  2024 hsperfdata_root/\n",
      "-rw-------  1 app  app  3.9K Dec 20 21:02 livyConf3890629206090919141.properties\n",
      "-rw-------  1 app  app  8.4K Dec 20 21:01 magics.ipynb\n",
      "drwx------  2 app  app  4.0K Dec 20 21:02 rsc-tmp4569829120044733179/\n",
      "drwx------  3 app  app  4.0K Jan  9  2025 sample-dir/\n",
      "drwxr-xr-x  2 app  app  4.0K Dec 20 20:18 spark/\n",
      "drwx------ 13 app  app  4.0K Dec 20 21:02 spark1650519457699558222/\n",
      "drwx------  4 app  app  4.0K Dec 20 21:02 spark-432eb187-770a-4346-9cef-e53de08657ca/\n",
      "drwx------ 13 app  app  4.0K Dec 20 21:01 spark7950148411163509628/\n",
      "drwxr-xr-x  2 app  app  4.0K Dec 20 21:02 spark-ae56d3e8-f839-4291-817b-44800913c2ec/\n",
      "drwxr-xr-x  2 app  app  4.0K Dec 20 21:01 tmp/\n",
      "drwx------  2 app  app  4.0K Dec 20 21:02 tmp3psq_fth/\n",
      "$ command exited with code 0"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "\n",
    "ls -lahF ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97109de1-6baf-47f1-a0f0-fd48aae9a6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n",
      "$ command exited with code 42"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "\n",
    "bash -c 'echo foo && exit 42'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "336cf21a-049e-494b-8aca-bbad2dcb07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "assert shell_output == 'foo\\n'\n",
    "assert shell_returncode == 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33add3-c506-4a0a-b40f-02278b554530",
   "metadata": {},
   "source": [
    "## Sending local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dd00633-fc4d-4d35-9625-c649c6324a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 100K\n",
      "drwxrwxr-x  5 app app 4.0K Dec 20 21:02 ./\n",
      "drwxrwxr-x 20 app app 4.0K Dec 20 19:59 ../\n",
      "drwxr-xr-x  2 app app 4.0K Jan 13  2025 .ipynb_checkpoints/\n",
      "-rw-rw-r--  1 app app  42K Dec 20 21:02 magics.ipynb\n",
      "drwxrwxr-x  3 app app 4.0K Jan  9  2025 sample-dir/\n",
      "drwxrwxr-x  2 app app 4.0K Dec 20 19:45 spark/\n",
      "-rw-rw-r--  1 app app  33K Dec 20 19:45 test-spark-another-version.ipynb\n"
     ]
    }
   ],
   "source": [
    "%local !ls -lahF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7c16f76-a199-4c4b-bbf1-76dbfe9827a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded magics.ipynb to /tmp/magics.ipynb"
     ]
    }
   ],
   "source": [
    "%send_path_to_spark -p magics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62c00b15-92bb-4256-b8c5-8972dc68867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-------  1 app  app   42K Dec 20 21:02 magics.ipynb\n",
      "$ command exited with code 0"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "\n",
    "ls -lahF | grep magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75bb149a-e7d9-498b-afe0-ea34b7b255f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "assert 'magics.ipynb' in shell_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffb420-c3b7-4867-9d70-ee63fa6af659",
   "metadata": {},
   "source": [
    "## Sending local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "464567bf-bd72-467d-b0e3-8f754b75ffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample-dir/\n",
      "sample-dir/inner\n",
      "sample-dir/inner/bar.txt\n",
      "sample-dir/foo.txt\n"
     ]
    }
   ],
   "source": [
    "%local !find sample-dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "739881b0-f362-4b16-b090-bb02403b1a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded sample-dir to /tmp/sample-dir"
     ]
    }
   ],
   "source": [
    "%send_path_to_spark -p sample-dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81f65cad-1264-4cc7-befd-2054f7081af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\n",
      "$ command exited with code 0"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff2d46be-e9ac-4bd0-ba23-823ae14ec601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/sample-dir\n",
      "/tmp/sample-dir/inner\n",
      "/tmp/sample-dir/inner/bar.txt\n",
      "/tmp/sample-dir/foo.txt\n",
      "$ command exited with code 0"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "\n",
    "find \"$PWD/sample-dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed081aa9-f3cf-4655-a94b-6d0c21e40000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "assert 'sample-dir/' in shell_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602cd86-34ab-4a64-b785-f70616557e28",
   "metadata": {},
   "source": [
    "## Following session logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "165a6270-434f-4232-9e8d-67d053896c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "25/12/20 21:02:12 INFO RSCDriver: Received job request 25181c0f-c239-473d-bed2-87b06a984908\n",
      "25/12/20 21:02:12 INFO RSCDriver: SparkContext not yet up, queueing job request.\n",
      "25/12/20 21:02:14 INFO SparkEntries: Starting Spark context...\n",
      "25/12/20 21:02:14 INFO SparkContext: Running Spark version 3.3.2\n",
      "25/12/20 21:02:14 INFO ResourceUtils: ==============================================================\n",
      "25/12/20 21:02:14 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/12/20 21:02:14 INFO ResourceUtils: ==============================================================\n",
      "25/12/20 21:02:14 INFO SparkContext: Submitted application: livy-session-1\n",
      "25/12/20 21:02:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1000, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/12/20 21:02:14 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "25/12/20 21:02:14 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/12/20 21:02:14 INFO SecurityManager: Changing view acls to: app\n",
      "25/12/20 21:02:14 INFO SecurityManager: Changing modify acls to: app\n",
      "25/12/20 21:02:14 INFO SecurityManager: Changing view acls groups to: \n",
      "25/12/20 21:02:14 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/12/20 21:02:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(app); groups with view permissions: Set(); users  with modify permissions: Set(app); groups with modify permissions: Set()\n",
      "25/12/20 21:02:14 INFO Utils: Successfully started service 'sparkDriver' on port 33783.\n",
      "25/12/20 21:02:14 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/12/20 21:02:14 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/12/20 21:02:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/12/20 21:02:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/12/20 21:02:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/12/20 21:02:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d98206a4-d920-4f49-b294-9378ee6c5d5b\n",
      "25/12/20 21:02:14 INFO MemoryStore: MemoryStore started with capacity 353.4 MiB\n",
      "25/12/20 21:02:14 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/12/20 21:02:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-common-4.1.86.Final.jar at spark://livy:33783/jars/netty-common-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/kryo-shaded-4.0.2.jar at spark://livy:33783/jars/kryo-shaded-4.0.2.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/livy-rsc-0.8.0-incubating.jar at spark://livy:33783/jars/livy-rsc-0.8.0-incubating.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/objenesis-2.5.1.jar at spark://livy:33783/jars/objenesis-2.5.1.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-sctp-4.1.86.Final.jar at spark://livy:33783/jars/netty-transport-sctp-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-smtp-4.1.86.Final.jar at spark://livy:33783/jars/netty-codec-smtp-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-dns-4.1.86.Final.jar at spark://livy:33783/jars/netty-resolver-dns-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-redis-4.1.86.Final.jar at spark://livy:33783/jars/netty-codec-redis-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-handler-ssl-ocsp-4.1.86.Final.jar at spark://livy:33783/jars/netty-handler-ssl-ocsp-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-dns-native-macos-4.1.86.Final-osx-aarch_64.jar at spark://livy:33783/jars/netty-resolver-dns-native-macos-4.1.86.Final-osx-aarch_64.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-all-4.1.86.Final.jar at spark://livy:33783/jars/netty-all-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-classes-epoll-4.1.86.Final.jar at spark://livy:33783/jars/netty-transport-classes-epoll-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-4.1.86.Final.jar at spark://livy:33783/jars/netty-codec-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-handler-proxy-4.1.86.Final.jar at spark://livy:33783/jars/netty-handler-proxy-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-kqueue-4.1.86.Final-osx-aarch_64.jar at spark://livy:33783/jars/netty-transport-native-kqueue-4.1.86.Final-osx-aarch_64.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-http-4.1.86.Final.jar at spark://livy:33783/jars/netty-codec-http-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-xml-4.1.86.Final.jar at spark://livy:33783/jars/netty-codec-xml-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-rxtx-4.1.86.Final.jar at spark://livy:33783/jars/netty-transport-rxtx-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-dns-classes-macos-4.1.86.Final.jar at spark://livy:33783/jars/netty-resolver-dns-classes-macos-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-stomp-4.1.86.Final.jar at spark://livy:33783/jars/netty-codec-stomp-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-classes-kqueue-4.1.86.Final.jar at spark://livy:33783/jars/netty-transport-classes-kqueue-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-kqueue-4.1.86.Final-osx-x86_64.jar at spark://livy:33783/jars/netty-transport-native-kqueue-4.1.86.Final-osx-x86_64.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/minlog-1.3.0.jar at spark://livy:33783/jars/minlog-1.3.0.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/livy-api-0.8.0-incubating.jar at spark://livy:33783/jars/livy-api-0.8.0-incubating.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-memcache-4.1.86.Final.jar at spark://livy:33783/jars/netty-codec-memcache-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-socks-4.1.86.Final.jar at spark://livy:33783/jars/netty-codec-socks-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-haproxy-4.1.86.Final.jar at spark://livy:33783/jars/netty-codec-haproxy-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-dns-4.1.86.Final.jar at spark://livy:33783/jars/netty-codec-dns-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-4.1.86.Final.jar at spark://livy:33783/jars/netty-resolver-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-buffer-4.1.86.Final.jar at spark://livy:33783/jars/netty-buffer-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-unix-common-4.1.86.Final.jar at spark://livy:33783/jars/netty-transport-native-unix-common-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-4.1.86.Final.jar at spark://livy:33783/jars/netty-transport-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-http2-4.1.86.Final.jar at spark://livy:33783/jars/netty-codec-http2-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-dns-native-macos-4.1.86.Final-osx-x86_64.jar at spark://livy:33783/jars/netty-resolver-dns-native-macos-4.1.86.Final-osx-x86_64.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-epoll-4.1.86.Final-linux-aarch_64.jar at spark://livy:33783/jars/netty-transport-native-epoll-4.1.86.Final-linux-aarch_64.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-handler-4.1.86.Final.jar at spark://livy:33783/jars/netty-handler-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-epoll-4.1.86.Final-linux-x86_64.jar at spark://livy:33783/jars/netty-transport-native-epoll-4.1.86.Final-linux-x86_64.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-udt-4.1.86.Final.jar at spark://livy:33783/jars/netty-transport-udt-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-mqtt-4.1.86.Final.jar at spark://livy:33783/jars/netty-codec-mqtt-4.1.86.Final.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/repl_2.12-jars/commons-codec-1.9.jar at spark://livy:33783/jars/commons-codec-1.9.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 ERROR SparkContext: Failed to add file:///etc/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar to Spark environment\n",
      "java.lang.IllegalArgumentException: requirement failed: File kryo-shaded-4.0.2.jar was already registered with a different path (old path = /etc/livy/rsc-jars/kryo-shaded-4.0.2.jar, new path = /etc/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.rpc.netty.NettyStreamManager.addJar(NettyStreamManager.scala:81)\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1965)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2014)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2714)\n",
      "\tat org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:52)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:66)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:71)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.$anonfun$start$1(SparkInterpreter.scala:85)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:342)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:60)\n",
      "\tat org.apache.livy.repl.Session.$anonfun$start$1(Session.scala:128)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/12/20 21:02:15 ERROR SparkContext: Failed to add file:///etc/livy/repl_2.12-jars/objenesis-2.5.1.jar to Spark environment\n",
      "java.lang.IllegalArgumentException: requirement failed: File objenesis-2.5.1.jar was already registered with a different path (old path = /etc/livy/rsc-jars/objenesis-2.5.1.jar, new path = /etc/livy/repl_2.12-jars/objenesis-2.5.1.jar\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.rpc.netty.NettyStreamManager.addJar(NettyStreamManager.scala:81)\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1965)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2014)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2714)\n",
      "\tat org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:52)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:66)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:71)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.$anonfun$start$1(SparkInterpreter.scala:85)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:342)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:60)\n",
      "\tat org.apache.livy.repl.Session.$anonfun$start$1(Session.scala:128)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/12/20 21:02:15 ERROR SparkContext: Failed to add file:///etc/livy/repl_2.12-jars/minlog-1.3.0.jar to Spark environment\n",
      "java.lang.IllegalArgumentException: requirement failed: File minlog-1.3.0.jar was already registered with a different path (old path = /etc/livy/rsc-jars/minlog-1.3.0.jar, new path = /etc/livy/repl_2.12-jars/minlog-1.3.0.jar\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.rpc.netty.NettyStreamManager.addJar(NettyStreamManager.scala:81)\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1965)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2014)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2714)\n",
      "\tat org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:52)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:66)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:71)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.$anonfun$start$1(SparkInterpreter.scala:85)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:342)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:60)\n",
      "\tat org.apache.livy.repl.Session.$anonfun$start$1(Session.scala:128)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/repl_2.12-jars/livy-repl_2.12-0.8.0-incubating.jar at spark://livy:33783/jars/livy-repl_2.12-0.8.0-incubating.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added JAR file:///etc/livy/repl_2.12-jars/livy-core_2.12-0.8.0-incubating.jar at spark://livy:33783/jars/livy-core_2.12-0.8.0-incubating.jar with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO SparkContext: Added file file:///etc/spark/python/lib/pyspark.zip at spark://livy:33783/files/pyspark.zip with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO Utils: Copying /etc/spark/python/lib/pyspark.zip to /tmp/spark-432eb187-770a-4346-9cef-e53de08657ca/userFiles-13061cad-7d92-457e-af9d-92e4f423fb97/pyspark.zip\n",
      "25/12/20 21:02:15 INFO SparkContext: Added file file:///etc/spark/python/lib/py4j-0.10.9.5-src.zip at spark://livy:33783/files/py4j-0.10.9.5-src.zip with timestamp 1766264534740\n",
      "25/12/20 21:02:15 INFO Utils: Copying /etc/spark/python/lib/py4j-0.10.9.5-src.zip to /tmp/spark-432eb187-770a-4346-9cef-e53de08657ca/userFiles-13061cad-7d92-457e-af9d-92e4f423fb97/py4j-0.10.9.5-src.zip\n",
      "25/12/20 21:02:15 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://master.docker.internal:7077...\n",
      "25/12/20 21:02:15 INFO TransportClientFactory: Successfully created connection to master.docker.internal/172.18.0.2:7077 after 7 ms (0 ms spent in bootstraps)\n",
      "25/12/20 21:02:15 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20251220210215-0001\n",
      "25/12/20 21:02:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251220210215-0001/0 on worker-20251220210106-172.18.0.5-36015 (172.18.0.5:36015) with 1 core(s)\n",
      "25/12/20 21:02:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20251220210215-0001/0 on hostPort 172.18.0.5:36015 with 1 core(s), 1000.0 MiB RAM\n",
      "25/12/20 21:02:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251220210215-0001/1 on worker-20251220210106-172.18.0.5-36015 (172.18.0.5:36015) with 1 core(s)\n",
      "25/12/20 21:02:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20251220210215-0001/1 on hostPort 172.18.0.5:36015 with 1 core(s), 1000.0 MiB RAM\n",
      "25/12/20 21:02:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34949.\n",
      "25/12/20 21:02:15 INFO NettyBlockTransferService: Server created on livy:34949\n",
      "25/12/20 21:02:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/12/20 21:02:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, livy, 34949, None)\n",
      "25/12/20 21:02:15 INFO BlockManagerMasterEndpoint: Registering block manager livy:34949 with 353.4 MiB RAM, BlockManagerId(driver, livy, 34949, None)\n",
      "25/12/20 21:02:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, livy, 34949, None)\n",
      "25/12/20 21:02:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, livy, 34949, None)\n",
      "25/12/20 21:02:15 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251220210215-0001/0 is now RUNNING\n",
      "25/12/20 21:02:15 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251220210215-0001/1 is now RUNNING\n",
      "25/12/20 21:02:15 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "25/12/20 21:02:15 INFO SparkEntries: Spark context finished initialization in 651ms\n",
      "25/12/20 21:02:15 INFO SparkEntries: Created Spark session.\n",
      "25/12/20 21:02:18 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:58766) with ID 0,  ResourceProfileId 0\n",
      "25/12/20 21:02:18 INFO SparkEntries: Created SQLContext.\n",
      "25/12/20 21:02:18 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:37209 with 353.4 MiB RAM, BlockManagerId(0, 172.18.0.5, 37209, None)\n",
      "25/12/20 21:02:19 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:58784) with ID 1,  ResourceProfileId 0\n",
      "25/12/20 21:02:19 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:34235 with 353.4 MiB RAM, BlockManagerId(1, 172.18.0.5, 34235, None)\n",
      "25/12/20 21:02:27 INFO RSCDriver: Received job request c9f45d32-ff1f-47c1-8d57-9c010d75f41c\n",
      "25/12/20 21:02:27 INFO SparkContext: Added file file:/home/app/.livy-sessions/cd7e7dd2-2f35-4fb5-a2f9-ad35b4354e4a/upload-file-85ff0852-ff28-4128-9ece-21b67f64ff30.0 at spark://livy:33783/files/upload-file-85ff0852-ff28-4128-9ece-21b67f64ff30.0 with timestamp 1766264547276\n",
      "25/12/20 21:02:27 INFO Utils: Copying /home/app/.livy-sessions/cd7e7dd2-2f35-4fb5-a2f9-ad35b4354e4a/upload-file-85ff0852-ff28-4128-9ece-21b67f64ff30.0 to /tmp/spark-432eb187-770a-4346-9cef-e53de08657ca/userFiles-13061cad-7d92-457e-af9d-92e4f423fb97/upload-file-85ff0852-ff28-4128-9ece-21b67f64ff30.0\n",
      "25/12/20 21:02:27 WARN SparkContext: The path file:/home/app/.livy-sessions/cd7e7dd2-2f35-4fb5-a2f9-ad35b4354e4a/upload-file-85ff0852-ff28-4128-9ece-21b67f64ff30.0 has been added already. Overwriting of added paths is not supported in the current version.\n",
      "25/12/20 21:02:28 INFO RSCDriver: Received job request d97f3e00-20af-48fc-8b72-8e55870ad8f0\n",
      "25/12/20 21:02:28 INFO SparkContext: Added file file:/home/app/.livy-sessions/cd7e7dd2-2f35-4fb5-a2f9-ad35b4354e4a/upload-file-2ef3cd25-1998-45a7-844d-dabb3fafc4c9.0 at spark://livy:33783/files/upload-file-2ef3cd25-1998-45a7-844d-dabb3fafc4c9.0 with timestamp 1766264548879\n",
      "25/12/20 21:02:28 INFO Utils: Copying /home/app/.livy-sessions/cd7e7dd2-2f35-4fb5-a2f9-ad35b4354e4a/upload-file-2ef3cd25-1998-45a7-844d-dabb3fafc4c9.0 to /tmp/spark-432eb187-770a-4346-9cef-e53de08657ca/userFiles-13061cad-7d92-457e-af9d-92e4f423fb97/upload-file-2ef3cd25-1998-45a7-844d-dabb3fafc4c9.0\n",
      "25/12/20 21:02:28 WARN SparkContext: The path file:/home/app/.livy-sessions/cd7e7dd2-2f35-4fb5-a2f9-ad35b4354e4a/upload-file-2ef3cd25-1998-45a7-844d-dabb3fafc4c9.0 has been added already. Overwriting of added paths is not supported in the current version.\n",
      "\n",
      "stderr: \n"
     ]
    }
   ],
   "source": [
    "%logs_follow -p 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1d8e857-6c1c-47d8-ba47-da936761ecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new logs"
     ]
    }
   ],
   "source": [
    "%logs_follow -p 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2e594ac-505f-42f1-948b-ca47f317d381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc._gateway.jvm.java.lang.System.err.println('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d262c71-789d-40c1-9970-fedf4443a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "%logs_follow -p 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41290b65-3939-460b-9250-3256cfb79fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "assert 'Hello World' in '\\n'.join(logs_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f9366-7f5d-432c-915e-aef0617344da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
