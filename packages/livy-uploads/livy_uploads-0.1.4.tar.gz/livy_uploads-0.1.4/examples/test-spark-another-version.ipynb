{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa0c5b0-0eaf-4514-be64-089dbc232382",
   "metadata": {},
   "source": [
    "### Extracting the archive to ./spark-home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20414aa2-5c61-4268-ac44-c69eeb6d7e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/examples'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "\n",
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a46993-01ae-4608-8ea3-b48cae0938e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'livy'"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "socket.getfqdn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "264da52f-5304-4536-ae60-fa527b182002",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext livy_uploads.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec422042-d657-4768-a87a-f74863038d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark-3.5.4-bin-spark3.5.4-scala2.12-hadoop3.1.1-v1.tgz\n",
      "spark-3.5.4-bin-spark3.5.4-scala2.12-hadoop3.1.1-v2.tgz\n",
      "$ process finished with return code 0\n"
     ]
    }
   ],
   "source": [
    "%%remote_command\n",
    "\n",
    "ls /app/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d79630d-9fa6-4c83-b0c9-f6cd87b82e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.addFile('file:/app/tmp/spark-3.5.4-bin-spark3.5.4-scala2.12-hadoop3.1.1-v1.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19597704-50ff-473b-b433-303cc65566e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./spark-home'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "archive_path = SparkFiles.get('spark-3.5.4-bin-spark3.5.4-scala2.12-hadoop3.1.1-v1.tgz')\n",
    "\n",
    "with TemporaryDirectory() as tmpdir:\n",
    "    output_path = tmpdir + '/out'\n",
    "    shutil.unpack_archive(archive_path, output_path)\n",
    "    items = list(Path(output_path).glob('*'))\n",
    "    if not items or len(items) > 1 or not items[0].is_dir():\n",
    "        raise Exception(f'expected only one directory in the archive: {items}')\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree('./spark-home')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    shutil.move(str(items[0]), './spark-home')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce08af71-219c-42ad-9419-149f9baaa535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3641"
     ]
    }
   ],
   "source": [
    "# monkeypatching the bin/spark-class script, so we can inject some VM args to the launched JVM itself\n",
    "\n",
    "import re\n",
    "\n",
    "exec_line = r'exec \"${CMD[@]}\"'\n",
    "repl = r'''\n",
    "eval set -- \"${SPARK_JAVA_ADHOC_ARGS:-}\"\n",
    "CMD=(\"${CMD[0]}\" \"$@\" \"${CMD[@]:1}\")\n",
    "exec \"${CMD[@]}\"\n",
    "'''\n",
    "\n",
    "pattern = re.compile('^' + re.escape(exec_line) + '$', re.MULTILINE)\n",
    "\n",
    "with open('./spark-home/bin/spark-class') as fp:\n",
    "    content = fp.read()\n",
    "\n",
    "if 'SPARK_JAVA_ADHOC_ARGS' not in content:\n",
    "    with open('./spark-home/bin/spark-class', 'w') as fp:\n",
    "        fp.write(pattern.sub(repl, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf1dc4b7-ca17-4814-9719-bfeb250bb585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./spark-home/LICENSE\n",
      "./spark-home/examples\n",
      "./spark-home/RELEASE\n",
      "./spark-home/sbin\n",
      "./spark-home/NOTICE\n",
      "./spark-home/conf\n",
      "./spark-home/bin\n",
      "./spark-home/python\n",
      "./spark-home/data\n",
      "./spark-home/licenses\n",
      "./spark-home/jars\n",
      "./spark-home/yarn\n",
      "./spark-home/README.md\n",
      "$ process finished with return code 0\n"
     ]
    }
   ],
   "source": [
    "%%remote_command\n",
    "\n",
    "find ./spark-home -mindepth 1 -maxdepth 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c6c8f-9eba-4c77-b702-90a39d900f7a",
   "metadata": {},
   "source": [
    "### Sending config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62835e77-0d46-4823-95d6-3a66cf2d1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "%send_path_to_spark -p spark/ -d ./spark-home/conf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50355bf7-1e48-4413-bd3a-376696d80cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing SPARK_MASTER_IP=172.21.0.2\n",
      "removing SPARK_HOME=/etc/spark\n",
      "removing SPARK_WORKER_DIR=/tmp/spark\n",
      "removing SPARK_LOG_DIR=/var/log/spark\n",
      "removing SPARK_DAEMON_JAVA_OPTS=-XX:+UseContainerSupport\n",
      "removing PYTHONPATH=:/etc/spark/python/lib/pyspark.zip:/etc/spark/python/lib/py4j-0.10.9.3-src.zip:/etc/spark/python/lib...\n",
      "removing SPARK_PID_DIR=/run/spark\n",
      "removing SPARK_BUFFER_SIZE=65536\n",
      "removing SPARK_CONF_DIR=/etc/spark/conf\n",
      "removing PYTHONHASHSEED=0\n",
      "removing PYSPARK_GATEWAY_PORT=40829\n",
      "removing SPARK_PUBLIC_DNS=localhost\n",
      "removing PYSPARK_GATEWAY_SECRET=JM1bPwLfRhO2bBhE0TewKtP0b7dzfmva1P2JZ6VM3qs=\n",
      "removing SPARK_ENV_LOADED=1\n",
      "removing PYSPARK_PYTHON=/usr/bin/python3\n",
      "removing SPARK_SCALA_VERSION=2.12\n",
      "removing PYSPARK_DRIVER_PYTHON=/usr/bin/python3\n",
      "removing PYTHONUNBUFFERED=YES\n",
      "removing SPARK_AUTH_SOCKET_TIMEOUT=15"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "removable_prefixes = ('SPARK_', 'PYSPARK_', 'PYTHON', 'CLASSPATH')\n",
    "for env_name in set(os.environ):\n",
    "    for prefix in removable_prefixes:\n",
    "        if env_name.startswith(prefix):\n",
    "            value = os.environ[env_name]\n",
    "            if len(value) > 100:\n",
    "                value = value[:100] + '...'\n",
    "            print(f'removing {env_name}={value}')\n",
    "            del os.environ[env_name]\n",
    "            break\n",
    "\n",
    "os.environ['SPARK_HOME'] = os.path.abspath('./spark-home')\n",
    "os.environ['SPARK_CONF_DIR'] = os.path.abspath('./spark-home/conf')\n",
    "\n",
    "python_lib = Path(os.environ['SPARK_HOME']) / 'python' / 'lib'\n",
    "py_files = list(sorted(map(str, {\n",
    "    *python_lib.glob('pyspark.zip'),\n",
    "    *python_lib.glob('py4j-*-src.zip'),\n",
    "})))\n",
    "os.environ['SPARK_PYTHON_PATH'] = ':'.join(py_files)\n",
    "os.environ['SPARK_PYFILES'] = ','.join(py_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eabbeb56-6459-419c-a72a-d94647534238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "templating /tmp/spark-home/conf/spark-env.tmpl.sh to /tmp/spark-home/conf/spark-env.sh\n",
      "4694\n",
      "templating /tmp/spark-home/conf/spark-defaults.tmpl.conf to /tmp/spark-home/conf/spark-defaults.conf\n",
      "1292"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "for p in Path(os.environ['SPARK_CONF_DIR']).glob('*'):\n",
    "    if '.tmpl.' not in p.name:\n",
    "        continue\n",
    "\n",
    "    new_name = p.name.replace('.tmpl', '')\n",
    "    dest = p.with_name(new_name)\n",
    "    print(f'templating {p} to {dest}')\n",
    "    content = p.read_text()\n",
    "    dest.write_text(content % os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "720d319d-8d07-4a50-aaff-af2df5bbf24a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#\n",
      "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
      "# contributor license agreements.  See the NOTICE file distributed with\n",
      "# this work for additional information regarding copyright ownership.\n",
      "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
      "# (the \"License\"); you may not use this file except in compliance with\n",
      "# the License.  You may obtain a copy of the License at\n",
      "#\n",
      "#    http\n",
      "//www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "#\n",
      "\n",
      "# This file is sourced when running various Spark programs.\n",
      "# Copy it as spark-env.sh and edit that to configure Spark for your site.\n",
      "\n",
      "# Options read when launching programs locally with\n",
      "# ./bin/run-example or ./bin/spark-submit\n",
      "# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files\n",
      "# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node\n",
      "# - SPARK_PUBLIC_DNS, to set the public dns name of the driver program\n",
      "\n",
      "# Options read by executors and drivers running inside the cluster\n",
      "# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node\n",
      "# - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program\n",
      "# - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data\n",
      "# - MESOS_NATIVE_JAVA_LIBRARY, to point to your libmesos.so if you use Mesos\n",
      "\n",
      "# Options read in any mode\n",
      "# - SPARK_CONF_DIR, Alternate conf dir. (Default\n",
      " ${SPARK_HOME}/conf)\n",
      "# - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default\n",
      " 1).\n",
      "# - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default\n",
      " 1G)\n",
      "# - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default\n",
      " 1G)\n",
      "\n",
      "# Options read in any cluster manager using HDFS\n",
      "# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files\n",
      "\n",
      "# Options read in YARN client/cluster mode\n",
      "# - YARN_CONF_DIR, to point Spark towards YARN configuration files when you use YARN\n",
      "\n",
      "# Options for the daemons used in the standalone deploy mode\n",
      "# - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname\n",
      "# - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master\n",
      "# - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. \"-Dx=y\")\n",
      "# - SPARK_WORKER_CORES, to set the number of cores to use on this machine\n",
      "# - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)\n",
      "# - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker\n",
      "# - SPARK_WORKER_DIR, to set the working directory of worker processes\n",
      "# - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. \"-Dx=y\")\n",
      "# - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default\n",
      " 1g).\n",
      "# - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. \"-Dx=y\")\n",
      "# - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. \"-Dx=y\")\n",
      "# - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. \"-Dx=y\")\n",
      "# - SPARK_DAEMON_CLASSPATH, to set the classpath for all daemons\n",
      "# - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers\n",
      "\n",
      "# Options for launcher\n",
      "# - SPARK_LAUNCHER_OPTS, to set config properties and Java options for the launcher (e.g. \"-Dx=y\")\n",
      "\n",
      "# Generic options for the daemons used in the standalone deploy mode\n",
      "# - SPARK_CONF_DIR      Alternate conf dir. (Default\n",
      " ${SPARK_HOME}/conf)\n",
      "# - SPARK_LOG_DIR       Where log files are stored.  (Default\n",
      " ${SPARK_HOME}/logs)\n",
      "# - SPARK_LOG_MAX_FILES Max log files of Spark daemons can rotate to. Default is 5.\n",
      "# - SPARK_PID_DIR       Where the pid file is stored. (Default\n",
      " /tmp)\n",
      "# - SPARK_IDENT_STRING  A string representing this instance of spark. (Default\n",
      " $USER)\n",
      "# - SPARK_NICENESS      The scheduling priority for daemons. (Default\n",
      " 0)\n",
      "# - SPARK_NO_DAEMONIZE  Run the proposed command in the foreground. It will not output a PID file.\n",
      "# Options for native BLAS, like Intel MKL, OpenBLAS, and so on.\n",
      "# You might get better performance to enable these options if using native BLAS (see SPARK-21305).\n",
      "# - MKL_NUM_THREADS=1        Disable multi-threading of Intel MKL\n",
      "# - OPENBLAS_NUM_THREADS=1   Disable multi-threading of OpenBLAS\n",
      "\n",
      "# Options for beeline\n",
      "# - SPARK_BEELINE_OPTS, to set config properties only for the beeline cli (e.g. \"-Dx=y\")\n",
      "# - SPARK_BEELINE_MEMORY, Memory for beeline (e.g. 1000M, 2G) (Default\n",
      " 1G)\n",
      "$ process finished with return code 0\n"
     ]
    }
   ],
   "source": [
    "%%remote_command\n",
    "\n",
    "cat spark-home/conf/spark-env.sh | tr ':' '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf33f71-160e-497e-8035-036d80490e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%remote_command\n",
    "\n",
    "hostname\n",
    "kdestroy -A\n",
    "klist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3d982-b4e1-4af1-9c07-814226460423",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Finally, try to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80e55113-898c-42c0-a7e1-60b18e23a994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/11 02:23:21 INFO SparkContext: Running Spark version 3.5.4\n",
      "25/01/11 02:23:21 INFO SparkContext: OS info Linux, 5.15.0-130-generic, amd64\n",
      "25/01/11 02:23:21 INFO SparkContext: Java version 1.8.0_422\n",
      "25/01/11 02:23:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/11 02:23:21 INFO ResourceUtils: ==============================================================\n",
      "25/01/11 02:23:21 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/01/11 02:23:21 INFO ResourceUtils: ==============================================================\n",
      "25/01/11 02:23:21 INFO SparkContext: Submitted application: Spark Pi\n",
      "25/01/11 02:23:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/01/11 02:23:21 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/01/11 02:23:21 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/01/11 02:23:21 INFO SecurityManager: Changing view acls to: app\n",
      "25/01/11 02:23:21 INFO SecurityManager: Changing modify acls to: app\n",
      "25/01/11 02:23:21 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/11 02:23:21 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/11 02:23:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: app; groups with view permissions: EMPTY; users with modify permissions: app; groups with modify permissions: EMPTY\n",
      "25/01/11 02:23:22 INFO Utils: Successfully started service 'sparkDriver' on port 34633.\n",
      "25/01/11 02:23:22 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/01/11 02:23:22 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/01/11 02:23:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/01/11 02:23:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/01/11 02:23:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/01/11 02:23:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-61ce4b2b-1ebe-4d8b-b5c1-05c0529e0d8a\n",
      "25/01/11 02:23:22 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB\n",
      "25/01/11 02:23:22 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/01/11 02:23:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/01/11 02:23:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/01/11 02:23:23 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "25/01/11 02:23:23 INFO SparkContext: Added JAR file:/tmp/spark-home/examples/jars/spark-examples_2.12-3.5.4.jar at spark://livy:34633/jars/spark-examples_2.12-3.5.4.jar with timestamp 1736562201538\n",
      "25/01/11 02:23:23 INFO Executor: Starting executor ID driver on host livy\n",
      "25/01/11 02:23:23 INFO Executor: OS info Linux, 5.15.0-130-generic, amd64\n",
      "25/01/11 02:23:23 INFO Executor: Java version 1.8.0_422\n",
      "25/01/11 02:23:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/01/11 02:23:23 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@66bacdbc for default.\n",
      "25/01/11 02:23:23 INFO Executor: Fetching spark://livy:34633/jars/spark-examples_2.12-3.5.4.jar with timestamp 1736562201538\n",
      "25/01/11 02:23:23 INFO TransportClientFactory: Successfully created connection to livy/172.21.0.3:34633 after 21 ms (0 ms spent in bootstraps)\n",
      "25/01/11 02:23:23 INFO Utils: Fetching spark://livy:34633/jars/spark-examples_2.12-3.5.4.jar to /tmp/spark-45981aa3-9db5-4289-a782-24b8f64b93d8/userFiles-b96f7636-af30-498a-998c-78513e39c1a1/fetchFileTemp1399223280950450654.tmp\n",
      "25/01/11 02:23:23 INFO Executor: Adding file:/tmp/spark-45981aa3-9db5-4289-a782-24b8f64b93d8/userFiles-b96f7636-af30-498a-998c-78513e39c1a1/spark-examples_2.12-3.5.4.jar to class loader default\n",
      "25/01/11 02:23:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39053.\n",
      "25/01/11 02:23:23 INFO NettyBlockTransferService: Server created on livy:39053\n",
      "25/01/11 02:23:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/01/11 02:23:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, livy, 39053, None)\n",
      "25/01/11 02:23:23 INFO BlockManagerMasterEndpoint: Registering block manager livy:39053 with 413.9 MiB RAM, BlockManagerId(driver, livy, 39053, None)\n",
      "25/01/11 02:23:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, livy, 39053, None)\n",
      "25/01/11 02:23:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, livy, 39053, None)\n",
      "25/01/11 02:23:24 INFO SparkContext: Starting job: reduce at SparkPi.scala:38\n",
      "25/01/11 02:23:24 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 10 output partitions\n",
      "25/01/11 02:23:24 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n",
      "25/01/11 02:23:24 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/01/11 02:23:24 INFO DAGScheduler: Missing parents: List()\n",
      "25/01/11 02:23:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n",
      "25/01/11 02:23:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KiB, free 413.9 MiB)\n",
      "25/01/11 02:23:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 413.9 MiB)\n",
      "25/01/11 02:23:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on livy:39053 (size: 2.3 KiB, free: 413.9 MiB)\n",
      "25/01/11 02:23:24 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "25/01/11 02:23:24 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "25/01/11 02:23:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks resource profile 0\n",
      "25/01/11 02:23:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (livy, executor driver, partition 0, PROCESS_LOCAL, 9304 bytes) \n",
      "25/01/11 02:23:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/01/11 02:23:25 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1055 bytes result sent to driver\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (livy, executor driver, partition 1, PROCESS_LOCAL, 9304 bytes) \n",
      "25/01/11 02:23:25 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 496 ms on livy (executor driver) (1/10)\n",
      "25/01/11 02:23:25 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1055 bytes result sent to driver\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (livy, executor driver, partition 2, PROCESS_LOCAL, 9304 bytes) \n",
      "25/01/11 02:23:25 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 86 ms on livy (executor driver) (2/10)\n",
      "25/01/11 02:23:25 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 969 bytes result sent to driver\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (livy, executor driver, partition 3, PROCESS_LOCAL, 9304 bytes) \n",
      "25/01/11 02:23:25 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 91 ms on livy (executor driver) (3/10)\n",
      "25/01/11 02:23:25 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "25/01/11 02:23:25 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1012 bytes result sent to driver\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (livy, executor driver, partition 4, PROCESS_LOCAL, 9304 bytes) \n",
      "25/01/11 02:23:25 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 15 ms on livy (executor driver) (4/10)\n",
      "25/01/11 02:23:25 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
      "25/01/11 02:23:25 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 969 bytes result sent to driver\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (livy, executor driver, partition 5, PROCESS_LOCAL, 9304 bytes) \n",
      "25/01/11 02:23:25 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 80 ms on livy (executor driver) (5/10)\n",
      "25/01/11 02:23:25 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
      "25/01/11 02:23:25 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1012 bytes result sent to driver\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (livy, executor driver, partition 6, PROCESS_LOCAL, 9304 bytes) \n",
      "25/01/11 02:23:25 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 20 ms on livy (executor driver) (6/10)\n",
      "25/01/11 02:23:25 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 969 bytes result sent to driver\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (livy, executor driver, partition 7, PROCESS_LOCAL, 9304 bytes) \n",
      "25/01/11 02:23:25 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 77 ms on livy (executor driver) (7/10)\n",
      "25/01/11 02:23:25 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1012 bytes result sent to driver\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (livy, executor driver, partition 8, PROCESS_LOCAL, 9304 bytes) \n",
      "25/01/11 02:23:25 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 19 ms on livy (executor driver) (8/10)\n",
      "25/01/11 02:23:25 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)\n",
      "25/01/11 02:23:25 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 969 bytes result sent to driver\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (livy, executor driver, partition 9, PROCESS_LOCAL, 9304 bytes) \n",
      "25/01/11 02:23:25 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 80 ms on livy (executor driver) (9/10)\n",
      "25/01/11 02:23:25 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1012 bytes result sent to driver\n",
      "25/01/11 02:23:25 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 87 ms on livy (executor driver) (10/10)\n",
      "25/01/11 02:23:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/01/11 02:23:25 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 1.277 s\n",
      "25/01/11 02:23:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/01/11 02:23:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/01/11 02:23:25 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 1.385903 s\n",
      "Pi is roughly 3.141791141791142\n",
      "25/01/11 02:23:25 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "25/01/11 02:23:25 INFO SparkUI: Stopped Spark web UI at http://livy:4041\n",
      "25/01/11 02:23:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "25/01/11 02:23:25 INFO MemoryStore: MemoryStore cleared\n",
      "25/01/11 02:23:25 INFO BlockManager: BlockManager stopped\n",
      "25/01/11 02:23:25 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "25/01/11 02:23:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "25/01/11 02:23:25 INFO SparkContext: Successfully stopped SparkContext\n",
      "25/01/11 02:23:25 INFO ShutdownHookManager: Shutdown hook called\n",
      "25/01/11 02:23:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-45981aa3-9db5-4289-a782-24b8f64b93d8\n",
      "25/01/11 02:23:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-34efe69a-b702-4096-8a87-870f77f719e0\n",
      "$ process finished with return code 0\n"
     ]
    }
   ],
   "source": [
    "%%remote_command\n",
    "\n",
    "./spark-home/bin/spark-submit --name test-spark-pi --master 'local[*]' --deploy-mode client \\\n",
    "--class org.apache.spark.examples.SparkPi ./spark-home/examples/jars/spark-examples_2.12-3.5.4.jar 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9a18f-bccd-4b2c-b40d-6836b2d804a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
