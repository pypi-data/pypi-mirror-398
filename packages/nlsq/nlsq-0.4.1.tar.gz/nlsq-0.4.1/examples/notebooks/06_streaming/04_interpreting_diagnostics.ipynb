{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af1b18d",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/imewei/NLSQ/blob/main/examples/notebooks/06_streaming/04_interpreting_diagnostics.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb37a3",
   "metadata": {
    "id": "colab-install"
   },
   "outputs": [],
   "source": [
    "# @title Install NLSQ (run once in Colab)\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab - installing NLSQ...\")\n",
    "    !pip install -q nlsq\n",
    "    print(\"âœ… NLSQ installed successfully!\")\n",
    "else:\n",
    "    print(\"Not running in Colab - assuming NLSQ is already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b41140a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:00:40.159327Z",
     "iopub.status.busy": "2025-12-18T21:00:40.159131Z",
     "iopub.status.idle": "2025-12-18T21:00:40.448790Z",
     "shell.execute_reply": "2025-12-18T21:00:40.448242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579ea5ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:00:40.450538Z",
     "iopub.status.busy": "2025-12-18T21:00:40.450271Z",
     "iopub.status.idle": "2025-12-18T21:00:41.493982Z",
     "shell.execute_reply": "2025-12-18T21:00:41.493377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Streaming Optimizer: Interpreting Diagnostics Example\n",
      "======================================================================\n",
      "\n",
      "Dataset: 5000 samples\n",
      "True parameters: a=1.0, b=2.0, c=-0.5\n",
      "\n",
      "Running optimization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization complete!\n",
      "\n",
      "DIAGNOSTICS STRUCTURE\n",
      "======================================================================\n",
      "\n",
      "Available diagnostic fields:\n",
      "  - aggregate_stats                : dict\n",
      "  - batch_padding                  : dict\n",
      "  - batch_success_rate             : float\n",
      "  - checkpoint_info                : dict\n",
      "  - convergence_achieved           : bool\n",
      "  - elapsed_time                   : float\n",
      "  - error_types                    : dict\n",
      "  - failed_batches                 : list\n",
      "  - final_epoch                    : int\n",
      "  - recent_batch_stats             : list\n",
      "  - retry_counts                   : dict\n",
      "  - total_batches_attempted        : int\n",
      "  - total_retries                  : int\n",
      "\n",
      "SUCCESS METRICS\n",
      "======================================================================\n",
      "Batch success rate: 100.0%\n",
      "Total batches attempted: 250\n",
      "Failed batches: 0\n",
      "Total retries: 0\n",
      "Convergence achieved: False\n",
      "Final epoch: 4\n",
      "Elapsed time: 0.27s\n",
      "\n",
      "FAILURE ANALYSIS\n",
      "======================================================================\n",
      "No failed batches!\n",
      "\n",
      "AGGREGATE STATISTICS\n",
      "======================================================================\n",
      "Mean loss:          1.402242e+01\n",
      "Std loss:           2.656516e+01\n",
      "Mean gradient norm: 67.390432\n",
      "Std gradient norm:  119.519512\n",
      "Mean batch time:    0.57ms\n",
      "Std batch time:     0.27ms\n",
      "\n",
      "Interpretation:\n",
      "  - Coefficient of variation (loss): 189.45%\n",
      "    => High variability in loss\n",
      "\n",
      "RECENT BATCH STATISTICS (last 10 batches)\n",
      "======================================================================\n",
      "Showing 10 most recent batches:\n",
      "\n",
      "   Batch     Status         Loss   GradNorm     Time  Retries\n",
      "----------------------------------------------------------------------\n",
      "      40    SUCCESS   5.3999e-01    14.3886     0.8ms        0\n",
      "      41    SUCCESS   3.5484e-01    12.6708     0.7ms        0\n",
      "      42    SUCCESS   1.7551e-01     9.1965     0.4ms        0\n",
      "      43    SUCCESS   6.3852e-02     4.5913     0.6ms        0\n",
      "      44    SUCCESS   4.4350e-02     1.7245     0.5ms        0\n",
      "      45    SUCCESS   1.1181e-01     9.7515     0.5ms        0\n",
      "      46    SUCCESS   3.6009e-01    21.3581     0.5ms        0\n",
      "      47    SUCCESS   7.7338e-01    35.3558     0.5ms        0\n",
      "      48    SUCCESS   1.3590e+00    51.8964     0.4ms        0\n",
      "      49    SUCCESS   2.0940e+00    70.2738     0.5ms        0\n",
      "\n",
      "Recent batch statistics:\n",
      "  Success rate: 100.0%\n",
      "  Mean loss: 5.876851e-01\n",
      "  Min loss: 4.434964e-02\n",
      "  Max loss: 2.094022e+00\n",
      "\n",
      "CHECKPOINT INFORMATION\n",
      "======================================================================\n",
      "Latest checkpoint:\n",
      "  Path: checkpoints_diagnostics/checkpoint_iter_250.h5\n",
      "  Saved at: 2025-12-18T15:00:41.487220\n",
      "  Batch index: 49\n",
      "\n",
      "Resume using:\n",
      "  config = StreamingConfig(resume_from_checkpoint='checkpoints_diagnostics/checkpoint_iter_250.h5')\n",
      "\n",
      "EXPORT DIAGNOSTICS\n",
      "======================================================================\n",
      "Diagnostics exported to: streaming_diagnostics_example.json\n",
      "File size: 15953 bytes\n",
      "\n",
      "FINAL RESULTS\n",
      "======================================================================\n",
      "Best parameters:\n",
      "  a = 0.506970 (true: 1.0)\n",
      "  b = 1.037821 (true: 2.0)\n",
      "  c = -0.208632 (true: -0.5)\n",
      "  Best loss = 4.139387e-02\n",
      "\n",
      "======================================================================\n",
      "Example complete!\n",
      "\n",
      "Key takeaways:\n",
      "  - streaming_diagnostics contains comprehensive information\n",
      "  - Aggregate statistics summarize overall performance\n",
      "  - Recent batch statistics show optimization trajectory\n",
      "  - Checkpoint information enables recovery\n",
      "  - Error analysis helps diagnose issues\n",
      "  - Diagnostics can be exported to JSON for further analysis\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from nlsq import StreamingConfig, StreamingOptimizer\n",
    "\n",
    "\n",
    "def polynomial_model(x, a, b, c):\n",
    "    \"\"\"Polynomial model: y = a + b*x + c*x^2\"\"\"\n",
    "    return a + b * x + c * x**2\n",
    "\n",
    "\n",
    "def print_diagnostics_structure(diagnostics):\n",
    "    \"\"\"Print the structure and contents of diagnostics dictionary\"\"\"\n",
    "    print(\"DIAGNOSTICS STRUCTURE\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"Available diagnostic fields:\")\n",
    "    for key in sorted(diagnostics.keys()):\n",
    "        value_type = type(diagnostics[key]).__name__\n",
    "        print(f\"  - {key:30s} : {value_type}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def analyze_success_metrics(diagnostics):\n",
    "    \"\"\"Analyze overall success metrics\"\"\"\n",
    "    print(\"SUCCESS METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Batch success rate: {diagnostics['batch_success_rate']:.1%}\")\n",
    "    print(f\"Total batches attempted: {diagnostics['total_batches_attempted']}\")\n",
    "    print(f\"Failed batches: {len(diagnostics['failed_batches'])}\")\n",
    "    print(f\"Total retries: {diagnostics['total_retries']}\")\n",
    "    print(f\"Convergence achieved: {diagnostics['convergence_achieved']}\")\n",
    "    print(f\"Final epoch: {diagnostics['final_epoch']}\")\n",
    "    print(f\"Elapsed time: {diagnostics['elapsed_time']:.2f}s\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def analyze_failure_patterns(diagnostics):\n",
    "    \"\"\"Analyze failure patterns and error types\"\"\"\n",
    "    print(\"FAILURE ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if not diagnostics[\"failed_batches\"]:\n",
    "        print(\"No failed batches!\")\n",
    "        print()\n",
    "        return\n",
    "\n",
    "    print(f\"Failed batch indices: {diagnostics['failed_batches']}\")\n",
    "    print()\n",
    "\n",
    "    print(\"Error Type Distribution:\")\n",
    "    error_types = diagnostics[\"error_types\"]\n",
    "    total_errors = sum(error_types.values())\n",
    "    for error_type, count in sorted(\n",
    "        error_types.items(), key=lambda x: x[1], reverse=True\n",
    "    ):\n",
    "        pct = count / total_errors * 100\n",
    "        print(f\"  {error_type:20s}: {count:3d} ({pct:5.1f}%)\")\n",
    "    print()\n",
    "\n",
    "    print(\"Retry Patterns:\")\n",
    "    retry_counts = diagnostics[\"retry_counts\"]\n",
    "    if retry_counts:\n",
    "        retry_values = list(retry_counts.values())\n",
    "        print(f\"  Batches with retries: {len(retry_values)}\")\n",
    "        print(f\"  Min retries: {min(retry_values)}\")\n",
    "        print(f\"  Max retries: {max(retry_values)}\")\n",
    "        print(f\"  Avg retries: {np.mean(retry_values):.2f}\")\n",
    "        print(f\"  Total retries: {sum(retry_values)}\")\n",
    "    else:\n",
    "        print(\"  No retries performed\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def analyze_aggregate_statistics(diagnostics):\n",
    "    \"\"\"Analyze aggregate statistics from batch buffer\"\"\"\n",
    "    print(\"AGGREGATE STATISTICS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    agg = diagnostics[\"aggregate_stats\"]\n",
    "    print(f\"Mean loss:          {agg['mean_loss']:.6e}\")\n",
    "    print(f\"Std loss:           {agg['std_loss']:.6e}\")\n",
    "    print(f\"Mean gradient norm: {agg['mean_grad_norm']:.6f}\")\n",
    "    print(f\"Std gradient norm:  {agg['std_grad_norm']:.6f}\")\n",
    "    print(f\"Mean batch time:    {agg['mean_batch_time'] * 1000:.2f}ms\")\n",
    "    print(f\"Std batch time:     {agg['std_batch_time'] * 1000:.2f}ms\")\n",
    "    print()\n",
    "\n",
    "    print(\"Interpretation:\")\n",
    "    cv_loss = agg[\"std_loss\"] / max(agg[\"mean_loss\"], 1e-10)\n",
    "    print(f\"  - Coefficient of variation (loss): {cv_loss:.2%}\")\n",
    "    if cv_loss < 0.1:\n",
    "        print(\"    => Very stable optimization\")\n",
    "    elif cv_loss < 0.5:\n",
    "        print(\"    => Moderately stable optimization\")\n",
    "    else:\n",
    "        print(\"    => High variability in loss\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def analyze_recent_batches(diagnostics, n_recent=10):\n",
    "    \"\"\"Analyze recent batch statistics\"\"\"\n",
    "    print(f\"RECENT BATCH STATISTICS (last {n_recent} batches)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    recent_stats = diagnostics[\"recent_batch_stats\"]\n",
    "    if not recent_stats:\n",
    "        print(\"No batch statistics available\")\n",
    "        print()\n",
    "        return\n",
    "\n",
    "    last_n = recent_stats[-n_recent:]\n",
    "    print(f\"Showing {len(last_n)} most recent batches:\")\n",
    "    print()\n",
    "    print(\n",
    "        f\"{'Batch':>8s} {'Status':>10s} {'Loss':>12s} {'GradNorm':>10s} {'Time':>8s} {'Retries':>8s}\"\n",
    "    )\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for stats in last_n:\n",
    "        batch_idx = stats[\"batch_idx\"]\n",
    "        status = \"SUCCESS\" if stats[\"success\"] else \"FAILED\"\n",
    "        loss = stats[\"loss\"]\n",
    "        grad_norm = stats[\"grad_norm\"]\n",
    "        batch_time = stats[\"batch_time\"] * 1000  # Convert to ms\n",
    "        retry_count = stats[\"retry_count\"]\n",
    "\n",
    "        loss_str = f\"{loss:.4e}\" if np.isfinite(loss) else \"inf\"\n",
    "        print(\n",
    "            f\"{batch_idx:8d} {status:>10s} {loss_str:>12s} {grad_norm:10.4f} {batch_time:7.1f}ms {retry_count:8d}\"\n",
    "        )\n",
    "\n",
    "    print()\n",
    "\n",
    "    successful_recent = [s for s in last_n if s[\"success\"]]\n",
    "    if successful_recent:\n",
    "        recent_losses = [s[\"loss\"] for s in successful_recent]\n",
    "        print(\"Recent batch statistics:\")\n",
    "        print(f\"  Success rate: {len(successful_recent) / len(last_n):.1%}\")\n",
    "        print(f\"  Mean loss: {np.mean(recent_losses):.6e}\")\n",
    "        print(f\"  Min loss: {min(recent_losses):.6e}\")\n",
    "        print(f\"  Max loss: {max(recent_losses):.6e}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def analyze_checkpoint_info(diagnostics):\n",
    "    \"\"\"Analyze checkpoint information\"\"\"\n",
    "    print(\"CHECKPOINT INFORMATION\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    cp_info = diagnostics.get(\"checkpoint_info\")\n",
    "    if not cp_info:\n",
    "        print(\"No checkpoint information available\")\n",
    "        print(\"(Checkpoints may be disabled or not saved yet)\")\n",
    "        print()\n",
    "        return\n",
    "\n",
    "    print(\"Latest checkpoint:\")\n",
    "    print(f\"  Path: {cp_info['path']}\")\n",
    "    print(f\"  Saved at: {cp_info['saved_at']}\")\n",
    "    print(f\"  Batch index: {cp_info['batch_idx']}\")\n",
    "    print()\n",
    "\n",
    "    print(\"Resume using:\")\n",
    "    print(f\"  config = StreamingConfig(resume_from_checkpoint='{cp_info['path']}')\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def export_diagnostics_json(diagnostics, filename=\"diagnostics.json\"):\n",
    "    \"\"\"Export diagnostics to JSON for further analysis\"\"\"\n",
    "    print(\"EXPORT DIAGNOSTICS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    diagnostics_copy = diagnostics.copy()\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(diagnostics_copy, f, indent=2)\n",
    "\n",
    "    print(f\"Diagnostics exported to: {filename}\")\n",
    "    print(f\"File size: {len(json.dumps(diagnostics_copy))} bytes\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Streaming Optimizer: Interpreting Diagnostics Example\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "\n",
    "    np.random.seed(42)\n",
    "    n_samples = 5000\n",
    "    x_data = np.linspace(-5, 5, n_samples)\n",
    "    true_a, true_b, true_c = 1.0, 2.0, -0.5\n",
    "    y_true = polynomial_model(x_data, true_a, true_b, true_c)\n",
    "    y_data = y_true + 0.2 * np.random.randn(n_samples)\n",
    "\n",
    "    print(f\"Dataset: {n_samples} samples\")\n",
    "    print(f\"True parameters: a={true_a}, b={true_b}, c={true_c}\")\n",
    "    print()\n",
    "\n",
    "    config = StreamingConfig(\n",
    "        batch_size=100,\n",
    "        max_epochs=5,\n",
    "        learning_rate=0.001,\n",
    "        enable_fault_tolerance=True,\n",
    "        checkpoint_dir=\"checkpoints_diagnostics\",\n",
    "        checkpoint_frequency=10,\n",
    "        enable_checkpoints=True,\n",
    "        batch_stats_buffer_size=100,  # Track last 100 batches\n",
    "    )\n",
    "\n",
    "    optimizer = StreamingOptimizer(config)\n",
    "    p0 = np.array([0.5, 1.0, -0.2])\n",
    "\n",
    "    print(\"Running optimization...\")\n",
    "    result = optimizer.fit(\n",
    "        (x_data, y_data),\n",
    "        polynomial_model,\n",
    "        p0,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    print()\n",
    "    print(\"Optimization complete!\")\n",
    "    print()\n",
    "\n",
    "    diagnostics = result[\"streaming_diagnostics\"]\n",
    "\n",
    "    print_diagnostics_structure(diagnostics)\n",
    "    analyze_success_metrics(diagnostics)\n",
    "    analyze_failure_patterns(diagnostics)\n",
    "    analyze_aggregate_statistics(diagnostics)\n",
    "    analyze_recent_batches(diagnostics, n_recent=10)\n",
    "    analyze_checkpoint_info(diagnostics)\n",
    "    export_diagnostics_json(diagnostics, \"streaming_diagnostics_example.json\")\n",
    "\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    best_params = result[\"x\"]\n",
    "    print(\"Best parameters:\")\n",
    "    print(f\"  a = {best_params[0]:.6f} (true: {true_a})\")\n",
    "    print(f\"  b = {best_params[1]:.6f} (true: {true_b})\")\n",
    "    print(f\"  c = {best_params[2]:.6f} (true: {true_c})\")\n",
    "    print(f\"  Best loss = {result['best_loss']:.6e}\")\n",
    "    print()\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example complete!\")\n",
    "    print()\n",
    "    print(\"Key takeaways:\")\n",
    "    print(\"  - streaming_diagnostics contains comprehensive information\")\n",
    "    print(\"  - Aggregate statistics summarize overall performance\")\n",
    "    print(\"  - Recent batch statistics show optimization trajectory\")\n",
    "    print(\"  - Checkpoint information enables recovery\")\n",
    "    print(\"  - Error analysis helps diagnose issues\")\n",
    "    print(\"  - Diagnostics can be exported to JSON for further analysis\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
