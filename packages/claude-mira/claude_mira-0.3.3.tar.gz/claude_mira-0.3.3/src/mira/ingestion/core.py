"""
MIRA Ingestion Core Module

Handles the full pipeline of parsing, extracting, archiving, and indexing conversations.

Primary: Central Qdrant + Postgres storage (full semantic search, cross-machine sync)
Fallback: Local SQLite with FTS (keyword search only, single machine)
"""

import hashlib
import json
import os
import time
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

from mira.core import log, MIRA_PATH
from mira.core.parsing import parse_conversation
from mira.extraction.metadata import extract_metadata, build_document_content
from mira.extraction.artifacts import extract_file_operations_from_messages, extract_artifacts_from_messages
from mira.custodian import extract_custodian_learnings
from mira.extraction.insights import extract_insights_from_conversation
from mira.extraction.concepts import extract_concepts_from_conversation

# Module-level tracking of active ingestions for status reporting
_active_ingestions: Dict[str, Dict[str, Any]] = {}
_active_lock = threading.Lock()


def get_active_ingestions() -> list:
    """Return list of currently active ingestion jobs."""
    with _active_lock:
        return [
            {
                'session_id': sid,
                'file_path': info['file_path'],
                'project_path': info['project_path'],
                'started_at': info['started_at'],
                'worker': info['worker'],
                'elapsed_ms': int((time.time() - info['started_at']) * 1000)
            }
            for sid, info in _active_ingestions.items()
        ]


def _mark_ingestion_active(session_id: str, file_path: str, project_path: str, worker: str):
    """Mark a session as currently being ingested."""
    with _active_lock:
        _active_ingestions[session_id] = {
            'file_path': file_path,
            'project_path': project_path,
            'started_at': time.time(),
            'worker': worker
        }


def _mark_ingestion_done(session_id: str):
    """Mark a session as done ingesting."""
    with _active_lock:
        _active_ingestions.pop(session_id, None)


def ingest_conversation(
    file_info: dict,
    collection,
    mira_path: Path = None,
    storage=None
) -> bool:
    """
    Ingest a single conversation: parse, extract, archive, index.

    Supports incremental ingestion - only processes new messages since last run.

    Args:
        file_info: Dict with session_id, file_path, project_path, last_modified
        collection: Deprecated - kept for API compatibility, ignored
        mira_path: Path to .mira directory (optional, uses default if not provided)
        storage: Storage instance for central Qdrant + Postgres

    Returns True if successfully ingested, False if skipped or failed.
    """
    if mira_path is None:
        mira_path = MIRA_PATH

    # Get storage instance if not provided
    if storage is None:
        try:
            from mira.storage import get_storage
            storage = get_storage()
        except ImportError:
            log("ERROR: Storage not available")
            return False

    session_id = file_info['session_id']
    file_path = Path(file_info['file_path'])

    archives_path = mira_path / "archives"
    metadata_path = mira_path / "metadata"

    # Ensure directories exist
    archives_path.mkdir(parents=True, exist_ok=True)
    metadata_path.mkdir(parents=True, exist_ok=True)

    # Check if already ingested and track incremental state
    meta_file = metadata_path / f"{session_id}.json"
    existing_meta = None
    last_indexed_message_count = 0
    is_incremental = False

    if meta_file.exists():
        try:
            existing_meta = json.loads(meta_file.read_text())
            last_indexed_message_count = existing_meta.get('last_indexed_message_count', 0)

            if existing_meta.get('last_modified') == file_info.get('last_modified'):
                # File hasn't changed - but check if we need to sync to central
                if storage.using_central:
                    if not storage.session_exists_in_central(session_id):
                        log(f"[{session_id[:12]}] Local metadata exists but not in central, will sync")
                        # Continue with full processing for central sync
                    else:
                        # Already in central and unchanged - skip silently
                        return False
                else:
                    return False  # Local only mode, already processed
            else:
                # File changed - check if we can do incremental
                is_incremental = last_indexed_message_count > 0
        except (json.JSONDecodeError, IOError, OSError):
            pass

    short_id = session_id[:12]
    t_start = time.time()
    log(f"[{short_id}] Starting ingestion{'(incremental)' if is_incremental else ''}...")

    # Parse conversation
    t0 = time.time()
    conversation = parse_conversation(file_path)
    messages = conversation.get('messages', [])
    msg_count = len(messages)
    if not msg_count:
        log(f"[{short_id}] Skipped: no messages")
        return False
    t_parse = (time.time() - t0) * 1000

    # Determine new messages to process
    new_message_start = 0
    if is_incremental and last_indexed_message_count < msg_count:
        new_message_start = last_indexed_message_count
        new_msg_count = msg_count - last_indexed_message_count
        log(f"[{short_id}] Parsed {msg_count} messages, {new_msg_count} NEW (from idx {new_message_start}) ({t_parse:.0f}ms)")
    else:
        log(f"[{short_id}] Parsed {msg_count} messages ({t_parse:.0f}ms)")

    # Extract metadata (always from full conversation for accurate summary)
    t0 = time.time()
    metadata = extract_metadata(conversation, file_info)
    kw_count = len(metadata.get('keywords', []))
    facts_count = len(metadata.get('key_facts', []))
    t_meta = (time.time() - t0) * 1000
    log(f"[{short_id}] Metadata: {kw_count} keywords, {facts_count} facts ({t_meta:.0f}ms)")

    # Read file content for remote archiving
    try:
        file_content = file_path.read_text(encoding='utf-8')
        content_hash = hashlib.sha256(file_content.encode('utf-8')).hexdigest()
    except Exception as e:
        log(f"[{short_id}] Failed to read file: {e}")
        return False

    # NOTE: metadata is saved AFTER successful indexing to prevent data loss on crash
    # Add incremental tracking to metadata (will be saved at the end)
    metadata['last_indexed_message_count'] = msg_count

    # Read raw messages for extraction
    raw_messages = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    raw_messages.append(json.loads(line))
                except json.JSONDecodeError:
                    pass

    # For incremental, only process new messages
    raw_messages_to_process = raw_messages[new_message_start:] if is_incremental else raw_messages

    # Build document content for vector search
    doc_content = build_document_content(conversation, metadata)
    log(f"[{short_id}] Built doc ({len(doc_content)} chars)")

    # Determine project path for central storage
    project_path_encoded = file_info.get('project_path', '')
    project_path_normalized = ''
    if project_path_encoded:
        # Convert from "-workspaces-MIRA3" to "/workspaces/MIRA3"
        project_path_normalized = project_path_encoded.replace('-', '/')
        # Ensure exactly one leading slash
        project_path_normalized = '/' + project_path_normalized.lstrip('/')

    # Get git remote for cross-machine project identification
    from mira.core.utils import get_git_remote_for_claude_path
    git_remote = get_git_remote_for_claude_path(project_path_encoded)
    if git_remote:
        log(f"[{short_id}] Git remote: {git_remote}")

    # Index to storage (central preferred, local fallback)
    try:
        # Upsert session
        t0 = time.time()
        db_session_id = storage.upsert_session(
            project_path=project_path_normalized,
            session_id=session_id,
            git_remote=git_remote,
            summary=metadata.get('summary', ''),
            keywords=metadata.get('keywords', []),
            facts=metadata.get('key_facts', []),
            task_description=metadata.get('task_description', ''),
            git_branch=metadata.get('git_branch'),
            models_used=metadata.get('models_used', []),
            tools_used=metadata.get('tools_used', []),
            files_touched=metadata.get('files_touched', []),
            message_count=metadata.get('message_count', 0),
            started_at=metadata.get('started_at'),
            ended_at=metadata.get('last_modified'),
        )
        t_session = (time.time() - t0) * 1000

        if db_session_id is None:
            log(f"[{short_id}] ERROR: Failed to create session")
            return False

        storage_mode = "central" if storage.using_central else "local"
        log(f"[{short_id}] Session upserted ({storage_mode}, id={db_session_id}) ({t_session:.0f}ms)")

        # Populate vocabulary for fuzzy search typo correction
        try:
            from mira.search.fuzzy import add_terms_to_vocabulary, extract_terms_from_text

            keywords = metadata.get('keywords', [])
            if keywords:
                add_terms_to_vocabulary(keywords, source="keyword")

            summary = metadata.get('summary', '')
            if summary:
                summary_terms = extract_terms_from_text(summary)
                if summary_terms:
                    add_terms_to_vocabulary(summary_terms, source="summary")

            task_desc = metadata.get('task_description', '')
            if task_desc:
                task_terms = extract_terms_from_text(task_desc)
                if task_terms:
                    add_terms_to_vocabulary(task_terms, source="task")

        except Exception as e:
            log(f"[{short_id}] Vocabulary population failed: {e}")

        # Archive conversation
        try:
            t0 = time.time()
            archive_id = storage.upsert_archive(
                postgres_session_id=db_session_id,
                content=file_content,
                content_hash=content_hash,
                session_id=session_id,
            )
            t_archive = (time.time() - t0) * 1000
            log(f"[{short_id}] Archived (archive_id={archive_id}) ({t_archive:.0f}ms)")
        except Exception as e:
            log(f"[{short_id}] Archive failed: {e}")

        # Extract artifacts
        try:
            t0 = time.time()
            artifact_count = extract_artifacts_from_messages(
                raw_messages_to_process, session_id,
                postgres_session_id=db_session_id,
                storage=storage,
                message_start_index=new_message_start,
            )
            t_artifacts = (time.time() - t0) * 1000
            if artifact_count > 0:
                incr_note = f" (incremental from msg {new_message_start})" if is_incremental else ""
                log(f"[{short_id}] Artifacts: {artifact_count} ({t_artifacts:.0f}ms){incr_note}")
        except Exception as e:
            log(f"[{short_id}] Artifacts failed: {e}")

        # Extract file operations
        try:
            t0 = time.time()
            file_ops_count = extract_file_operations_from_messages(
                raw_messages_to_process,
                session_id,
                postgres_session_id=db_session_id,
                storage=storage,
            )
            t_file_ops = (time.time() - t0) * 1000
            if file_ops_count > 0:
                incr_note = f" (incremental from msg {new_message_start})" if is_incremental else ""
                log(f"[{short_id}] File ops: {file_ops_count} ({t_file_ops:.0f}ms){incr_note}")
        except Exception as e:
            log(f"[{short_id}] File ops failed: {e}")

        # Extract code history
        try:
            from mira.extraction.code_history import extract_and_store_from_session
            t0 = time.time()
            code_history_ops = extract_and_store_from_session(
                session_id,
                file_path,
                project_path_encoded
            )
            t_code_history = (time.time() - t0) * 1000
            if code_history_ops > 0:
                log(f"[{short_id}] Code history: {code_history_ops} ops ({t_code_history:.0f}ms)")
        except Exception as e:
            log(f"[{short_id}] Code history failed: {e}")

        # Create incremental conversation for custodian/insights/concepts
        if is_incremental and new_message_start > 0:
            conversation_to_process = {
                **conversation,
                'messages': messages[new_message_start:]
            }
            incr_msg_count = len(messages) - new_message_start
        else:
            conversation_to_process = conversation
            incr_msg_count = len(messages)

        # Learn about the custodian
        try:
            t0 = time.time()
            custodian_result = extract_custodian_learnings(conversation_to_process, session_id)
            learned = custodian_result.get('learned', 0) if isinstance(custodian_result, dict) else 0
            t_custodian = (time.time() - t0) * 1000
            if learned > 0:
                incr_note = f" (from {incr_msg_count} new msgs)" if is_incremental else ""
                log(f"[{short_id}] Custodian: {learned} learnings ({t_custodian:.0f}ms){incr_note}")
        except Exception as e:
            log(f"[{short_id}] Custodian failed: {e}")

        # Extract insights
        try:
            t0 = time.time()
            insights = extract_insights_from_conversation(
                conversation_to_process, session_id,
                project_path=project_path_normalized,
                postgres_session_id=db_session_id,
                storage=storage
            )
            err_count = insights.get('errors_found', 0)
            dec_count = insights.get('decisions_found', 0)
            t_insights = (time.time() - t0) * 1000
            if err_count > 0 or dec_count > 0:
                incr_note = f" (from {incr_msg_count} new msgs)" if is_incremental else ""
                log(f"[{short_id}] Insights: {err_count} errors, {dec_count} decisions ({t_insights:.0f}ms){incr_note}")
        except Exception as e:
            log(f"[{short_id}] Insights failed: {e}")

        # Extract codebase concepts
        try:
            t0 = time.time()
            concepts = extract_concepts_from_conversation(
                conversation_to_process, session_id,
                project_path=project_path_normalized,
                storage=storage
            )
            concept_count = concepts.get('concepts_found', 0)
            t_concepts = (time.time() - t0) * 1000
            if concept_count > 0:
                incr_note = f" (from {incr_msg_count} new msgs)" if is_incremental else ""
                log(f"[{short_id}] Concepts: {concept_count} ({t_concepts:.0f}ms){incr_note}")
        except Exception as e:
            log(f"[{short_id}] Concepts failed: {e}")

        t_total = (time.time() - t_start) * 1000
        log(f"[{short_id}] Ingestion complete ({storage_mode} mode) - TOTAL: {t_total:.0f}ms")

        # Save metadata AFTER successful indexing
        meta_file.write_text(json.dumps(metadata, indent=2))

        # Optional LLM extraction
        if os.environ.get("MIRA_LLM_EXTRACTION", "").lower() in ("true", "1", "yes"):
            try:
                from mira.extraction.llm_extractor import trigger_llm_extraction
                trigger_llm_extraction(session_id)
            except Exception as e:
                log(f"[{short_id}] LLM extraction trigger failed (non-fatal): {e}")

        # Queue for local semantic indexing
        try:
            from mira.search.local_semantic import queue_session_for_indexing
            summary = metadata.get('summary', '')
            queue_session_for_indexing(session_id, file_content, summary)
        except Exception:
            pass

        return True
    except Exception as e:
        log(f"[{short_id}] Ingestion failed: {e}")
        return False


def sync_active_session(
    file_path: str,
    session_id: str,
    project_path: str,
    mira_path: Path = None,
    storage=None
) -> bool:
    """
    Sync the active session to remote storage.

    Lightweight sync that updates archive content without full re-extraction.
    """
    if mira_path is None:
        mira_path = MIRA_PATH

    if storage is None:
        try:
            from mira.storage import get_storage
            storage = get_storage()
        except ImportError:
            log("[active-sync] Storage not available")
            return False

    if not storage.using_central:
        return False

    short_id = f"{session_id[:12]}"
    file_path = Path(file_path)

    try:
        parsed = parse_conversation(file_path)
        messages = parsed.get('messages', [])

        if not messages:
            return False

        file_stat = file_path.stat()
        file_size = file_stat.st_size
        archive_content = file_path.read_text()
        line_count = archive_content.count('\n')

        if not storage.session_exists_in_central(session_id):
            log(f"[{short_id}] Session not in central, doing full ingest")
            file_info = {
                'session_id': session_id,
                'file_path': str(file_path),
                'project_path': project_path,
                'last_modified': datetime.fromtimestamp(file_stat.st_mtime).isoformat()
            }
            return ingest_conversation(file_info, None, mira_path, storage)

        try:
            storage.update_archive(
                session_id=session_id,
                content=archive_content,
                size_bytes=file_size,
                line_count=line_count
            )
            log(f"[{short_id}] Archive updated ({file_size} bytes)")
        except Exception as e:
            log(f"[{short_id}] Archive update failed: {e}")

        file_info = {'project_path': project_path}
        raw_meta = extract_metadata(parsed, file_info)

        try:
            storage.update_session_metadata(
                session_id=session_id,
                summary=raw_meta.get('summary', ''),
                keywords=raw_meta.get('keywords', []),
            )
        except Exception as e:
            log(f"[{short_id}] Metadata update failed: {e}")

        return True

    except Exception as e:
        log(f"[{short_id}] Active sync failed: {e}")
        return False


def discover_conversations(claude_path: Path = None) -> list:
    """
    Discover all conversation files from Claude Code projects.

    Returns list of file_info dicts with:
    - session_id: Unique identifier
    - file_path: Full path to JSONL file
    - project_path: Project directory
    - last_modified: ISO timestamp
    """
    from mira.core.utils import get_claude_projects_path, get_project_filter

    if claude_path is None:
        claude_path = get_claude_projects_path()

    if not claude_path.exists():
        return []

    project_filter = get_project_filter()
    if project_filter:
        filtered_path = claude_path / project_filter
        if filtered_path.exists():
            claude_path = filtered_path
            log(f"Filtering discovery to project: {project_filter}")
        else:
            log(f"Project filter path not found: {filtered_path}, scanning all projects")

    conversations = []

    for jsonl_file in claude_path.rglob("*.jsonl"):
        session_id = jsonl_file.stem
        is_agent_file = jsonl_file.name.startswith("agent-")
        project_dir = jsonl_file.parent.name

        try:
            mtime = jsonl_file.stat().st_mtime
            last_modified = datetime.fromtimestamp(mtime).isoformat()
        except (OSError, ValueError):
            last_modified = ""

        conversations.append({
            'session_id': session_id,
            'file_path': str(jsonl_file),
            'project_path': project_dir,
            'last_modified': last_modified,
            'is_agent': is_agent_file,
        })

    return conversations
