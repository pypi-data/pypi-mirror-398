import requests
import os
import torch
import torch.nn as nn
import zipfile
import colorama
from colorama import Fore, Style

# Enable colors
colorama.init(autoreset=True)

API_TOKEN = None
BASE_URL = "https://oneurai.com/api"

# âœ… WAF Bypass: User-Agent Header
COMMON_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "application/json"
}

# =====================================================
# 1. Authentication
# =====================================================
def login(token):
    global API_TOKEN
    API_TOKEN = token
    COMMON_HEADERS["Authorization"] = f"Bearer {API_TOKEN}"
    
    print(f"""{Fore.CYAN}{Style.BRIGHT}
    ____                             _ 
   / __ \                           (_)
  | |  | |_ __   ___ _   _ _ __ __ _ _   
  | |  | | '_ \ / _ \ | | | '__/ _` | |  
  | |__| | | | |  __/ |_| | | | (_| | |  
   \____/|_| |_|\___|\__,_|_|  \__,_|_|  
      {Fore.GREEN}>> AI & MLOps Library <<{Style.RESET_ALL}
""")
    print(f"{Fore.CYAN}ğŸ“¡ Checking connection...{Style.RESET_ALL}")
    
    try:
        response = requests.get(f"{BASE_URL}/user", headers=COMMON_HEADERS, timeout=10)
        if response.status_code == 200:
            user = response.json()
            name = user.get('username') or user.get('name')
            print(f"{Fore.GREEN}âœ… Connected successfully as: {name}{Style.RESET_ALL}")
        else:
            print(f"{Fore.YELLOW}âš ï¸  Warning: Could not fetch username (Code {response.status_code}).{Style.RESET_ALL}")
    except Exception as e:
        print(f"{Fore.RED}âŒ Connection Warning: {e}{Style.RESET_ALL}")

# =====================================================
# 2. Dataset Management (New & Improved)
# =====================================================
def load_dataset(repo_id, cache_dir="datasets"):
    """
    Ø¯Ø§Ù„Ø© Ø°ÙƒÙŠØ© Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¯Ø§ØªØ§ Ø³Øª ÙƒØ§Ù…Ù„Ø© (Ù…Ø¶ØºÙˆØ·Ø©) Ù…Ù† Ø§Ù„Ø³ÙŠØ±ÙØ± ÙˆØªØºÙ„ÙŠÙÙ‡Ø§.
    
    Args:
        repo_id (str): Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ù…Ø«Ù„ "mtma/wiki-arabic-full"
        cache_dir (str): Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
    
    Returns:
        str: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙÙƒÙˆÙƒØ©.
    """
    if "/" not in repo_id:
        print(f"{Fore.RED}âŒ Error: Invalid format. Use 'username/dataset'{Style.RESET_ALL}")
        return None

    username, repo_name = repo_id.split("/", 1)

    # 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù„ÙŠ (Ø¯Ø§Ø¦Ù…Ø§Ù‹ zip Ù„Ø£Ù† Ø§Ù„Ø³ÙŠØ±ÙØ± ÙŠØ±Ø³Ù„Ù‡ Ù…Ø¶ØºÙˆØ·Ø§Ù‹)
    filename = f"{repo_name}.zip"

    # 2. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ø¬Ù„Ø¯ (Ø§Ù„ØªØºÙ„ÙŠÙ)
    # Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯: datasets/mtma_wiki-arabic-full/
    folder_name = repo_id.replace("/", "_")
    save_dir = os.path.join(cache_dir, folder_name)
    
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
        print(f"{Fore.CYAN}ğŸ“‚ Created directory: {save_dir}{Style.RESET_ALL}")

    file_path = os.path.join(save_dir, filename)

    # 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„ÙØ¹Ù„ (Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªÙƒØ±Ø±)
    # Ù†ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù€ zip ÙÙ‚Ø·
    if os.path.exists(save_dir) and len(os.listdir(save_dir)) > 1: 
        # > 1 because file_path might be there/created
        print(f"{Fore.YELLOW}â„¹ï¸  Dataset seems ready at: {save_dir}{Style.RESET_ALL}")
        return save_dir

    # 4. Ø§Ù„ØªØ­Ù…ÙŠÙ„ (Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¬ÙˆÙ‡Ø±ÙŠ Ù‡Ù†Ø§)
    print(f"â¬‡ï¸  Downloading dataset: {repo_id}...")
    
    # âœ… Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„ØµØ­ÙŠØ­ Ø§Ù„Ø¢Ù† (Ø¨Ø¯ÙˆÙ† Ø§Ø³Ù… Ù…Ù„Ù ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©)
    url = f"{BASE_URL}/datasets/{repo_id}/download"
    
    success = _download_file(url, file_path)
    
    if success:
        # 5. ÙÙƒ Ø§Ù„Ø¶ØºØ· Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
        print(f"ğŸ“¦ Extracting {filename}...")
        try:
            with zipfile.ZipFile(file_path, 'r') as zip_ref:
                zip_ref.extractall(save_dir)
            print(f"{Fore.GREEN}âœ… Extracted successfully to: {save_dir}{Style.RESET_ALL}")
            
            # ØªÙ†Ø¸ÙŠÙ: Ø­Ø°Ù Ù…Ù„Ù Ø§Ù„Ù€ zip Ø¨Ø¹Ø¯ ÙÙƒ Ø§Ù„Ø¶ØºØ· Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©
            if os.path.exists(file_path):
                os.remove(file_path)
                
            return save_dir
        except zipfile.BadZipFile:
            print(f"{Fore.RED}âŒ Error: Server returned an invalid zip file. (Check if files exist on Wasabi){Style.RESET_ALL}")
            return None
    else:
        return None

def upload_dataset(file_path, full_repo_name, description="Dataset"):
    """
    Ø¯Ø§Ù„Ø© Ù„Ø±ÙØ¹ Ù…Ù„Ù Ø¥Ù„Ù‰ Ø§Ù„Ø¯Ø§ØªØ§ Ø³Øª.
    """
    if not os.path.exists(file_path):
        print(f"{Fore.RED}âŒ File not found: {file_path}{Style.RESET_ALL}")
        return
    
    print(f"ğŸ“¦ Preparing to upload: {file_path} ...")
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© _upload_file Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ api.py
    # ØªØ£ÙƒØ¯ Ø£Ù† _upload_file ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø§Ø¨Ø·: /datasets/{username}/{repo}/upload
    _upload_file(full_repo_name, file_path, "datasets", description)
# =====================================================
# 3. Models Logic (Standard)
# =====================================================
class SimpleNN(nn.Module):
    def __init__(self, layers_config):
        super(SimpleNN, self).__init__()
        layers = []
        for i in range(len(layers_config) - 1):
            layers.append(nn.Linear(layers_config[i], layers_config[i+1]))
            if i < len(layers_config) - 2:
                layers.append(nn.ReLU())
            else:
                layers.append(nn.Sigmoid())
        self.model = nn.Sequential(*layers)
        self.config = layers_config

    def forward(self, x): return self.model(x)
    
    def train_model(self, X, y, epochs=1000): pass 

    def save(self, path):
        torch.save({'state_dict': self.state_dict(), 'config': self.config}, path)

    def load(self, path):
        checkpoint = torch.load(path)
        self.load_state_dict(checkpoint['state_dict'])
        self.config = checkpoint['config']
        self.eval()

class Model:
    def __init__(self, layers):
        self.engine = SimpleNN(layers)
    
    def train(self, X, y, epochs=1000):
        self.engine.train_model(X, y, epochs)

    def predict(self, val):
        with torch.no_grad():
            return self.engine(torch.tensor(val, dtype=torch.float32)).tolist()

    def push_to_hub(self, full_repo_name, description="AI Model uploaded via Oneurai"):
        if "/" not in full_repo_name:
            print(f"{Fore.RED}âŒ Format Error{Style.RESET_ALL}")
            return
        _, repo_name = full_repo_name.split("/", 1)
        
        pt_filename = f"{repo_name}.pt"
        zip_filename = f"{repo_name}.zip"

        self.engine.save(pt_filename)
        
        print(f"ğŸ“¦ Compressing model to {zip_filename}...")
        try:
            with zipfile.ZipFile(zip_filename, 'w') as zipf:
                zipf.write(pt_filename)
        except Exception as e:
            print(f"{Fore.RED}âŒ Compression Failed: {e}{Style.RESET_ALL}")
            return

        _upload_file(full_repo_name, zip_filename, "models", description)
        
        if os.path.exists(pt_filename): os.remove(pt_filename)
        if os.path.exists(zip_filename): os.remove(zip_filename)

def create_model(layers): return Model(layers)

def load_model(full_repo_name, layers):
    _, repo_name = full_repo_name.split("/", 1)
    filename = f"{repo_name}.zip" 
    url = f"{BASE_URL}/models/{full_repo_name}/download/{filename}"
    return _download_and_load_model(url, filename, layers)

# =====================================================
# 4. Helper Internal Functions
# =====================================================
def _upload_file(full_repo_name, file_path, type_category, description):
    if "/" not in full_repo_name: return

    username, repo_name = full_repo_name.split("/", 1)
    url = f"{BASE_URL}/{type_category}/{username}/{repo_name}/upload"
    data = {'description': description}
    filename = os.path.basename(file_path)
    
    print(f"â˜ï¸ Uploading to [{type_category.upper()}] -> {Fore.BLUE}{full_repo_name}{Style.RESET_ALL} ...")
    
    try:
        with open(file_path, 'rb') as f:
            files = {'file': (filename, f, 'application/octet-stream')}
            response = requests.post(url, headers=COMMON_HEADERS, files=files, data=data)
        
        if response.status_code in [200, 201]:
            print(f"{Fore.GREEN}âœ… Upload Successful!{Style.RESET_ALL}")
        else:
            print(f"{Fore.RED}âŒ Server Error ({response.status_code}): {response.text}{Style.RESET_ALL}")
    except Exception as e:
        print(f"{Fore.RED}âŒ Connection Failed: {e}{Style.RESET_ALL}")

def _download_file(url, save_path):
    try:
        r = requests.get(url, headers=COMMON_HEADERS, stream=True)
        if r.status_code == 200:
            total_size = int(r.headers.get('content-length', 0))
            # Ù‡Ù†Ø§ Ù…Ù…ÙƒÙ† Ù†Ø¶ÙŠÙ Progress Bar Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… tqdm
            with open(save_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
            print(f"{Fore.GREEN}âœ… Downloaded: {save_path}{Style.RESET_ALL}")
            return True
        else:
            print(f"{Fore.RED}âŒ Failed to download (Code {r.status_code}): {r.text}{Style.RESET_ALL}")
            return False
    except Exception as e:
        print(f"{Fore.RED}âŒ Error: {e}{Style.RESET_ALL}")
        return False

def _download_and_load_model(url, zip_filename, layers):
    print(f"â¬‡ï¸ Downloading Model Package...")
    if _download_file(url, zip_filename):
        pt_filename = zip_filename.replace('.zip', '.pt')
        print("ğŸ“¦ Extracting model...")
        try:
            with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
                extracted_name = zip_ref.namelist()[0]
                zip_ref.extractall()
                if extracted_name != pt_filename and extracted_name.endswith('.pt'):
                        if os.path.exists(pt_filename): os.remove(pt_filename)
                        os.rename(extracted_name, pt_filename)
        except Exception as z_err:
            print(f"{Fore.RED}âŒ Extraction Error: {z_err}{Style.RESET_ALL}")
            return None

        m = Model(layers)
        m.engine.load(pt_filename)
        print(f"{Fore.GREEN}âœ… Model loaded successfully.{Style.RESET_ALL}")
        
        if os.path.exists(zip_filename): os.remove(zip_filename)
        if os.path.exists(pt_filename): os.remove(pt_filename)
        return m
    return None