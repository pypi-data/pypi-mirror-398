# Generated by tools/codegen/gen.py
# Do not edit directly

import tensorplay
import tensorplay._C as _C
from tensorplay._C import DType

def _ensure_device(device):
    if device is None or device is Ellipsis:
        return tensorplay.device("cpu")
    if isinstance(device, str):
        return tensorplay.device(device)
    return device

def embedding(weight, indices, padding_idx=-1, scale_grad_by_freq=False, sparse=False):
    return _C.embedding(weight=weight, indices=indices, padding_idx=padding_idx, scale_grad_by_freq=scale_grad_by_freq, sparse=sparse)

def embedding_dense_backward(grad_output, indices, num_weights, padding_idx, scale_grad_by_freq):
    return _C.embedding_dense_backward(grad_output=grad_output, indices=indices, num_weights=num_weights, padding_idx=padding_idx, scale_grad_by_freq=scale_grad_by_freq)

def conv1d(input, weight, bias={}, stride={1}, padding={0}, dilation={1}, groups=1):
    return _C.conv1d(input=input, weight=weight, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups)

def conv1d_grad_input(grad_output, input, weight, stride, padding, dilation, groups):
    return _C.conv1d_grad_input(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, dilation=dilation, groups=groups)

def conv1d_grad_weight(grad_output, input, weight, stride, padding, dilation, groups):
    return _C.conv1d_grad_weight(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, dilation=dilation, groups=groups)

def conv1d_grad_bias(grad_output, input, weight, stride, padding, dilation, groups):
    return _C.conv1d_grad_bias(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, dilation=dilation, groups=groups)

def conv2d(input, weight, bias={}, stride={1, 1}, padding={0, 0}, dilation={1, 1}, groups=1):
    return _C.conv2d(input=input, weight=weight, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups)

def conv2d_grad_input(grad_output, input, weight, stride, padding, dilation, groups):
    return _C.conv2d_grad_input(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, dilation=dilation, groups=groups)

def conv2d_grad_weight(grad_output, input, weight, stride, padding, dilation, groups):
    return _C.conv2d_grad_weight(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, dilation=dilation, groups=groups)

def conv2d_grad_bias(grad_output, input, weight, stride, padding, dilation, groups):
    return _C.conv2d_grad_bias(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, dilation=dilation, groups=groups)

def conv3d(input, weight, bias={}, stride={1, 1, 1}, padding={0, 0, 0}, dilation={1, 1, 1}, groups=1):
    return _C.conv3d(input=input, weight=weight, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups)

def conv3d_grad_input(grad_output, input, weight, stride, padding, dilation, groups):
    return _C.conv3d_grad_input(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, dilation=dilation, groups=groups)

def conv3d_grad_weight(grad_output, input, weight, stride, padding, dilation, groups):
    return _C.conv3d_grad_weight(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, dilation=dilation, groups=groups)

def conv3d_grad_bias(grad_output, input, weight, stride, padding, dilation, groups):
    return _C.conv3d_grad_bias(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, dilation=dilation, groups=groups)

def conv_transpose2d(input, weight, bias={}, stride={1, 1}, padding={0, 0}, output_padding={0, 0}, groups=1, dilation={1, 1}):
    return _C.conv_transpose2d(input=input, weight=weight, bias=bias, stride=stride, padding=padding, output_padding=output_padding, groups=groups, dilation=dilation)

def conv_transpose2d_grad_input(grad_output, input, weight, stride, padding, output_padding, groups, dilation):
    return _C.conv_transpose2d_grad_input(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, output_padding=output_padding, groups=groups, dilation=dilation)

def conv_transpose2d_grad_weight(grad_output, input, weight, stride, padding, output_padding, groups, dilation):
    return _C.conv_transpose2d_grad_weight(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, output_padding=output_padding, groups=groups, dilation=dilation)

def conv_transpose2d_grad_bias(grad_output, input, weight, stride, padding, output_padding, groups, dilation):
    return _C.conv_transpose2d_grad_bias(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, output_padding=output_padding, groups=groups, dilation=dilation)

def conv_transpose3d(input, weight, bias={}, stride={1, 1, 1}, padding={0, 0, 0}, output_padding={0, 0, 0}, groups=1, dilation={1, 1, 1}):
    return _C.conv_transpose3d(input=input, weight=weight, bias=bias, stride=stride, padding=padding, output_padding=output_padding, groups=groups, dilation=dilation)

def conv_transpose3d_grad_input(grad_output, input, weight, stride, padding, output_padding, groups, dilation):
    return _C.conv_transpose3d_grad_input(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, output_padding=output_padding, groups=groups, dilation=dilation)

def constant_pad_nd(input, pad, value):
    if not isinstance(value, (tensorplay.Scalar, tensorplay.Tensor)):
        value = tensorplay.Scalar(value)
    return _C.constant_pad_nd(self=input, pad=pad, value=value)

def constant_pad_nd_backward(grad_output, pad):
    return _C.constant_pad_nd_backward(grad_output=grad_output, pad=pad)

def conv_transpose3d_grad_weight(grad_output, input, weight, stride, padding, output_padding, groups, dilation):
    return _C.conv_transpose3d_grad_weight(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, output_padding=output_padding, groups=groups, dilation=dilation)

def conv_transpose3d_grad_bias(grad_output, input, weight, stride, padding, output_padding, groups, dilation):
    return _C.conv_transpose3d_grad_bias(grad_output=grad_output, input=input, weight=weight, stride=stride, padding=padding, output_padding=output_padding, groups=groups, dilation=dilation)

def add(input, other, alpha=1):
    return input.add(other=other, alpha=alpha)

def add_(input, other, alpha=1):
    return input.add_(other=other, alpha=alpha)

def sub(input, other, alpha=1):
    return input.sub(other=other, alpha=alpha)

def sub_(input, other, alpha=1):
    return input.sub_(other=other, alpha=alpha)

def mul(input, other):
    return input.mul(other=other)

def mul_(input, other):
    return input.mul_(other=other)

def div(input, other):
    return input.div(other=other)

def div_(input, other):
    return input.div_(other=other)

def mm(input, other):
    return _C.mm(self=input, other=other)

def matmul(input, other):
    return _C.matmul(self=input, other=other)

def eq(input, other):
    return _C.eq(self=input, other=other)

def ne(input, other):
    return _C.ne(self=input, other=other)

def lt(input, other):
    return _C.lt(self=input, other=other)

def le(input, other):
    return _C.le(self=input, other=other)

def gt(input, other):
    return _C.gt(self=input, other=other)

def ge(input, other):
    return _C.ge(self=input, other=other)

def copy_(input, src):
    return input.copy_(src=src)

def view(input, shape):
    return input.view(shape=shape)

def fill_(input, value):
    return input.fill_(value=value)

def transpose(input, dim0, dim1):
    return _C.transpose(self=input, dim0=dim0, dim1=dim1)

def t(input):
    return _C.t(self=input)

def permute(input, dims):
    return _C.permute(self=input, dims=dims)

def permute_backward(grad_output, input, dims):
    return _C.permute_backward(grad_output=grad_output, self=input, dims=dims)

def squeeze(input):
    return _C.squeeze(self=input)

def squeeze_backward(grad_output, input):
    return _C.squeeze_backward(grad_output=grad_output, self=input)

def unsqueeze(input, dim):
    return _C.unsqueeze(self=input, dim=dim)

def cat(tensors, dim=0):
    return _C.cat(tensors=tensors, dim=dim)

def stack(tensors, dim=0):
    return _C.stack(tensors=tensors, dim=dim)

def split(input, split_size, dim=0):
    return _C.split(self=input, split_size=split_size, dim=dim)

def chunk(input, chunks, dim=0):
    return _C.chunk(self=input, chunks=chunks, dim=dim)

def reshape(input, shape):
    return _C.reshape(self=input, shape=shape)

def unbind(input, dim=0):
    return _C.unbind(self=input, dim=dim)

def rand(*size, dtype=DType.float32, device=None, requires_grad=False):
    if len(size) == 1 and isinstance(size[0], (list, tuple)):
        _size = size[0]
    else:
        _size = size
    return _C.rand(size=list(_size), dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)

def rand_like(input, dtype=DType.undefined, device=None, requires_grad=False):
    return _C.rand_like(self=input, dtype=dtype, device=device, requires_grad=requires_grad)

def randint(low, high, size, dtype=DType.int64, device=..., requires_grad=False):
    return _C.randint(low=low, high=high, size=size, dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)

def randint_like(input, low, high, dtype=DType.undefined, device=None, requires_grad=False):
    return _C.randint_like(self=input, low=low, high=high, dtype=dtype, device=device, requires_grad=requires_grad)

def randn(*size, dtype=DType.float32, device=None, requires_grad=False):
    if len(size) == 1 and isinstance(size[0], (list, tuple)):
        _size = size[0]
    else:
        _size = size
    return _C.randn(size=list(_size), dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)

def randn_like(input, dtype=DType.undefined, device=None, requires_grad=False):
    return _C.randn_like(self=input, dtype=dtype, device=device, requires_grad=requires_grad)

def randperm(n, dtype=DType.int64, device=..., requires_grad=False):
    return _C.randperm(n=n, dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)

def bernoulli(input):
    return input.bernoulli()

def normal(mean, std):
    return _C.normal(mean=mean, std=std)

def abs(input):
    return _C.abs(self=input)

def acos(input):
    return _C.acos(self=input)

def acosh(input):
    return _C.acosh(self=input)

def angle(input):
    return _C.angle(self=input)

def asin(input):
    return _C.asin(self=input)

def asinh(input):
    return _C.asinh(self=input)

def atan(input):
    return _C.atan(self=input)

def atan2(input, other):
    return _C.atan2(self=input, other=other)

def poisson(input):
    return _C.poisson(self=input)

def atanh(input):
    return _C.atanh(self=input)

def ceil(input):
    return _C.ceil(self=input)

def clamp(input, min=None, max=None):
    if min is not None and not isinstance(min, (tensorplay.Scalar, tensorplay.Tensor)):
        min = tensorplay.Scalar(min)
    if max is not None and not isinstance(max, (tensorplay.Scalar, tensorplay.Tensor)):
        max = tensorplay.Scalar(max)
    return _C.clamp(self=input, min=min, max=max)

def clamp_backward(grad_output, input, min=None, max=None):
    if min is not None and not isinstance(min, (tensorplay.Scalar, tensorplay.Tensor)):
        min = tensorplay.Scalar(min)
    if max is not None and not isinstance(max, (tensorplay.Scalar, tensorplay.Tensor)):
        max = tensorplay.Scalar(max)
    return _C.clamp_backward(grad_output=grad_output, self=input, min=min, max=max)

def cos(input):
    return _C.cos(self=input)

def cosh(input):
    return _C.cosh(self=input)

def exp(input):
    return _C.exp(self=input)

def floor(input):
    return _C.floor(self=input)

def lerp(input, end, weight):
    if not isinstance(weight, (tensorplay.Scalar, tensorplay.Tensor)):
        weight = tensorplay.Scalar(weight)
    return _C.lerp(self=input, end=end, weight=weight)

def log(input):
    return _C.log(self=input)

def neg(input):
    return _C.neg(self=input)

def pow(input, exponent):
    if not isinstance(exponent, (tensorplay.Scalar, tensorplay.Tensor)):
        exponent = tensorplay.Scalar(exponent)
    return _C.pow(self=input, exponent=exponent)

def round(input):
    return _C.round(self=input)

def rsqrt(input):
    return _C.rsqrt(self=input)

def sigmoid(input):
    return _C.sigmoid(self=input)

def sign(input):
    return _C.sign(self=input)

def sin(input):
    return _C.sin(self=input)

def sinh(input):
    return _C.sinh(self=input)

def softmax(input, dim, dtype=DType.undefined):
    return _C.softmax(self=input, dim=dim, dtype=dtype)

def log_softmax(input, dim, dtype=DType.undefined):
    return _C.log_softmax(self=input, dim=dim, dtype=dtype)

def sqrt(input):
    return _C.sqrt(self=input)

def square(input):
    return _C.square(self=input)

def tan(input):
    return _C.tan(self=input)

def tanh(input):
    return _C.tanh(self=input)

def relu(input):
    return _C.relu(self=input)

def relu_(input):
    return _C.relu_(self=input)

def threshold_backward(grad_output, output, threshold):
    if not isinstance(threshold, (tensorplay.Scalar, tensorplay.Tensor)):
        threshold = tensorplay.Scalar(threshold)
    return _C.threshold_backward(grad_output=grad_output, output=output, threshold=threshold)

def gelu(input):
    return _C.gelu(self=input)

def silu(input):
    return _C.silu(self=input)

def empty(*size, dtype=DType.float32, device=None, requires_grad=False):
    if len(size) == 1 and isinstance(size[0], (list, tuple)):
        _size = size[0]
    else:
        _size = size
    return _C.empty(size=list(_size), dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)

def full(size, fill_value, dtype=DType.undefined, device=..., requires_grad=False):
    if not isinstance(fill_value, (tensorplay.Scalar, tensorplay.Tensor)):
        fill_value = tensorplay.Scalar(fill_value)
    return _C.full(size=size, fill_value=fill_value, dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)

def zeros(*size, dtype=DType.float32, device=None, requires_grad=False):
    if len(size) == 1 and isinstance(size[0], (list, tuple)):
        _size = size[0]
    else:
        _size = size
    return _C.zeros(size=list(_size), dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)

def ones(*size, dtype=DType.float32, device=None, requires_grad=False):
    if len(size) == 1 and isinstance(size[0], (list, tuple)):
        _size = size[0]
    else:
        _size = size
    return _C.ones(size=list(_size), dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)

def eye(n, m=-1, dtype=DType.float32, device=..., requires_grad=False):
    return _C.eye(n=n, m=m, dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)

def arange(*args, dtype=DType.undefined, device=None, requires_grad=False):
    if len(args) == 1:
        return _C.arange(end=args[0], dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)
    elif len(args) == 2:
        return _C.arange(start=args[0], end=args[1], dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)
    elif len(args) == 3:
        return _C.arange(start=args[0], end=args[1], step=args[2], dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)
    else:
        raise TypeError(f'arange expected 1-3 positional arguments, got {len(args)}')

def linspace(start, end, steps, dtype=DType.float32, device=..., requires_grad=False):
    if not isinstance(start, (tensorplay.Scalar, tensorplay.Tensor)):
        start = tensorplay.Scalar(start)
    if not isinstance(end, (tensorplay.Scalar, tensorplay.Tensor)):
        end = tensorplay.Scalar(end)
    return _C.linspace(start=start, end=end, steps=steps, dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)

def logspace(start, end, steps, base=10.0, dtype=DType.float32, device=..., requires_grad=False):
    if not isinstance(start, (tensorplay.Scalar, tensorplay.Tensor)):
        start = tensorplay.Scalar(start)
    if not isinstance(end, (tensorplay.Scalar, tensorplay.Tensor)):
        end = tensorplay.Scalar(end)
    return _C.logspace(start=start, end=end, steps=steps, base=base, dtype=dtype, device=_ensure_device(device), requires_grad=requires_grad)

def sum(input, dtype=DType.undefined):
    return _C.sum(self=input, dtype=dtype)

def mean(input, dtype=DType.undefined):
    return _C.mean(self=input, dtype=dtype)

def max(input):
    return _C.max(self=input)

def min(input):
    return _C.min(self=input)

def prod(input, dtype=DType.undefined):
    return _C.prod(self=input, dtype=dtype)

def argmax(input, dim=None, keepdim=False):
    return _C.argmax(self=input, dim=dim, keepdim=keepdim)

def argmin(input, dim=None, keepdim=False):
    return _C.argmin(self=input, dim=dim, keepdim=keepdim)

def all(input):
    return _C.all(self=input)

def any(input):
    return _C.any(self=input)

def var(input, correction=1):
    return _C.var(self=input, correction=correction)

def std(input, correction=1):
    return _C.std(self=input, correction=correction)

def median(input):
    return _C.median(self=input)

def norm(input, p=2.0):
    return _C.norm(self=input, p=p)

def exponential_(input, lambd=1.0):
    return input.exponential_(lambd=lambd)

def geometric_(input, p):
    return input.geometric_(p=p)

def log_normal_(input, mean=1.0, std=2.0):
    return input.log_normal_(mean=mean, std=std)

def normal_(input, mean=0.0, std=1.0):
    return input.normal_(mean=mean, std=std)

def random_(input, low=0, high=0):
    return input.random_(low=low, high=high)

def uniform_(input, from_=0.0, to=1.0):
    return input.uniform_(from_=from_, to=to)

def bernoulli_(input):
    return input.bernoulli_()

def cauchy_(input, median=0.0, sigma=1.0):
    return input.cauchy_(median=median, sigma=sigma)

def empty_like(input, dtype=DType.undefined, device=None, requires_grad=False):
    return _C.empty_like(self=input, dtype=dtype, device=device, requires_grad=requires_grad)

def zeros_like(input, dtype=DType.undefined, device=None, requires_grad=False):
    return _C.zeros_like(self=input, dtype=dtype, device=device, requires_grad=requires_grad)

def ones_like(input, dtype=DType.undefined, device=None, requires_grad=False):
    return _C.ones_like(self=input, dtype=dtype, device=device, requires_grad=requires_grad)

def full_like(input, fill_value, dtype=DType.undefined, device=None, requires_grad=False):
    if not isinstance(fill_value, (tensorplay.Scalar, tensorplay.Tensor)):
        fill_value = tensorplay.Scalar(fill_value)
    return _C.full_like(self=input, fill_value=fill_value, dtype=dtype, device=device, requires_grad=requires_grad)

def masked_select(input, mask):
    return _C.masked_select(self=input, mask=mask)

def zero_(input):
    return input.zero_()

def max_pool2d(input, kernel_size, stride={}, padding={0, 0}, dilation={1, 1}, ceil_mode=False):
    return _C.max_pool2d(input=input, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, ceil_mode=ceil_mode)

def avg_pool2d(input, kernel_size, stride={}, padding={0, 0}, ceil_mode=False, count_include_pad=True, divisor_override=None):
    return _C.avg_pool2d(input=input, kernel_size=kernel_size, stride=stride, padding=padding, ceil_mode=ceil_mode, count_include_pad=count_include_pad, divisor_override=divisor_override)

def adaptive_avg_pool2d(input, output_size):
    return _C.adaptive_avg_pool2d(input=input, output_size=output_size)

def adaptive_max_pool2d(input, output_size):
    return _C.adaptive_max_pool2d(input=input, output_size=output_size)

def nll_loss(input, target, weight=None, reduction=1, ignore_index=-100):
    return _C.nll_loss(self=input, target=target, weight=weight, reduction=reduction, ignore_index=ignore_index)

def nll_loss_backward(grad_output, input, target, weight=None, reduction=1, ignore_index=-100, total_weight={}):
    return _C.nll_loss_backward(grad_output=grad_output, self=input, target=target, weight=weight, reduction=reduction, ignore_index=ignore_index, total_weight=total_weight)

def mse_loss(input, target, reduction=1):
    return _C.mse_loss(self=input, target=target, reduction=reduction)

def mse_loss_backward(grad_output, input, target, reduction=1):
    return _C.mse_loss_backward(grad_output=grad_output, self=input, target=target, reduction=reduction)

def max_pool2d_backward(grad_output, input, kernel_size, stride={}, padding={0, 0}, dilation={1, 1}, ceil_mode=False):
    return _C.max_pool2d_backward(grad_output=grad_output, input=input, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, ceil_mode=ceil_mode)

def avg_pool2d_backward(grad_output, input, kernel_size, stride={}, padding={0, 0}, ceil_mode=False, count_include_pad=True, divisor_override=None):
    return _C.avg_pool2d_backward(grad_output=grad_output, input=input, kernel_size=kernel_size, stride=stride, padding=padding, ceil_mode=ceil_mode, count_include_pad=count_include_pad, divisor_override=divisor_override)

def adaptive_avg_pool2d_backward(grad_output, input):
    return _C.adaptive_avg_pool2d_backward(grad_output=grad_output, input=input)

def adaptive_max_pool2d_backward(grad_output, input):
    return _C.adaptive_max_pool2d_backward(grad_output=grad_output, input=input)

def batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps):
    return _C.batch_norm(input=input, weight=weight, bias=bias, running_mean=running_mean, running_var=running_var, training=training, momentum=momentum, eps=eps)

def layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-5):
    return _C.layer_norm(input=input, normalized_shape=normalized_shape, weight=weight, bias=bias, eps=eps)

def group_norm(input, num_groups, weight=None, bias=None, eps=1e-5):
    return _C.group_norm(input=input, num_groups=num_groups, weight=weight, bias=bias, eps=eps)

def instance_norm(input, weight=None, bias=None, running_mean=None, running_var=None, use_input_stats=True, momentum=0.1, eps=1e-5):
    return _C.instance_norm(input=input, weight=weight, bias=bias, running_mean=running_mean, running_var=running_var, use_input_stats=use_input_stats, momentum=momentum, eps=eps)

def batch_norm_backward(grad_output, input, weight=None, running_mean=None, running_var=None, training=True, eps=1e-5):
    return _C.batch_norm_backward(grad_output=grad_output, input=input, weight=weight, running_mean=running_mean, running_var=running_var, training=training, eps=eps)

def layer_norm_backward(grad_output, input, normalized_shape, weight=None, bias=None, eps=1e-5):
    return _C.layer_norm_backward(grad_output=grad_output, input=input, normalized_shape=normalized_shape, weight=weight, bias=bias, eps=eps)

def group_norm_backward(grad_output, input, num_groups, weight=None, bias=None, eps=1e-5):
    return _C.group_norm_backward(grad_output=grad_output, input=input, num_groups=num_groups, weight=weight, bias=bias, eps=eps)

def instance_norm_backward(grad_output, input, weight=None, bias=None, running_mean=None, running_var=None, use_input_stats=True, eps=1e-5):
    return _C.instance_norm_backward(grad_output=grad_output, input=input, weight=weight, bias=bias, running_mean=running_mean, running_var=running_var, use_input_stats=use_input_stats, eps=eps)
