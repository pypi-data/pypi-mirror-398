"""
Unit Tests for MirrorMind Framework

Comprehensive test suite covering all core components:
- ConfigValidator
- AdaptiveFramework
- EWC/SI Memory Handlers
- Consciousness Layer
- Meta-Controller
- Feedback Buffer
"""

import unittest
import torch
import torch.nn as nn
from pathlib import Path
import sys
import tempfile
import shutil

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from airbornehrs import (
    AdaptiveFramework,
    AdaptiveFrameworkConfig,
    ConfigValidator,
    validate_config
)


class TestConfigValidator(unittest.TestCase):
    """Test configuration validation."""
    
    def test_valid_config(self):
        """Test validation of valid configuration."""
        config = AdaptiveFrameworkConfig(
            learning_rate=0.001,
            meta_learning_rate=0.0001,
            model_dim=256,
            num_layers=6,
            num_heads=8
        )
        
        result = validate_config(config, raise_on_error=False)
        is_valid, errors, warnings = result if isinstance(result, tuple) else (result, [], [])
        self.assertTrue(is_valid, f"Valid config marked invalid. Errors: {errors}\")\n    \n    def test_invalid_learning_rate(self):\n        \"\"\"Test detection of invalid learning rate.\"\"\"  \n        config = AdaptiveFrameworkConfig(\n            learning_rate=-0.001,  # Invalid: negative\n            model_dim=256,\n            num_layers=6,\n            num_heads=8\n        )\n        \n        result = validate_config(config, raise_on_error=False)\n        is_valid, errors, warnings = result if isinstance(result, tuple) else (result, [], [])\n        self.assertFalse(is_valid, \"Invalid learning rate not detected\")\n        self.assertTrue(any('learning_rate' in str(e).lower() for e in errors))\n    \n    def test_invalid_architecture(self):\n        \"\"\"Test detection of invalid architecture (model_dim % num_heads != 0).\"\"\"\n        config = AdaptiveFrameworkConfig(\n            learning_rate=0.001,\n            model_dim=256,\n            num_heads=7  # 256 % 7 != 0, invalid\n        )\n        \n        result = validate_config(config, raise_on_error=False)\n        is_valid, errors, warnings = result if isinstance(result, tuple) else (result, [], [])\n        self.assertFalse(is_valid, \"Invalid architecture not detected\")\n    \n    def test_config_with_warnings(self):\n        \"\"\"Test that valid but unusual configs generate warnings.\"\"\"\n        config = AdaptiveFrameworkConfig(\n            learning_rate=0.001,\n            meta_learning_rate=0.0001,\n            dropout=0.7,  # Unusually high\n            model_dim=256,\n            num_heads=8\n        )\n        \n        result = validate_config(config, raise_on_error=False)\n        is_valid, errors, warnings = result if isinstance(result, tuple) else (result, [], [])\n        self.assertTrue(is_valid, \"Config with high dropout should be valid\")\n        # Should have warnings but no errors\n        self.assertEqual(len(errors), 0)\n\n\nclass TestFrameworkInitialization(unittest.TestCase):\n    \"\"\"Test AdaptiveFramework initialization.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Create simple model for testing.\"\"\"\n        self.model = nn.Sequential(\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )\n        \n        self.config = AdaptiveFrameworkConfig(\n            learning_rate=0.001,\n            device='cpu'\n        )\n    \n    def test_basic_initialization(self):\n        \"\"\"Test basic framework initialization.\"\"\"\n        framework = AdaptiveFramework(self.model, self.config, device='cpu')\n        self.assertIsNotNone(framework)\n        self.assertIsNotNone(framework.model)\n        self.assertIsNotNone(framework.optimizer)\n    \n    def test_has_required_components(self):\n        \"\"\"Test that framework has all required components.\"\"\"\n        framework = AdaptiveFramework(self.model, self.config, device='cpu')\n        \n        # Check required attributes\n        self.assertTrue(hasattr(framework, 'model'))\n        self.assertTrue(hasattr(framework, 'optimizer'))\n        self.assertTrue(hasattr(framework, 'ewc'))\n        self.assertTrue(hasattr(framework, 'feedback_buffer'))\n        self.assertTrue(hasattr(framework, 'meta_controller'))\n    \n    def test_consciousness_initialization(self):\n        \"\"\"Test consciousness layer initialization.\"\"\"\n        config = AdaptiveFrameworkConfig(\n            learning_rate=0.001,\n            device='cpu',\n            enable_consciousness=True\n        )\n        \n        framework = AdaptiveFramework(self.model, config, device='cpu')\n        self.assertIsNotNone(framework.consciousness)\n        self.assertIsNotNone(framework.attention)\n        self.assertIsNotNone(framework.intrinsic_motivation)\n\n\nclass TestForwardPass(unittest.TestCase):\n    \"\"\"Test forward pass and gradient computation.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Create model and framework.\"\"\"\n        self.model = nn.Sequential(\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )\n        \n        self.config = AdaptiveFrameworkConfig(\n            learning_rate=0.001,\n            device='cpu'\n        )\n        \n        self.framework = AdaptiveFramework(self.model, self.config, device='cpu')\n    \n    def test_forward_pass(self):\n        \"\"\"Test basic forward pass.\"\"\"\n        X = torch.randn(4, 64)\n        output = self.framework(X)\n        \n        self.assertEqual(output.shape, (4, 10))\n        self.assertTrue(torch.isfinite(output).all())\n    \n    def test_gradient_computation(self):\n        \"\"\"Test gradient computation.\"\"\"\n        X = torch.randn(4, 64)\n        y = torch.randint(0, 10, (4,))\n        \n        self.framework.train()\n        output = self.framework(X)\n        loss = nn.CrossEntropyLoss()(output, y)\n        \n        # Check gradients before backward\n        for param in self.framework.parameters():\n            self.assertIsNone(param.grad)\n        \n        # Compute gradients\n        loss.backward()\n        \n        # Check gradients after backward\n        has_gradients = False\n        for param in self.framework.parameters():\n            if param.grad is not None:\n                has_gradients = True\n                self.assertTrue(torch.isfinite(param.grad).all())\n        \n        self.assertTrue(has_gradients, \"No gradients computed\")\n    \n    def test_eval_mode(self):\n        \"\"\"Test evaluation mode (no gradients).\"\"\"\n        X = torch.randn(4, 64)\n        \n        self.framework.eval()\n        with torch.no_grad():\n            output = self.framework(X)\n        \n        self.assertEqual(output.shape, (4, 10))\n        self.assertTrue(torch.isfinite(output).all())\n\n\nclass TestMemoryConsolidation(unittest.TestCase):\n    \"\"\"Test EWC memory consolidation.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Create model and framework.\"\"\"\n        self.model = nn.Sequential(\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )\n        \n        self.config = AdaptiveFrameworkConfig(\n            learning_rate=0.001,\n            memory_type='ewc',\n            device='cpu'\n        )\n        \n        self.framework = AdaptiveFramework(self.model, self.config, device='cpu')\n    \n    def test_ewc_initialized(self):\n        \"\"\"Test that EWC handler is initialized.\"\"\"\n        self.assertIsNotNone(self.framework.ewc)\n    \n    def test_fisher_computation(self):\n        \"\"\"Test Fisher information computation.\"\"\"\n        X = torch.randn(10, 64)\n        y = torch.randint(0, 10, (10,))\n        \n        if hasattr(self.framework.ewc, 'compute_fisher'):\n            # Compute Fisher on random data\n            criterion = nn.CrossEntropyLoss()\n            \n            for _ in range(2):\n                output = self.framework(X)\n                loss = criterion(output, y)\n                loss.backward()\n                self.framework.optimizer.step()\n                self.framework.optimizer.zero_grad()\n            \n            # Framework should have stored some statistics\n            self.assertTrue(len(self.framework.loss_history) > 0)\n\n\nclass TestFeedbackBuffer(unittest.TestCase):\n    \"\"\"Test feedback buffer (experience replay).\"\"\"\n    \n    def setUp(self):\n        \"\"\"Create framework with buffer.\"\"\"\n        self.model = nn.Sequential(\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )\n        \n        self.config = AdaptiveFrameworkConfig(\n            learning_rate=0.001,\n            feedback_buffer_size=100,\n            device='cpu'\n        )\n        \n        self.framework = AdaptiveFramework(self.model, self.config, device='cpu')\n    \n    def test_buffer_exists(self):\n        \"\"\"Test that feedback buffer exists.\"\"\"\n        self.assertIsNotNone(self.framework.feedback_buffer)\n    \n    def test_add_to_buffer(self):\n        \"\"\"Test adding samples to buffer.\"\"\"\n        X = torch.randn(4, 64)\n        output = self.framework(X)\n        y = torch.randint(0, 10, (4,))\n        \n        initial_size = len(self.framework.feedback_buffer.buffer)\n        \n        self.framework.feedback_buffer.add(\n            X, output.detach(), y, reward=1.0, loss=0.5\n        )\n        \n        final_size = len(self.framework.feedback_buffer.buffer)\n        self.assertGreater(final_size, initial_size)\n\n\nclass TestOptimizationStep(unittest.TestCase):\n    \"\"\"Test optimization step.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Create model and framework.\"\"\"\n        self.model = nn.Sequential(\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )\n        \n        self.config = AdaptiveFrameworkConfig(\n            learning_rate=0.001,\n            device='cpu'\n        )\n        \n        self.framework = AdaptiveFramework(self.model, self.config, device='cpu')\n    \n    def test_parameters_update(self):\n        \"\"\"Test that parameters are updated during optimization.\"\"\"\n        X = torch.randn(4, 64)\n        y = torch.randint(0, 10, (4,))\n        \n        # Store initial parameters\n        initial_params = [p.clone() for p in self.framework.parameters()]\n        \n        # Training step\n        self.framework.train()\n        output = self.framework(X)\n        loss = nn.CrossEntropyLoss()(output, y)\n        loss.backward()\n        self.framework.optimizer.step()\n        self.framework.optimizer.zero_grad()\n        \n        # Check that parameters changed\n        params_changed = False\n        for initial, current in zip(initial_params, self.framework.parameters()):\n            if not torch.equal(initial, current):\n                params_changed = True\n                break\n        \n        self.assertTrue(params_changed, \"Parameters did not update\")\n\n\nclass TestMetaController(unittest.TestCase):\n    \"\"\"Test meta-controller integration.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Create framework with meta-controller.\"\"\"\n        self.model = nn.Sequential(\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )\n        \n        self.config = AdaptiveFrameworkConfig(\n            learning_rate=0.001,\n            device='cpu'\n        )\n        \n        self.framework = AdaptiveFramework(self.model, self.config, device='cpu')\n    \n    def test_meta_controller_exists(self):\n        \"\"\"Test that meta-controller is initialized.\"\"\"\n        self.assertIsNotNone(self.framework.meta_controller)\n    \n    def test_meta_optimizer_exists(self):\n        \"\"\"Test that meta optimizer exists.\"\"\"\n        self.assertIsNotNone(self.framework.meta_optimizer)\n\n\nclass TestEndToEnd(unittest.TestCase):\n    \"\"\"End-to-end integration tests.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Create model and framework for integration test.\"\"\"\n        self.model = nn.Sequential(\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )\n        \n        self.config = AdaptiveFrameworkConfig(\n            learning_rate=0.001,\n            meta_learning_rate=0.0001,\n            device='cpu',\n            enable_consciousness=True\n        )\n        \n        self.framework = AdaptiveFramework(self.model, self.config, device='cpu')\n    \n    def test_training_step(self):\n        \"\"\"Test complete training step.\"\"\"\n        X = torch.randn(8, 64)\n        y = torch.randint(0, 10, (8,))\n        \n        self.framework.train()\n        criterion = nn.CrossEntropyLoss()\n        \n        # Forward pass\n        output = self.framework(X)\n        loss = criterion(output, y)\n        \n        # Backward pass\n        self.framework.optimizer.zero_grad()\n        loss.backward()\n        self.framework.optimizer.step()\n        \n        # Check loss is finite\n        self.assertTrue(torch.isfinite(torch.tensor(loss.item())))\n    \n    def test_multiple_training_steps(self):\n        \"\"\"Test multiple training steps without divergence.\"\"\"\n        self.framework.train()\n        criterion = nn.CrossEntropyLoss()\n        \n        losses = []\n        for step in range(10):\n            X = torch.randn(8, 64)\n            y = torch.randint(0, 10, (8,))\n            \n            output = self.framework(X)\n            loss = criterion(output, y)\n            \n            self.framework.optimizer.zero_grad()\n            loss.backward()\n            self.framework.optimizer.step()\n            \n            losses.append(loss.item())\n            self.assertTrue(torch.isfinite(torch.tensor(loss.item())))\n        \n        # Check that training doesn't diverge\n        self.assertFalse(any(l > 1e6 for l in losses), \"Loss diverged during training\")\n\n\nif __name__ == '__main__':\n    unittest.main()\n