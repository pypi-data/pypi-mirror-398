[build-system]
requires = [
  "setuptools>=64",
  "wheel",
  "packaging",
  "psutil",
  "ninja",
  "scikit-build-core>=0.10",
  "torch>=2.8.0",
]
build-backend = "setuptools.build_meta"

[project]
name = "flash-sparse-attn"
dynamic = ["version"]
description = "Flash Sparse Attention: Fast and Memory-Efficient Trainable Dynamic Mask Sparse Attention"
readme = "README.md"
license = { file = "LICENSE" }
authors = [
  { name = "Jingze Shi", email = "losercheems@gmail.com" },
  { name = "Yifan Wu", email = "ywu012@connect.hkust-gz.edu.cn" },
  { name = "Bingheng Wu", email = "wubingheng52136@gmail.com" },
  { name = "Yiran Peng", email = "amagipeng@gmail.com" },
  { name = "Liangdong Wang", email = "wangliangdong@baai.ac.cn" },
  { name = "Guang Li", email = "liuguang@baai.ac.cn" },
  { name = "Yuyu Luo", email = "yuyuluo@hkust-gz.edu.cn" }
]
maintainers = [
  { name = "Jingze Shi", email = "losercheems@gmail.com" }
]
requires-python = ">=3.9"
dependencies = [
  "torch",
  "einops"
]
classifiers = [
  "Programming Language :: Python :: 3",
  "License :: OSI Approved :: BSD License",
  "Operating System :: Unix"
]

[project.urls]
Homepage = "https://github.com/flash-algo/flash-sparse-attention"
Source = "https://github.com/flash-algo/flash-sparse-attention"
Issues = "https://github.com/flash-algo/flash-sparse-attention/issues"

[project.optional-dependencies]
triton = [
  "triton>=2.0.0"
]
flex = [
  "transformers>=4.38.0"
]
all = [
  "triton>=2.0.0",
  "transformers>=4.38.0"
]
test = [
  "pytest>=6.0",
  "pytest-benchmark",
  "numpy"
]
dev = [
  "triton>=2.0.0",
  "transformers>=4.38.0",
  "pytest>=6.0",
  "pytest-benchmark",
  "numpy"
]

[tool.setuptools.dynamic]
version = { attr = "flash_sparse_attn.__version__" }

[tool.setuptools.packages.find]
where = ["."]
include = ["flash_sparse_attn*"]
exclude = [
  "build",
  "csrc",
  "include",
  "tests",
  "dist",
  "docs",
  "benchmarks",
  "flash_sparse_attn.egg-info"
]

[tool.setuptools.package-data]
flash_sparse_attn = ["*.py"]

[tool.setuptools]
