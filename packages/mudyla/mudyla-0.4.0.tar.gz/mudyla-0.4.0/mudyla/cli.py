"""Command-line interface for Mudyla."""

import argparse
import json
import os
import platform
import sys
import time
from dataclasses import dataclass
from glob import glob
from pathlib import Path
from typing import Optional

from .ast.models import ParsedDocument, ActionDefinition
from .dag.builder import DAGBuilder
from .dag.compiler import DAGCompiler, CompilationError
from .dag.graph import ActionKey, Dependency
from .dag.validator import DAGValidator, ValidationError
from .executor.engine import ExecutionEngine
from .executor.retainer_executor import RetainerExecutor, RetainerResult
from .parser.markdown_parser import MarkdownParser
from .cli_args import (
    AXIS_OPTIONS,
    ActionInvocation,
    ArgValue,
    CLIParseError,
    parse_custom_inputs,
    ParsedCLIInputs,
)
from .cli_builder import build_arg_parser
from .axis_wildcards import expand_all_wildcards
from .utils.project_root import find_project_root
from .utils.colors import ColorFormatter
from .utils.context_ids import build_context_mapping, format_action_label
from .utils.output import OutputFormatter
from .ast.expansions import ArgsExpansion, FlagsExpansion, EnvExpansion, ActionExpansion


@dataclass(frozen=True)
class ExecutionSetup:
    """Prepared state required to run the engine."""

    document: ParsedDocument
    project_root: Path
    markdown_files: list[Path]
    goals: list[str]
    custom_args: dict[str, str]
    axis_values: dict[str, str]
    all_flags: dict[str, bool]
    parsed_inputs: ParsedCLIInputs


class CLI:
    """Command-line interface for Mudyla."""

    def __init__(self):
        self.parser = build_arg_parser()

    def run(self, argv: Optional[list[str]] = None) -> int:
        """Run the CLI.

        Args:
            argv: Command-line arguments (defaults to sys.argv[1:])

        Returns:
            Exit code
        """
        args, unknown = self.parser.parse_known_args(argv)
        quiet_mode = args.autocomplete is not None
        self._apply_platform_defaults(args, quiet_mode)

        if args.autocomplete:
            return self._handle_autocomplete(args)

        color, output = self._build_formatters(args.no_color)

        try:
            # All arguments (goals, axes, args, flags) are in 'unknown' since we don't
            # define a positional 'goals' parameter in argparse (to preserve order)
            parsed_inputs = parse_custom_inputs([], unknown)
        except CLIParseError as e:
            output.print(f"{output.emoji('âŒ', 'âœ—')} {color.error('Error:')} {e}")
            return 1

        try:
            setup = self._prepare_execution_setup(args, parsed_inputs, color, output)
            self._validate_required_env(setup.document)

            document = setup.document
            goals = setup.goals
            custom_args = setup.custom_args
            axis_values = setup.axis_values
            all_flags = setup.all_flags
            project_root = setup.project_root

            if args.list_actions:
                self._list_actions(document, args.no_color)
                return 0

            parallel_execution = args.parallel or (
                not args.sequential
                and not args.verbose
                and not args.github_actions
                and not document.properties.sequential_execution_default
            )

            output.print(f"{output.emoji('ðŸ“š', 'â–¸')} {color.dim('Found')} {color.bold(str(len(setup.markdown_files)))} {color.dim('definition file(s) with')} {color.bold(str(len(document.actions)))} {color.dim('actions')}")

            for warning in setup.parsed_inputs.goal_warnings:
                output.print(f"{output.emoji('âš ï¸', '!')} {color.warning('Warning:')} {warning}")

            # Use the new compiler for multi-context support
            planning_start = time.perf_counter()
            compiler = DAGCompiler(document, setup.parsed_inputs)
            compiler.validate_action_invocations()
            graph = compiler.compile()
            planning_elapsed_ms = (time.perf_counter() - planning_start) * 1000

            # Build context mapping early (from full graph) for logging
            use_short_ids = not args.full_ctx_reprs
            context_mapping: dict[str, str] = {}
            if use_short_ids:
                context_mapping = build_context_mapping(graph.nodes.keys())

            # Show contexts and goals BEFORE retainer execution
            goal_keys = sorted(graph.goals, key=str)
            if use_short_ids and context_mapping:
                goal_strs = [
                    color.format_action_key(format_action_label(goal, use_short_ids=True))
                    for goal in goal_keys
                ]
            else:
                goal_strs = [color.format_action_key(str(goal)) for goal in goal_keys]
            goal_keys_str = ", ".join(goal_strs)

            if use_short_ids and context_mapping:
                output.print(f"\n{output.emoji('ðŸ”—', 'â–¸')} {color.bold('Contexts:')}")
                for short_id in sorted(context_mapping.keys()):
                    full_ctx = context_mapping[short_id]
                    short_id_colored = color.format_short_context_id(short_id)
                    full_ctx_colored = color.format_context_string(full_ctx)
                    output.print(f"  {short_id_colored}: {full_ctx_colored}")

            output.print(f"\n{output.emoji('ðŸŽ¯', 'â–¸')} {color.dim('Goals:')} {goal_keys_str}")

            # Execute retainers for soft dependencies to determine which to retain
            retainer_executor = RetainerExecutor(
                graph=graph,
                document=document,
                project_root=project_root,
                environment_vars=document.environment_vars,
                passthrough_env_vars=document.passthrough_env_vars,
                args=custom_args,
                flags=all_flags,
                axis_values=axis_values,
                without_nix=args.without_nix,
                verbose=args.verbose,
            )
            retained_soft_targets, retainer_results = retainer_executor.execute_retainers()

            # Log retainer results with context info
            if retainer_results:
                output.print(f"\n{output.emoji('ðŸ”„', 'â–¸')} {color.bold('Retainers:')}")
            for result in retainer_results:
                retainer_label = format_action_label(result.retainer_key, use_short_ids=use_short_ids)
                time_str = f"{result.execution_time_ms:.0f}ms"
                if result.retained:
                    # Deduplicate targets (multiple soft deps may point to the same target)
                    unique_targets = list(dict.fromkeys(result.soft_dep_targets))
                    targets = [
                        format_action_label(t, use_short_ids=use_short_ids)
                        for t in unique_targets
                    ]
                    targets_str = ", ".join(color.highlight(t) for t in targets)
                    output.print(
                        f"  {color.highlight(retainer_label)} {color.dim('ran in')} {time_str} "
                        f"{color.dim('â†’ retained')} {targets_str}"
                    )
                else:
                    output.print(
                        f"  {color.highlight(retainer_label)} {color.dim('ran in')} {time_str} "
                        f"{color.dim('â†’ retained nothing')}"
                    )
                # Log stdout/stderr in verbose mode
                if args.verbose and (result.stdout or result.stderr):
                    if result.stdout:
                        for line in result.stdout.rstrip().split("\n"):
                            output.print(f"    {color.dim('stdout:')} {line}")
                    if result.stderr:
                        for line in result.stderr.rstrip().split("\n"):
                            output.print(f"    {color.dim('stderr:')} {line}")

            pruned_graph = graph.prune_to_goals(retained_soft_targets)

            # Show execution mode
            if not quiet_mode:
                mode_label = "dry-run" if args.dry_run else ("parallel" if parallel_execution else "sequential")
                output.print(f"\n{output.emoji('âš™ï¸', 'â–¸')} {color.dim('Execution mode:')} {color.highlight(mode_label)}")

            validator = DAGValidator(document, pruned_graph)
            validator.validate_all(custom_args, all_flags, axis_values)
            if not quiet_mode:
                output.print(f"{output.emoji('âœ…', 'âœ“')} {color.dim('Built plan graph with')} {color.bold(str(len(pruned_graph.nodes)))} {color.dim('required action(s)')} {color.dim(f'(planning took {planning_elapsed_ms:.0f}ms)')}")

            execution_order = pruned_graph.get_execution_order()
            if not quiet_mode:
                output.print(f"\n{output.emoji('ðŸ“‹', 'â–¸')} {color.bold('Execution plan:')}")
                self._visualize_execution_plan(pruned_graph, execution_order, goals, color, output, use_short_ids)

            if args.dry_run:
                output.print(f"\n{output.emoji('â„¹ï¸', 'i')} {color.info('Dry run - not executing')}")
                return 0

            previous_run_dir = None
            if args.continue_run:
                runs_dir = project_root / ".mdl" / "runs"
                if runs_dir.exists():
                    run_dirs = sorted([d for d in runs_dir.iterdir() if d.is_dir()])
                    if run_dirs:
                        previous_run_dir = run_dirs[-1]
                        output.print(f"\n{output.emoji('ðŸ”„', 'â–¸')} {color.info('Continuing from previous run:')} {color.highlight(previous_run_dir.name)}")
                    else:
                        output.print(f"\n{output.emoji('âš ï¸', '!')} {color.warning('Warning:')} No previous runs found, starting fresh")
                else:
                    output.print(f"\n{output.emoji('âš ï¸', '!')} {color.warning('Warning:')} No runs directory found, starting fresh")

            engine = ExecutionEngine(
                graph=pruned_graph,
                project_root=project_root,
                args=custom_args,
                flags=all_flags,
                axis_values=axis_values,
                environment_vars=document.environment_vars,
                passthrough_env_vars=document.passthrough_env_vars,
                previous_run_directory=previous_run_dir,
                github_actions=args.github_actions,
                without_nix=args.without_nix,
                verbose=args.verbose,
                no_output_on_fail=args.no_out_on_fail,
                keep_run_dir=args.keep_run_dir,
                no_color=args.no_color,
                simple_log=args.simple_log,
                show_dirs=args.show_dirs,
                parallel_execution=parallel_execution,
                use_short_context_ids=use_short_ids,
                context_id_mapping=context_mapping,
            )

            # Print run ID
            run_id = engine.run_directory.name
            output.print(f"\n{output.emoji('ðŸ†”', 'â–¸')} {color.dim('Run ID:')} {color.highlight(run_id)}")
            output.print(f"{output.emoji('ðŸš€', 'â†’')} {color.bold('Executing actions...')}")

            result = engine.execute_all()
            if not result.success:
                output.print(f"\n{output.emoji('âŒ', 'âœ—')} {color.error('Execution failed!')}")
                return 1

            # Get outputs using ActionKeys (with context) instead of just action names
            if args.full_output:
                outputs_to_report = result.get_all_outputs(pruned_graph.nodes.keys())
            else:
                outputs_to_report = result.get_goal_outputs(graph.goals)
            output.print(f"\n{output.emoji('âœ…', 'âœ“')} {color.success('Execution completed successfully!')}")

            output_json = json.dumps(outputs_to_report, indent=2)
            output.print(f"\n{output.emoji('ðŸ“Š', 'â–¸')} {color.bold('Outputs:')}")

            if not args.no_color:
                from rich.console import Console
                from rich.json import JSON
                console = Console()
                console.print(JSON(output_json))
            else:
                output.print(output_json)

            if args.out:
                out_path = Path(args.out)
                out_path.write_text(output_json)
                output.print(f"\n{output.emoji('ðŸ’¾', 'â–¸')} {color.dim('Outputs saved to:')} {color.highlight(str(out_path))}")

            if args.keep_run_dir:
                output.print(f"\n{output.emoji('ðŸ“‚', 'â–¸')} {color.dim('Run directory:')} {color.highlight(str(result.run_directory))}")

            return 0

        except ValueError as err:
            output.print(f"{output.emoji('âŒ', 'âœ—')} {color.error('Error:')} {err}")
            if "No goals specified" in str(err):
                self.parser.print_help()
            return 1
        except ValidationError as e:
            try:
                output.print(f"\n{output.emoji('âŒ', 'âœ—')} {color.error('Validation error:')}\n{e}")
            except (NameError, UnicodeEncodeError):
                print(f"\n[!] Validation error:\n{e}")
            return 1
        except CompilationError as e:
            try:
                output.print(f"\n{output.emoji('âŒ', 'âœ—')} {color.error('Compilation error:')}\n{e}")
            except (NameError, UnicodeEncodeError):
                print(f"\n[!] Compilation error:\n{e}")
            return 1
        except Exception as e:
            try:
                output.print(f"\n{output.emoji('âŒ', 'âœ—')} {color.error('Error:')} {e}")
            except (NameError, UnicodeEncodeError):
                print(f"\n[!] Error: {e}")
            import traceback

            traceback.print_exc()
            return 1

    def _validate_required_env(self, document: ParsedDocument):
        missing_vars = []
        for var in document.required_env_vars:
            if var not in os.environ:
                missing_vars.append(var)
        
        if missing_vars:
            raise ValueError(f"Missing required environment variables: {', '.join(missing_vars)}")

    def _apply_platform_defaults(self, args: argparse.Namespace, quiet_mode: bool) -> None:
        """Apply platform specific defaults."""
        system = platform.system()
        
        # Determine Nix usage
        use_nix_env = os.environ.get("MUDYLA_USE_NIX", "").lower()
        
        # Nix is enabled by default only on Linux
        nix_default_on = system == "Linux"
        
        using_nix = False
        reason = ""
        
        if args.force_nix:
            using_nix = True
            reason = "forced with --force-nix"
        elif args.without_nix:
            using_nix = False
            reason = "disabled with --without-nix"
        elif use_nix_env == "force-on":
            using_nix = True
            reason = "forced with MUDYLA_USE_NIX=force-on"
        elif use_nix_env == "force-off":
            using_nix = False
            reason = "disabled with MUDYLA_USE_NIX=force-off"
        else:
            using_nix = nix_default_on
            reason = f"default for {system}"
            
        # Update args
        args.without_nix = not using_nix
        
        if not quiet_mode:
            state = "Yes" if using_nix else "No"
            print(f"Using Nix: {state} ({reason})")

        if args.github_actions and system == "Windows" and not args.no_color:
            args.no_color = True


    def _handle_autocomplete(self, args: argparse.Namespace) -> int:
        """Handle autocomplete mode without noisy output."""
        mode = args.autocomplete or "actions"
        try:
            project_root = find_project_root()
            md_files = self._discover_markdown_files(args.defs, project_root)
            if not md_files:
                return 1

            parser = MarkdownParser()
            document = parser.parse_files(md_files)

            if mode == "actions":
                suggestions = self._list_action_names_ordered(document)
            elif mode == "flags":
                suggestions = self._list_all_flags(document)
            elif mode == "axis-names":
                suggestions = self._list_axis_names(document)
            elif mode == "axis-values":
                axis_name = args.autocomplete_axis
                if not axis_name:
                    return 1
                suggestions = self._list_axis_values(document, axis_name)
            else:
                return 1

            for name in suggestions:
                print(name)
            return 0
        except Exception:
            return 1

    def _list_axis_names(self, document: ParsedDocument) -> list[str]:
        """Return all axis names defined in the document."""
        return sorted(document.axis.keys())

    def _list_axis_values(self, document: ParsedDocument, axis_name: str) -> list[str]:
        """Return all values for a specific axis."""
        if axis_name not in document.axis:
            return []
        axis_def = document.axis[axis_name]
        return [av.value for av in axis_def.values]

    def _build_formatters(self, no_color: bool) -> tuple[ColorFormatter, OutputFormatter]:
        color = ColorFormatter(no_color=no_color)
        output = OutputFormatter(color)
        return color, output

    def _prepare_execution_setup(
        self,
        args: argparse.Namespace,
        parsed_inputs: ParsedCLIInputs,
        color: ColorFormatter,
        output: OutputFormatter,
    ) -> ExecutionSetup:
        """Load markdown definitions and merge CLI inputs with defaults."""
        project_root = find_project_root()
        output.print(f"{color.dim('Project root:')} {color.highlight(str(project_root))}")

        md_files = self._discover_markdown_files(args.defs, project_root)
        if not md_files:
            raise ValueError(f"No markdown files found matching pattern: {args.defs}")

        parser = MarkdownParser()
        document = parser.parse_files(md_files)

        # Expand wildcards in axis specifications
        parsed_inputs = expand_all_wildcards(parsed_inputs, document)

        # Resolve argument aliases (e.g., --ml -> --message-local)
        parsed_inputs = self._resolve_argument_aliases(document, parsed_inputs)

        custom_args = dict(parsed_inputs.custom_args)
        axis_values = dict(parsed_inputs.axis_values)
        goals = list(parsed_inputs.goals)

        self._apply_default_axis_values(document, axis_values, color)
        self._apply_default_argument_values(document, custom_args)
        self._normalize_array_arguments(document, custom_args)

        all_flags = {name: False for name in document.flags}
        all_flags.update(parsed_inputs.custom_flags)

        if not goals and not args.list_actions:
            raise ValueError("No goals specified")

        return ExecutionSetup(
            document=document,
            project_root=project_root,
            markdown_files=md_files,
            goals=goals,
            custom_args=custom_args,
            axis_values=axis_values,
            all_flags=all_flags,
            parsed_inputs=parsed_inputs,
        )

    def _discover_markdown_files(self, defs_pattern: str, project_root: Path) -> list[Path]:
        pattern = Path(defs_pattern)
        if not pattern.is_absolute():
            pattern = project_root / defs_pattern
        matches = [
            Path(path) for path in glob(str(pattern), recursive=True)
            if Path(path).is_file() and path.endswith('.md')
        ]
        return matches

    def _apply_default_axis_values(
        self,
        document: ParsedDocument,
        axis_values: dict[str, str],
        color: ColorFormatter,
    ) -> None:
        for axis_name, axis_def in document.axis.items():
            if axis_name in axis_values:
                continue
            default_value = axis_def.get_default_value()
            if default_value:
                axis_values[axis_name] = default_value
                # Format axis name and value with consistent colors
                from .utils.colors import Colors
                axis_colored = color.colorize(axis_name, Colors.MAGENTA)
                separator = color.dim(":")
                value_colored = color.colorize(default_value, Colors.YELLOW)
                print(f"{color.dim('Using default axis value:')} {axis_colored}{separator}{value_colored}")

    def _resolve_argument_aliases(
        self,
        document: ParsedDocument,
        parsed_inputs: ParsedCLIInputs,
    ) -> ParsedCLIInputs:
        """Resolve argument aliases to their canonical names.

        Args:
            document: Parsed document with argument definitions
            parsed_inputs: Parsed CLI inputs (immutable, returns new instance)

        Returns:
            New ParsedCLIInputs with aliases resolved in both global and per-action args
        """
        # Build alias -> canonical name mapping
        alias_to_canonical: dict[str, str] = {}
        for arg_name, arg_def in document.arguments.items():
            if arg_def.alias:
                if arg_def.alias in alias_to_canonical:
                    raise ValueError(
                        f"Duplicate alias '{arg_def.alias}': used by both "
                        f"'args.{alias_to_canonical[arg_def.alias]}' and 'args.{arg_name}'"
                    )
                alias_to_canonical[arg_def.alias] = arg_name

        if not alias_to_canonical:
            return parsed_inputs

        def resolve_args(args: dict[str, ArgValue], context: str) -> dict[str, ArgValue]:
            """Resolve aliases in a dict of arguments."""
            resolved = dict(args)
            for alias, canonical_name in alias_to_canonical.items():
                if alias in resolved:
                    alias_value = resolved.pop(alias)
                    if canonical_name in resolved:
                        # Merge values (both alias and canonical were used)
                        existing = resolved[canonical_name]
                        if isinstance(existing, list):
                            if isinstance(alias_value, list):
                                existing.extend(alias_value)
                            else:
                                existing.append(alias_value)
                        else:
                            if isinstance(alias_value, list):
                                resolved[canonical_name] = [existing] + alias_value
                            else:
                                resolved[canonical_name] = [existing, alias_value]
                    else:
                        resolved[canonical_name] = alias_value
            return resolved

        # Resolve global args
        resolved_global_args = resolve_args(parsed_inputs.global_args, "global scope")

        # Resolve per-action args
        resolved_invocations = []
        for inv in parsed_inputs.action_invocations:
            resolved_invocations.append(ActionInvocation(
                action_name=inv.action_name,
                args=resolve_args(inv.args, f"action '{inv.action_name}'"),
                flags=inv.flags,
                axes=inv.axes,
            ))

        return ParsedCLIInputs(
            global_args=resolved_global_args,
            global_flags=parsed_inputs.global_flags,
            global_axes=parsed_inputs.global_axes,
            action_invocations=resolved_invocations,
            goal_warnings=parsed_inputs.goal_warnings,
        )

    def _apply_default_argument_values(
        self,
        document: ParsedDocument,
        custom_args: dict[str, ArgValue],
    ) -> None:
        """Apply default values for missing arguments."""
        for arg_name, arg_def in document.arguments.items():
            if arg_name in custom_args:
                continue
            if arg_def.default_value is not None:
                custom_args[arg_name] = arg_def.default_value

    def _normalize_array_arguments(
        self,
        document: ParsedDocument,
        custom_args: dict[str, ArgValue],
    ) -> None:
        """Ensure array arguments are always lists, scalar args are always strings.

        - For array arguments: convert single string to list[str]
        - For scalar arguments: fail if multiple values were provided
        """
        for arg_name, arg_def in document.arguments.items():
            if arg_name not in custom_args:
                continue

            value = custom_args[arg_name]

            if arg_def.is_array:
                # Array argument: ensure it's a list
                if isinstance(value, str):
                    custom_args[arg_name] = [value]
                # Already a list, nothing to do
            else:
                # Scalar argument: must be a single string
                if isinstance(value, list):
                    raise ValueError(
                        f"Argument 'args.{arg_name}' is not an array type but was "
                        f"specified multiple times. Use type 'array[{arg_def.arg_type.element_type.value}]' "
                        f"if you want to specify multiple values."
                    )

    def _compute_sharing_counts(
        self,
        graph,
        execution_order,
        goals: list[str],
    ) -> dict:
        """Compute how many unique goal contexts use each action.

        An action is "shared" if multiple goal contexts depend on it (directly or
        transitively). This helps visualize context reduction benefits.

        Args:
            graph: The execution graph
            execution_order: List of action keys in execution order
            goals: List of goal action names

        Returns:
            Dictionary mapping ActionKey to count of goal contexts that use it
        """
        from .dag.graph import ActionKey

        # Find all goal action keys
        goal_keys = [ak for ak in execution_order if ak.id.name in goals]

        # For each action, collect which goal contexts reach it
        action_to_goal_contexts: dict[ActionKey, set[str]] = {}

        def collect_reachable_goals(action_key: ActionKey, visited: set[ActionKey]) -> set[str]:
            """Recursively find all goal contexts that depend on this action."""
            if action_key in visited:
                return set()
            visited.add(action_key)

            contexts: set[str] = set()

            # If this is a goal, add its context
            if action_key.id.name in goals:
                contexts.add(str(action_key.context_id))

            # Check all dependents (actions that depend on this one)
            node = graph.get_node(action_key)
            for dep in node.dependents:
                contexts.update(collect_reachable_goals(dep.action, visited))

            return contexts

        # Compute for each action
        for action_key in execution_order:
            contexts = collect_reachable_goals(action_key, set())
            action_to_goal_contexts[action_key] = contexts

        # Convert to counts
        return {ak: len(contexts) for ak, contexts in action_to_goal_contexts.items()}

    def _visualize_execution_plan(
        self,
        graph,
        execution_order,
        goals: list[str],
        color,
        output: OutputFormatter,
        use_short_ids: bool = False,
    ) -> None:
        """Visualize execution plan as a rich table.

        Args:
            graph: The execution graph
            execution_order: List of action keys in execution order
            goals: List of goal action names
            color: Color formatter
            output: Output formatter
            use_short_ids: Whether to use short context IDs
        """
        from rich.console import Console
        from rich.table import Table

        # Compute sharing counts: how many unique goal contexts use each action
        sharing_counts = self._compute_sharing_counts(graph, execution_order, goals)

        table = Table(show_header=True, header_style="bold")
        table.add_column("#", justify="right")
        table.add_column("Context", style="dim")
        table.add_column("Action", style="cyan")
        table.add_column("Goal", justify="center")
        table.add_column("Deps", style="dim")
        table.add_column("Shared", justify="right", style="blue")

        for i, action_key in enumerate(execution_order, 1):
            node = graph.get_node(action_key)
            is_goal = action_key.id.name in goals

            # Number column
            num_str = str(i)

            # Context column
            if use_short_ids:
                context_str = format_action_label(action_key, use_short_ids=True).split("#")[0]
            else:
                context_str = str(action_key.context_id) if action_key.context_id else "default"

            # Action column
            action_str = action_key.id.name

            # Goal column
            goal_str = output.emoji("ðŸŽ¯", "*") if is_goal else ""

            # Dependencies column
            if node.dependencies:
                sorted_deps = sorted(node.dependencies, key=lambda d: d.action.id.name)
                dep_parts = []
                for dep in sorted_deps:
                    if dep.action not in execution_order:
                        continue  # Skip dependencies not in execution order (e.g., pruned soft deps)
                    dep_num = execution_order.index(dep.action) + 1
                    if dep.weak:
                        dep_parts.append(f"~{dep_num}")
                    elif dep.soft:
                        dep_parts.append(f"?{dep_num}")  # Show soft deps with question mark
                    else:
                        dep_parts.append(str(dep_num))
                deps_str = ", ".join(dep_parts) if dep_parts else "-"
            else:
                deps_str = "-"

            # Shared column - show how many contexts share this action
            share_count = sharing_counts.get(action_key, 1)
            shared_str = str(share_count) if share_count > 1 else "-"

            table.add_row(num_str, context_str, action_str, goal_str, deps_str, shared_str)

        console = Console()
        console.print(table)
        output.print("")  # Empty line after plan

    def _list_actions(self, document: ParsedDocument, no_color: bool = False) -> None:
        """List all available actions."""
        from .utils.colors import ColorFormatter
        from .utils.output import OutputFormatter

        color = ColorFormatter(no_color=no_color)
        output = OutputFormatter(color)

        # Show available axes first
        if document.axis:
            print(f"\n{color.info('Available axes:')}\n")
            for axis_name in sorted(document.axis.keys()):
                axis_def = document.axis[axis_name]

                # Format values with default marked
                value_strs = []
                for axis_val in axis_def.values:
                    if axis_val.is_default:
                        value_strs.append(f"{color.success(axis_val.value)}*")
                    else:
                        value_strs.append(axis_val.value)

                values_str = ', '.join(value_strs)
                print(f"  {color.highlight(axis_name)}: {values_str}")
            print()

        print(f"{color.info('Available actions:')}\n")

        root_actions, non_root_actions = self._partition_actions(document)
        metadata = {
            name: self._collect_action_metadata(action)
            for name, action in document.actions.items()
        }

        # Display root actions first, then non-root actions
        for action_name in root_actions + non_root_actions:
            action = document.actions[action_name]
            info = metadata[action_name]
            typed_deps = info["typed_dependencies"]
            is_root = len(typed_deps) == 0

            # Format action name
            if is_root:
                # Root actions are bold with goal emoji
                goal_marker = output.emoji("ðŸŽ¯", "*")
                formatted_name = f"{goal_marker} {color.bold(color.highlight(action_name))}"
            else:
                formatted_name = f"  {color.highlight(action_name)}"

            print(formatted_name)

            if action.description:
                for line in action.description.splitlines():
                    stripped_line = line.strip()
                    if stripped_line:
                        print(f"    {color.dim(stripped_line)}")

            if typed_deps:
                dep_strs = []
                for dep_name in sorted(typed_deps.keys()):
                    dep_type = typed_deps[dep_name]
                    if dep_type == "weak":
                        dep_strs.append(f"~{dep_name}")
                    elif dep_type == "soft":
                        dep_strs.append(f"?{dep_name}")
                    else:
                        dep_strs.append(dep_name)
                dep_str = ', '.join(dep_strs)
                print(f"    {color.dim('Dependencies:')} {dep_str}")

            args_used = info["args_used"]
            if args_used:
                args_str = ', '.join(sorted(args_used))
                print(f"    {color.dim('Arguments:')} {color.warning(args_str)}")
            flags_used = info["flags_used"]
            if flags_used:
                flags_str = ', '.join(sorted(flags_used))
                print(f"    {color.dim('Flags:')} {color.warning(flags_str)}")

            all_env_vars = info["env_vars"]
            if all_env_vars:
                env_str = ', '.join(sorted(all_env_vars))
                print(f"    {color.dim('Env vars:')} {env_str}")

            inputs_map = info["inputs"]
            if inputs_map:
                input_strs = []
                for act_name in sorted(inputs_map.keys()):
                    vars_str = ', '.join(sorted(inputs_map[act_name]))
                    input_strs.append(f"{color.highlight(act_name)}.{{{vars_str}}}")
                print(f"    {color.dim('Inputs:')} {', '.join(input_strs)}")

            all_returns = info["returns"]
            if all_returns:
                return_strs = [
                    f"{color.success(r.name)}:{color.dim(r.return_type.value)}"
                    for r in all_returns
                ]
                print(f"    {color.dim('Returns:')} {', '.join(return_strs)}")

            # Show versions if action has multiple versions
            if len(action.versions) > 1:
                from .ast.models import AxisCondition, PlatformCondition
                version_strs = []
                for i, version in enumerate(action.versions, 1):
                    cond_parts = []
                    for cond in version.conditions:
                        if isinstance(cond, AxisCondition):
                            cond_parts.append(f"{cond.axis_name}: {cond.axis_value}")
                        elif isinstance(cond, PlatformCondition):
                            cond_parts.append(f"platform: {cond.platform_value}")

                    if cond_parts:
                        version_strs.append(f"{i} ({', '.join(cond_parts)})")
                    else:
                        version_strs.append(str(i))

                print(f"    {color.dim('Versions:')} {', '.join(version_strs)}")

            print()

    def _list_action_names_ordered(self, document: ParsedDocument) -> list[str]:
        """Return action names in the same order as _list_actions prints them."""
        root_actions, non_root_actions = self._partition_actions(document)
        return root_actions + non_root_actions

    def _partition_actions(self, document: ParsedDocument) -> tuple[list[str], list[str]]:
        root_actions: list[str] = []
        non_root_actions: list[str] = []
        for action_name, action in document.actions.items():
            deps = action.get_typed_action_dependencies()
            if len(deps) == 0:
                root_actions.append(action_name)
            else:
                non_root_actions.append(action_name)
        root_actions.sort()
        non_root_actions.sort()
        return root_actions, non_root_actions

    def _list_cli_flag_options(self) -> list[str]:
        """Return CLI-level flag options (long form only, excluding autocomplete)."""
        cli_flags: set[str] = set()
        for action in self.parser._actions:
            for option in action.option_strings:
                if not option.startswith("--"):
                    continue
                if option == "--autocomplete":
                    continue
                cli_flags.add(option)
        # Add all axis option aliases
        for axis_opt in AXIS_OPTIONS:
            cli_flags.add(axis_opt)
        return sorted(cli_flags)

    def _list_all_flags(self, document: ParsedDocument) -> list[str]:
        """Return combined list of CLI and document flags (prefixed with --)."""
        document_flags = {f"--{flag_name}" for flag_name in document.flags.keys()}
        all_flags = set(self._list_cli_flag_options())
        all_flags.update(document_flags)
        return sorted(all_flags)

    def _collect_action_metadata(self, action: ActionDefinition) -> dict[str, object]:
        args_used: set[str] = set()
        flags_used: set[str] = set()
        env_vars_used: set[str] = set()
        inputs: dict[str, set[str]] = {}

        for expansion in action.get_all_expansions():
            if isinstance(expansion, ArgsExpansion):
                args_used.add(expansion.argument_name)
            elif isinstance(expansion, FlagsExpansion):
                flags_used.add(expansion.flag_name)
            elif isinstance(expansion, EnvExpansion):
                env_vars_used.add(expansion.variable_name)
            elif isinstance(expansion, ActionExpansion):
                inputs.setdefault(expansion.action_name, set()).add(expansion.variable_name)

        returns_map: dict[str, object] = {}
        for version in action.versions:
            for ret_decl in version.return_declarations:
                returns_map[ret_decl.name] = ret_decl

        all_env_vars = set(action.required_env_vars.keys()) | env_vars_used

        return {
            "dependencies": action.get_action_dependencies(),
            "typed_dependencies": action.get_typed_action_dependencies(),
            "args_used": args_used,
            "flags_used": flags_used,
            "env_vars": all_env_vars,
            "inputs": inputs,
            "returns": list(returns_map.values()),
        }


def main() -> int:
    """Main entry point."""
    cli = CLI()
    return cli.run()


if __name__ == "__main__":
    sys.exit(main())
