# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                         QUORUM CONFIGURATION                             ║
# ╚══════════════════════════════════════════════════════════════════════════╝
#
# Copy this file to .env and fill in your API keys and preferred models.
#
# IMPORTANT: Cloud provider models (OpenAI, Anthropic, Google, xAI) must be
# listed in their respective *_MODELS variables to be available in Quorum.
# Ollama models are auto-discovered and don't need to be listed.


# ══════════════════════════════════════════════════════════════════════════════
# OPENAI
# ══════════════════════════════════════════════════════════════════════════════
# Get your API key: https://platform.openai.com/api-keys
# Available models: https://platform.openai.com/docs/models
#
OPENAI_API_KEY=sk-...
OPENAI_MODELS=gpt-5.2,gpt-5.1,gpt-5


# ══════════════════════════════════════════════════════════════════════════════
# ANTHROPIC
# ══════════════════════════════════════════════════════════════════════════════
# Get your API key: https://console.anthropic.com/settings/keys
# Available models: https://docs.anthropic.com/en/docs/about-claude/models
#
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODELS=claude-opus-4-5-20251124,claude-sonnet-4-5-20250929


# ══════════════════════════════════════════════════════════════════════════════
# GOOGLE
# ══════════════════════════════════════════════════════════════════════════════
# Get your API key: https://aistudio.google.com/apikey
# Available models: https://ai.google.dev/gemini-api/docs/models
#
GOOGLE_API_KEY=...
GOOGLE_MODELS=gemini-3-pro,gemini-2.5-flash


# ══════════════════════════════════════════════════════════════════════════════
# XAI (GROK)
# ══════════════════════════════════════════════════════════════════════════════
# Get your API key: https://console.x.ai/
# Available models: https://docs.x.ai/docs/models
#
XAI_API_KEY=xai-...
XAI_MODELS=grok-4.1,grok-4


# ══════════════════════════════════════════════════════════════════════════════
# OLLAMA (LOCAL MODELS)
# ══════════════════════════════════════════════════════════════════════════════
# Install Ollama: https://ollama.com/download
# Available models: https://ollama.com/library
#
# Models are auto-discovered - no configuration needed for same-machine setup!
# Just run "ollama pull llama3" and it appears in /models automatically.
#
# Only configure these if Ollama runs on a different machine:
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_API_KEY=                          # Optional, for proxy authentication
#
# WSL users with Ollama on Windows: See README.md troubleshooting section


# ══════════════════════════════════════════════════════════════════════════════
# OPENAI-COMPATIBLE PROVIDERS
# ══════════════════════════════════════════════════════════════════════════════
# Connect to any service that uses the OpenAI API format.
# These work alongside native providers - you can use them simultaneously.

# ──────────────────────────────────────────────────────────────────────────────
# OpenRouter (Multi-model API aggregator)
# ──────────────────────────────────────────────────────────────────────────────
# Access Claude, GPT, Llama, Mistral and 200+ models through one API.
# Get your API key: https://openrouter.ai/keys
#
# OPENROUTER_API_KEY=sk-or-v1-...
# OPENROUTER_MODELS=anthropic/claude-3-opus,openai/gpt-4o,meta-llama/llama-3.1-70b
#
# Optional: Override base URL (default: https://openrouter.ai/api/v1)
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# ──────────────────────────────────────────────────────────────────────────────
# LM Studio (Local desktop app)
# ──────────────────────────────────────────────────────────────────────────────
# Run local models with a nice GUI. Download: https://lmstudio.ai/
# No API key required - just list your loaded models.
#
# LMSTUDIO_MODELS=llama-3.2-3b,deepseek-coder-v2
#
# Optional: Override base URL (default: http://localhost:1234/v1)
# LMSTUDIO_BASE_URL=http://localhost:1234/v1
# LMSTUDIO_API_KEY=                         # Optional, usually not needed

# ──────────────────────────────────────────────────────────────────────────────
# llama-swap (Hot-swap server for local models)
# ──────────────────────────────────────────────────────────────────────────────
# Automatically load/unload models based on requests.
# GitHub: https://github.com/mostlygeek/llama-swap
#
# LLAMASWAP_BASE_URL=http://localhost:8080/v1
# LLAMASWAP_MODELS=llama3,mistral-7b,qwen2
# LLAMASWAP_API_KEY=                        # Optional, usually not needed

# ──────────────────────────────────────────────────────────────────────────────
# Custom OpenAI-compatible endpoint
# ──────────────────────────────────────────────────────────────────────────────
# Generic fallback for any OpenAI-compatible server not listed above.
# Examples: vLLM, LocalAI, text-generation-webui, TabbyAPI, etc.
#
# CUSTOM_BASE_URL=http://localhost:5000/v1
# CUSTOM_MODELS=model-name-1,model-name-2
# CUSTOM_API_KEY=                           # Required if your server needs auth


# ══════════════════════════════════════════════════════════════════════════════
# QUORUM SETTINGS
# ══════════════════════════════════════════════════════════════════════════════

# Language setting
# Leave empty/unset to respond in the same language as the question
# Or set a specific language to always use
# QUORUM_DEFAULT_LANGUAGE=Swedish

# ──────────────────────────────────────────────────────────────────────────────
# Discussion Methods
# ──────────────────────────────────────────────────────────────────────────────
# Choose your method in the terminal UI. Each has authentic structure:
#
# STANDARD (2+ models)   - Configurable consensus-seeking discussion
# OXFORD (even number)   - Formal parliamentary debate with FOR/AGAINST teams
# ADVOCATE (3+ models)   - Devil's advocate challenges the consensus
# SOCRATIC (2+ models)   - Question-driven Socratic dialogue
# DELPHI (3+ models)     - Iterative consensus for estimates and forecasts
# BRAINSTORM (2+ models) - Creative ideation (diverge → build → converge)
# TRADEOFF (2+ models)   - Structured comparison of alternatives
#
# NOTE: Only Standard method uses the settings below.
# Other methods have fixed authentic structures.

# ──────────────────────────────────────────────────────────────────────────────
# Standard Method Settings
# ──────────────────────────────────────────────────────────────────────────────

# Rounds per agent in Standard's Phase 3 discussion (default: 2)
# Total discussion messages = rounds_per_agent * number_of_models
QUORUM_ROUNDS_PER_AGENT=2

# Which model creates the final synthesis in Standard's Phase 5
# first  - Always use the first selected model (default, predictable)
# random - Randomly select a model each time
# rotate - Rotate through models across discussions
QUORUM_SYNTHESIZER=first

# ──────────────────────────────────────────────────────────────────────────────
# Model Timeout
# ──────────────────────────────────────────────────────────────────────────────
# Maximum time to wait for a model response (in seconds).
# Default: 60 seconds
# Increase for slow local models (e.g., Ollama on CPU): 120-300
#
# QUORUM_MODEL_TIMEOUT=60

# ──────────────────────────────────────────────────────────────────────────────
# Execution Mode (VRAM Optimization)
# ──────────────────────────────────────────────────────────────────────────────
# Controls how models are executed in parallel phases.
# This is critical for local Ollama models that share GPU VRAM.
#
# auto       - Smart default: Cloud APIs run in parallel, Ollama runs sequentially
# parallel   - Always run all models simultaneously (best for cloud-only setups)
# sequential - Always run models one at a time (best for limited VRAM)
#
# Default: auto (recommended for most users)
#
# QUORUM_EXECUTION_MODE=auto

# ──────────────────────────────────────────────────────────────────────────────
# Auto-save Reports
# ──────────────────────────────────────────────────────────────────────────────
# All discussions are automatically saved as markdown files.
# Configure the directory here (supports ~ for home directory expansion).
# Default: ~/reports
#
# QUORUM_REPORT_DIR=~/reports

# ──────────────────────────────────────────────────────────────────────────────
# Export Settings
# ──────────────────────────────────────────────────────────────────────────────
# Directory for exported discussions (manual /export command).
# Supports ~ for home directory expansion.
# If not set, exports to home directory (~).
#
# QUORUM_EXPORT_DIR=~/.quorum/exports

# Default export format: md (markdown), text (plain text), pdf, json
# Can be overridden per export: /export text, /export pdf, /export json
#
# QUORUM_EXPORT_FORMAT=md
