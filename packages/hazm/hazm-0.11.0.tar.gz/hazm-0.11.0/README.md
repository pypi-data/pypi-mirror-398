# Hazm - Persian NLP Toolkit

![Tests](https://img.shields.io/github/actions/workflow/status/roshan-research/hazm/test.yml?branch=master)
![PyPI - Downloads](https://img.shields.io/github/downloads/roshan-research/hazm/total)
![PyPI - Python Version](https://img.shields.io/pypi/pyversions/hazm)
![GitHub](https://img.shields.io/github/license/roshan-research/hazm)

[**Hazm**](https://www.roshan-ai.ir/hazm/) is a python library to perform natural language processing tasks on Persian text. It offers various features for analyzing, processing, and understanding Persian text. You can use Hazm to normalize text, tokenize sentences and words, lemmatize words, assign part-of-speech tags, identify dependency relations, create word and sentence embeddings, or read popular Persian corpora.

[![sample](documentation/assets/sample.png)](documentation/assets/sample.png)

## Features

- **Normalization:** Converts text to a standard form (diacritics removal, ZWNJ correction, etc).
- **Tokenization:** Splits text into sentences and words.
- **Lemmatization:** Reduces words to their base forms.
- **POS tagging:** Assigns a part of speech to each word.
- **Dependency parsing:** Identifies the syntactic relations between words.
- **Embedding:** Creates vector representations of words and sentences.
- **Hugging Face Integration:** Automatically download and cache pretrained models from the Hub.
- **Persian corpora reading:** Easily read popular Persian corpora with ready-made scripts.

## Installation

To install the latest version of Hazm (requires Python 3.12+), run:

    pip install hazm

To use the pretrained models from Hugging Face, ensure you have the `huggingface-hub` package:

    pip install huggingface-hub

## Pretrained-Models

Hazm supports automatic downloading of pretrained models. You can find all available models (POS Tagger, Chunker, Embeddings, etc.) on our official Hugging Face page:

ğŸ‘‰ [**Roshan Research on Hugging Face**](https://huggingface.co/roshan-research/models)

When using Hazm, simply provide the `repo_id` and `model_filename` as shown in the examples below, and the library will handle the rest.

## Usage

```python
from hazm import *

# ===============================
# Stemming
# ===============================
stemmer = Stemmer()
stem = stemmer.stem('Ú©ØªØ§Ø¨â€ŒÙ‡Ø§')
print(stem) # Ú©ØªØ§Ø¨

# ===============================
# Normalizing
# ===============================
normalizer = Normalizer()
normalized_text = normalizer.normalize('Ù…Ù† Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒ Ø²ÛŒÙ€Ù€Ù€Ù€Ø§Ø¯ÛŒ Ø¯Ø§Ø±Ù… .')
print(normalized_text) # Ù…Ù† Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ§Ø¯ÛŒ Ø¯Ø§Ø±Ù….

# ===============================
# Lemmatizing
# ===============================
lemmatizer = Lemmatizer()
lem = lemmatizer.lemmatize('Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³ÛŒÙ…')
print(lem) # Ù†ÙˆØ´Øª#Ù†ÙˆÛŒØ³

# ===============================
# Sentence tokenizing
# ===============================
sentence_tokenizer = SentenceTokenizer()
sent_tokens = sentence_tokenizer.tokenize('Ù…Ø§ Ú©ØªØ§Ø¨ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ…. ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¨ Ø§Ø³Øª.')
print(sent_tokens) # ['Ù…Ø§ Ú©ØªØ§Ø¨ Ù…ÛŒ\u200cØ®ÙˆØ§Ù†ÛŒÙ….', 'ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¨ Ø§Ø³Øª.']

# ===============================
# Word tokenizing
# ===============================
word_tokenizer = WordTokenizer()
word_tokens = word_tokenizer.tokenize('Ù…Ø§ Ú©ØªØ§Ø¨ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ…')
print(word_tokens) # ['Ù…Ø§', 'Ú©ØªØ§Ø¨', 'Ù…ÛŒ\u200cØ®ÙˆØ§Ù†ÛŒÙ…']

# ===============================
# Part of speech tagging
# ===============================
tagger = POSTagger(repo_id="roshan-research/hazm-postagger", model_filename="pos_tagger.model")
tagged_words = tagger.tag(word_tokens)
print(tagged_words) # [('Ù…Ø§', 'PRON'), ('Ú©ØªØ§Ø¨', 'NOUN'), ('Ù…ÛŒ\u200cØ®ÙˆØ§Ù†ÛŒÙ…', 'VERB')]

# ===============================
# Chunking
# ===============================
chunker = Chunker(repo_id="roshan-research/hazm-chunker", model_filename="chunker.model")
chunked_tree = tree2brackets(chunker.parse(tagged_words))
print(chunked_tree) # [Ù…Ø§ NP] [Ú©ØªØ§Ø¨ NP] [Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ… VP]

# ===============================
# Word embedding
# ===============================
word_embedding = WordEmbedding.load(repo_id='roshan-research/hazm-word-embedding', model_filename='fasttext_skipgram_300.bin', model_type='fasttext')
odd_word = word_embedding.doesnt_match(['Ú©ØªØ§Ø¨', 'Ø¯ÙØªØ±', 'Ù‚Ù„Ù…', 'Ù¾Ù†Ø¬Ø±Ù‡'])
print(odd_word) # Ù¾Ù†Ø¬Ø±Ù‡

# ===============================
# Sentence embedding
# ===============================
sent_embedding = SentEmbedding.load(repo_id='roshan-research/hazm-sent-embedding', model_filename='sent2vec-naab.model')
sentence_similarity = sent_embedding.similarity('Ø§Ùˆ Ø´ÛŒØ± Ù…ÛŒØ®ÙˆØ±Ø¯','Ø´ÛŒØ± ØºØ°Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ±Ø¯')
print(sentence_similarity) # 0.4643607437610626

# ===============================
# Dependency parsing
# ===============================
parser = DependencyParser(tagger=tagger, lemmatizer=lemmatizer, repo_id="roshan-research/hazm-dependency-parser", model_filename="langModel.mco")
dependency_graph = parser.parse(word_tokens)
print(dependency_graph)
"""
{0:  {'address': 0,
      'ctag': 'TOP',
      'deps': defaultdict(<class 'list'>, {'root': [3]}),
      'feats': None,
      'head': None,
      'lemma': None,
      'rel': None,
      'tag': 'TOP',
      'word': None},
  1: {'address': 1,
      'ctag': 'PRON',
      'deps': defaultdict(<class 'list'>, {}),
      'feats': '_',
      'head': 3,
      'lemma': 'Ù…Ø§',
      'rel': 'SBJ',
      'tag': 'PRON',
      'word': 'Ù…Ø§'},
  2: {'address': 2,
      'ctag': 'NOUN',
      'deps': defaultdict(<class 'list'>, {}),
      'feats': '_',
      'head': 3,
      'lemma': 'Ú©ØªØ§Ø¨',
      'rel': 'OBJ',
      'tag': 'NOUN',
      'word': 'Ú©ØªØ§Ø¨'},
  3: {'address': 3,
      'ctag': 'VERB',
      'deps': defaultdict(<class 'list'>, {'SBJ': [1], 'OBJ': [2]}),
      'feats': '_',
      'head': 0,
      'lemma': 'Ø®ÙˆØ§Ù†Ø¯#Ø®ÙˆØ§Ù†',
      'rel': 'root',
      'tag': 'VERB',
      'word': 'Ù…ÛŒ\u200cØ®ÙˆØ§Ù†ÛŒÙ…'}})

"""
```

## Documentation

Visit https://roshan-ai.ir/hazm to view the full documentation.

## Evaluation

| Module name      |           |
| :--------------- | --------- |
| DependencyParser | **85.6%** |
| POSTagger        | **98.8%** |
| Chunker          | **93.4%** |
| Lemmatizer       | **89.9%** |

|                                | Metric          | Value   |
| ------------------------------ | --------------- | ------- |
| **SpacyPOSTagger**             | Precision       | 0.99250 |
|                                | Recall          | 0.99249 |
|                                | F1-Score        | 0.99249 |
| **EZ Detection in SpacyPOSTagger** | Precision   | 0.99301 |
|                                | Recall          | 0.99297 |
|                                | F1-Score        | 0.99298 |
| **SpacyChunker**                | Accuracy        | 96.53%  |
|                                | F-Measure       | 95.00%  |
|                                | Recall          | 95.17%  |
|                                | Precision       | 94.83%  |
| **SpacyDependencyParser**       | TOK Accuracy    | 99.06   |
|                                | UAS             | 92.30   |
|                                | LAS             | 89.15   |
|                                | SENT Precision  | 98.84   |
|                                | SENT Recall     | 99.38   |
|                                | SENT F-Measure  | 99.11   |

### Code contributores

![Alt](https://repobeats.axiom.co/api/embed/ae42bda158791645d143c3e3c7f19d8a68d06d08.svg "Repobeats analytics image")

<a href="https://github.com/roshan-research/hazm/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=roshan-research/hazm" />
</a>

[![Star History Chart](https://api.star-history.com/svg?repos=roshan-research/hazm&type=Date)](https://star-history.com/#roshan-research/hazm&Date)

