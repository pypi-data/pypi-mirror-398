"""This module includes classes and functions for extracting words from text."""

import re
from pathlib import Path

from flashtext import KeywordProcessor
from nltk.tokenize.api import TokenizerI

from hazm.api import TokenizerProtocol
from hazm.utils import abbreviations
from hazm.utils import default_verbs
from hazm.utils import default_words
from hazm.utils import words_list


class WordTokenizer(TokenizerI, TokenizerProtocol):
    """This class includes methods for extracting words from text.

    Args:
        words_file: Path to the file containing the list of words.
            Hazm provides a default file; however, you can introduce your own
            file. Refer to the default file to understand its structure.
        verbs_file: Path to the file containing verbs.
            Hazm provides a default file; however, you can introduce your own
            file. Refer to the default file to understand its structure.
        join_verb_parts: If `True`, joins multi-part verbs with an underscore;
            for example, 'Ú¯ÙØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª' becomes 'Ú¯ÙØªÙ‡_Ø´Ø¯Ù‡_Ø§Ø³Øª'.
        join_abbreviations: If `True`, prevents abbreviations from being split
            and returns them as a single token.
        separate_emoji: If `True`, separates emojis with a space.
        replace_links: If `True`, replaces links with the word `LINK`.
        replace_ids: If `True`, replaces IDs with the word `ID`.
        replace_emails: If `True`, replaces email addresses with the word `EMAIL`.
        replace_numbers: If `True`, replaces decimal numbers with `NUMF` and
            integers with `NUM`. For non-decimal numbers, the number of digits
            is appended to `NUM`.
        replace_hashtags: If `True`, replaces the `#` symbol with `TAG`.
    """

    def __init__(
        self,
        words_file: str | Path = default_words,
        verbs_file: str | Path = default_verbs,
        join_verb_parts: bool = True,
        join_abbreviations: bool = False,
        separate_emoji: bool = False,
        replace_links: bool = False,
        replace_ids: bool = False,
        replace_emails: bool = False,
        replace_numbers: bool = False,
        replace_hashtags: bool = False,
    ) -> None:
        """Initializes the WordTokenizer with the specified configurations."""
        self._join_verb_parts = join_verb_parts
        self._join_abbreviation = join_abbreviations
        self.separate_emoji = separate_emoji
        self.replace_links = replace_links
        self.replace_ids = replace_ids
        self.replace_emails = replace_emails
        self.replace_numbers = replace_numbers
        self.replace_hashtags = replace_hashtags

        self.pattern = re.compile(r'([ØŸ!?]+|[\d.:]+|[:.ØŒØ›Â»\])}"Â«\[({/\\])')

        self.emoji_pattern = re.compile(
            "["
            "\U0001f600-\U0001f64f"
            "\U0001f300-\U0001f5ff"
            "\U0001f4cc\U0001f4cd"
            "]",
            flags=re.UNICODE,
        )
        self.id_pattern = re.compile(r"(?<![\w._])(@[\w_]+)")
        self.link_pattern = re.compile(
            r"((https?|ftp)://)?(?<!@)(([a-zA-Z0-9-]+\.)+[a-zA-Z]{2,})[-\w@:%_.+/~#?=&]*",
        )
        self.email_pattern = re.compile(
            r"[a-zA-Z0-9._+-]+@([a-zA-Z0-9-]+\.)+[A-Za-z]{2,}",
        )
        self.number_int_pattern = re.compile(
            r"\b(?<![\dÛ°-Û¹][.Ù«Ù¬,])([\dÛ°-Û¹]+)(?![.Ù«Ù¬,][\dÛ°-Û¹])\b",
        )
        self.number_float_pattern = re.compile(
            r"\b(?<!\.)([\dÛ°-Û¹,Ù¬]+[.Ù«Ù¬][\dÛ°-Û¹]+)\b(?!\.)",
        )
        self.hashtag_pattern = re.compile(r"#(\S+)")

        self.words = {item[0]: (item[1], item[2]) for item in words_list(words_file)}

        self.verbs: list[str] = []
        self.bons: set[str] = set()
        self.verbe: set[str] = set()

        if join_verb_parts:
            self._init_verb_parts(verbs_file)

        self.abbreviations: list[str] = []
        if join_abbreviations:
            with Path(abbreviations).open("r", encoding="utf-8") as f:
                self.abbreviations = [line.strip() for line in f]

    def _init_verb_parts(self, verbs_file: str | Path):
        """Initializes the internal sets used for joining verb parts.

        Args:
            verbs_file: Path to the verbs file.
        """
        self.after_verbs = {
            "Ø§Ù…", "Ø§ÛŒ", "Ø§Ø³Øª", "Ø§ÛŒÙ…", "Ø§ÛŒØ¯", "Ø§Ù†Ø¯", "Ø¨ÙˆØ¯Ù…", "Ø¨ÙˆØ¯ÛŒ", "Ø¨ÙˆØ¯", "Ø¨ÙˆØ¯ÛŒÙ…", "Ø¨ÙˆØ¯ÛŒØ¯", "Ø¨ÙˆØ¯Ù†Ø¯",
            "Ø¨Ø§Ø´Ù…", "Ø¨Ø§Ø´ÛŒ", "Ø¨Ø§Ø´Ø¯", "Ø¨Ø§Ø´ÛŒÙ…", "Ø¨Ø§Ø´ÛŒØ¯", "Ø¨Ø§Ø´Ù†Ø¯", "Ø´Ø¯Ù‡_Ø§Ù…", "Ø´Ø¯Ù‡_Ø§ÛŒ", "Ø´Ø¯Ù‡_Ø§Ø³Øª",
            "Ø´Ø¯Ù‡_Ø§ÛŒÙ…", "Ø´Ø¯Ù‡_Ø§ÛŒØ¯", "Ø´Ø¯Ù‡_Ø§Ù†Ø¯", "Ø´Ø¯Ù‡_Ø¨ÙˆØ¯Ù…", "Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒ", "Ø´Ø¯Ù‡_Ø¨ÙˆØ¯", "Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒÙ…",
            "Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒØ¯", "Ø´Ø¯Ù‡_Ø¨ÙˆØ¯Ù†Ø¯", "Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ù…", "Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒ", "Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ø¯", "Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒÙ…",
            "Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒØ¯", "Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ù†Ø¯", "Ù†Ø´Ø¯Ù‡_Ø§Ù…", "Ù†Ø´Ø¯Ù‡_Ø§ÛŒ", "Ù†Ø´Ø¯Ù‡_Ø§Ø³Øª", "Ù†Ø´Ø¯Ù‡_Ø§ÛŒÙ…", "Ù†Ø´Ø¯Ù‡_Ø§ÛŒØ¯",
            "Ù†Ø´Ø¯Ù‡_Ø§Ù†Ø¯", "Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯Ù…", "Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒ", "Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯", "Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒÙ…", "Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒØ¯",
            "Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯Ù†Ø¯", "Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ù…", "Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒ", "Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ø¯", "Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒÙ…", "Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒØ¯",
            "Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ù†Ø¯", "Ø´ÙˆÙ…", "Ø´ÙˆÛŒ", "Ø´ÙˆØ¯", "Ø´ÙˆÛŒÙ…", "Ø´ÙˆÛŒØ¯", "Ø´ÙˆÙ†Ø¯", "Ø´Ø¯Ù…", "Ø´Ø¯ÛŒ", "Ø´Ø¯",
            "Ø´Ø¯ÛŒÙ…", "Ø´Ø¯ÛŒØ¯", "Ø´Ø¯Ù†Ø¯", "Ù†Ø´ÙˆÙ…", "Ù†Ø´ÙˆÛŒ", "Ù†Ø´ÙˆØ¯", "Ù†Ø´ÙˆÛŒÙ…", "Ù†Ø´ÙˆÛŒØ¯", "Ù†Ø´ÙˆÙ†Ø¯", "Ù†Ø´Ø¯Ù…",
            "Ù†Ø´Ø¯ÛŒ", "Ù†Ø´Ø¯", "Ù†Ø´Ø¯ÛŒÙ…", "Ù†Ø´Ø¯ÛŒØ¯", "Ù†Ø´Ø¯Ù†Ø¯", "Ù…ÛŒâ€ŒØ´ÙˆÙ…", "Ù…ÛŒâ€ŒØ´ÙˆÛŒ", "Ù…ÛŒâ€ŒØ´ÙˆØ¯", "Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ…",
            "Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯", "Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯", "Ù…ÛŒâ€ŒØ´Ø¯Ù…", "Ù…ÛŒâ€ŒØ´Ø¯ÛŒ", "Ù…ÛŒâ€ŒØ´Ø¯", "Ù…ÛŒâ€ŒØ´Ø¯ÛŒÙ…", "Ù…ÛŒâ€ŒØ´Ø¯ÛŒØ¯", "Ù…ÛŒâ€ŒØ´Ø¯Ù†Ø¯",
            "Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ…", "Ù†Ù…ÛŒâ€ŒØ´ÙˆÛŒ", "Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯", "Ù†Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ…", "Ù†Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯", "Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯", "Ù†Ù…ÛŒâ€ŒØ´Ø¯Ù…",
            "Ù†Ù…ÛŒâ€ŒØ´Ø¯ÛŒ", "Ù†Ù…ÛŒâ€ŒØ´Ø¯", "Ù†Ù…ÛŒâ€ŒØ´Ø¯ÛŒÙ…", "Ù†Ù…ÛŒâ€ŒØ´Ø¯ÛŒØ¯", "Ù†Ù…ÛŒâ€ŒØ´Ø¯Ù†Ø¯", "Ø®ÙˆØ§Ù‡Ù…_Ø´Ø¯", "Ø®ÙˆØ§Ù‡ÛŒ_Ø´Ø¯",
            "Ø®ÙˆØ§Ù‡Ø¯_Ø´Ø¯", "Ø®ÙˆØ§Ù‡ÛŒÙ…_Ø´Ø¯", "Ø®ÙˆØ§Ù‡ÛŒØ¯_Ø´Ø¯", "Ø®ÙˆØ§Ù‡Ù†Ø¯_Ø´Ø¯", "Ù†Ø®ÙˆØ§Ù‡Ù…_Ø´Ø¯", "Ù†Ø®ÙˆØ§Ù‡ÛŒ_Ø´Ø¯",
            "Ù†Ø®ÙˆØ§Ù‡Ø¯_Ø´Ø¯", "Ù†Ø®ÙˆØ§Ù‡ÛŒÙ…_Ø´Ø¯", "Ù†Ø®ÙˆØ§Ù‡ÛŒØ¯_Ø´Ø¯", "Ù†Ø®ÙˆØ§Ù‡Ù†Ø¯_Ø´Ø¯",
        }

        self.before_verbs = {
            "Ø®ÙˆØ§Ù‡Ù…", "Ø®ÙˆØ§Ù‡ÛŒ", "Ø®ÙˆØ§Ù‡Ø¯", "Ø®ÙˆØ§Ù‡ÛŒÙ…", "Ø®ÙˆØ§Ù‡ÛŒØ¯", "Ø®ÙˆØ§Ù‡Ù†Ø¯",
            "Ù†Ø®ÙˆØ§Ù‡Ù…", "Ù†Ø®ÙˆØ§Ù‡ÛŒ", "Ù†Ø®ÙˆØ§Ù‡Ø¯", "Ù†Ø®ÙˆØ§Ù‡ÛŒÙ…", "Ù†Ø®ÙˆØ§Ù‡ÛŒØ¯", "Ù†Ø®ÙˆØ§Ù‡Ù†Ø¯",
        }

        with Path(verbs_file).open(encoding="utf-8") as file:
            self.verbs = list(reversed([verb.strip() for verb in file if verb]))
            self.bons = {verb.split("#")[0] for verb in self.verbs}
            self.verbe = set(
                [bon + "Ù‡" for bon in self.bons]
                + ["Ù†" + bon + "Ù‡" for bon in self.bons],
            )

    def tokenize(self, text: str) -> list[str]:
        """Extracts tokens from the given text.

        Examples:
            >>> tokenizer = WordTokenizer()
            >>> tokenizer.tokenize('Ø§ÛŒÙ† Ø¬Ù…Ù„Ù‡ (Ø®ÛŒÙ„ÛŒ) Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ù†ÛŒØ³Øª!!!')
            ['Ø§ÛŒÙ†', 'Ø¬Ù…Ù„Ù‡', '(', 'Ø®ÛŒÙ„ÛŒ', ')', 'Ù¾ÛŒÚ†ÛŒØ¯Ù‡', 'Ù†ÛŒØ³Øª', '!!!']
            >>> tokenizer = WordTokenizer(join_verb_parts=False)
            >>> print(' '.join(tokenizer.tokenize('Ø³Ù„Ø§Ù….')))
            Ø³Ù„Ø§Ù… .
            >>> tokenizer = WordTokenizer(join_verb_parts=False, replace_links=True)
            >>> print(' '.join(tokenizer.tokenize('Ø¯Ø± Ù‚Ø·Ø± Ù‡Ú© Ø´Ø¯ https://t.co/tZOurPSXzi https://t.co/vtJtwsRebP')))
            Ø¯Ø± Ù‚Ø·Ø± Ù‡Ú© Ø´Ø¯ LINK LINK
            >>> tokenizer = WordTokenizer(join_verb_parts=False, replace_ids=True, replace_numbers=True)
            >>> print(' '.join(tokenizer.tokenize('Ø²Ù„Ø²Ù„Ù‡ Û´.Û¸ Ø±ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ù‡Ø¬Ø¯Ú© Ú©Ø±Ù…Ø§Ù† @bourse24ir')))
            Ø²Ù„Ø²Ù„Ù‡ NUMF Ø±ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ù‡Ø¬Ø¯Ú© Ú©Ø±Ù…Ø§Ù† ID
            >>> tokenizer = WordTokenizer(join_verb_parts=False, separate_emoji=True)
            >>> print(' '.join(tokenizer.tokenize('Ø¯ÛŒÚ¯Ù‡ Ù…ÛŒØ®ÙˆØ§Ù… ØªØ±Ú© ØªØ­ØµÛŒÙ„ Ú©Ù†Ù… ğŸ˜‚ğŸ˜‚ğŸ˜‚')))
            Ø¯ÛŒÚ¯Ù‡ Ù…ÛŒØ®ÙˆØ§Ù… ØªØ±Ú© ØªØ­ØµÛŒÙ„ Ú©Ù†Ù… ğŸ˜‚ ğŸ˜‚ ğŸ˜‚
            >>> tokenizer = WordTokenizer(join_abbreviations=True)
            >>> tokenizer.tokenize('Ø§Ù…Ø§Ù… Ø¹Ù„ÛŒ (Ø¹) ÙØ±Ù…ÙˆØ¯: Ø¨Ø±ØªØ±ÛŒÙ† Ø²Ù‡Ø¯ØŒ Ù¾Ù†Ù‡Ø§Ù† Ø¯Ø§Ø´ØªÙ† Ø²Ù‡Ø¯ Ø§Ø³Øª')
            ['Ø§Ù…Ø§Ù…', 'Ø¹Ù„ÛŒ', '(Ø¹)', 'ÙØ±Ù…ÙˆØ¯', ':', 'Ø¨Ø±ØªØ±ÛŒÙ†', 'Ø²Ù‡Ø¯', 'ØŒ', 'Ù¾Ù†Ù‡Ø§Ù†', 'Ø¯Ø§Ø´ØªÙ†', 'Ø²Ù‡Ø¯', 'Ø§Ø³Øª']

        Args:
            text: The text from which tokens should be extracted.

        Returns:
            A list of extracted tokens.
        """
        keyword_processor = None

        if self._join_abbreviation:
            keyword_processor = KeywordProcessor()
            rnd = 313
            while str(rnd) in text:
                rnd += 1
            rnd_str = str(rnd)

            text = text.replace(" ", " " * 3)

            for i, abbr in enumerate(self.abbreviations):
                keyword_processor.add_keyword(f" {abbr} ", f"{rnd_str}{i}")

            text = keyword_processor.replace_keywords(text)

        if self.separate_emoji:
            text = self.emoji_pattern.sub(r"\g<0> ", text)
        if self.replace_emails:
            text = self.email_pattern.sub(" EMAIL ", text)
        if self.replace_links:
            text = self.link_pattern.sub(" LINK ", text)
        if self.replace_ids:
            text = self.id_pattern.sub(" ID ", text)
        if self.replace_hashtags:
            text = self.hashtag_pattern.sub(
                lambda m: "TAG " + m.group(1).replace("_", " "), text,
            )
        if self.replace_numbers:
            text = self.number_int_pattern.sub(
                lambda m: f" NUM{len(m.group(1))} ", text,
            )
            text = self.number_float_pattern.sub(" NUMF ", text)

        text = self.pattern.sub(r" \1 ", text.replace("\n", " ").replace("\t", " "))
        tokens = [word for word in text.split(" ") if word]

        if self._join_verb_parts:
            tokens = self.join_verb_parts(tokens)

        if self._join_abbreviation and keyword_processor:
            reversed_dict = {
                value: key for key, value in keyword_processor.get_all_keywords().items()
            }
            for i, token in enumerate(tokens):
                if token in reversed_dict:
                    tokens[i] = reversed_dict[token].strip()

        return tokens

    def join_verb_parts(self, tokens: list[str]) -> list[str]:
        """Joins multi-part verbs with an underscore.

        Examples:
            >>> tokenizer = WordTokenizer()
            >>> tokenizer.join_verb_parts(['Ø®ÙˆØ§Ù‡Ø¯', 'Ø±ÙØª'])
            ['Ø®ÙˆØ§Ù‡Ø¯_Ø±ÙØª']
            >>> tokenizer.join_verb_parts(['Ø±ÙØªÙ‡', 'Ø§Ø³Øª'])
            ['Ø±ÙØªÙ‡_Ø§Ø³Øª']
            >>> tokenizer.join_verb_parts(['Ú¯ÙØªÙ‡', 'Ø´Ø¯Ù‡', 'Ø§Ø³Øª'])
            ['Ú¯ÙØªÙ‡_Ø´Ø¯Ù‡_Ø§Ø³Øª']
            >>> tokenizer.join_verb_parts(['Ú¯ÙØªÙ‡', 'Ø®ÙˆØ§Ù‡Ø¯', 'Ø´Ø¯'])
            ['Ú¯ÙØªÙ‡_Ø®ÙˆØ§Ù‡Ø¯_Ø´Ø¯']
            >>> tokenizer.join_verb_parts(['Ø®Ø³ØªÙ‡', 'Ø´Ø¯ÛŒØ¯'])
            ['Ø®Ø³ØªÙ‡_Ø´Ø¯ÛŒØ¯']

        Args:
            tokens: A list of word components of a multi-part verb.

        Returns:
            A list where parts of multi-part verbs are joined by underscores if necessary.
        """
        if len(tokens) <= 1:
            return tokens

        result = [""]
        for token in reversed(tokens):
            if token in self.before_verbs or (
                result[-1] in self.after_verbs and token in self.verbe
            ):
                result[-1] = f"{token}_{result[-1]}"
            else:
                result.append(token)

        return list(reversed(result[1:]))


def word_tokenize(text: str) -> list[str]:
    """A helper function to tokenize text into words.

    Args:
        text: The input text.

    Returns:
        A list of tokens.
    """
    return WordTokenizer().tokenize(text)
